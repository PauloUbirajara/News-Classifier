{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Importar bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar seções do dataset para treinar os modelos\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
    "\n",
    "# Obter uma representação vetorial a partir de um texto\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# Redução de dimensionalidade\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Mostrar árvore de decisões\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "# Mostrar visualmente a matrix de confusão\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "# Balancear dados\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Modelos a serem treinados\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Gerar a imagem contendo as palavras mais frequentes\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Operações para baixar o dataset\n",
    "from zipfile import ZipFile\n",
    "from os import remove\n",
    "\n",
    "# Filtrar stopwords\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import download\n",
    "from string import punctuation\n",
    "download('stopwords')\n",
    "download('punkt')\n",
    "\n",
    "# JSON\n",
    "from json import dumps, loads\n",
    "\n",
    "# Arquivos\n",
    "from os import system\n",
    "from os.path import isfile, exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Criar funções auxiliares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1) Filtrar stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = stopwords.words('english')\n",
    "STOPWORDS.extend(punctuation)\n",
    "\n",
    "# TODO Aplicar o NER\n",
    "\n",
    "def filter_stopwords(word):\n",
    "\tword_tokens = word_tokenize(word)\n",
    "\tfiltered_word = [x.lower() for x in word_tokens if x.lower() not in STOPWORDS]\n",
    "\treturn ' '.join(filtered_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2) Representação Vetorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "  stop_words=STOPWORDS,\n",
    "  # max_features=50,\n",
    "\tlowercase=True\n",
    ")\n",
    "\n",
    "def TFIDF(word_list):\n",
    "  try:\n",
    "    tfidf = vectorizer.fit_transform(word_list.apply(str))\n",
    "  except:\n",
    "    tfidf = vectorizer.fit_transform(word_list)\n",
    "\n",
    "  df_tfidf = pd.DataFrame(\n",
    "    tfidf.toarray(),\n",
    "    columns=vectorizer.get_feature_names_out()\n",
    "  )\n",
    "\n",
    "  return df_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BERT(word_list):\n",
    "\tdf_bert = word_list.copy()\n",
    "\t\n",
    "\t# TODO Importar e aplicar BERT no df_bert (tensorflow)\n",
    "\n",
    "\treturn df_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(\n",
    "\tlowercase=True, \n",
    ")\n",
    "\n",
    "def BAG_OF_WORDS(word_list: pd.Series):\n",
    "\ttry:\n",
    "\t\tbag = count_vectorizer.fit_transform(word_list.apply(str))\n",
    "\texcept:\n",
    "\t\tbag = count_vectorizer.fit_transform(word_list)\n",
    "\n",
    "\tdf_bag = pd.DataFrame(bag.toarray(), columns=count_vectorizer.get_feature_names_out())\n",
    "\treturn df_bag\n",
    "\n",
    "\t\n",
    "# BAG_OF_WORDS(df_dados['title'].head(5) + df_dados['title'].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def representacao_vetorial(x):\n",
    "\treturn TFIDF(x)\n",
    "\treturn BAG_OF_WORDS(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3) Redução de dimensionalidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reducao_dimensionalidade(x):\n",
    "\treturn TSNE(n_components=2, init='random', learning_rate='auto').fit_transform(x)\n",
    "\treturn PCA(n_components=2).fit_transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4) Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(\n",
    "\t# stopwords=STOPWORDS,\n",
    "\tcollocations=False,\n",
    "\tbackground_color='white',\n",
    "\twidth=800,\n",
    "\theight=800\n",
    ")\n",
    "\n",
    "def show_wordcloud(wordlist):\n",
    "\tdata = ' '.join(wordlist)\n",
    "\twc = wordcloud.generate_from_text(data)\n",
    "\n",
    "\tplt.figure(figsize=(15, 15))\n",
    "\tplt.imshow(wc)\n",
    "\tplt.axis('off')\n",
    "\tplt.tight_layout()\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Preparar dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1) Importar/Extrair dataset do arquivo .zip do kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (exists('topic_classifier.csv') and isfile('topic_classifier.csv')):\n",
    "\tsystem('''!curl --silent -o out.zip \"https://storage.googleapis.com/kaggle-data-sets/1115257/1873557/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20220426%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20220426T174557Z&X-Goog-Expires=259199&X-Goog-SignedHeaders=host&X-Goog-Signature=0953e29a79852b0e242947618441064722538d9bd68fe6e8594ca7a7cd04a61959bb9a2aa8063a11b3edf02fcd690cdcf8be20428fc62251eedcf7cfbf9be23b6acd553e6cb9c96726a1750590013717b8ce194276693310dd318ae558b83d210e0123cdf96a320bd47843c2a0056608f9c07be4d1db8e06acdf3c32dfe201ef0df9503cbc91f8a711e4172f1e3904e0afd5ddee490f622c5af2e0f7f4166bcb17a00adec125995b8cfc0bf957cb0ab61dab36d701e7d3b1745dbd69b856fd79e19b97ba9e4d26526c6e6ce764119eef4a1ef7df305af6ceeb46b2849fefeff7c3d4d928a4a984596cb3361ae31f80ceedc054a5506b25ad8698f9e1f3435075\"''')\n",
    "\tsystem('''f = ZipFile('out.zip', mode='r')''')\n",
    "\tsystem('''f.extractall()''')\n",
    "\tsystem('''f.close()''')\n",
    "\tsystem('''remove('out.zip')''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('topic_classifier.csv')\n",
    "df_dados = dataset.sample(frac=1)\n",
    "df_dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2) Filtrar stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dados['title'] = df_dados['title'].apply(lambda x: filter_stopwords(str(x)))\n",
    "df_dados['c1body'] = df_dados['c1body'].apply(lambda x: filter_stopwords(str(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3) Separar variáveis de features e target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = df_dados['Topic']\n",
    "X = df_dados.drop(columns=['Topic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4) Balancear dados com SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=42)\n",
    "x2 = representacao_vetorial(X['title'])\n",
    "res = sm.fit_resample(x2, y_true)\n",
    "\n",
    "X_smote = res[0]\n",
    "y_true_smote = res[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5) Salvar notícias em arquivos separados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir news\n",
    "\n",
    "for topico in y_true.unique():\n",
    "\t# if exists(f'news/{topico}.csv'):\n",
    "\t# \tcontinue\n",
    "\n",
    "\twith open(f'news/{topico}.csv', mode='w') as f:\n",
    "\t\tf.write(df_dados[df_dados['Topic'] == topico].to_csv(index=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6) Plottar scatter com redução de dimensionalidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noticias = sorted(y_true_smote.unique().tolist())\n",
    "cores = ['red', 'orange', 'green', 'blue', 'purple', 'gray', 'brown', 'cyan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_reducao_dimensionalidade(data, title, xlabel, ylabel):\n",
    "\tplt.figure(figsize=(20, 10))\n",
    "\n",
    "\tfor i in range(len(noticias)):\n",
    "\t\tnoticia = noticias[i]\n",
    "\n",
    "\t\t# Transformar texto pra número\n",
    "\t\t# dados_representacao_vetorial = representacao_vetorial(data[data['Topic'] == noticia])\n",
    "\t\t# dados_representacao_vetorial = representacao_vetorial(dados['c1body'])\n",
    "\t\t# dados_representacao_vetorial = representacao_vetorial(dados['title'] + dados['c1body'])\n",
    "\n",
    "\t\t# Reduzir dimensionalidade\n",
    "\t\tdados_reducao_dimensionalidade = reducao_dimensionalidade(\n",
    "\t\t\tdata[data['target'] == noticia].drop(columns=['target'])\n",
    "\t\t)\n",
    "\n",
    "\t\tprint(noticia)\n",
    "\t\tplt.scatter(\n",
    "\t\t\t[y[0] for y in dados_reducao_dimensionalidade],\n",
    "\t\t\t[y[1] for y in dados_reducao_dimensionalidade],\n",
    "\t\t\tcolor=cores[i],\n",
    "\t\t\talpha=0.5,\n",
    "\t\t\tlabel=noticia\n",
    "\t\t)\n",
    "\n",
    "\tplt.title(title)\n",
    "\tplt.xlabel(xlabel)\n",
    "\tplt.ylabel(ylabel)\n",
    "\tplt.legend()\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Visualizações"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1) Redução de dimensionalidade com representações vetoriais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dados_smote = X_smote.copy()\n",
    "df_dados_smote['target'] = y_true_smote.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_reducao_dimensionalidade(\n",
    "\tdata=df_dados_smote,\n",
    "\ttitle=\"BOW - TSNE - Títulos\",\n",
    "\txlabel='TSNE1',\n",
    "\tylabel='TSNE2'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2) Distribuição dos dados por tópico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_true_plot_data = y_true.value_counts()\n",
    "y_true_plot_data = y_true_smote.value_counts()\n",
    "\n",
    "print(y_true_smote.value_counts())\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.bar(y_true_plot_data.keys(), y_true_plot_data.values)\n",
    "plt.show()\n",
    "\n",
    "# Sugestões\n",
    "# SMOTE para oversampling\n",
    "# 1 classificador para cada target\n",
    "# Rodar vários modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3) Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_wordcloud(X['title'])\n",
    "show_wordcloud(X['c1body'])\n",
    "show_wordcloud(X['title'] + X['c1body'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Separação de dados para treino/teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_representacao_vetorial = representacao_vetorial(X_smote['title'])\n",
    "# df_representacao_vetorial = representacao_vetorial(X_smote['c1body'])\n",
    "# df_representacao_vetorial = representacao_vetorial(X_smote['title'] + X_smote['c1body'])\n",
    "\n",
    "# df_representacao_vetorial = representacao_vetorial(X['title'])\n",
    "# df_representacao_vetorial = representacao_vetorial(X['c1body'])\n",
    "# df_representacao_vetorial = representacao_vetorial(X['title'] + X['c1body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_representacao_vetorial, y_true_smote, test_size=0.3)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(df_representacao_vetorial, y_true, test_size=0.3)\n",
    "\n",
    "print('X_train', len(X_train))\n",
    "print('y_train', len(y_train))\n",
    "print('X_test', len(X_test))\n",
    "print('y_test', len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "\t# return LogisticRegression()\n",
    "\t# return RandomForestClassifier()\n",
    "\t# return DecisionTreeClassifier(max_depth=None)\n",
    "\treturn DecisionTreeClassifier(max_depth=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1) Treino e teste previamente separados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = model()\n",
    "m.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = m.score(X_test, y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2) Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(model(), df_representacao_vetorial, y_true_smote, cv=10)\n",
    "print(scores)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3) Matrix de confusão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cross_val_predict(model(), df_representacao_vetorial, y_true_smote)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true_smote, y_pred)\n",
    "f = ConfusionMatrixDisplay(\n",
    "\tcm,\n",
    "\tdisplay_labels=y_true.unique()\n",
    ")\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "f.plot(ax=ax, xticks_rotation='vertical')\n",
    "plt.title('Regressão Logística - TFIDF - Títulos')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
