"{\"title\":{\"4311\":\"iclr 2020 yann lecun energy-based models\",\"499\":\"got chance try nvidia a100 gpu via qblocks.cloud\",\"486\":\"medical test paradox redesigning bayes rule help\",\"1507\":\"eric weinstein revolutionary ideas science math society\",\"1632\":\"top 20 image datasets machine learning computer vision\",\"1547\":\"david chalmers hard problem consciousness\",\"4094\":\"machine learning laptops becoming worth\",\"1228\":\"neural networks part 1 inside black box\",\"4084\":\"data science without statistics\",\"1672\":\"natural language processing brief overview\",\"1653\":\"fintech 2021 fintech companies use big data effectively\",\"4090\":\"cuda work tensorflow\",\"1601\":\"visiting austin check places still torn austin sf boston went terry black 's bbq 's good everyone says best brisket 've ever\",\"1624\":\"full article svm classification kernel selection outlier detection code r\\u2026\",\"3187\":\"univariate function optimization python\",\"1640\":\"nlp model able prevent adversarial attack\",\"1579\":\"pieter abbeel deep reinforcement learning\",\"4092\":\"google coral mini pcie clustering\",\"1046\":\"applications non-euclidean distance metric spaces\",\"710\":\"careers microsoft research panel discussion microsoft phd summit 2020\",\"5097\":\"get started recommender systems\",\"4270\":\"choose activation function deep learning\",\"1222\":\"backpropagation details part 1 optimizing 3 parameters simultaneously\",\"1535\":\"133 \\u2013 manolis kellis biology disease\",\"476\":\"intuition power lockdown math ep 9\",\"1461\":\"jeff hawkins thousand brains theory intelligence\",\"646\":\"charles isbell michael littman machine learning education lex fridman podcast 148\",\"259\":\"image\",\"1649\":\"5 top tech careers consider studying towards 2021\",\"4280\":\"social dilemma part 1\",\"2320\":\"r would like study trends word frequency academic papers existing way arxiv instance\",\"709\":\"remote video game coding camp improved autistic college students \\u2019 self-efficacy comms\",\"5686\":\"research ml research permutations combinations ol existing models datasets\",\"1481\":\"76 \\u2013 john hopfield physics view mind neurobiology\",\"3746\":\"p building new machine learning competition platform would love get feedback\",\"1462\":\"stuart russell long-term future ai\",\"1136\":\"drag equation empire state building v eiffel tower numberphile\",\"1647\":\"need apache spark\",\"650\":\"dmitri dolgov waymo future self-driving cars lex fridman podcast 147\",\"4273\":\"gentle introduction machine learning modeling pipelines\",\"1545\":\"tomaso poggio brains minds machines\",\"712\":\"empowering k12 students learn computer science\",\"503\":\"best nlp competitions kaggle learn\",\"1660\":\"prevent embarrassment ai\",\"3749\":\"possible train cycle gan two image sets different axes ie one set horizontal image object ground bird 's eye view object\",\"1611\":\"partial dependence plots discover variables influencing model\",\"1635\":\"use marketing automation acquiring qualifying e-commerce leads\",\"1575\":\"111 \\u2013 richard karp algorithms computational complexity\",\"498\":\"best computer vision competitions kaggle beginners\",\"255\":\"fact social media algorithms give rise legitimate problems mean solution `` ban opinions n't like '' also legitimate\",\"496\":\"talks 15 shubhadeep roychowdhury applying machine learning source code\",\"1454\":\"noam chomsky language cognition deep learning\",\"3872\":\"data scientists know multi-output multi-label training\",\"1518\":\"yoshua bengio deep learning\",\"1438\":\"77 \\u2013 alex garland ex machina devs annihilation poetry science\",\"1348\":\"language models open knowledge graphs paper explained\",\"1236\":\"alternative hypotheses main ideas\",\"1869\":\"optimization models subset cover\",\"1873\":\"p-values error rates false positives\",\"4277\":\"facebook research unsupervised translation programming languages\",\"1613\":\"complete introduction graph data structure\",\"5684\":\"deal classes located small area feature space\",\"1147\":\"bomb blast radius numberphile\",\"3754\":\"abandon ml research project\",\"4282\":\"iclr 2020 yoshua bengio nature consciousness\",\"4492\":\"use auroc ovr vs. auroc ovo\",\"1537\":\"whitney cummings comedy robotics neurology love\",\"5837\":\"project inside machine 's dream juan carlos garfias tovar 2021\",\"4294\":\"ai alignment agi fire alarm connor leahy\",\"749\":\"deep learning empirical success large extent proportional raw experimental throughput ability babysit large number experiments staring plots tweaking\\/re-launching works necessary sufficient\",\"1149\":\"butterflies gyroids numberphile\",\"1516\":\"rajat monga tensorflow\",\"1143\":\"brains count numberphile\",\"2329\":\"discussion good software review annotations csv file\",\"1655\":\"10 amazing articles python programming machine learning week 3\",\"1514\":\"110 \\u2013 jitendra malik computer vision\",\"5683\":\"proposed dot-product attention\",\"207\":\"gaining sense control covid-19 pandemic winner \\u2019 interview daniel wolffram\",\"1145\":\"quasiperfect numbers eric lander numberphile\",\"2333\":\"r degenerative adversarial neuroimage nets brain scan simulations application ageing dementia\",\"1642\":\"making money organization 's data 's done\",\"4298\":\"programming languages software engineering machine learning\",\"1234\":\"live 2020-06-15 bootstrapping main ideas\",\"1230\":\"us census data contest\",\"2328\":\"psa buggy caching behaviour huggingfaces old run_glue script\",\"1352\":\"rebel combining deep reinforcement learning search imperfect-information games explained\",\"1344\":\"stochastic meme descent deep learning meme review episode 2 part 2 2\",\"1656\":\"top 3 things forget building saas product\",\"3767\":\"image format impact significantly training inference instance segmentation models\",\"1867\":\"searching rh counterexamples \\u2014 unbounded integers\",\"3752\":\"theoretical papers transformers attention mechanism seq2seq\",\"1681\":\"becoming data scientist easier think\",\"1511\":\"115 \\u2013 dileep george brain-inspired ai\",\"4286\":\"034 eray \\u00f6zkural- agi simulations safety\",\"1685\":\"big data machine learning fundamentals using gcp data professionals\",\"1492\":\"75 \\u2013 marcus hutter universal artificial intelligence aixi agi\",\"1631\":\"take good data scientist\",\"1868\":\"searching rh counterexamples \\u2014 adding database\",\"1503\":\"dava newman space exploration space suits life mars\",\"1583\":\"gilbert strang linear algebra deep learning teaching mit opencourseware\",\"1466\":\"jeff atwood stack overflow coding horror\",\"1593\":\"feels like amazon removing parler aws created dangerous world less dangerous one may wrong 'm listening learning thinking hope\",\"1880\":\"assessing covid-19 vaccination experiment results\",\"3758\":\"p datasets behave like git repositories\",\"643\":\"michael littman reinforcement learning future ai lex fridman podcast 144\",\"258\":\"rt ykilcher \\ud83d\\udd25new video\\ud83d\\udd25 everybody talking openai 's new dall\\u00b7e model \\ud83d\\udc40 takes piece text turns image absolutely crazy \\ud83d\\ude31 watch video learn more\\ud83d\\udcaa https \\/\\/youtu.be\\/j4xgkjwlfl4 dalle ilyasut _jongwook_kim mikhailpavlov5 gabeeegoooh scottgray76\",\"1351\":\"underspecification presents challenges credibility modern machine learning paper explained\",\"3140\":\"ai financial industry 8 key takeaways bill.com h2o.ai fireside chat\",\"1650\":\"integrate ai data mapping drive business decision making\",\"1638\":\"interview data science product manager get job\",\"1625\":\"\\u2019 difference data science machine learning\",\"245\":\"wow\",\"1689\":\"2 rookie mistakes avoid training predictive analytics models\",\"1144\":\"build giant dome numberphile\",\"1555\":\"paola arlotta brain development stem cell organoid\",\"3873\":\"choose clone data github\",\"4674\":\"extract features ensure sparsity sparse convolutional autoencoders\",\"2339\":\"machine learning wayr reading week 104\",\"1872\":\"independent identically distributed data iid\",\"1435\":\"jim keller moore \\u2019 law microprocessors abstractions first principles\",\"3756\":\"r finding words say hidden state visualizations language models\",\"4312\":\"uk algoshambles neuralink gpt-3 intelligence\",\"1615\":\"cure imposter syndrome data science\",\"4900\":\"asking friend\",\"1591\":\"`` dwell beauty life watch stars see running '' marcus aurelius\",\"1665\":\"interface r using python bioinformatics\",\"1050\":\"happens infinity cantor set\",\"5682\":\"recommendations startups solid ml\",\"256\":\"n't seen much bs legal statements google co since fired timnitgebru attacks free speech wrong matter dangerous done monopolies\",\"1436\":\"153 \\u2013 dmitry korkin evolution proteins viruses life ai\",\"641\":\"diana walsh pasulka aliens technology religion nature belief lex fridman podcast 149\",\"1687\":\"planning startup data team 's guide 2021\",\"3195\":\"regression metrics machine learning\",\"247\":\"rt ykilcher openai clip https \\/\\/openai.com\\/blog\\/clip\\/ came got curious adversarial examples turns easy find generalize semantically related adversary classes blog post https \\/\\/stanislavfort.github.io\\/2021\\/01\\/12\\/openai_clip_adversarial_examples.html googlecolab https \\/\\/github.com\\/stanislavfort\\/openai_clip_adversarial_examples\\/blob\\/main\\/openai_clip_adversarial_images_playground.ipynb\",\"4467\":\"`` every moment fresh beginning '' \\u2013 t.s eliot\",\"1525\":\"garry kasparov chess deep blue ai putin\",\"2330\":\"p ai telegram api cool bot\",\"714\":\"valuing expertise people serve\",\"2322\":\"would like join ml slack\",\"3748\":\"anomaly detection quality assurance\",\"645\":\"dan gable olympic wrestling mental toughness making champions lex fridman podcast 152\",\"1708\":\"machine learning predict loan defaults\",\"1629\":\"best data science overviews\",\"1571\":\"george hotz comma.ai openpilot autonomous vehicles\",\"1146\":\"poncelet 's porism numberphile\",\"1521\":\"119 \\u2013 david eagleman neuroplasticity livewired brain\",\"3855\":\"definitely awesome kenneth0stanley 's tutorial icml kickoff made put work channel 's great honor talk\",\"1482\":\"117 \\u2013 sheldon solomon death meaning\",\"4093\":\"discussion much data turns useless\",\"702\":\"welcome microsoft phd summit 2020 fireside chat johannes gehrke\",\"1549\":\"gustav soderstrom spotify\",\"1148\":\"brussels choice numberphile\",\"1478\":\"chris lattner compilers llvm swift tpu ml accelerators\",\"1699\":\"probability machine learning\",\"1548\":\"99 \\u2013 karl friston neuroscience free energy principle\",\"1586\":\"104 \\u2013 david patterson computer architecture data storage\",\"2332\":\"benchmarking gpus right way\",\"3143\":\"new improvements h2o 3.32.0.2\",\"1487\":\"103 \\u2013 ben goertzel artificial general intelligence\",\"1707\":\"data used shape future media entertainment\",\"1443\":\"93 \\u2013 daphne koller biomedicine machine learning\",\"1614\":\"10 questions consider setting corporate a.i project\",\"1618\":\"dags grow people trust data source 'll ask\",\"1709\":\"hacking marketing campaigns data science\",\"971\":\"banned social media trump found way communicate duolingo\",\"2327\":\"r facial recognition expose political orientation facial images\",\"3138\":\"grandmaster series inspiring journey \\u2018 beluga \\u2019 kaggle world \\ud83d\\udc0b\",\"4289\":\"032- simon kornblith googleai simclr paper haul\",\"1515\":\"peter norvig artificial intelligence modern approach\",\"1694\":\"python beginners learning one-liners practice\",\"1499\":\"judea pearl causal reasoning counterfactuals bayesian networks path agi\",\"1455\":\"paul krugman economics innovation automation safety nets universal basic income\",\"264\":\"pinned \\ud83d\\udd25deep learning meme review episode 2 here\\ud83d\\udd25 antonio go state art memes 2020 \\ud83d\\udcaa check https \\/\\/youtu.be\\/7dglelsvygo\",\"1488\":\"lex solo 1 \\u2013 seven levels coronavirus impact\",\"640\":\"matthew johnson psychedelics lex fridman podcast 145\",\"1520\":\"82 \\u2013 simon sinek leadership hard work optimism infinite game\",\"1639\":\"17 open crime datasets data science machine learning projects\",\"1472\":\"84 \\u2013 william macaskill effective altruism\",\"1688\":\"engineering metrics dashboards part 1\",\"1226\":\"chain rule\",\"3191\":\"visualization function optimization python\",\"2337\":\"list novel ml hardware companies january 2021\",\"713\":\"big data research ethics human rights collide mary l. gray microsoft phd summit 2020\",\"1693\":\"scikit-learn library machine learning nutshell\",\"478\":\"logarithm fundamentals lockdown math ep 6\",\"1568\":\"vijay kumar flying robots\",\"653\":\"john clarke art fighting pursuit excellence lex fridman podcast 143\",\"1569\":\"kyle vogt cruise automation\",\"1696\":\"apply unsupervised learning audio data\",\"204\":\"marcin pionnier finishing 5th rta competition\",\"651\":\"michael malice white pill freedom hope happiness amidst chaos lex fridman podcast 150\",\"974\":\"rt ykilcher \\u23f0video psa\\u23f0end week healthy dose new deep learning memes \\ud83e\\uddd9 collaboration infamous orvieto_antonio think best one far \\ud83c\\udf89 watch enjoy https \\/\\/youtu.be\\/hhzsa9z_abe science ai memes machinelearning technology\",\"1440\":\"149 \\u2013 diana walsh pasulka aliens technology religion nature belief\",\"3135\":\"liqui.do speeds credit scoring fair lending h2o.ai\",\"3141\":\"mitos e verdades sobre automl\",\"1523\":\"124 \\u2013 stephen wolfram fundamental theory physics life universe\",\"2805\":\"`` easy denounce evildoer difficult understand '' dostoevsky\",\"1609\":\"npc character chaotic good understand accept pre-programmed storyline ethical alignment amor fati\",\"263\":\"pretty sure `` guy '' took couple pictures dad wig \\ud83d\\ude02\",\"1139\":\"colouring knots numberphile\",\"504\":\"subword tokenization byte pair encoding\",\"1480\":\"fran\\u00e7ois chollet keras deep learning progress ai\",\"4288\":\"openai gpt-3 language models few-shot learners\",\"254\":\"god bless ml reddit \\ud83d\\ude02\",\"262\":\"thank god machine learning 12 none others least 12 \\ud83d\\ude43\",\"1546\":\"125 \\u2013 ryan hall martial arts philosophy violence power grace\",\"1527\":\"cristos goodrow youtube algorithm\",\"205\":\"trained model next\",\"1630\":\"accountable algorithms get wrong\",\"1675\":\"master \\u2019 data science amount cumulated data\",\"1540\":\"147 \\u2013 dmitri dolgov waymo future self-driving cars\",\"1433\":\"95 \\u2013 dawn song adversarial machine learning computer security\",\"5690\":\"n john leonard `` marine robotics simultaneous localization mapping slam ''\",\"1465\":\"116 \\u2013 sara seager search planets life outside solar system\",\"1664\":\"stripe cohort analysis python pandas\",\"1442\":\"greg brockman openai agi\",\"489\":\"power tower puzzle lockdown math ep 8\",\"4896\":\"rt ykilcher go catch ykilcher latest deep learning video\",\"1877\":\"using applied statistics expand human knowledge\",\"237\":\"r karpathy eg tonight random walk around markets cairo egypt nice background track late night email https \\/\\/www.youtube.com\\/watch v=yvydz_cosfy\",\"1651\":\"serp analysis google search console+python\",\"1484\":\"ian goodfellow generative adversarial networks gans\",\"1637\":\"going able code deep learning hero\",\"1451\":\"sean carroll quantum mechanics many-worlds interpretation\",\"1692\":\"7 challenges marketing ai machine learning solutions\",\"976\":\"rt ykilcher how.rl.works\\/\",\"1223\":\"backpropagation details part 2 going bonkers chain rule\",\"972\":\"rt ykilcher hilarious lot truth ykilcher\",\"1619\":\"ai-driven features highest potential enterprise development\",\"253\":\"image\",\"1677\":\"data science people trouble buying superheroine movies\",\"1705\":\"spotify audio features time series additive spotify analyzer\",\"2325\":\"cvpr 2021 reviews\",\"1596\":\"switched signal texting introversion loneliness secure\",\"4087\":\"r best research papers look ml bias\",\"1679\":\"applications python programming language\",\"1526\":\"141 \\u2013 erik brynjolfsson economics ai social networks technology\",\"1356\":\"rethinking attention performers paper explained\",\"1052\":\"would see lived hypersphere\",\"3875\":\"story flash fill shaped\",\"1588\":\"146 \\u2013 michael mina rapid testing viruses engineering mindset\",\"1508\":\"vladimir vapnik statistical learning\",\"3759\":\"tool format data nicely ex convert ros bags nuscenes kitti\",\"1467\":\"80 \\u2013 vitalik buterin ethereum cryptocurrency future money\",\"1233\":\"automl conversation gnosis data analysis\",\"1562\":\"108 \\u2013 sergey levine robotics machine learning\",\"1517\":\"109 \\u2013 brian kernighan unix c awk ampl go programming\",\"1448\":\"140 \\u2013 lisa feldman barrett love evolution human brain\",\"2321\":\"p big sleep text-to-image generation using biggan openai 's clip via google colab notebook twitter user adverb\",\"1663\":\"neural network paints depressing russian cityscapes\",\"1560\":\"guido van rossum python\",\"1543\":\"chris urmson self-driving cars aurora google cmu darpa\",\"1623\":\"brett lantz shows manage data r\",\"1550\":\"bjarne stroustrup c++\",\"644\":\"avi loeb aliens black holes mystery oumuamua lex fridman podcast 154\",\"973\":\"rt ykilcher openai dall\\u00b7e fighter jet mind \\u2708\\ufe0f \\u25b6\\ufe0ffull video https \\/\\/youtu.be\\/c7d5ezkht6a \\ud83d\\udcdcsource post https \\/\\/openai.com\\/blog\\/dall-e\\/ dalle openai gpt3 ai deeplearning machinelearning twominutepapers whatatimetobealive\",\"1533\":\"121 \\u2013 eugenia kuyda friendship ai companion\",\"1572\":\"144 \\u2013 michael littman reinforcement learning future ai\",\"1469\":\"101 \\u2013 joscha bach artificial consciousness nature reality\",\"1229\":\"decision trees python start finish\",\"1231\":\"hypothesis testing null hypothesis\",\"1559\":\"73 \\u2013 andrew ng deep learning education real-world ai\",\"1224\":\"neural networks part 2 backpropagation main ideas\",\"1497\":\"steven pinker ai age reason\",\"2334\":\"p stylegan ~5k images\",\"1676\":\"ai dungeon ai-generated adventure game nick walton\",\"228\":\"finding increasingly hard keep activity deep learning right tabs \\\\infty tab width 0 n't even decade since alexnet ~sep 2012 ~8.5yrs lot happened another 8.5 ~2030 wonder 's like\",\"198\":\"quan sun finishing second place predict grant applications\",\"1686\":\"forecasting future customer inflow numbers using arima fbprophet\",\"1058\":\"could avoid hit laser room mirrors\",\"1621\":\"top datasets climate change data science projects\",\"488\":\"hamming codes part 2 elegance\",\"1670\":\"crypto-economic networks technological enablers scalable tribe-like collaboration\",\"242\":\"binge watching journey microcosmos https \\/\\/www.youtube.com\\/c\\/microcosmos\\/videos\",\"4302\":\"037 tour de bayesian connor tann\",\"3874\":\"microsoft research reinforcement learning day 2021\",\"1704\":\"future capitalism age ai\",\"4284\":\"francois chollet measure intelligence\",\"1458\":\"regina barzilay deep learning cancer diagnosis treatment\",\"1876\":\"variance inflation factors vifs\",\"3753\":\"thoughts optimal hyperparams gpt-2 used mathematical functions\",\"1475\":\"155 \\u2013 max tegmark ai physics\",\"1504\":\"michio kaku future humans aliens space travel physics\",\"4297\":\"sayak paul\",\"1622\":\"decentralized ai rest us\",\"3854\":\"rt ykilcher epic special edition kenneth0stanley greatness planned abandoning objectives open-endedness joelbot3000 jeffclune ykilcher https \\/\\/www.youtube.com\\/watch v=lhygxyemq_e\",\"1563\":\"89 \\u2013 stephen wolfram cellular automata computation physics\",\"701\":\"evidence based cs education\",\"4085\":\"towards understanding ensemble knowledge distillation self-distillation deep learning microsoft research\",\"1646\":\"use plaidml machine learning macos amd gpu\",\"4276\":\"robert lange nn pruning collective intelligence\",\"1493\":\"sebastian thrun flying cars autonomous vehicles education\",\"477\":\"trigonometry fundamentals lockdown math ep 2\",\"241\":\"r karpathy classical robotics computer graphics stacks re-written neural net modules typically building closely classical algorithms whenever possible swapping differentiable versions propagate gradients 's plugged wider system\",\"1345\":\"predictive coding approximates backprop along arbitrary computation graphs paper explained\",\"1491\":\"92 \\u2013 harry cliff particle physics large hadron collider\",\"479\":\"complex number fundamentals lockdown math ep 3\",\"244\":\"pinned looks like bombard earth photons emit roadster hah\",\"1668\":\"10 free python programming courses beginners learn online\",\"3142\":\"automate model documentation using h2o autodoc\",\"1698\":\"objects classification using cnn-based model\",\"234\":\"r karpathy 8.5 years ago training restricted boltzmann machines matlab cpu machine desk\",\"4469\":\"hope 2021 let 's make year amazing let engineers scientists small businesses best solve problems\",\"1566\":\"88 \\u2013 eric weinstein geometric unity call new ideas leaders institutions\",\"4086\":\"starting business line\",\"232\":\"r karpathy impressiveness judged distribution prompt\\/output likely e.g `` collection glasses table '' giving generic images nice rendering arbitrary text prompt textures rare\\/specific prompts \\ud83d\\udca5\",\"1513\":\"123 \\u2013 manolis kellis origin life humans ideas suffering happiness\",\"1654\":\"10 best image classification datasets ml projects\",\"2682\":\"cvpr2021 review met unqualified reviewer\",\"1702\":\"best python tutorial ever need watch\",\"231\":\"retweeting one time excellent describes nicely mrna vaccines direct hacking life 's assembly code spike protein headers metadata tweaking stable likely evade immune system defenses \\ud83d\\udc4c\\ud83d\\udc4c\",\"750\":\"nick lane 's books good https \\/\\/www.amazon.com\\/vital-question-evolution-origins-complex\\/dp\\/0393352978\",\"250\":\"rt ykilcher week connossor explore history practical utility unique capabilities bayesian methods also discuss computational difficulties inherent bayesian methods https \\/\\/www.youtube.com\\/watch v=rq31mhw84o0 tim ecsquendor alex keith\",\"1141\":\"2.920050977316 numberphile\",\"647\":\"dan kokotov speech recognition ai humans lex fridman podcast 151\",\"1489\":\"kai-fu lee ai superpowers \\u2013 china silicon valley\",\"1662\":\"ethical ai libraries critical every data scientist know\",\"1570\":\"132 \\u2013 george hotz hacking simulation learning drive neural nets\",\"1573\":\"118 \\u2013 grant sanderson math manim neural networks teaching 3blue1brown\",\"505\":\"leaf disease classification using pytorch\",\"4089\":\"3090 get\",\"4083\":\"n questions john leonard- mit csail\",\"1592\":\"changes everything\",\"203\":\"yuanchen finishing third melbourne university competition\",\"1494\":\"eric schmidt google\",\"257\":\"\\ud83c\\udf89new video\\ud83c\\udf89 clip openai huge step connecting images text beats fully supervised models zero-shot imagenet\\ud83d\\udd25trained 400 million scraped samples w\\/ huge potential applications\\ud83d\\udcaa https \\/\\/youtu.be\\/t9xsu0pkx2e alecrad _jongwook_kim ilyasut\",\"4269\":\"3 books optimization machine learning\",\"4088\":\"p dm crowd counting model day 3\",\"711\":\"fireside chat susan dumais microsoft phd summit 2020\",\"1565\":\"81 \\u2013 anca dragan human-robot interaction reward engineering\",\"704\":\"directions ml ai adaptive experiment design caltech professor yisong yue\",\"1678\":\"docker dev workflow apache spark\",\"1342\":\"openai dall\\u00b7e creating images text blog post explained\",\"1648\":\"automation girl scout events\",\"3761\":\"apple ai residency program coding stage invitation\",\"1496\":\"127 \\u2013 joe rogan conversations ideas love freedom joe rogan experience\",\"1582\":\"stephen kotkin stalin putin nature power\",\"1048\":\"bolzano\\u2013weierstrass theorem proof real analysis\",\"1449\":\"new name lex fridman podcast\",\"1879\":\"using moving averages smooth time series data\",\"1620\":\"knowledge graphs new black year graph newsletter may 2019\",\"4971\":\"039 lena voita nlp\",\"1542\":\"90 \\u2013 dmitry korkin computational biology coronavirus\",\"4898\":\"rt ykilcher great video summary recent work thanks ykilcher\",\"4279\":\"kernels\",\"249\":\"rt ykilcher 'm really excited share latest survey paper deep learning applications covid-19 \\ud83d\\udcdc product months research really hope helps people find research projects impact pandemic https \\/\\/journalofbigdata.springeropen.com\\/articles\\/10.1186\\/s40537-020-00392-9 100daysofmlcode\",\"1680\":\"using data social graphs clinical trials\",\"5677\":\"discussion someone donated gaming pc streaming video content creation \\u2019 overwhelmed joy \\u2019 know anything pcs gaming could guys tell good specs give advice general gaming machine\",\"1599\":\"photo earlier discarded version t-800 buggy lacked personality kept talking dostoevksy later repurposed human-like podcast host thanks ethos_pictures instagram drawing https \\/\\/www.instagram.com\\/ethos_pictures\\/\",\"2341\":\"recent advances metric monocular depth estimation\",\"1661\":\"6 tips working analysts data engineers\",\"1691\":\"14 open datasets text classification machine learning\",\"1053\":\"random things\",\"3139\":\"maximizing value ai\",\"2335\":\"r p random shadows highlights data augmentation\",\"4274\":\"035 christmas community edition\",\"5722\":\"r prefix-tuning optimizing continuous prompts generation\",\"1871\":\"tour survival analysis\",\"229\":\"rt karpathy https \\/\\/berthub.eu\\/articles\\/posts\\/reverse-engineering-source-code-of-the-biontech-pfizer-vaccine\\/ great job breaking pfizer vaccine\",\"1597\":\"'s conversation max tegmark tegmark first chat episode 1 podcast 's back talk intersection machine learning physics also avoid near-term long-term existential threats ai https \\/\\/www.youtube.com\\/watch v=rl4j4kpwngm\",\"1432\":\"juergen schmidhuber godel machines meta-learning lstms\",\"2837\":\"microsoft research sponsors popl 2021\",\"1576\":\"128 \\u2013 michael malice anarchy democracy libertarianism love trolling\",\"3194\":\"gentle introduction machine learning modeling pipelines\",\"4673\":\"precision recall different loss functions\",\"4296\":\"036 max welling quantum manifolds symmetries ml\",\"3764\":\"p output gan match text 's input via clipping gradient clip openai 's new model\",\"243\":\"rt karpathy new blog post great stagnation ending technologies watching decade ahead going get life extension treatments soon 5700+ words please read share https \\/\\/elidourado.com\\/blog\\/notes-on-technology-2020s\\/\",\"1464\":\"grant sanderson 3blue1brown beauty mathematics\",\"5675\":\"one solve coding interviews stats\\/ml background\",\"1578\":\"143 \\u2013 john clarke art fighting pursuit excellence\",\"5667\":\"video -- lena voita nlp\",\"201\":\"hobbies went hiatus kaggler made fighting covid-19 data mission a\\u2026\",\"500\":\"talks 14 martin henze knowledge power understanding data eda visualisations\",\"1538\":\"97 \\u2013 sertac karaman robots fly robots drive\",\"4299\":\"computation bayesian model selection interactive articles\",\"1138\":\"569936821221962380720 numberphile\",\"3190\":\"semi-supervised learning label propagation\",\"748\":\"going phase obsessively trying evaluating flavors philz today \\u2019 \\u201c silken splendor \\u201d allegedly claimed \\u201c dark cocoa citrus butterscotch \\u201d wonder determine\",\"3188\":\"code adam gradient descent optimization scratch\",\"4295\":\"harri valpola system 2 ai planning model-based reinforcement learning\",\"3876\":\"actors behind flash fill\",\"1450\":\"106 \\u2013 matt botvinick neuroscience psychology ai deepmind\",\"483\":\"dp-3t algorithm contact tracing via nicky case\",\"1580\":\"sean carroll nature universe life intelligence\",\"4267\":\"semi-supervised learning label propagation\",\"1607\":\"wikipedia arguably greatest website ever made happy 20th\",\"1446\":\"151 \\u2013 dan kokotov speech recognition ai humans\",\"1703\":\"data science love letters\",\"491\":\"deep learning almost text classification problem binary multi-class multi-label\",\"706\":\"teaching learning neurodiverse students\",\"4287\":\"explainability reasoning priors gpt-3\",\"501\":\"stemming lemmatization\",\"1501\":\"137 \\u2013 alex filippenko supernovae dark energy aliens expanding universe\",\"240\":\"maybe 's travel starved really getting enjoying growing genre 4k walking videos around world e.g https \\/\\/www.openculture.com\\/2020\\/03\\/explore-the-entire-world-from-the-comfort-of-quarantine-with-4k-walking-tours.html examples interesting leave running tv background unscripted samples human condition\",\"492\":\"nvidia broadcast app ai-based application streaming broadcasting meeting features\",\"703\":\"different career paths academia industry panel microsoft phd summit 2020\",\"4468\":\"hope everyone stays safe today\",\"1054\":\"engineering students like ... part 2\",\"1553\":\"keoki jackson lockheed martin\",\"4281\":\"033 prof. karl friston free energy principle\",\"2343\":\"r evolving reinforcement learning algorithms\",\"1643\":\"use google api save cat \\u2019 photo cloud\",\"1468\":\"ayanna howard human-robot interaction ethics safety-critical systems\",\"1142\":\"predators prey numberphile\",\"1479\":\"112 \\u2013 ian hutchinson nuclear fusion plasma physics religion\",\"1047\":\"teaching upper level pure math course almost died\",\"493\":\"natural language processing tokenization basic\",\"4305\":\"nlp nlu gpt-3 walid saba\",\"5680\":\"p langhuan nlp labeling app ner classify driven dataframe\",\"4306\":\"one shot metric learning quadruplet loss machine learning dojo\",\"1626\":\"last bi vendor please turn lights\",\"199\":\"top marks student kaggler bengali.ai winner \\u2019 interview linsho kaku\",\"4879\":\"'m running 48 miles 48 hours davidgoggins march 5-7 starting 8pm pst join us challenge see page live videos details podcast 's throwing stuff refuses tell think thinks break good luck \\ud83d\\ude0e\",\"1444\":\"87 \\u2013 richard dawkins evolution intelligence simulation memes\",\"1628\":\"4 ways data science helps streamline business operations\",\"654\":\"lex plays cyberpunk 2077\",\"1608\":\"rt lexfridman feel honored excited back lexfridman 's podcast 's really deep unique knack getting important exciting questions tech life reality https \\/\\/youtu.be\\/rl4j4kpwngm\",\"202\":\"max lin finishing second r challenge\",\"1059\":\"imaginary numbers needed understand radius convergence\",\"494\":\"introducing datamuni no-nonsense platform machine learning data science articles\",\"2340\":\"different approaches pre-training transfer learning machine learning models masking method\",\"4880\":\"joerogan 's podcast yesterday air one special moments life joe took favorite watch gifted one heroes means put words 'm worthy 'll work hard live\",\"260\":\"openai code dangerous complex mere mortals lucidrains hold beer\",\"1602\":\"rt lexfridman tegmark exceptionally smart good human\",\"230\":\"\\u201c would aliens also x \\u201d almost x tickles brain lot x primed stainless steel almost generalization works\",\"1353\":\"news soccer ai fails mixes ball referee 's bald head\",\"1457\":\"tuomas sandholm poker game theory\",\"5674\":\"aistats 2021 decisions\",\"1652\":\"2020 world university ranking ai safety\",\"1498\":\"134 \\u2013 eric weinstein nature good evil genius madness\",\"4275\":\"curl contrastive unsupervised representations reinforcement learning\",\"4285\":\"jordan edwards ml engineering devops azureml\",\"1539\":\"126 \\u2013 james gosling java jvm emacs early days computing\",\"1641\":\"nevermined organizations manage monetize data next level solution\",\"1603\":\"'s conversation avi loeb astrophysicist harvard argues oumuamua interstellar object passed earth 2017 may alien technology talk aliens black holes space exploration truly fascinating https \\/\\/www.youtube.com\\/watch v=plcc6e-e1uu\",\"1866\":\"searching rh counterexamples \\u2014 deploying docker\",\"1486\":\"oriol vinyals deepmind alphastar starcraft language sequences\",\"772\":\"max tegmark ai physics lex fridman podcast 155\",\"1683\":\"behaviors trees ai ditch event framework\",\"1604\":\"every time\",\"1878\":\"coefficient variation statistics\",\"4676\":\"looking inspiration bachelor thesis\",\"1051\":\"intuition implications complex derivative\",\"4278\":\"welcomeaioverlords zak jost\",\"2324\":\"r data movement need case study optimizing transformers\",\"4308\":\"exploring open-ended algorithms poet\",\"3762\":\"find importance weights layers\",\"1355\":\"memes need deep learning meme review episode 2 part 1 2\",\"1606\":\"pinned social media press currently incentivized drastically exaggerate narratives division turn creates division downward spiral continues hope build tech changes incentives believe much love hate world\",\"1634\":\"python first step data science\",\"1616\":\"scanta named finalist datatribe 2020 cybersecurity startup challenge\",\"1617\":\"computational journalism data behind stories jonathan stray\",\"251\":\"rt ykilcher temporarily breaking twitter-minimization short thread issues around free speech mass deplatformings last week obviously riots terrible people still supporting dt crazy moving things yet said ...\",\"1135\":\"statistics storks babies numberphile\",\"1657\":\"5 papers face recognition every data scientist read\",\"1350\":\"lambdanetworks modeling long-range interactions without attention paper explained\",\"1700\":\"podcast machine learning meets privacy\",\"1512\":\"131 \\u2013 chris lattner future computing programming languages\",\"233\":\"actually think barely even scratches surface weirdness inverted computers tenet universe legitimately breaks brain love quite bit depends understandably skimmed physics interaction fwd\\/bwd objects\",\"1045\":\"hour minute hand clock look identical always determine time\",\"3757\":\"visualize network activation non-classification models\",\"5678\":\"anyone knows ml algorithms behind ads accurate\",\"236\":\"`` love '' new boston dynamics video \\ud83e\\udd16 https \\/\\/youtu.be\\/fn3kwm1kuaw\",\"975\":\"rt ykilcher yearning little openai dall\\u00b7e action make sure check ykilcher 's video exhaustive think 's best one good https \\/\\/www.youtube.com\\/watch v=j4xgkjwlfl4\",\"5669\":\"p prompty mcpromptface telegram writing prompt bot gpt-2\",\"1645\":\"detecting fraudulent transactions\",\"4082\":\"tensorflow model.predict vs model.predict_on_batch impact predictions result\",\"4293\":\"exploring limits transfer learning unified text-to-text transformer\",\"484\":\"makes natural log `` natural '' lockdown math ep 7\",\"1529\":\"colin angle irobot\",\"1456\":\"melanie mitchell concepts analogies common sense future ai\",\"1495\":\"michael kearns algorithmic fairness bias privacy ethics machine learning\",\"1556\":\"79 \\u2013 lee smolin quantum gravity einstein \\u2019 unfinished revolution\",\"5691\":\"video tutorial mocking neural networks\",\"206\":\"jeremy howard winning predict grant applications competition\",\"487\":\"group theory 196,883-dimensional monster\",\"1636\":\"know happens data\",\"1658\":\"python panda package tutorial\",\"252\":\"rt ykilcher fchollet see like new tensorflow honestly quite beautiful therefore 'd curious know think mesh tensorflow personally find much easier use pytorch even keras backend definitions\",\"1870\":\"searching rh counterexamples \\u2014 search strategies\",\"1532\":\"lex solo 2 \\u2013 future neuralink\",\"4897\":\"rt ykilcher bit late party \\ud83d\\udc83new video\\ud83d\\udd7a switch transformers googleai hard routing selective dropout mixed precision achieve \\ud83d\\udd25one trillion parameters\\ud83d\\udd25 language model watch learn 's done\\ud83e\\uddd9\\ud83d\\udcaa https \\/\\/youtu.be\\/iar8lkkmmim liamfedus barret_zoph\",\"1666\":\"15 best machine learning courses coursera free\",\"490\":\"euler 's formula actually saying lockdown math ep 4\",\"4310\":\"lottery ticket hypothesis jonathan frankle\",\"2323\":\"p preview pdf arxiv abstract page\",\"1483\":\"kevin scott microsoft cto\",\"4264\":\"visualization function optimization python\",\"248\":\"rt ykilcher wanted give openai 's newest text-to-image transformer dall\\u00b7e try pytorch thanks lucidrains \\ud83d\\udc0d\\ud83d\\udd25 breathtaking possibilities generating high-quality images arbitrary descriptions \\u25b6\\ufe0f https \\/\\/github.com\\/lucidrains\\/dalle-pytorch h\\/t andfanilo ykilcher\",\"649\":\"lex plays stanley parable\",\"1490\":\"113 \\u2013 manolis kellis human genome evolutionary dynamics\",\"502\":\"talks 16 issam laradji build large-scale ml projects manage thousands experiments\",\"200\":\"recruit ponpare japan \\u2019 leading joint coupon site offering huge discounts everything from\\u2026\",\"261\":\"conform\",\"1558\":\"pamela mccorduck machines think early days ai\",\"1510\":\"leonard susskind quantum mechanics string theory black holes\",\"1232\":\"elements statquest webinar\",\"1659\":\"10 computer vision startups product hunt upvotes\",\"642\":\"dmitry korkin evolution proteins viruses life ai lex fridman podcast 153\",\"1057\":\"likely live one 18 universes\",\"2336\":\"p machine learning flow-based visual coding environment using ryven\",\"1605\":\"accurate\",\"1674\":\"self-supervised machine learning story far trends 2021\",\"4268\":\"multinomial logistic regression python\",\"1528\":\"lex solo 3 \\u2013 memory grandmother\",\"1471\":\"152 \\u2013 dan gable olympic wrestling mental toughness making champions\",\"4303\":\"sara hooker hardware lottery sparsity fairness\",\"1600\":\"love america\",\"495\":\"unboxing installing nvidia rtx 3090 bfgpu\",\"4877\":\"using ai unearth unconscious bias job descriptions\",\"705\":\"helping young students build career research msr india research fellow program\",\"4899\":\"r\\/woooosh\",\"3897\":\"stem students non stem classes like ...\",\"5670\":\"experience training cloud tracking experiments feedback wanted\",\"1439\":\"yann lecun deep learning convolutional neural networks self-supervised learning\",\"2345\":\"n learnings deployments synthetic data\",\"1541\":\"max tegmark life 3.0\",\"482\":\"impossible chessboard puzzle\",\"5671\":\"range prediction general approaches\",\"4878\":\"rt lexfridman one favorite podcasts ever russian brother lexfridman really great flow available video audio spotify https \\/\\/www.instagram.com\\/p\\/ckzyl4if338\\/ igshid=67qa2drbtvzg\",\"1554\":\"94 \\u2013 ilya sutskever deep learning\",\"4304\":\"social dilemma part 2\",\"497\":\"build pytorch trainer\",\"485\":\"tips better problem solver last lecture lockdown math ep 10\",\"235\":\"somehow missed tenet new nolan movie back august watched last night bracing disappointment mediocre reviews disorientation settled realized may one favorite movies ever certain yet watch times\",\"2342\":\"p persistent anti-muslim bias large language models\",\"1706\":\"25 favorite data science courses harvard udemy\",\"4081\":\"train simsiam cifar10 91 accuracy\",\"1551\":\"72 \\u2013 scott aaronson quantum computing\",\"1673\":\"top 10 javascript charting libraries every data visualization need\",\"2346\":\"video tutorial visualizing activations forward hooks\",\"239\":\"rt karpathy cc amprobotics https \\/\\/www.rollingstone.com\\/culture\\/culture-features\\/plastic-problem-recycling-myth-big-oil-950957\\/ fbclid=iwar0g5nbjlzeg7ukymretyevtpw9bvtrvbopuryfctnv1chmj51lqgtdie6i\",\"1595\":\"people die suicide forms violence combined people struggling see lot outrage online moment n't see much compassion gandhi said `` eye eye make whole world blind ''\",\"707\":\"need accessible technology\",\"1552\":\"136 \\u2013 dan carlin hardcore history\",\"1585\":\"leslie kaelbling reinforcement learning planning robotics\",\"4283\":\"031 got access gpt-3 gary marcus walid saba connor leahy\",\"1589\":\"david ferrucci ibm watson jeopardy deep conversations ai\",\"708\":\"reinforcement learning rl open source fest day 2 demos\",\"1701\":\"5 simple ways kickstart freelance data science career\",\"4307\":\"038 professor kenneth stanley greatness planned\",\"4300\":\"029 gpt-3 prompt engineering trading ai alignment intelligence\",\"1544\":\"130 \\u2013 scott aaronson computational complexity consciousness\",\"5676\":\"paper explained switch transformers scaling trillion parameter models simple efficient sparsity full video analysis\",\"1690\":\"road become python ninja \\u2014 handling exceptions\",\"1875\":\"perform regression analysis using excel\",\"3751\":\"dealing new possible outputs data driven recommendation engine\",\"3136\":\"h2o driverless ai 1.9.1 continuing push boundaries responsible ai\",\"4290\":\"kaggle ml community engineering sanyam bhutani\",\"1564\":\"129 \\u2013 lisa feldman barrett counterintuitive ideas brain works\",\"3763\":\"tool track relate key ideas papers readen\",\"3760\":\"ai studies quantum computer systems consequences\",\"700\":\"access mean \\u200b re-thinking accessibility research problems tackle\",\"5679\":\"p current sota really detailed image similarity search\",\"1502\":\"jeremy howard fast.ai deep learning courses research\",\"5067\":\"get started recommender systems\",\"1612\":\"presidential debate sentiment analysis lstm onevsrest linearsvc nlp step-by-step guide\",\"1534\":\"100 \\u2013 alexander fridman dad plasma physicist\",\"1627\":\"data science toolkit concepts code\",\"2338\":\"p made nlprule library fast grammatical error correction\",\"1349\":\"openai clip connectingtext images paper explained\",\"5687\":\"dialogpt paper walkthrough\",\"1531\":\"gavin miller adobe research\",\"1610\":\"create bubble map javascript visualize election results\",\"1644\":\"big data bring transformative improvements medical care\",\"1536\":\"74 \\u2013 michael i. jordan machine learning recommender systems future ai\",\"3750\":\"video -- kenneth stanley greatness planned ml street talk\",\"1485\":\"120 \\u2013 fran\\u00e7ois chollet measures intelligence\",\"2326\":\"simple questions thread december 20 2020\",\"1463\":\"ray dalio principles economic machine artificial intelligence arc life\",\"3189\":\"semi-supervised learning label spreading\",\"1874\":\"independent dependent samples statistics\",\"3193\":\"3 books optimization machine learning\",\"1598\":\"`` n't join book burners n't think 're going conceal faults concealing evidence ever existed n't afraid go library read every book '' dwight d. eisenhower\",\"246\":\"site really n't want love true\",\"1441\":\"donald knuth algorithms tex life art computer programming\",\"1434\":\"83 \\u2013 nick bostrom simulation superintelligence\",\"1445\":\"145 \\u2013 matthew johnson psychedelics\",\"1590\":\"139 \\u2013 andrew huberman neuroscience optimal performance\",\"1225\":\"support vector machines python start finish\",\"4291\":\"capsule networks education targets\",\"1049\":\"things get weird infinity\",\"1557\":\"150 \\u2013 michael malice white pill freedom hope happiness amidst chaos\",\"3134\":\"introducing h2o wave\",\"1667\":\"introductory guide variables data types go\",\"1594\":\"happy birthday andrewyang 46 new 42\",\"3755\":\"witnessed malpractices ml\\/cv research papers\",\"5681\":\"r strange interview question feature selection using test null hypothesis definition\",\"1447\":\"85 \\u2013 roger penrose physics consciousness infinite universe\",\"1505\":\"98 \\u2013 kate darling emotional connection humans robots\",\"1509\":\"138 \\u2013 yaron brook ayn rand philosophy objectivism\",\"1473\":\"christof koch consciousness\",\"4272\":\"code adam gradient descent optimization scratch\",\"3192\":\"multinomial logistic regression python\",\"4954\":\"switch transformers scaling trillion parameter models simple efficient sparsity\",\"4301\":\"social dilemma part 3 dr. rebecca roache\",\"3747\":\"r counterfactual latent dance\",\"1453\":\"148 \\u2013 charles isbell michael littman machine learning education\",\"1567\":\"elon musk neuralink ai autopilot pale blue dot\",\"1452\":\"elon musk tesla autopilot\",\"1574\":\"86 \\u2013 david silver alphago alphazero deep reinforcement learning\",\"1437\":\"michael stevens vsauce\",\"1561\":\"96 \\u2013 stephen schwarzman going big business investing ai\",\"4309\":\"030 multi-armed bandits pure-exploration wouter m. koolen\",\"1460\":\"102 \\u2013 steven pressfield war art\",\"1522\":\"jim gates supersymmetry string theory proving einstein right\",\"1587\":\"154 \\u2013 avi loeb aliens black holes mystery oumuamua\",\"1671\":\"5 companies developing computer vision technology 2020\",\"1227\":\"xgboost python start finish\",\"648\":\"many alien civilizations\",\"1459\":\"107 \\u2013 peter singer suffering humans animals ai\",\"5668\":\"human brain work neurobio recommendations thread\",\"1500\":\"78 \\u2013 ann druyan cosmos carl sagan voyager beauty science\",\"1682\":\"10 great articles python development\",\"1669\":\"12 key lessons ml researchers practitioners\",\"1476\":\"105 \\u2013 robert langer edison medicine\",\"4265\":\"univariate function optimization python\",\"238\":\"impressive surprising https \\/\\/openai.com\\/blog\\/dall-e\\/ use words sparingly\",\"1343\":\"deepmind 's alphafold 2 explained ai breakthrough protein folding know n't\",\"1235\":\"neural networks part 3 relu action\",\"5673\":\"r characterizing signal propagation close performance gap unnormalized resnets\",\"5689\":\"think shortcomings arguments argue artificial intelligence never created important penrose 's theories academic responses\",\"1581\":\"gary marcus toward hybrid deep learning symbolic ai\",\"1354\":\"extracting training data large language models paper explained\",\"1056\":\"inverted pendulum puzzle\",\"5838\":\"materials learning gpu programming computer vision\",\"4292\":\"swav unsupervised learning visual features contrasting cluster assignments mathilde caron\",\"1697\":\"issue machine ethics robot rights\",\"1524\":\"rosalind picard affective computing emotion privacy health\",\"3765\":\"awesome list equivariance neural networks\",\"1684\":\"complete guide build machine learning model deployed production using aws sagemaker\",\"1474\":\"rohit prasad amazon alexa conversational ai\",\"1137\":\"forgotten number system numberphile\",\"1695\":\"10 great articles data science data engineering\",\"4271\":\"semi-supervised learning label spreading\",\"4675\":\"approach recommended brain \\ud83e\\udde0 tumor segmentation mri scans dicom files\",\"1140\":\"inca knot numbers numberphile\",\"1347\":\"2m all-in 5 pot wwyd daniel negreanu 's no-limit hold'em challenge poker hand analysis\",\"4266\":\"regression metrics machine learning\",\"5688\":\"technical lessons learn job\",\"1055\":\"donut sphere things one surface\",\"1530\":\"122 \\u2013 david fravor ufos aliens fighter jets aerospace engineering\",\"3137\":\"meet data scientist stop winning kaggle\",\"481\":\"imaginary interest rates lockdown math ep 5\",\"1633\":\"guide logistic regression sas\",\"227\":\"rt karpathy \\ud83c\\udf89 introducing new papers code newsletter newsletter helps manage firehose new ml papers highlighting trending papers new state-of-the-art results community contributions around pwc https \\/\\/paperswithcode.com\\/newsletter\\/1\",\"1506\":\"vladimir vapnik predicates invariants essence intelligence\",\"2344\":\"mila 2021 msc\\/phd program supervision request\",\"5685\":\"industry would categorize low-hanging fruit provide value use ml\",\"1577\":\"142 \\u2013 manolis kellis meaning life universe everything\",\"1477\":\"114 \\u2013 russ tedrake underactuated robotics control dynamics touch\",\"1470\":\"daniel kahneman thinking fast slow deep learning ai\",\"1519\":\"91 \\u2013 jack dorsey square cryptocurrency artificial intelligence\",\"1346\":\"fourier neural operator parametric partial differential equations paper explained\",\"480\":\"hamming codes error correction\",\"2331\":\"r flavr fast efficient technique video frame interpolation\",\"3196\":\"choose activation function deep learning\",\"652\":\"michael mina rapid testing viruses engineering mindset lex fridman podcast 146\",\"3766\":\"reproducible ml struggle\",\"1584\":\"135 \\u2013 charles isbell computing interactive ai race america\",\"4091\":\"r paper reinforcement learning solving mazes\",\"5672\":\"10 insightful practical ai\\/ml books read 2021\"},\"c1body\":{\"4311\":\"This week Connor Shorten, Yannic Kilcher and Tim Scarfe reacted to Yann LeCun's keynote speech at this year's ICLR conference which just passed. ICLR is the number two ML conference and was completely open this year, with all the sessions publicly accessible via the internet. Yann spent most of his talk speaking about self-supervised learning, Energy-based models (EBMs) and manifold learning. Don't worry if you hadn't heard of EBMs before, neither had we! Thanks for watching! Please Subscribe! Paper Links: ICLR 2020 Keynote Talk: https:\\/\\/iclr.cc\\/virtual_2020\\/speaker_7.html A Tutorial on Energy-Based Learning: http:\\/\\/yann.lecun.com\\/exdb\\/publis\\/pdf\\/lecun-06.pdf Concept Learning with Energy-Based Models (Yannic's Explanation): https:\\/\\/www.youtube.com\\/watch?v=Cs_j-oNwGgg Concept Learning with Energy-Based Models (Paper): https:\\/\\/arxiv.org\\/pdf\\/1811.02486.pdf Concept Learning with Energy-Based Models (OpenAI Blog Post): https:\\/\\/openai.com\\/blog\\/learning-concepts-with-energy-functions\\/ #deeplearning #machinelearning #iclr #iclr2020 #yannlecun\",\"499\":\"In this video, I show you how fast the training is using an #NVIDIA #A100 GPU that I got to try via qblocks.cloud Please subscribe and like the video to help me keep motivated to make awesome videos like this one. :) To buy my book, Approaching (Almost) Any Machine Learning problem, please visit: https:\\/\\/bit.ly\\/buyaaml Follow me on: Twitter: https:\\/\\/twitter.com\\/abhi1thakur LinkedIn: https:\\/\\/www.linkedin.com\\/in\\/abhi1thakur\\/ Kaggle: https:\\/\\/kaggle.com\\/abhishek Instagram: https:\\/\\/instagram.com\\/abhi4ml\",\"486\":\"About Likelihood Ratios, also sometimes called Bayes Factors*. Viewer-supported: https:\\/\\/3b1b.co\\/bayes-factor-thanks Home page: https:\\/\\/www.3blue1brown.com The book by my friend Matt Cook about paradoxes mentioned at the end: https:\\/\\/amzn.to\\/3aBrEzg On the topic, I can't help also mentioning another paradox book I'm rather fond of by Bunch: https:\\/\\/amzn.to\\/3mBDSKE Another video on Bayes' theorem: https:\\/\\/youtu.be\\/HZGCoVF3YvM *As mentioned in the on-screen note at the end, while the terms \\\"Bayes Factor\\\" and \\\"Likelihood Ratio\\\" refer to the same ratio in this setting, where Bayes rule is used on the probability of an event with only two possible outcomes (you either have the disease or you don't), they do take on divergent meanings in more general contexts. Namely, if you have a continuous parameter you are trying to estimate, the two terms reflect two alternate approaches you can use in comparing hypotheses. In fact, some people take the phrase \\\"Bayes factor\\\" to _specifically_ refer to its use in this more continuous context. If you want more details, Wikipedia actually has a really nice example discussing the difference: https:\\/\\/en.wikipedia.org\\/wiki\\/Bayes_factor#Example This post has some nice discussion of the distinction: https:\\/\\/stats.stackexchange.com\\/questions\\/27345\\/likelihood-ratio-vs-bayes-factor ------------------ These animations are largely made using manim, a scrappy open source python library: https:\\/\\/github.com\\/3b1b\\/manim If you want to check it out, I feel compelled to warn you that it's not the most well-documented tool, and it has many other quirks you might expect in a library someone wrote with only their own use in mind. Music by Vincent Rubinetti. Download the music on Bandcamp: https:\\/\\/vincerubinetti.bandcamp.com\\/album\\/the-music-of-3blue1brown Stream the music on Spotify: https:\\/\\/open.spotify.com\\/album\\/1dVyjwS8FBqXhRunaG5W5u If you want to contribute translated subtitles or to help review those that have already been made by others and need approval, you can click the gear icon in the video and go to subtitles\\/cc, then \\\"add subtitles\\/cc\\\". I really appreciate those who do this, as it helps make the lessons accessible to more people. ------------------ 3blue1brown is a channel about animating math, in all senses of the word animate. And you know the drill with YouTube, if you want to stay posted on new videos, subscribe: http:\\/\\/3b1b.co\\/subscribe Various social media stuffs: Website: https:\\/\\/www.3blue1brown.com Twitter: https:\\/\\/twitter.com\\/3blue1brown Reddit: https:\\/\\/www.reddit.com\\/r\\/3blue1brown Instagram: https:\\/\\/www.instagram.com\\/3blue1brown_animations\\/ Patreon: https:\\/\\/patreon.com\\/3blue1brown Facebook: https:\\/\\/www.facebook.com\\/3blue1brown\",\"1507\":\"Eric Weinstein is a mathematician, economist, physicist, and managing director of Thiel Capital. He formed the \\u201cintellectual dark web\\u201d which is a loosely assembled group of public intellectuals including Sam Harris, Jordan Peterson, Steven Pinker, Joe Rogan, Michael Shermer, and a few others. Video version is available on YouTube. If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman on Twitter, LinkedIn, Facebook, Medium, or YouTube where you can watch the video versions of these conversations.\",\"1632\":\"Computer vision enables computers to understand the content of images and videos. The goal in computer vision is to automate tasks that the human visual system can do. Read the full story\",\"1547\":\"David Chalmers is a philosopher and cognitive scientist specializing in philosophy of mind, philosophy of language, and consciousness. He is perhaps best known for formulating the hard problem of consciousness which could be stated as \\u201cwhy does the feeling which accompanies awareness of sensory information exist at all?\\u201d This conversation is part of the Artificial Intelligence podcast. If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman on Twitter, LinkedIn, Facebook, Medium, or YouTube where you can watch the video versions of these conversations. If you enjoy the podcast, please rate it 5\",\"4094\":\"I've come to realize that my workflow is somewhat inefficient when it come to debugging GPU code. I have access to a cluster, but there is a small cost (moving your code over, requesting a GPU from the schedule manager etc.) that adds up when you're debugging code. You can debug on CPU but you're limited to toy models\\/datasets and the intuition you build from those is often wrong. So let's assume it's worth it to get a GPU locally. During Covid times, many of us are working in several locations and you can't move a rig around. Finally, the new gaming laptops pack surprisingly good GPUs. You can get an RTX 2080 on the Alienware R3. The new version of this laptop (available end of Jan) is expected to offer the RTX 3080. I understand that the laptop GPUs are less efficient than their desktop counterparts, but still. Should be enough to run a small WRN on 10 epochs of CIFAR10 in ~1 min or so. Thoughts? [link] [comments]\",\"1228\":\"Neural Networks are one of the most popular Machine Learning algorithms, but they are also one of the most poorly understood. Everyone says Neural Networks are \\\"black boxes\\\", but that's not true at all. In this video I break each piece down and show how it works, step-by-step, using simple mathematics that is still true to the algorithm. By the end of this video you will have a deep understanding of what Neural Networks do. \\u2b50 NOTE: When I code, I use Kite, a free AI-powered coding assistant that will help you code faster and smarter. The Kite plugin integrates with all the top editors and IDEs to give you smart completions and documentation while you\\u2019re typing. I love it! https:\\/\\/www.kite.com\\/get-kite\\/?utm_medium=referral&utm_source=youtube&utm_campaign=statquest&utm_content=description-only For a complete index of all the StatQuest videos, check out: https:\\/\\/statquest.org\\/video-index\\/ If you'd like to support StatQuest, please consider... Patreon: https:\\/\\/www.patreon.com\\/statquest ...or... YouTube Membership: https:\\/\\/www.youtube.com\\/channel\\/UCtYLUTtgS3k1Fg4y5tAhLbw\\/join ...a cool StatQuest t-shirt or sweatshirt (USA\\/Europe): https:\\/\\/teespring.com\\/stores\\/statquest (everywhere): https:\\/\\/www.redbubble.com\\/people\\/starmer\\/works\\/40421224-statquest-double-bam?asc=u&p=t-shirt ...buying one or two of my songs (or go large and get a whole album!) https:\\/\\/joshuastarmer.bandcamp.com\\/ ...or just donating to StatQuest! https:\\/\\/www.paypal.me\\/statquest Lastly, if you want to keep up with me as I research and create new StatQuests, follow me on twitter: https:\\/\\/twitter.com\\/joshuastarmer 0:00 Awesome song and introduction 2:01 A simple dataset and problem 3:37 Description of Neural Networks 7:54 Creating a squiggle from curved lines 15:25 Using the Neural Network to make a prediction 16:38 Some more Neural Network terminology #StatQuest #NeuralNetworks\",\"4084\":\"I was thinking about the high bar to enter this field lately. Is it that everyone needs to be a statistician, mathematician and computer scientist to work in data science today? EDIT: I'm not suggesting that people should not learn stats, the question is more about which direction the wider field goes with automation and abstraction in tools evolving rapdily. https:\\/\\/medium.com\\/datadriveninvestor\\/is-there-data-science-without-statistics-70d671649dc3 [link] [comments]\",\"1672\":\"Natural language processing (NLP) is a subfield of artificial intelligence. It is the ability to analyze and process a natural language. Read the full story\",\"1653\":\"According to a study, 90% of the whole world\\u2019s data was created in the last two years. This sounds quite cool but what does the world do with all that data? How does one analyze it? Read the full story\",\"4090\":\"How does CUDA (specifally the CUDNN library) implement layers in the Tensorflow Python API on the GPU. For instance, if I import Conv2D from tf.keras.layers to be run on the GPU, what sequence of imports from tf.keras.layers will lead to the CUDA code which actually implements the convolution layer on the GPU? In a more broader sense, how does the python API interact with C++ based low level implementations? [link] [comments]\",\"1601\":\"Visiting Austin to check out some places. Still torn between Austin, SF, and Boston. Went to Terry Black's BBQ and it's as good as everyone says. Best brisket I've ever had.\",\"1624\":\"Read the full story\",\"3187\":\"How to Optimize a Function with One Variable? Univariate function optimization involves finding the input to a function that results in the optimal output from an objective function. This is a common procedure in machine learning when fitting a model with one parameter or tuning a model that has a single hyperparameter. An efficient algorithm is required to solve optimization problems of this type that will find the best solution with the minimum number of evaluations of the objective function, given that each evaluation of the objective function could be computationally expensive, such as fitting and evaluating a model on a dataset. This excludes expensive grid search and random search algorithms and in favor of efficient algorithms like Brent\\u2019s method. In this tutorial, you will discover how to perform univariate function optimization in Python. After completing this tutorial, you will know: Univariate function optimization involves finding an optimal input for an objective function that takes a single continuous argument. How to perform univariate function optimization for an unconstrained convex function. How to perform univariate function optimization for an unconstrained non-convex function. Let\\u2019s get started. Univariate Function Optimization in Python Photo by Robert Haandrikman, some rights reserved. Tutorial Overview This tutorial is divided into three parts; they are: Univariate Function Optimization Convex Univariate Function Optimization Non-Convex Univariate Function Optimization Univariate Function Optimization We may need to find an optimal value of a function that takes a single parameter. In machine learning, this may occur in many situations, such as: Finding the coefficient of a model to fit to a training dataset. Finding the value of a single hyperparameter that results in the best model performance. This is called univariate function optimization. We may be interested in the minimum outcome or maximum outcome of the function, although this can be simplified to minimization as a maximizing function can be made minimizing by adding a negative sign to all outcomes of the function. There may or may not be limits on the inputs to the function, so-called unconstrained or constrained optimization, and we assume that small changes in input correspond to small changes in the output of the function, e.g. that it is smooth. The function may or may not have a single optima, although we prefer that it does have a single optima and that shape of the function looks like a large basin. If this is the case, we know we can sample the function at one point and find the path down to the minima of the function. Technically, this is referred to as a convex function for minimization (concave for maximization), and functions that don\\u2019t have this basin shape are referred to as non-convex. Convex Target Function: There is a single optima and the shape of the target function leads to this optima. Nevertheless, the target function is sufficiently complex that we don\\u2019t know the derivative, meaning we cannot just use calculus to analytically compute the minimum or maximum of the function where the gradient is zero. This is referred to as a function that is non-differentiable. Although we might be able to sample the function with candidate values, we don\\u2019t know the input that will result in the best outcome. This may be because of the many reasons it is expensive to evaluate candidate solutions. Therefore, we require an algorithm that efficiently samples input values to the function. One approach to solving univariate function optimization problems is to use Brent\\u2019s method. Brent\\u2019s method is an optimization algorithm that combines a bisecting algorithm (Dekker\\u2019s method) and inverse quadratic interpolation. It can be used for constrained and unconstrained univariate function optimization. The Brent-Dekker method is an extension of the bisection method. It is a root-finding algorithm that combines elements of the secant method and inverse quadratic interpolation. It has reliable and fast convergence properties, and it is the univariate optimization algorithm of choice in many popular numerical optimization packages. \\u2014 Pages 49-51, Algorithms for Optimization, 2019. Bisecting algorithms use a bracket (lower and upper) of input values and split up the input domain, bisecting it in order to locate where in the domain the optima is located, much like a binary search. Dekker\\u2019s method is one way this is achieved efficiently for a continuous domain. Dekker\\u2019s method gets stuck on non-convex problems. Brent\\u2019s method modifies Dekker\\u2019s method to avoid getting stuck and also approximates the second derivative of the objective function (called the Secant Method) in an effort to accelerate the search. As such, Brent\\u2019s method for univariate function optimization is generally preferred over most other univariate function optimization algorithms given its efficiency. Brent\\u2019s method is available in Python via the minimize_scalar() SciPy function that takes the name of the function to be minimized. If your target function is constrained to a range, it can be specified via the \\u201cbounds\\u201d argument. It returns an OptimizeResult object that is a dictionary containing the solution. Importantly, the \\u2018x\\u2018 key summarizes the input for the optima, the \\u2018fun\\u2018 key summarizes the function output for the optima, and the \\u2018nfev\\u2018 summarizes the number of evaluations of the target function that were performed. ... # minimize the function result = minimize_scalar(objective, method='brent') Now that we know how to perform univariate function optimization in Python, let\\u2019s look at some examples. Convex Univariate Function Optimization In this section, we will explore how to solve a convex univariate function optimization problem. First, we can define a function that implements our function. In this case, we will use a simple offset version of the x^2 function e.g. a simple parabola (u-shape) function. It is a minimization objective function with an optima at -5.0. # objective function def objective(x): return (5.0 + x)**2.0 We can plot a coarse grid of this function with input values from -10 to 10 to get an idea of the shape of the target function. The complete example is listed below. # plot a convex target function from numpy import arange from matplotlib import pyplot # objective function def objective(x): return (5.0 + x)**2.0 # define range r_min, r_max = -10.0, 10.0 # prepare inputs inputs = arange(r_min, r_max, 0.1) # compute targets targets = [objective(x) for x in inputs] # plot inputs vs target pyplot.plot(inputs, targets, '--') pyplot.show() Running the example evaluates input values in our specified range using our target function and creates a plot of the function inputs to function outputs. We can see the U-shape of the function and that the objective is at -5.0. Line Plot of a Convex Objective Function Note: in a real optimization problem, we would not be able to perform so many evaluations of the objective function so easily. This simple function is used for demonstration purposes so we can learn how to use the optimization algorithm. Next, we can use the optimization algorithm to find the optima. ... # minimize the function result = minimize_scalar(objective, method='brent') Once optimized, we can summarize the result, including the input and evaluation of the optima and the number of function evaluations required to locate the optima. ... # summarize the result opt_x, opt_y = result['x'], result['fun'] print('Optimal Input x: %.6f' % opt_x) print('Optimal Output f(x): %.6f' % opt_y) print('Total Evaluations n: %d' % result['nfev']) Finally, we can plot the function again and mark the optima to confirm it was located in the place we expected for this function. ... # define the range r_min, r_max = -10.0, 10.0 # prepare inputs inputs = arange(r_min, r_max, 0.1) # compute targets targets = [objective(x) for x in inputs] # plot inputs vs target pyplot.plot(inputs, targets, '--') # plot the optima pyplot.plot([opt_x], [opt_y], 's', color='r') # show the plot pyplot.show() The complete example of optimizing an unconstrained convex univariate function is listed below. # optimize convex objective function from numpy import arange from scipy.optimize import minimize_scalar from matplotlib import pyplot # objective function def objective(x): return (5.0 + x)**2.0 # minimize the function result = minimize_scalar(objective, method='brent') # summarize the result opt_x, opt_y = result['x'], result['fun'] print('Optimal Input x: %.6f' % opt_x) print('Optimal Output f(x): %.6f' % opt_y) print('Total Evaluations n: %d' % result['nfev']) # define the range r_min, r_max = -10.0, 10.0 # prepare inputs inputs = arange(r_min, r_max, 0.1) # compute targets targets = [objective(x) for x in inputs] # plot inputs vs target pyplot.plot(inputs, targets, '--') # plot the optima pyplot.plot([opt_x], [opt_y], 's', color='r') # show the plot pyplot.show() Running the example first solves the optimization problem and reports the result. Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome. In this case, we can see that the optima was located after 10 evaluations of the objective function with an input of -5.0, achieving an objective function value of 0.0. Optimal Input x: -5.000000 Optimal Output f(x): 0.000000 Total Evaluations n: 10 A plot of the function is created again and this time, the optima is marked as a red square. Line Plot of a Convex Objective Function with Optima Marked Non-Convex Univariate Function Optimization A convex function is one that does not resemble a basin, meaning that it may have more than one hill or valley. This can make it more challenging to locate the global optima as the multiple hills and valleys can cause the search to get stuck and report a false or local optima instead. We can define a non-convex univariate function as follows. # objective function def objective(x): return (x - 2.0) * x * (x + 2.0)**2.0 We can sample this function and create a line plot of input values to objective values. The complete example is listed below. # plot a non-convex univariate function from numpy import arange from matplotlib import pyplot # objective function def objective(x): return (x - 2.0) * x * (x + 2.0)**2.0 # define range r_min, r_max = -3.0, 2.5 # prepare inputs inputs = arange(r_min, r_max, 0.1) # compute targets targets = [objective(x) for x in inputs] # plot inputs vs target pyplot.plot(inputs, targets, '--') pyplot.show() Running the example evaluates input values in our specified range using our target function and creates a plot of the function inputs to function outputs. We can see a function with one false optima around -2.0 and a global optima around 1.2. Line Plot of a Non-Convex Objective Function Note: in a real optimization problem, we would not be able to perform so many evaluations of the objective function so easily. This simple function is used for demonstration purposes so we can learn how to use the optimization algorithm. Next, we can use the optimization algorithm to find the optima. As before, we can call the minimize_scalar() function to optimize the function, then summarize the result and plot the optima on a line plot. The complete example of optimization of an unconstrained non-convex univariate function is listed below. # optimize non-convex objective function from numpy import arange from scipy.optimize import minimize_scalar from matplotlib import pyplot # objective function def objective(x): return (x - 2.0) * x * (x + 2.0)**2.0 # minimize the function result = minimize_scalar(objective, method='brent') # summarize the result opt_x, opt_y = result['x'], result['fun'] print('Optimal Input x: %.6f' % opt_x) print('Optimal Output f(x): %.6f' % opt_y) print('Total Evaluations n: %d' % result['nfev']) # define the range r_min, r_max = -3.0, 2.5 # prepare inputs inputs = arange(r_min, r_max, 0.1) # compute targets targets = [objective(x) for x in inputs] # plot inputs vs target pyplot.plot(inputs, targets, '--') # plot the optima pyplot.plot([opt_x], [opt_y], 's', color='r') # show the plot pyplot.show() Running the example first solves the optimization problem and reports the result. Want to Get Started With Ensemble Learning? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course In this case, we can see that the optima was located after 15 evaluations of the objective function with an input of about 1.28, achieving an objective function value of about -9.91. Optimal Input x: 1.280776 Optimal Output f(x): -9.914950 Total Evaluations n: 15 A plot of the function is created again, and this time, the optima is marked as a red square. We can see that the optimization was not deceived by the false optima and successfully located the global optima. Line Plot of a Non-Convex Objective Function with Optima Marked Further Reading This section provides more resources on the topic if you are looking to go deeper. Books Algorithms for Optimization, 2019. APIs Optimization (scipy.optimize). Optimization and root finding (scipy.optimize) scipy.optimize.minimize_scalar API. Articles Brent\\u2019s method, Wikipedia. Secant method, Wikipedia. Summary In this tutorial, you discovered how to perform univariate function optimization in Python. Specifically, you learned: Univariate function optimization involves finding an optimal input for an objective function that takes a single continuous argument. How to perform univariate function optimization for an unconstrained convex function. How to perform univariate function optimization for an unconstrained non-convex function. Do you have any questions? Ask your questions in the comments below and I will do my best to answer. The post Univariate Function Optimization in Python appeared first on Machine Learning Mastery.\",\"1640\":\"Read the full story\",\"1579\":\"Pieter Abbeel is a professor at UC Berkeley, director of the Berkeley Robot Learning Lab, and is one of the top researchers in the world working on how to make robots understand and interact with the world around them, especially through imitation and deep reinforcement learning. Video version is available on YouTube. If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman on Twitter, LinkedIn, Facebook, or YouTube where you can watch the video versions of these conversations.\",\"4092\":\"My computer can accommodate 3 mini pcie corals. If I install 3, can I cluster them? And if so, how would I set about doing that? The OS is Ubuntu. [link] [comments]\",\"1046\":\"Get free access to over 2500 documentaries on CuriosityStream: http:\\/\\/go.thoughtleaders.io\\/1622620200907 (use promo code \\\"zachstar\\\" at sign up) STEMerch Store: https:\\/\\/stemerch.com\\/ Support the Channel: https:\\/\\/www.patreon.com\\/zachstar PayPal(one time donation): https:\\/\\/www.paypal.me\\/ZachStarYT Join this channel to get access to perks: https:\\/\\/www.youtube.com\\/channel\\/UCpCSAcbqs-sjEVfk_hMfY9w\\/join \\u25baFollow me Instagram: https:\\/\\/www.instagram.com\\/zachstar\\/ Twitter: https:\\/\\/twitter.com\\/ImZachStar Mathematics used to solve crime: https:\\/\\/youtu.be\\/-cXBgHgX5UE Hypersphere universe: https:\\/\\/youtu.be\\/iiGe2x8t6mA Lumberjack Feynman Lectures: https:\\/\\/www.youtube.com\\/watch?v=Gucaa-pwuD8&list=PLSuQRd4LfSUTmb_7IK7kAzxJtU2tpmEd3 Mathematics of Universe (Metric Tensor Video): https:\\/\\/www.youtube.com\\/watch?v=KT5Sk-62-pg Animations: Brainup Studios ( http:\\/\\/brainup.in\\/ ) Check out my Spanish channel here: https:\\/\\/www.youtube.com\\/channel\\/UCnkNu2xQBLASpj6cKC8vtpA \\u25baMy Setup: Space Pictures: https:\\/\\/amzn.to\\/2CC4Kqj Magnetic Floating Globe: https:\\/\\/amzn.to\\/2VgPdn0 Camera: https:\\/\\/amzn.to\\/2RivYu5 Mic: https:\\/\\/amzn.to\\/35bKiri Tripod: https:\\/\\/amzn.to\\/2RgMTNL Equilibrium Tube: https:\\/\\/amzn.to\\/2SowDrh \\u25baCheck out the my Amazon Store: https:\\/\\/www.amazon.com\\/shop\\/zachstar\",\"710\":\"Day 2 | December 2, 2020 A panel of four researchers at Microsoft discuss their experience working at Microsoft as PhDs and their career journeys since then. Microsoft's third PhD Summit was a two-day virtual workshop. It was an opportunity for top PhD students to enhance their skills, build a network, and discuss research within a community of peers and notable Microsoft researchers. Speakers: Denae Ford Robinson, Senior Researcher, Microsoft [Discussion Lead] Bita Darvish Rouhani, Senior Researcher and Manager, Microsoft Lukas Maas, Senior Research SDE, Microsoft Martez Mott, Senior Researcher, Microsoft More information on this workshop can be found at https:\\/\\/www.microsoft.com\\/en-us\\/research\\/event\\/phd-summit-2020\\/\",\"5097\":\"Tweet Share Share Recommender systems may be the most common type of predictive model that the average person may encounter. They provide the basis for recommendations on services such as Amazon, Spotify, and Youtube. Recommender systems are a huge daunting topic if you\\u2019re just getting started. There is a myriad of data preparation techniques, algorithms, and model evaluation methods. Not all of the techniques will be relevant, and in fact, the state-of-the-art can be ignored for now as you will likely get very good results by focusing on the fundamentals, e.g. treat it as a straightforward classification or regression problem. It is important to know the basics and have it all laid out for you in a systematic way. For this, I recommend skimming or reading the standard books and papers on the topic and looking at some of the popular libraries. In this tutorial, you will discover resources you can use to get started with recommender systems. After completing this tutorial, you will know: The top review papers on recommender systems you can use to quickly understand the state of the field. The top books on recommender systems from which you can learn the algorithms and techniques required when developing and evaluating recommender systems. The top Python libraries and APIs that you can use to prototype and develop your own recommender systems. Let\\u2019s get started. How to Get Started With Recommender Systems Photo by Paul Toogood, some right reserved. Tutorial Overview This tutorial is divided into three parts; they are: Papers on Recommender Systems Books on Recommender Systems Recommender Systems Libraries Papers on Recommender Systems Research papers on recommender systems can help you very quickly get up to speed on the state of the field. Specifically, review papers that use precise language to define what a recommender system is, the algorithms that can be used, standard datasets and metrics for comparing algorithms, and hints at the state of the art techniques. By skimming or reading a handful of review papers on recommender systems, you can quickly develop a foundation from which to dive deeper and start developing your own systems. The field does not change that quickly, and techniques from 10 or 20 years ago will give you solid results. Review papers on recommender systems I recommended to establish a foundational understanding include: Amazon.com Recommendations: Item-to-item Collaborative Filtering, 2003. Matrix Factorization Techniques for Recommender Systems, 2009. Recommender Systems, 2012. Recommender Systems Survey, 2013. Advances in Collaborative Filtering, 2015. Matrix Factorization Techniques for Recommender Systems Once you have questions about specific techniques, you can then find papers that focus on those techniques and dive deeper. You can search for papers on specific techniques here: Google Scholar Do you know of additional good review papers on recommender systems? Let me know in the comments below. Books on Recommender Systems Books on recommender systems provide the space to lay out the field and take you on a tour of the techniques and give you the detail you need to understand them, with more breadth and detail than a much shorter review paper. Again, given that the field is quite mature, older books, such as those published a decade ago, should not be immediately neglected. Some top textbooks published by key researchers in the field include the following: Recommender Systems: An Introduction, 2010. Recommender Systems: The Textbook, 2016. I own a hard copy of \\u201cRecommender Systems: An Introduction\\u201d and cannot recommend it highly enough. This book offers an overview of approaches to developing state-of-the-art recommender systems. The authors present current algorithmic approaches for generating personalized buying proposals, such as collaborative and content-based filtering, as well as more interactive and knowledge- based approaches. They also discuss how to measure the effectiveness of recommender systems and illustrate the methods with practical case studies. \\u2014 Recommender Systems: An Introduction, 2010. The table of contents for this book is as follows: Chapter 1: Introduction Chapter 2: Collaborative recommendation Chapter 3: Content-based recommendation Chapter 4: Knowledge-based recommendation Chapter 5: Hybrid recommendation approaches Chapter 6: Explanations in recommender systems Chapter 7: Evaluating recommender systems Chapter 8: Case study: Personalized game recommendations on the mobile Internet Chapter 9: Attacks on collaborative recommender systems Chapter 10: Online consumer decision making Chapter 11: Recommender systems and the next-generation web Chapter 12: Recommendations in ubiquitous environments Chapter 13: Summary and outlook Recommender Systems: An Introduction It can be good to get a handbook on the topic with chapters written by different academics summarizing or championing their preferred techniques and methods. I recommend this handbook: Recommender Systems Handbook, 2015. If you are looking for a more hands-on book, I recommend: Practical Recommender Systems, 2019. Have you read one of these books? Or do you know another great book on the topic? Let me know in the comments below. Recommender Systems Libraries You probably don\\u2019t need to dive into the start of the art, at least not immediately. As such, standard machine learning libraries are a great place to start. For example, you can develop an effective recommender system using matrix factorization methods (SVD) or even a straight forward k-nearest neighbors model by items or by users. As such, I recommend starting with some experiments with scikit-learn: Scikit-Learn Python Machine Learning Library. You can practice on standard recommender system datasets if your own data is not yet accessible or available, or you just want to get the hang of things first. Popular standard datasets for recommender systems include: MovieLens Yahoo datasets (music, urls, movies, etc.) If you are ready for state-of-the-art techniques, a great place to start is \\u201cpapers with code\\u201d that lists both academic papers and links to the source code for the methods described in the paper: Papers With Code: Recommendation Systems There are a number of proprietary and open-source libraries and services for recommender systems. I recommend sticking with open-source Python libraries in the beginning, such as: Surprise: A Python scikit for building and analyzing recommender systems Case Recommender: A Flexible and Extensible Python Framework for Recommender Systems Have you used any of these libraries to develop a recommender system? Let me know in the comments below. Summary In this tutorial, you discovered resources you can use to get started with recommender systems. Specifically, you learned: The top review papers on recommender systems you can use to quickly understand the state of the field. The top books on recommender systems from which you can learn the algorithms and techniques required when developing and evaluating recommender systems. The top Python libraries and APIs that you can use to prototype and develop your own recommender systems. Do you have any questions? Ask your questions in the comments below and I will do my best to answer. Tweet Share Share The post How to Get Started With Recommender Systems appeared first on Machine Learning Mastery.\",\"4270\":\"Tweet Share Share Last Updated on January 22, 2021 Activation functions are a critical part of the design of a neural network. The choice of activation function in the hidden layer will control how well the network model learns the training dataset. The choice of activation function in the output layer will define the type of predictions the model can make. As such, a careful choice of activation function must be made for each deep learning neural network project. In this tutorial, you will discover how to choose activation functions for neural network models. After completing this tutorial, you will know: Activation functions are a key part of neural network design. The modern default activation function for hidden layers is the ReLU function. The activation function for output layers depends on the type of prediction problem. Let\\u2019s get started. How to Choose an Activation Function for Deep Learning Photo by Peter Dowley, some rights reserved. Tutorial Overview This tutorial is divided into three parts; they are: Activation Functions Activation for Hidden Layers Activation for Output Layers Activation Functions An activation function in a neural network defines how the weighted sum of the input is transformed into an output from a node or nodes in a layer of the network. Sometimes the activation function is called a \\u201ctransfer function.\\u201d If the output range of the activation function is limited, then it may be called a \\u201csquashing function.\\u201d Many activation functions are nonlinear and may be referred to as the \\u201cnonlinearity\\u201d in the layer or the network design. The choice of activation function has a large impact on the capability and performance of the neural network, and different activation functions may be used in different parts of the model. Technically, the activation function is used within or after the internal processing of each node in the network, although networks are designed to use the same activation function for all nodes in a layer. A network may have three types of layers: input layers that take raw input from the domain, hidden layers that take input from another layer and pass output to another layer, and output layers that make a prediction. All hidden layers typically use the same activation function. The output layer will typically use a different activation function from the hidden layers and is dependent upon the type of prediction required by the model. Activation functions are also typically differentiable, meaning the first-order derivative can be calculated for a given input value. This is required given that neural networks are typically trained using the backpropagation of error algorithm that requires the derivative of prediction error in order to update the weights of the model. There are many different types of activation functions used in neural networks, although perhaps only a small number of functions used in practice for hidden and output layers. Let\\u2019s take a look at the activation functions used for each type of layer in turn. Activation for Hidden Layers A hidden layer in a neural network is a layer that receives input from another layer (such as another hidden layer or an input layer) and provides output to another layer (such as another hidden layer or an output layer). A hidden layer does not directly contact input data or produce outputs for a model, at least in general. A neural network may have zero or more hidden layers. Typically, a differentiable nonlinear activation function is used in the hidden layers of a neural network. This allows the model to learn more complex functions than a network trained using a linear activation function. In order to get access to a much richer hypothesis space that would benefit from deep representations, you need a non-linearity, or activation function. \\u2014 Page 72, Deep Learning with Python, 2017. There are perhaps three activation functions you may want to consider for use in hidden layers; they are: Rectified Linear Activation (ReLU) Logistic (Sigmoid) Hyperbolic Tangent (Tanh) This is not an exhaustive list of activation functions used for hidden layers, but they are the most commonly used. Let\\u2019s take a closer look at each in turn. ReLU Hidden Layer Activation Function The rectified linear activation function, or ReLU activation function, is perhaps the most common function used for hidden layers. It is common because it is both simple to implement and effective at overcoming the limitations of other previously popular activation functions, such as Sigmoid and Tanh. Specifically, it is less susceptible to vanishing gradients that prevent deep models from being trained, although it can suffer from other problems like saturated or \\u201cdead\\u201d units. The ReLU function is calculated as follows: max(0.0, x) This means that if the input value (x) is negative, then a value 0.0 is returned, otherwise, the value is returned. You can learn more about the details of the ReLU activation function in this tutorial: A Gentle Introduction to the Rectified Linear Unit (ReLU) We can get an intuition for the shape of this function with the worked example below. # example plot for the relu activation function from matplotlib import pyplot # rectified linear function def rectified(x): return max(0.0, x) # define input data inputs = [x for x in range(-10, 10)] # calculate outputs outputs = [rectified(x) for x in inputs] # plot inputs vs outputs pyplot.plot(inputs, outputs) pyplot.show() Running the example calculates the outputs for a range of values and creates a plot of inputs versus outputs. We can see the familiar kink shape of the ReLU activation function. Plot of Inputs vs. Outputs for the ReLU Activation Function. When using the ReLU function for hidden layers, it is a good practice to use a \\u201cHe Normal\\u201d or \\u201cHe Uniform\\u201d weight initialization and scale input data to the range 0-1 (normalize) prior to training. Sigmoid Hidden Layer Activation Function The sigmoid activation function is also called the logistic function. It is the same function used in the logistic regression classification algorithm. The function takes any real value as input and outputs values in the range 0 to 1. The larger the input (more positive), the closer the output value will be to 1.0, whereas the smaller the input (more negative), the closer the output will be to 0.0. The sigmoid activation function is calculated as follows: 1.0 \\/ (1.0 + e^-x) Where e is a mathematical constant, which is the base of the natural logarithm. We can get an intuition for the shape of this function with the worked example below. # example plot for the sigmoid activation function from math import exp from matplotlib import pyplot # sigmoid activation function def sigmoid(x): return 1.0 \\/ (1.0 + exp(-x)) # define input data inputs = [x for x in range(-10, 10)] # calculate outputs outputs = [sigmoid(x) for x in inputs] # plot inputs vs outputs pyplot.plot(inputs, outputs) pyplot.show() Running the example calculates the outputs for a range of values and creates a plot of inputs versus outputs. We can see the familiar S-shape of the sigmoid activation function. Plot of Inputs vs. Outputs for the Sigmoid Activation Function. When using the Sigmoid function for hidden layers, it is a good practice to use a \\u201cXavier Normal\\u201d or \\u201cXavier Uniform\\u201d weight initialization (also referred to Glorot initialization, named for Xavier Glorot) and scale input data to the range 0-1 (e.g. the range of the activation function) prior to training. Tanh Hidden Layer Activation Function The hyperbolic tangent activation function is also referred to simply as the Tanh (also \\u201ctanh\\u201d and \\u201cTanH\\u201c) function. It is very similar to the sigmoid activation function and even has the same S-shape. The function takes any real value as input and outputs values in the range -1 to 1. The larger the input (more positive), the closer the output value will be to 1.0, whereas the smaller the input (more negative), the closer the output will be to -1.0. The Tanh activation function is calculated as follows: (e^x \\u2013 e^-x) \\/ (e^x + e^-x) Where e is a mathematical constant that is the base of the natural logarithm. We can get an intuition for the shape of this function with the worked example below. # example plot for the tanh activation function from math import exp from matplotlib import pyplot # tanh activation function def tanh(x): return (exp(x) - exp(-x)) \\/ (exp(x) + exp(-x)) # define input data inputs = [x for x in range(-10, 10)] # calculate outputs outputs = [tanh(x) for x in inputs] # plot inputs vs outputs pyplot.plot(inputs, outputs) pyplot.show() Running the example calculates the outputs for a range of values and creates a plot of inputs versus outputs. We can see the familiar S-shape of the Tanh activation function. Plot of Inputs vs. Outputs for the Tanh Activation Function. When using the TanH function for hidden layers, it is a good practice to use a \\u201cXavier Normal\\u201d or \\u201cXavier Uniform\\u201d weight initialization (also referred to Glorot initialization, named for Xavier Glorot) and scale input data to the range -1 to 1 (e.g. the range of the activation function) prior to training. How to Choose a Hidden Layer Activation Function A neural network will almost always have the same activation function in all hidden layers. It is most unusual to vary the activation function through a network model. Traditionally, the sigmoid activation function was the default activation function in the 1990s. Perhaps through the mid to late 1990s to 2010s, the Tanh function was the default activation function for hidden layers. \\u2026 the hyperbolic tangent activation function typically performs better than the logistic sigmoid. \\u2014 Page 195, Deep Learning, 2016. Both the sigmoid and Tanh functions can make the model more susceptible to problems during training, via the so-called vanishing gradients problem. You can learn more about this problem in this tutorial: A Gentle Introduction to the Rectified Linear Unit (ReLU) The activation function used in hidden layers is typically chosen based on the type of neural network architecture. Modern neural network models with common architectures, such as MLP and CNN, will make use of the ReLU activation function, or extensions. In modern neural networks, the default recommendation is to use the rectified linear unit or ReLU \\u2026 \\u2014 Page 174, Deep Learning, 2016. Recurrent networks still commonly use Tanh or sigmoid activation functions, or even both. For example, the LSTM commonly uses the Sigmoid activation for recurrent connections and the Tanh activation for output. Multilayer Perceptron (MLP): ReLU activation function. Convolutional Neural Network (CNN): ReLU activation function. Recurrent Neural Network: Tanh and\\/or Sigmoid activation function. If you\\u2019re unsure which activation function to use for your network, try a few and compare the results. The figure below summarizes how to choose an activation function for the hidden layers of your neural network model. How to Choose a Hidden Layer Activation Function Activation for Output Layers The output layer is the layer in a neural network model that directly outputs a prediction. All feed-forward neural network models have an output layer. There are perhaps three activation functions you may want to consider for use in the output layer; they are: Linear Logistic (Sigmoid) Softmax This is not an exhaustive list of activation functions used for output layers, but they are the most commonly used. Let\\u2019s take a closer look at each in turn. Linear Output Activation Function The linear activation function is also called \\u201cidentity\\u201d (multiplied by 1.0) or \\u201cno activation.\\u201d This is because the linear activation function does not change the weighted sum of the input in any way and instead returns the value directly. We can get an intuition for the shape of this function with the worked example below. # example plot for the linear activation function from matplotlib import pyplot # linear activation function def linear(x): return x # define input data inputs = [x for x in range(-10, 10)] # calculate outputs outputs = [linear(x) for x in inputs] # plot inputs vs outputs pyplot.plot(inputs, outputs) pyplot.show() Running the example calculates the outputs for a range of values and creates a plot of inputs versus outputs. We can see a diagonal line shape where inputs are plotted against identical outputs. Plot of Inputs vs. Outputs for the Linear Activation Function Target values used to train a model with a linear activation function in the output layer are typically scaled prior to modeling using normalization or standardization transforms. Sigmoid Output Activation Function The sigmoid of logistic activation function was described in the previous section. Nevertheless, to add some symmetry, we can review for the shape of this function with the worked example below. # example plot for the sigmoid activation function from math import exp from matplotlib import pyplot # sigmoid activation function def sigmoid(x): return 1.0 \\/ (1.0 + exp(-x)) # define input data inputs = [x for x in range(-10, 10)] # calculate outputs outputs = [sigmoid(x) for x in inputs] # plot inputs vs outputs pyplot.plot(inputs, outputs) pyplot.show() Running the example calculates the outputs for a range of values and creates a plot of inputs versus outputs. We can see the familiar S-shape of the sigmoid activation function. Plot of Inputs vs. Outputs for the Sigmoid Activation Function. Target labels used to train a model with a sigmoid activation function in the output layer will have the values 0 or 1. Softmax Output Activation Function The softmax function outputs a vector of values that sum to 1.0 that can be interpreted as probabilities of class membership. It is related to the argmax function that outputs a 0 for all options and 1 for the chosen option. Softmax is a \\u201csofter\\u201d version of argmax that allows a probability-like output of a winner-take-all function. As such, the input to the function is a vector of real values and the output is a vector of the same length with values that sum to 1.0 like probabilities. The softmax function is calculated as follows: e^x \\/ sum(e^x) Where x is a vector of outputs and e is a mathematical constant that is the base of the natural logarithm. You can learn more about the details of the Softmax function in this tutorial: Softmax Activation Function with Python We cannot plot the softmax function, but we can give an example of calculating it in Python. from numpy import exp # softmax activation function def softmax(x): return exp(x) \\/ exp(x).sum() # define input data inputs = [1.0, 3.0, 2.0] # calculate outputs outputs = softmax(inputs) # report the probabilities print(outputs) # report the sum of the probabilities print(outputs.sum()) Running the example calculates the softmax output for the input vector. We then confirm that the sum of the outputs of the softmax indeed sums to the value 1.0. [0.09003057 0.66524096 0.24472847] 1.0 Target labels used to train a model with the softmax activation function in the output layer will be vectors with 1 for the target class and 0 for all other classes. How to Choose an Output Activation Function You must choose the activation function for your output layer based on the type of prediction problem that you are solving. Specifically, the type of variable that is being predicted. For example, you may divide prediction problems into two main groups, predicting a categorical variable (classification) and predicting a numerical variable (regression). If your problem is a regression problem, you should use a linear activation function. Regression: One node, linear activation. If your problem is a classification problem, then there are three main types of classification problems and each may use a different activation function. Predicting a probability is not a regression problem; it is classification. In all cases of classification, your model will predict the probability of class membership (e.g. probability that an example belongs to each class) that you can convert to a crisp class label by rounding (for sigmoid) or argmax (for softmax). If there are two mutually exclusive classes (binary classification), then your output layer will have one node and a sigmoid activation function should be used. If there are more than two mutually exclusive classes (multiclass classification), then your output layer will have one node per class and a softmax activation should be used. If there are two or more mutually inclusive classes (multilabel classification), then your output layer will have one node for each class and a sigmoid activation function is used. Binary Classification: One node, sigmoid activation. Multiclass Classification: One node per class, softmax activation. Multilabel Classification: One node per class, sigmoid activation. The figure below summarizes how to choose an activation function for the output layer of your neural network model. How to Choose an Output Layer Activation Function Further Reading This section provides more resources on the topic if you are looking to go deeper. Tutorials A Gentle Introduction to the Rectified Linear Unit (ReLU) Softmax Activation Function with Python 4 Types of Classification Tasks in Machine Learning How to Fix the Vanishing Gradients Problem Using the ReLU Books Deep Learning, 2016. Neural Smithing: Supervised Learning in Feedforward Artificial Neural Networks, 1999. Neural Networks for Pattern Recognition, 1996. Deep Learning with Python, 2017. Articles Activation function, Wikipedia. Summary In this tutorial, you discovered how to choose activation functions for neural network models. Specifically, you learned: Activation functions are a key part of neural network design. The modern default activation function for hidden layers is the ReLU function. The activation function for output layers depends on the type of prediction problem. Do you have any questions? Ask your questions in the comments below and I will do my best to answer. Tweet Share Share The post How to Choose an Activation Function for Deep Learning appeared first on Machine Learning Mastery.\",\"1222\":\"The main ideas behind Backpropagation are super simple, but there are tons of details when it comes time to implementing it. This video shows how to optimize three parameters in a Neural Network simultaneously and introduces some Fancy Notation. \\u2b50 NOTE: When I code, I use Kite, a free AI-powered coding assistant that will help you code faster and smarter. The Kite plugin integrates with all the top editors and IDEs to give you smart completions and documentation while you\\u2019re typing. I love it! https:\\/\\/www.kite.com\\/get-kite\\/?utm_medium=referral&utm_source=youtube&utm_campaign=statquest&utm_content=description-only NOTE: This StatQuest assumes that you already know the main ideas behind Backpropagation: https:\\/\\/youtu.be\\/IN2XmBhILt4 ...and that also means you should be familiar with... Neural Networks: https:\\/\\/youtu.be\\/CqOfi41LfDw The Chain Rule: https:\\/\\/youtu.be\\/wl1myxrtQHQ Gradient Descent: https:\\/\\/youtu.be\\/sDv4f4s2SB8 LAST NOTE: When I was researching this 'Quest, I found this page by Sebastian Raschka to be helpful: https:\\/\\/sebastianraschka.com\\/faq\\/docs\\/backprop-arbitrary.html For a complete index of all the StatQuest videos, check out: https:\\/\\/statquest.org\\/video-index\\/ If you'd like to support StatQuest, please consider... Patreon: https:\\/\\/www.patreon.com\\/statquest ...or... YouTube Membership: https:\\/\\/www.youtube.com\\/channel\\/UCtYLUTtgS3k1Fg4y5tAhLbw\\/join ...a cool StatQuest t-shirt or sweatshirt (USA\\/Europe): https:\\/\\/teespring.com\\/stores\\/statquest (everywhere): https:\\/\\/www.redbubble.com\\/people\\/starmer\\/works\\/40421224-statquest-double-bam?asc=u&p=t-shirt ...buying one or two of my songs (or go large and get a whole album!) https:\\/\\/joshuastarmer.bandcamp.com\\/ ...or just donating to StatQuest! https:\\/\\/www.paypal.me\\/statquest Lastly, if you want to keep up with me as I research and create new StatQuests, follow me on twitter: https:\\/\\/twitter.com\\/joshuastarmer 0:00 Awesome song and introduction 3:01 Derivatives do not change when we optimize multiple parameters 6:28 Fancy Notation 10:51 Derivatives with respect to two different weights 15:02 Gradient Descent for three parameters 17:19 Fancy Gradient Descent Animation #StatQuest #NeuralNetworks #Backpropagation\",\"1535\":\"Manolis Kellis is a computational biologist at MIT. Please support this podcast by checking out our sponsors: \\u2013 SEMrush: https:\\/\\/www.semrush.com\\/partner\\/lex\\/ to get a free month of Guru \\u2013 Pessimists Archive: https:\\/\\/pessimists.co\\/ \\u2013 Eight Sleep: https:\\/\\/www.eightsleep.com\\/lex and use code LEX to get $200 off \\u2013 BetterHelp: https:\\/\\/betterhelp.com\\/lex to get 10% off EPISODE LINKS: Manolis Website: http:\\/\\/web.mit.edu\\/manoli\\/ Manolis Twitter: https:\\/\\/twitter.com\\/manoliskellis Manolis YouTube: https:\\/\\/www.youtube.com\\/channel\\/UCkKlJ5LHrE3C7fgbnPA5DGA Manolis Wikipedia: https:\\/\\/en.wikipedia.org\\/wiki\\/Manolis_Kellis PODCAST INFO: Podcast website: https:\\/\\/lexfridman.com\\/podcast Apple Podcasts: https:\\/\\/apple.co\\/2lwqZIr Spotify: https:\\/\\/spoti.fi\\/2nEwCF8 RSS: https:\\/\\/lexfridman.com\\/feed\\/podcast\\/ YouTube Full Episodes: https:\\/\\/youtube.com\\/lexfridman YouTube Clips: https:\\/\\/youtube.com\\/lexclips SUPPORT & CONNECT: \\u2013 Check out the sponsors above, it\\u2019s the best way to support this podcast\",\"476\":\"i^i, visualized and explained. Full playlist: https:\\/\\/www.youtube.com\\/playlist?list=PLZHQObOWTQDP5CVelJJ1bNDouqrAhVPev Home page: https:\\/\\/www.3blue1brown.com Brought to you by you: https:\\/\\/3b1b.co\\/ldm-thanks Mistakes: At 1:06:20, when changing r to equal 0.69*i, I said \\\"this is what we might think of as (2i)^x\\\", but that is not correct. It's what we'd think of as [Exp(ln(2)*i)]^x for whatever complex number Exp(ln(2)*i) is. Beautiful notes by Ng\\u00e2n V\\u0169 https:\\/\\/twitter.com\\/ThuyNganVu\\/status\\/1263522876403011585 Video by Matt Parker: https:\\/\\/youtu.be\\/9tlHQOKMHGA Video by Red Pen Black Pen https:\\/\\/youtu.be\\/ABk1HK2AR2E ------------------ Video timeline (thanks to user \\\"nooonesperfect\\\") 0:18 Exponential function for i^i 1:26 Question 1 2:27 Plug-in imaginary number in exp(x) polynomial 3:38 Answer 1 and explanation 7:35 What it really means i^i? 9:14 e^it as a position vector 11:30 Question 2 12:39 Audience question from twitter 13:14 Answer 2 14:52 Where you get after traveling \\u03c0\\/2 units of time for position vector e^it 19:48 Question 3 20:42 Audience tweets 23:34 Answer 3 35:50 Question 4 37:11 Answer 4 40:11 How exp(rx) or b^x really works? 46:28 Question 5 47:49 Audience tweets 49:26 Answer 5 57:06 Visualization of f(x)= exp(r*x) i.e. e^(r*x), where r= unique complex number 1:06:06 Questions to think about 1:08:51 Audience tweets 1:09:09 Power tower for i ------------------ The live question setup with stats on-screen is powered by Itempool. https:\\/\\/itempool.com\\/ Curious about other animations? https:\\/\\/www.3blue1brown.com\\/faq#manim Music by Vincent Rubinetti. Download the music on Bandcamp: https:\\/\\/vincerubinetti.bandcamp.com\\/album\\/the-music-of-3blue1brown Stream the music on Spotify: https:\\/\\/open.spotify.com\\/album\\/1dVyjwS8FBqXhRunaG5W5u If you want to contribute translated subtitles or to help review those that have already been made by others and need approval, you can click the gear icon in the video and go to subtitles\\/cc, then \\\"add subtitles\\/cc\\\". I really appreciate those who do this, as it helps make the lessons accessible to more people. ------------------ 3blue1brown is a channel about animating math, in all senses of the word animate. And you know the drill with YouTube, if you want to stay posted on new videos, subscribe: http:\\/\\/3b1b.co\\/subscribe Various social media stuffs: Website: https:\\/\\/www.3blue1brown.com Twitter: https:\\/\\/twitter.com\\/3blue1brown Reddit: https:\\/\\/www.reddit.com\\/r\\/3blue1brown Instagram: https:\\/\\/www.instagram.com\\/3blue1brown_animations\\/ Patreon: https:\\/\\/patreon.com\\/3blue1brown Facebook: https:\\/\\/www.facebook.com\\/3blue1brown\",\"1461\":\"Jeff Hawkins is the founder of Redwood Center for Theoretical Neuroscience in 2002 and Numenta in 2005. In his 2004 book titled On Intelligence, and in his research before and after, he and his team have worked to reverse-engineer the neocortex and propose artificial intelligence architectures, approaches, and ideas that are inspired by the human brain. These ideas include Hierarchical Temporal Memory (HTM) from 2004 and The Thousand Brains Theory of Intelligence from 2017. If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman on Twitter, LinkedIn, Facebook, Medium, or YouTube where\",\"646\":\"Charles Isbell is the Dean of the College of Computing at Georgia Tech. Michael Littman is a computer scientist at Brown University. Please support this podcast by checking out our sponsors: - Athletic Greens: https:\\/\\/athleticgreens.com\\/lex and use code LEX to get 1 month of fish oil - Eight Sleep: https:\\/\\/www.eightsleep.com\\/lex and use code LEX to get special savings - MasterClass: https:\\/\\/masterclass.com\\/lex to get 2 for price of 1 - Cash App: https:\\/\\/cash.app\\/ and use code LexPodcast to get $10 EPISODE LINKS: Charles's Twitter: https:\\/\\/twitter.com\\/isbellHFh Charles's Website: https:\\/\\/www.cc.gatech.edu\\/~isbell\\/ Michael's Twitter: https:\\/\\/twitter.com\\/mlittmancs Michael's Website: https:\\/\\/www.littmania.com\\/ Michael's YouTube: https:\\/\\/www.youtube.com\\/user\\/mlittman PODCAST INFO: Podcast website: https:\\/\\/lexfridman.com\\/podcast Apple Podcasts: https:\\/\\/apple.co\\/2lwqZIr Spotify: https:\\/\\/spoti.fi\\/2nEwCF8 RSS: https:\\/\\/lexfridman.com\\/feed\\/podcast\\/ Full episodes playlist: https:\\/\\/www.youtube.com\\/playlist?list=PLrAXtmErZgOdP_8GztsuKi9nrraNbKKp4 Clips playlist: https:\\/\\/www.youtube.com\\/playlist?list=PLrAXtmErZgOeciFP3CBCIEElOJeitOr41 OUTLINE: 0:00 - Introduction 2:27 - Is machine learning just statistics? 6:49 - NeurIPS vs ICML 9:05 - Data is more important than algorithm 14:49 - The role of hardship in education 23:33 - How Charles and Michael met 28:05 - Key to success: never be satisfied 31:23 - Bell Labs 42:50 - Teaching machine learning 53:01 - Westworld and Ex Machina 1:01:00 - Simulation 1:07:49 - The college experience in the times of COVID 1:36:27 - Advice for young people 1:43:19 - How to learn to program 1:54:43 - Friendship CONNECT: - Subscribe to this YouTube channel - Twitter: https:\\/\\/twitter.com\\/lexfridman - LinkedIn: https:\\/\\/www.linkedin.com\\/in\\/lexfridman - Facebook: https:\\/\\/www.facebook.com\\/LexFridmanPage - Instagram: https:\\/\\/www.instagram.com\\/lexfridman - Medium: https:\\/\\/medium.com\\/@lexfridman - Support on Patreon: https:\\/\\/www.patreon.com\\/lexfridman\",\"259\":null,\"1649\":\"Gain entry into IT with knowledge of data science, engineering, cloud computing, cybersecurity, or devops. Read the full story\",\"4280\":\"In this first part of our three part series on the Social Dilemma Netflix film, Dr. Tim Scarfe, Yannic \\\"Lightspeed\\\" Kilcher and Zak Jost gang up with Cybersecurity expert Andy Smith. We give you our take on the film. We are super excited to get your feedback on this one! Hope you enjoy. 00:00:00 Introduction 00:06:11 Moral hypocrisy 00:12:38 Road to hell is paved with good intentions, attention economy 00:15:04 They know everything about you 00:18:02 Addiction 00:21:22 Differential realities 00:26:12 Self determination and Monetisation 00:29:08 AI: Overwhelm human strengths undermine human vulnerabilities 00:31:51 Conspiracy theory \\/ fake news 00:34:23 Overton window \\/ polarisation 00:39:12 Short attention span \\/ convergent behaviour 00:41:26 Is social media good for you 00:45:17 Your attention time is linear, the things you can pay attention to are a volume, anonymity 00:51:32 Andy question on security: social engineering 00:56:32 Is it a security risk having your information in social media 00:58:02 Retrospective judgement 01:03:06 Free speech and censorship 01:06:06 Technology accelerator\",\"2320\":\"I would like to look at the statistical frequency of certain words on arxiv (or academic papers in general) by year, topic, etc. Is there an existing dataset or database or library that would allow me to do this? For example, I would like to look at the frequency of the word \\\"quantum\\\" over the last five years in math subject areas. [link] [comments]\",\"709\":\"Day 1 | November 17, 2020 Theme: Envisioning the Future of Tech for Inclusion Andrew Begel, Microsoft The Accessible Computer Science Education Fall Workshop was hosted by Microsoft, University of Washington CREATE, and University of Colorado\\u2019s Coleman Institute. It took place November 17-19, 2020 and consisted of three half-days of talks, discussions, and planning for new research dedicated to making Computer Science education learning experiences more accessible for people with disabilities. More information on this workshop can be found at https:\\/\\/www.microsoft.com\\/en-us\\/research\\/event\\/accessible-cs-education-fall-workshop\\/\",\"5686\":\"I'm slightly new to the field of ML research (have 1 published conference paper and 2 journal papers under review), but from what I've seen so far, a vast majority of the papers in the field are just Permutations and Combinations of the same existing datasets and existing methods. When I worked with a research group on computational biology, most of the work there was something along the lines of: taking a biology dataset that had only statistical analysis done before, training a random ML\\/deep learning network, and publishing that as a \\\"novel\\\" contribution - low hanging fruit like that is everywhere from astronomy to healthcare. I've seen my friends trying 100s of different models on a dataset just to see if any one model or an ensemble of some of those models would beat the existing SOTA by even 0.5%. On popular datasets like the NSLKDD (an intrusion detection dataset), we have 100s of neural network models, each of which is a paper - even though all of them have more or less the same performance (some are better in accuracy, other have lower FPR, other have lesser training cost, or others are just ensembles). Sure, there are very interesting and novel ideas coming in - but the vast majority of people just seem to be throwing random models from random fields at random datasets hoping that it's faster\\/better\\/less memory usage\\/anything that can be used to claim \\\"novelty\\\". Is \\\"research\\\" like this even useful? [link] [comments]\",\"1481\":\"John Hopfield is professor at Princeton, whose life\\u2019s work weaved beautifully through biology, chemistry, neuroscience, and physics. Most crucially, he saw the messy world of biology through the piercing eyes of a physicist. He is perhaps best known for his work on associate neural networks, now known as Hopfield networks that were one of the early ideas that catalyzed the development of the modern field of deep learning. EPISODE LINKS: Now What? article: http:\\/\\/bit.ly\\/3843LeU John wikipedia: https:\\/\\/en.wikipedia.org\\/wiki\\/John_Hopfield Books mentioned: \\u2013 Einstein\\u2019s Dreams: https:\\/\\/amzn.to\\/2PBa96X \\u2013 Mind is Flat: https:\\/\\/amzn.to\\/2I3YB84 This conversation is part of the Artificial Intelligence podcast. If you would like\",\"3746\":\"Hi there, \\/r\\/MachineLearning community! So, me and a couple of my friends spent the last year building a machine learning competition platform (https:\\/\\/telesto.ai). We are slowly getting started and working on filling it up with competitions, but we would love to get some feedback on it! You might ask, why are we building another platform, when Kaggle, AIcrowd, DrivenData, and many others are available? Let me explain! TL;DR: crowdsourcing is not suitable for production-ready models, and the crowdsourcing process itself is not a part of the machine learning development cycle. We want to change that by deploying to production-like environments and rewarding top solutions weekly. Crowdsourcing and the development cycle Let's take a look into how the development of a machine learning solution is done! Just as for other software tools, the development workflow here is a cycle as well. Roughly, the main steps are the following. Data collection. Once the problem is formulated, data has to be collected to provide training and test sets. In some cases, data is collected even before the problems are clear. Model training. This is the part that is most exciting for data scientists: coming up with the architecture of the solution and training a machine learning model. However, this takes up only a small percentage of the total workload. Deployment to production. If the model is ready, it has to be prepared for production use. This involves quality assurance, packaging it into an API, securely deploying it to production servers, etc. Testing and validation. When a model is used in production, new and unexpected problems might arise. Suppose you train a classifier that recognizes the brand and the model of a car, but a manufacturer issues a new model. The classifier no longer works correctly, so you have to go back to the beginning, collect more data, train a new model, and so on. As we know it, a machine learning competition only covers a small part of this process: the model training. What happens before and after is unrelated to the competition and done by an internal team instead of the thousands of participants. Effectively, crowdsourcing only covers a small portion of the work. We want to change that. Crowdsourcing for production How can we involve the participants of a challenge in the entire process? In our opinion, there are several opportunities. For instance, I have participated in competitions where participants noticed problems with the training data labels right away, yet nothing changed for the entire duration. In others, participants have cheated their way to victory by downloading the private test dataset and overfitting the model on that. At best, the result is a proof of concept. At worst, it is useless. We've come up with the idea of making make the entire process dynamic, involving competitors and challenge hosts in the entire development cycle. Simply speaking, instead of rewarding the top 3-5 solutions at the end of a three-month period and conclude the competition, we would reward the best solutions every week and deploy them to production-like environments. (Currently, we ask the weekly winners to package their solution into a Docker image for which we provide a template, but this is under heavy development.) When issues arise, like mislabeled data, it can be fixed by the next round, and the solution can keep improving constantly. On the other hand, we believe that this would be more rewarding for participants as well. If you are working hard on the competition, staying on top of the leaderboard for months but get outcompeted during the final hours, you receive no reward. We think this is unfair, and consistency should be rewarded. So, our idea is to issue the rewards every week. In brief, our goals are to connect the machine learning community to the industry through exciting problems, to provide a more rewarding competition platform for developers, and to integrate crowdsourcing to the entire development life cycle. What do you think? We are asking for your feedback! So far, we are at the beginning of our journey. We set up an early release version that you can test at https:\\/\\/telesto.ai. Currently, we are just testing out our platform with a competition to diagnose COVID-19 based on cell microscopy. More challenges are coming soon! We are asking for your feedback on our idea and the execution so far! We are building this platform for the machine learning community, and we aim to provide the best experience out there! [link] [comments]\",\"1462\":\"Stuart Russell is a professor of computer science at UC Berkeley and a co-author of the book that introduced me and millions of other people to AI, called Artificial Intelligence: A Modern Approach. Video version is available on YouTube. If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman on Twitter, LinkedIn, Facebook, or YouTube where you can watch the video versions of these conversations.\",\"1136\":\"Tom Crawford discusses drag - then we drop the Empire State Building and Eiffel Tower into the ocean... Check out Brilliant (get 20% off their premium service): https:\\/\\/brilliant.org\\/numberphile (sponsor) More links & stuff in full description below \\u2193\\u2193\\u2193 Find out more about Tom Crawford: https:\\/\\/tomrocksmaths.com\\/about\\/ More Tom videos on Numberphile: http:\\/\\/bit.ly\\/Crawford_Videos Check out some raw slow motions from filming of this video: https:\\/\\/youtu.be\\/sLR-2sTu20g Numberphile is supported by the Mathematical Sciences Research Institute (MSRI): http:\\/\\/bit.ly\\/MSRINumberphile We are also supported by Science Sandbox, a Simons Foundation initiative dedicated to engaging everyone with the process of science. https:\\/\\/www.simonsfoundation.org\\/outreach\\/science-sandbox\\/ And support from Math For America - https:\\/\\/www.mathforamerica.org\\/ NUMBERPHILE Website: http:\\/\\/www.numberphile.com\\/ Numberphile on Facebook: http:\\/\\/www.facebook.com\\/numberphile Numberphile tweets: https:\\/\\/twitter.com\\/numberphile Subscribe: http:\\/\\/bit.ly\\/Numberphile_Sub Videos by Brady Haran Patreon: http:\\/\\/www.patreon.com\\/numberphile Numberphile T-Shirts and Merch: https:\\/\\/teespring.com\\/stores\\/numberphile Brady's videos subreddit: http:\\/\\/www.reddit.com\\/r\\/BradyHaran\\/ Brady's latest videos across all channels: http:\\/\\/www.bradyharanblog.com\\/ Sign up for (occasional) emails: http:\\/\\/eepurl.com\\/YdjL9\",\"1647\":\"Read the full story\",\"650\":\"Dmitri Dolgov is the CTO of Waymo, an autonomous vehicle company. Please support this podcast by checking out our sponsors: - Tryolabs: https:\\/\\/tryolabs.com\\/lex - Blinkist: https:\\/\\/blinkist.com\\/lex and use code LEX to get 25% off premium - BetterHelp: https:\\/\\/betterhelp.com\\/lex to get 10% off - Cash App: https:\\/\\/cash.app\\/ and use code LexPodcast to get $10 EPISODE LINKS: Waymo's Twitter: https:\\/\\/twitter.com\\/waymo Waymo's Website: https:\\/\\/waymo.com PODCAST INFO: Podcast website: https:\\/\\/lexfridman.com\\/podcast Apple Podcasts: https:\\/\\/apple.co\\/2lwqZIr Spotify: https:\\/\\/spoti.fi\\/2nEwCF8 RSS: https:\\/\\/lexfridman.com\\/feed\\/podcast\\/ Full episodes playlist: https:\\/\\/www.youtube.com\\/playlist?list=PLrAXtmErZgOdP_8GztsuKi9nrraNbKKp4 Clips playlist: https:\\/\\/www.youtube.com\\/playlist?list=PLrAXtmErZgOeciFP3CBCIEElOJeitOr41 OUTLINE: 0:00 - Introduction 2:16 - Computer games 7:23 - Childhood 9:55 - Robotics 10:44 - Moscow Institute of Physics and Technology 12:56 - DARPA Urban Challenge 23:16 - Waymo origin story 38:58 - Waymo self-driving hardware 47:31 - Connected cars 53:23 - Waymo fully driverless service in Phoenix 57:45 - Getting feedback from riders 1:05:58 - Creating a product that people love 1:11:49 - Do self-driving cars need to break the rules like humans do? 1:18:33 - Waymo Trucks 1:24:11 - Future of Waymo 1:37:23 - Role of lidar in autonomous driving 1:50:23 - Machine learning is essential for autonomous driving 1:54:25 - Pedestrians 2:01:02 - Trolley problem 2:05:30 - Book recommendations 2:16:56 - Meaning of life CONNECT: - Subscribe to this YouTube channel - Twitter: https:\\/\\/twitter.com\\/lexfridman - LinkedIn: https:\\/\\/www.linkedin.com\\/in\\/lexfridman - Facebook: https:\\/\\/www.facebook.com\\/LexFridmanPage - Instagram: https:\\/\\/www.instagram.com\\/lexfridman - Medium: https:\\/\\/medium.com\\/@lexfridman - Support on Patreon: https:\\/\\/www.patreon.com\\/lexfridman\",\"4273\":\"Tweet Share Share Applied machine learning is typically focused on finding a single model that performs well or best on a given dataset. Effective use of the model will require appropriate preparation of the input data and hyperparameter tuning of the model. Collectively, the linear sequence of steps required to prepare the data, tune the model, and transform the predictions is called the modeling pipeline. Modern machine learning libraries like the scikit-learn Python library allow this sequence of steps to be defined and used correctly (without data leakage) and consistently (during evaluation and prediction). Nevertheless, working with modeling pipelines can be confusing to beginners as it requires a shift in perspective of the applied machine learning process. In this tutorial, you will discover modeling pipelines for applied machine learning. After completing this tutorial, you will know: Applied machine learning is concerned with more than finding a good performing model; it also requires finding an appropriate sequence of data preparation steps and steps for the post-processing of predictions. Collectively, the operations required to address a predictive modeling problem can be considered an atomic unit called a modeling pipeline. Approaching applied machine learning through the lens of modeling pipelines requires a change in thinking from evaluating specific model configurations to sequences of transforms and algorithms. Let\\u2019s get started. A Gentle Introduction to Machine Learning Modeling Pipelines Photo by Jay Huang, some rights reserved. Tutorial Overview This tutorial is divided into three parts; they are: Finding a Skillful Model Is Not Enough What Is a Modeling Pipeline? Implications of a Modeling Pipeline Finding a Skillful Model Is Not Enough Applied machine learning is the process of discovering the model that performs best for a given predictive modeling dataset. In fact, it\\u2019s more than this. In addition to discovering which model performs the best on your dataset, you must also discover: Data transforms that best expose the unknown underlying structure of the problem to the learning algorithms. Model hyperparameters that result in a good or best configuration of a chosen model. There may also be additional considerations such as techniques that transform the predictions made by the model, like threshold moving or model calibration for predicted probabilities. As such, it is common to think of applied machine learning as a large combinatorial search problem across data transforms, models, and model configurations. This can be quite challenging in practice as it requires that the sequence of one or more data preparation schemes, the model, the model configuration, and any prediction transform schemes must be evaluated consistently and correctly on a given test harness. Although tricky, it may be manageable with a simple train-test split but becomes quite unmanageable when using k-fold cross-validation or even repeated k-fold cross-validation. The solution is to use a modeling pipeline to keep everything straight. What Is a Modeling Pipeline? A pipeline is a linear sequence of data preparation options, modeling operations, and prediction transform operations. It allows the sequence of steps to be specified, evaluated, and used as an atomic unit. Pipeline: A linear sequence of data preparation and modeling steps that can be treated as an atomic unit. To make the idea clear, let\\u2019s look at two simple examples: The first example uses data normalization for the input variables and fits a logistic regression model: [Input], [Normalization], [Logistic Regression], [Predictions] The second example standardizes the input variables, applies RFE feature selection, and fits a support vector machine. [Input], [Standardization], [RFE], [SVM], [Predictions] You can imagine other examples of modeling pipelines. As an atomic unit, the pipeline can be evaluated using a preferred resampling scheme such as a train-test split or k-fold cross-validation. This is important for two main reasons: Avoid data leakage. Consistency and reproducibility. A modeling pipeline avoids the most common type of data leakage where data preparation techniques, such as scaling input values, are applied to the entire dataset. This is data leakage because it shares knowledge of the test dataset (such as observations that contribute to a mean or maximum known value) with the training dataset, and in turn, may result in overly optimistic model performance. Instead, data transforms must be prepared on the training dataset only, then applied to the training dataset, test dataset, validation dataset, and any other datasets that require the transform prior to being used with the model. A modeling pipeline ensures that the sequence of data preparation operations performed is reproducible. Without a modeling pipeline, the data preparation steps may be performed manually twice: once for evaluating the model and once for making predictions. Any changes to the sequence must be kept consistent in both cases, otherwise differences will impact the capability and skill of the model. A pipeline ensures that the sequence of operations is defined once and is consistent when used for model evaluation or making predictions. The Python scikit-learn machine learning library provides a machine learning modeling pipeline via the Pipeline class. You can learn more about how to use this Pipeline API in this tutorial: How to Avoid Data Leakage When Performing Data Preparation Implications of a Modeling Pipeline The modeling pipeline is an important tool for machine learning practitioners. Nevertheless, there are important implications that must be considered when using them. The main confusion for beginners when using pipelines comes in understanding what the pipeline has learned or the specific configuration discovered by the pipeline. For example, a pipeline may use a data transform that configures itself automatically, such as the RFECV technique for feature selection. When evaluating a pipeline that uses an automatically-configured data transform, what configuration does it choose? or When fitting this pipeline as a final model for making predictions, what configuration did it choose? The answer is, it doesn\\u2019t matter. Another example is the use of hyperparameter tuning as the final step of the pipeline. The grid search will be performed on the data provided by any prior transform steps in the pipeline and will then search for the best combination of hyperparameters for the model using that data, then fit a model with those hyperparameters on the data. When evaluating a pipeline that grid searches model hyperparameters, what configuration does it choose? or When fitting this pipeline as a final model for making predictions, what configuration did it choose? The answer again is, it doesn\\u2019t matter. The answer applies when using a threshold moving or probability calibration step at the end of the pipeline. The reason is the same reason that we are not concerned about the specific internal structure or coefficients of the chosen model. For example, when evaluating a logistic regression model, we don\\u2019t need to inspect the coefficients chosen on each k-fold cross-validation round in order to choose the model. Instead, we focus on its out-of-fold predictive skill Similarly, when using a logistic regression model as the final model for making predictions on new data, we do not need to inspect the coefficients chosen when fitting the model on the entire dataset before making predictions. We can inspect and discover the coefficients used by the model as an exercise in analysis, but it does not impact the selection and use of the model. This same answer generalizes when considering a modeling pipeline. We are not concerned about which features may have been automatically selected by a data transform in the pipeline. We are also not concerned about which hyperparameters were chosen for the model when using a grid search as the final step in the modeling pipeline. In all three cases: the single model, the pipeline with automatic feature selection, and the pipeline with a grid search, we are evaluating the \\u201cmodel\\u201d or \\u201cmodeling pipeline\\u201d as an atomic unit. The pipeline allows us as machine learning practitioners to move up one level of abstraction and be less concerned with the specific outcomes of the algorithms and more concerned with the capability of a sequence of procedures. As such, we can focus on evaluating the capability of the algorithms on the dataset, not the product of the algorithms, i.e. the model. Once we have an estimate of the pipeline, we can apply it and be confident that we will get similar performance, on average. It is a shift in thinking and may take some time to get used to. It is also the philosophy behind modern AutoML (automatic machine learning) techniques that treat applied machine learning as a large combinatorial search problem. Further Reading This section provides more resources on the topic if you are looking to go deeper. How to Avoid Data Leakage When Performing Data Preparation Summary In this tutorial, you discovered modeling pipelines for applied machine learning. Specifically, you learned: Applied machine learning is concerned with more than finding a good performing model; it also requires finding an appropriate sequence of data preparation steps and steps for the post-processing of predictions. Collectively, the operations required to address a predictive modeling problem can be considered an atomic unit called a modeling pipeline. Approaching applied machine learning through the lens of modeling pipelines requires a change in thinking from evaluating specific model configurations to sequences of transforms and algorithms. Do you have any questions? Ask your questions in the comments below and I will do my best to answer. Tweet Share Share The post A Gentle Introduction to Machine Learning Modeling Pipelines appeared first on Machine Learning Mastery.\",\"1545\":\"Tomaso Poggio is a professor at MIT and is the director of the Center for Brains, Minds, and Machines. Cited over 100,000 times, his work has had a profound impact on our understanding of the nature of intelligence, in both biological neural networks and artificial ones. He has been an advisor to many highly-impactful researchers and entrepreneurs in AI, including Demis Hassabis of DeepMind, Amnon Shashua of MobileEye, and Christof Koch of the Allen Institute for Brain Science. Video version is available on YouTube. If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with\",\"712\":\"Day 3 | November 19, 2020 Theme: Unblocking the Pipeline from Education to Employment Ruthe Farmer, CS for All Consortium The Accessible Computer Science Education Fall Workshop was hosted by Microsoft, University of Washington CREATE, and University of Colorado\\u2019s Coleman Institute. It took place November 17-19, 2020 and consisted of three half-days of talks, discussions, and planning for new research dedicated to making Computer Science education learning experiences more accessible for people with disabilities. More information on this workshop can be found at https:\\/\\/www.microsoft.com\\/en-us\\/research\\/event\\/accessible-cs-education-fall-workshop\\/\",\"503\":\"In this video, I talk about the #NLP competitions on #Kaggle that I find are the best to learn from. The full list discussed in this video: - https:\\/\\/www.kaggle.com\\/c\\/predict-closed-questions-on-stack-overflow\\/ - https:\\/\\/www.kaggle.com\\/c\\/stumbleupon - https:\\/\\/www.kaggle.com\\/c\\/crowdflower-search-relevance\\/ - https:\\/\\/www.kaggle.com\\/c\\/dato-native\\/ - https:\\/\\/www.kaggle.com\\/c\\/home-depot-product-search-relevance - https:\\/\\/www.kaggle.com\\/c\\/avito-duplicate-ads-detection\\/ - https:\\/\\/www.kaggle.com\\/c\\/quora-question-pairs - https:\\/\\/www.kaggle.com\\/c\\/quora-insincere-questions-classification - https:\\/\\/www.kaggle.com\\/c\\/jigsaw-toxic-comment-classification-challenge - https:\\/\\/www.kaggle.com\\/c\\/jigsaw-unintended-bias-in-toxicity-classification - https:\\/\\/www.kaggle.com\\/c\\/jigsaw-multilingual-toxic-comment-classification - https:\\/\\/www.kaggle.com\\/c\\/tensorflow2-question-answering - https:\\/\\/www.kaggle.com\\/c\\/tweet-sentiment-extraction Please subscribe and like the video to help me keep motivated to make awesome videos like this one. :) To buy my book, Approaching (Almost) Any Machine Learning problem, please visit: https:\\/\\/bit.ly\\/buyaaml Follow me on: Twitter: https:\\/\\/twitter.com\\/abhi1thakur LinkedIn: https:\\/\\/www.linkedin.com\\/in\\/abhi1thakur\\/ Kaggle: https:\\/\\/kaggle.com\\/abhishek Instagram: https:\\/\\/instagram.com\\/abhi4ml\",\"1660\":\"Read the full story\",\"3749\":\"From the research I've done so far on cycle gans both image sets are usually from the same type of reference point. Right now I have two different medical datasets of the same area, but one is from the pov of looking towards you, and the other is a pov of above you. The goal would be to have a network that translates the two different reference points. I don't really know if this is possible, so some input would be nice. On the contrary if there is an architecture that can do this better than cycle gans, I'd be glad to know. This is a research topic for a group of people to work on, so I'd rather like to not end up wasting everyone's time by making a network that just turns everything into a black photo. [link] [comments]\",\"1611\":\"Quick techniques on how to find which variables are influencing the model results and by how much and how to visualize using Partial dependence plots. Read the full story\",\"1635\":\"How to acquire new customers using a sales funnel and marketing automation Read the full story\",\"1575\":\"Richard Karp is a professor at Berkeley and one of the most important figures in the history of theoretical computer science. In 1985, he received the Turing Award for his research in the theory of algorithms, including the development of the Edmonds\\u2013Karp algorithm for solving the maximum flow problem on networks, Hopcroft\\u2013Karp algorithm for finding maximum cardinality matchings in bipartite graphs, and his landmark paper in complexity theory called \\u201cReducibility Among Combinatorial Problems\\u201d, in which he proved 21 problems to be NP-complete. This paper was probably the most important catalyst in the explosion of interest in the study of NP-completeness\",\"498\":\"This is my list of best #computer #vision competitions on #kaggle for beginners. I hope you like it. Full list discussed in this video: - https:\\/\\/www.kaggle.com\\/c\\/dogs-vs-cats - https:\\/\\/www.kaggle.com\\/c\\/flower-classification-with-tpus - https:\\/\\/www.kaggle.com\\/c\\/cdiscount-image-classification-challenge\\/ - https:\\/\\/www.kaggle.com\\/c\\/rsna-pneumonia-detection-challenge\\/ - https:\\/\\/www.kaggle.com\\/c\\/facial-keypoints-detection\\/ - https:\\/\\/www.kaggle.com\\/c\\/noaa-right-whale-recognition\\/ - https:\\/\\/www.kaggle.com\\/c\\/tgs-salt-identification-challenge - https:\\/\\/www.kaggle.com\\/c\\/carvana-image-masking-challenge\\/ - https:\\/\\/www.kaggle.com\\/c\\/imaterialist-fashion-2019-FGVC6 - https:\\/\\/www.kaggle.com\\/c\\/global-wheat-detection - https:\\/\\/www.kaggle.com\\/c\\/generative-dog-images\\/ Please subscribe and like the video to help me keep motivated to make awesome videos like this one. :) To buy my book, Approaching (Almost) Any Machine Learning problem, please visit: https:\\/\\/bit.ly\\/buyaaml Follow me on: Twitter: https:\\/\\/twitter.com\\/abhi1thakur LinkedIn: https:\\/\\/www.linkedin.com\\/in\\/abhi1thakur\\/ Kaggle: https:\\/\\/kaggle.com\\/abhishek Instagram: https:\\/\\/instagram.com\\/abhi4ml\",\"255\":\"The fact that social media algorithms give rise to some legitimate problems does not mean the solution \\\"ban all opinions I don't like\\\" is also legitimate.\",\"496\":\"In recent times, Machine Learning, especially deep learning, has shown enormous potential to learn latent patterns from data and is practically breaking one benchmark after another. The main ingredients in this continuing success are - Massive amount of Open Data, GPU accelerated high-performance computing, and last but not the least clever algorithmic tricks such as Self Attention and Graph Neural Networks. One can argue that similar techniques and methods can be applied to more diverse fields such as Source Code written for computer softwares. In fact, in 2018 in their seminal paper \\\"A Survey of Machine Learning for Big Code and Naturalness\\\" (https:\\/\\/arxiv.org\\/pdf\\/1709.06182.pdf) Miltiadis Allamanis et al. came up with what we call Naturalness Hypothesis. Which states \\\"Software is a form of human communication; software corpora have similar statistical properties to natural language corpora; and these properties can be exploited to build better software engineering tools.\\\" Post this there has been a surge of new research papers which either explore already proven techniques from other discipline of Machine Learning (such as transformers from NLP, Graph Convolution Networks etc.) or devise novel techniques (such as Gated Graph Neural Network, Nueral Symbolic methods, Deep ProbLog etc.). And a new kind of companies (such as Kite, DiffBlue, Ponicode, Codist) who use \\/ advance these methods to produce a new set of tools that helps developers to write source code faster, safer, and easier. In today's talk we will look into this field. Discuss some recent advancements. Some new tools that came up thanks to those advancements, and finally write some code to explore one problem among many that the researchers are trying to solve. Discuss the present best approaches and their drawbacks and further research directions. ------------------------------------------------------------- Shubhadeep Roychowdhury - CTO and Co-founder of Codist. A Paris based start-up which has created docly (http:\\/\\/thedocly.io\\/) automated source code summarization tools for developers to help them maintain better code documentation. SRC has more than 17 years of experience in diverse areas such as Game Programming, Very Large Scale Data Engineering, and Cutting Edge Deep Learning based systems. He co-authored \\\"Data Wrangling with Python\\\" in 2019 which is a massive 500+ pages treasure trove of anything related to data handling and wrangling and descriptive statistics. An Active blogger in medium and a father of two kids, SRC lives and works in Paris, France. ------------------------------------------------------------ Please subscribe and like the video to help me keep motivated to make awesome videos like this one. :) To buy my book, Approaching (Almost) Any Machine Learning problem, please visit: https:\\/\\/bit.ly\\/buyaaml Follow me on: Twitter: https:\\/\\/twitter.com\\/abhi1thakur LinkedIn: https:\\/\\/www.linkedin.com\\/in\\/abhi1thakur\\/ Kaggle: https:\\/\\/kaggle.com\\/abhishek Instagram: https:\\/\\/instagram.com\\/abhi4ml\",\"1454\":\"Noam Chomsky is one of the greatest minds of our time and is one of the most cited scholars in history. He is a linguist, philosopher, cognitive scientist, historian, social critic, and political activist. He has spent over 60 years at MIT and recently also joined the University of Arizona. This conversation is part of the Artificial Intelligence podcast. If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman on Twitter, LinkedIn, Facebook, Medium, or YouTube where you can watch the video versions of these conversations. If you enjoy the podcast, please rate\",\"3872\":\"Multi-output Machine Learning \\u2014 MixedRandomForest Read the full story\",\"1518\":\"Yoshua Bengio, along with Geoffrey Hinton and Yann Lecun, is considered one of the three people most responsible for the advancement of deep learning during the 1990s, 2000s, and now. Cited 139,000 times, he has been integral to some of the biggest breakthroughs in AI over the past 3 decades. Video version is available on YouTube. If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman on Twitter, LinkedIn, Facebook, or YouTube where you can watch the video versions of these conversations.\",\"1438\":\"Alex Garland is a writer and director of many imaginative and philosophical films from the dreamlike exploration of human self-destruction in the movie Annihilation to the deep questions of consciousness and intelligence raised in the movie Ex Machina, which to me is one of the greatest movies on artificial intelligence ever made. I\\u2019m releasing this podcast to coincide with the release of his new series called Devs that will premiere this Thursday, March 5, on Hulu. EPISODE LINKS: Devs: https:\\/\\/hulu.tv\\/2x35HaH Annihilation: https:\\/\\/hulu.tv\\/3ai9Eqk Ex Machina: https:\\/\\/www.netflix.com\\/title\\/80023689 Alex IMDb: https:\\/\\/www.imdb.com\\/name\\/nm0307497\\/ Alex Wiki: https:\\/\\/en.wikipedia.org\\/wiki\\/Alex_Garland This conversation is part of the Artificial Intelligence podcast. If\",\"1348\":\"#ai #research #nlp Knowledge Graphs are structured databases that capture real-world entities and their relations to each other. KGs are usually built by human experts, which costs considerable amounts of time and money. This paper hypothesizes that language models, which have increased their performance dramatically in the last few years, contain enough knowledge to use them to construct a knowledge graph from a given corpus, without any fine-tuning of the language model itself. The resulting system can uncover new, unknown relations and outperforms all baselines in automated KG construction, even trained ones! OUTLINE: 0:00 - Intro & Overview 1:40 - TabNine Promotion 4:20 - Title Misnomer 6:45 - From Corpus To Knowledge Graph 13:40 - Paper Contributions 15:50 - Candidate Fact Finding Algorithm 25:50 - Causal Attention Confusion 31:25 - More Constraints 35:00 - Mapping Facts To Schemas 38:40 - Example Constructed Knowledge Graph 40:10 - Experimental Results 47:25 - Example Discovered Facts 50:40 - Conclusion & My Comments Paper: https:\\/\\/arxiv.org\\/abs\\/2010.11967 Abstract: This paper shows how to construct knowledge graphs (KGs) from pre-trained language models (e.g., BERT, GPT-2\\/3), without human supervision. Popular KGs (e.g, Wikidata, NELL) are built in either a supervised or semi-supervised manner, requiring humans to create knowledge. Recent deep language models automatically acquire knowledge from large-scale corpora via pre-training. The stored knowledge has enabled the language models to improve downstream NLP tasks, e.g., answering questions, and writing code and articles. In this paper, we propose an unsupervised method to cast the knowledge contained within language models into KGs. We show that KGs are constructed with a single forward pass of the pre-trained language models (without fine-tuning) over the corpora. We demonstrate the quality of the constructed KGs by comparing to two KGs (Wikidata, TAC KBP) created by humans. Our KGs also provide open factual knowledge that is new in the existing KGs. Our code and KGs will be made publicly available. Authors: Chenguang Wang, Xiao Liu, Dawn Song Links: YouTube: https:\\/\\/www.youtube.com\\/c\\/yannickilcher Twitter: https:\\/\\/twitter.com\\/ykilcher Discord: https:\\/\\/discord.gg\\/4H8xxDF BitChute: https:\\/\\/www.bitchute.com\\/channel\\/yannic-kilcher Minds: https:\\/\\/www.minds.com\\/ykilcher Parler: https:\\/\\/parler.com\\/profile\\/YannicKilcher LinkedIn: https:\\/\\/www.linkedin.com\\/in\\/yannic-kilcher-488534136\\/ If you want to support me, the best thing to do is to share out the content :) If you want to support me financially (completely optional and voluntary, but a lot of people have asked for this): SubscribeStar: https:\\/\\/www.subscribestar.com\\/yannickilcher Patreon: https:\\/\\/www.patreon.com\\/yannickilcher Bitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq Ethereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2 Litecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m Monero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\",\"1236\":\"In Statistics, when we do Hypothesis Testing, we are supposed to have two hypotheses: A primary, or Null Hypothesis and an Alternative Hypothesis. This StatQuest explains why we need the Alternative Hypothesis, even though Hypothesis Testing tends to focus on the Null. NOTE: This StatQuest follows up on Hypothesis Testing and the Null Hypothesis: https:\\/\\/youtu.be\\/0oc49DyA3hU And if you'd like to learn about p-values, check out... p-values: What they are and how to interpret them: https:\\/\\/youtu.be\\/vemZtEM63GY How to Calculate p-values: https:\\/\\/youtu.be\\/JQc3yx0-Q9E Lastly, if you would like to learn about Statistical Tests, see: https:\\/\\/www.youtube.com\\/playlist?list=PLblh5JKOoLUIzaEkCLIUxQFjPIlapw8nU \\u2b50 NOTE: When I code, I use Kite, a free AI-powered coding assistant that will help you code faster and smarter. The Kite plugin integrates with all the top editors and IDEs to give you smart completions and documentation while you\\u2019re typing. I love it! https:\\/\\/www.kite.com\\/get-kite\\/?utm_medium=referral&utm_source=youtube&utm_campaign=statquest&utm_content=description-only For a complete index of all the StatQuest videos, check out: https:\\/\\/statquest.org\\/video-index\\/ If you'd like to support StatQuest, please consider... Patreon: https:\\/\\/www.patreon.com\\/statquest ...or... YouTube Membership: https:\\/\\/www.youtube.com\\/channel\\/UCtYLUTtgS3k1Fg4y5tAhLbw\\/join ...a cool StatQuest t-shirt or sweatshirt (USA\\/Europe): https:\\/\\/teespring.com\\/stores\\/statquest (everywhere): https:\\/\\/www.redbubble.com\\/people\\/starmer\\/works\\/40421224-statquest-double-bam?asc=u&p=t-shirt ...buying one or two of my songs (or go large and get a whole album!) https:\\/\\/joshuastarmer.bandcamp.com\\/ ...or just donating to StatQuest! https:\\/\\/www.paypal.me\\/statquest Lastly, if you want to keep up with me as I research and create new StatQuests, follow me on twitter: https:\\/\\/twitter.com\\/joshuastarmer 0:00 Awesome song and introduction 1:49 The Alternative Hypothesis 3:21 Testing the Null vs Alternative for 2 groups 5:38 Testing the Null vs Alternative for 3 or more groups 8:00 Summary and next steps #StatQuest #AlternativeHypothesis\",\"1869\":\"In a recent newsletter article I complained about how researchers mislead about the applicability of their work. I gave SAT solvers as an example. People provided interesting examples in response, but what was new to me was the concept of SMT (Satisfiability Modulo Theories), an extension to SAT. SMT seems to have more practical uses than vanilla SAT (see the newsletter for details). I wanted to take some time to explore SMT solvers, and I landed on Z3, an open-source SMT solver from Microsoft. In particular, I wanted to compare it to ILP (Integer Linear Programing) solvers, which I know relatively well. I picked a problem that I thought would work better for SAT-ish solvers than for ILPs: subset covering (explained in the next section). If ILP still wins against Z3, then that would be not so great for the claim that SMT is a production strength solver. All the code used for this post is on Github. Subset covering A subset covering is a kind of combinatorial design, which can be explained in terms of magic rings. An adventurer stumbles upon a chest full of magic rings. Each ring has a magical property, but some pairs of rings, when worn together on the same hand, produce a combined special magical effect distinct to that pair. The adventurer would like to try all pairs of rings to catalogue the magical interactions. With only five fingers, how can we minimize the time spent trying on rings? Mathematically, the rings can be described as a set of size . We want to choose a family of subsets of , with each subset having size 5 (five fingers), such that each subset of of size 2 (pairs of rings) is contained in some subset of . And we want to be as small as possible. Subset covering is not a \\u201cproduction worthy\\u201d problem. Rather, I could imagine it\\u2019s useful in some production settings, but I haven\\u2019t heard of one where it is actually used. I can imagine, for instance, that a cluster of machines has some bug occurring seemingly at random for some point-to-point RPCs, and in tracking down the problem, you want to deploy a test change to subsets of servers to observe the bug occurring. Something like an experiment design problem. If you generalize the \\u201c5\\u201d in \\u201c5 fingers\\u201d to an arbitrary positive integer , and the \\u201c2\\u201d in \\u201c2 rings\\u201d to SolveStatus.SOLVED: 1\\/preimg src=\\\"https:\\/\\/s0.wp.com\\/latex.php?latex=n%3D12%2C+k%3D6&bg=ffffff&fg=36312d&s=0&c=20201002\\\" alt=\\\"n=12, k=6\\\" title=\\\"n=12, k=6\\\" class=\\\"latex\\\" \\/pre class=\\\"brush: plain; title: ; notranslate\\\"\\/prepre class=\\\"brush: plain; title: ; notranslate\\\"\\/preh2 class=\\\"has-text-align-center\\\"\\/h2a href=\\\"https:\\/\\/www.scipopt.org\\/\\\"\\/aa href=\\\"https:\\/\\/www.gurobi.com\\/\\\"\\/aa href=\\\"https:\\/\\/github.com\\/j2kun\\/subset-cover\\/blob\\/main\\/subset_cover_ilp.py\\\"\\/aimg src=\\\"https:\\/\\/s0.wp.com\\/latex.php?latex=%5Ctextup%7BMember%7D_%7BS%2Ci%7D+%5Cin+%5C%7B+0%2C+1+%5C%7D&bg=ffffff&fg=36312d&s=0&c=20201002\\\" alt=\\\"\\\\textup{Member}_{S,i} \\\\in \\\\{ 0, 1 \\\\}\\\" title=\\\"\\\\textup{Member}_{S,i} \\\\in \\\\{ 0, 1 \\\\}\\\" class=\\\"latex\\\" \\/img src=\\\"https:\\/\\/s0.wp.com\\/latex.php?latex=i&bg=ffffff&fg=36312d&s=0&c=20201002\\\" alt=\\\"i\\\" title=\\\"i\\\" class=\\\"latex\\\" \\/img src=\\\"https:\\/\\/s0.wp.com\\/latex.php?latex=S&bg=ffffff&fg=36312d&s=0&c=20201002\\\" alt=\\\"S\\\" title=\\\"S\\\" class=\\\"latex\\\" \\/img src=\\\"https:\\/\\/s0.wp.com\\/latex.php?latex=%5Ctextup%7BIsHit%7D_%7BT%2C+S%7D+%5Cin+%5C%7B0%2C+1%5C%7D&bg=ffffff&fg=36312d&s=0&c=20201002\\\" alt=\\\"\\\\textup{IsHit}_{T, S} \\\\in \\\\{0, 1\\\\}\\\" title=\\\"\\\\textup{IsHit}_{T, S} \\\\in \\\\{0, 1\\\\}\\\" class=\\\"latex\\\" \\/img src=\\\"https:\\/\\/s0.wp.com\\/latex.php?latex=T&bg=ffffff&fg=36312d&s=0&c=20201002\\\" alt=\\\"T\\\" title=\\\"T\\\" class=\\\"latex\\\" \\/img src=\\\"https:\\/\\/s0.wp.com\\/latex.php?latex=S&bg=ffffff&fg=36312d&s=0&c=20201002\\\" alt=\\\"S\\\" title=\\\"S\\\" class=\\\"latex\\\" \\/img src=\\\"https:\\/\\/s0.wp.com\\/latex.php?latex=S&bg=ffffff&fg=36312d&s=0&c=20201002\\\" alt=\\\"S\\\" title=\\\"S\\\" class=\\\"latex\\\" \\/img src=\\\"https:\\/\\/s0.wp.com\\/latex.php?latex=S&bg=ffffff&fg=36312d&s=0&c=20201002\\\" alt=\\\"S\\\" title=\\\"S\\\" class=\\\"latex\\\" \\/img src=\\\"https:\\/\\/s0.wp.com\\/latex.php?latex=S&bg=ffffff&fg=36312d&s=0&c=20201002\\\" alt=\\\"S\\\" title=\\\"S\\\" class=\\\"latex\\\" \\/img src=\\\"https:\\/\\/s0.wp.com\\/latex.php?latex=k&bg=ffffff&fg=36312d&s=0&c=20201002\\\" alt=\\\"k\\\" title=\\\"k\\\" class=\\\"latex\\\" \\/p class=\\\"has-text-align-center\\\"img src=\\\"https:\\/\\/s0.wp.com\\/latex.php?latex=%5Cdisplaystyle+%5Csum_%7Bi+%5Cin+X%7D+%5Ctextup%7BMember%7D_%7BS%2C+i%7D+%3D+k&bg=ffffff&fg=36312d&s=0&c=20201002\\\" alt=\\\"\\\\displaystyle \\\\sum_{i \\\\in X} \\\\textup{Member}_{S, i} = k\\\" title=\\\"\\\\displaystyle \\\\sum_{i \\\\in X} \\\\textup{Member}_{S, i} = k\\\" class=\\\"latex\\\" \\/img src=\\\"https:\\/\\/s0.wp.com\\/latex.php?latex=T&bg=ffffff&fg=36312d&s=0&c=20201002\\\" alt=\\\"T\\\" title=\\\"T\\\" class=\\\"latex\\\" \\/p class=\\\"has-text-align-center\\\"img src=\\\"https:\\/\\/s0.wp.com\\/latex.php?latex=%5Cdisplaystyle+%5Csum_%7BS%7D+%5Ctextup%7BIsHit%7D_%7BT%2C+S%7D+%5Cgeq+1&bg=ffffff&fg=36312d&s=0&c=20201002\\\" alt=\\\"\\\\displaystyle \\\\sum_{S} \\\\textup{IsHit}_{T, S} \\\\geq 1\\\" title=\\\"\\\\displaystyle \\\\sum_{S} \\\\textup{IsHit}_{T, S} \\\\geq 1\\\" class=\\\"latex\\\" \\/img src=\\\"https:\\/\\/s0.wp.com\\/latex.php?latex=T&bg=ffffff&fg=36312d&s=0&c=20201002\\\" alt=\\\"T\\\" title=\\\"T\\\" class=\\\"latex\\\" \\/img src=\\\"https:\\/\\/s0.wp.com\\/latex.php?latex=S&bg=ffffff&fg=36312d&s=0&c=20201002\\\" alt=\\\"S\\\" title=\\\"S\\\" class=\\\"latex\\\" \\/img src=\\\"https:\\/\\/s0.wp.com\\/latex.php?latex=%5Ctextup%7BIsHit%7D_%7BT%2C+S%7D+%3D+1&bg=ffffff&fg=36312d&s=0&c=20201002\\\" alt=\\\"\\\\textup{IsHit}_{T, S} = 1\\\" title=\\\"\\\\textup{IsHit}_{T, S} = 1\\\" class=\\\"latex\\\" \\/img src=\\\"https:\\/\\/s0.wp.com\\/latex.php?latex=T&bg=ffffff&fg=36312d&s=0&c=20201002\\\" alt=\\\"T\\\" title=\\\"T\\\" class=\\\"latex\\\" \\/img src=\\\"https:\\/\\/s0.wp.com\\/latex.php?latex=S&bg=ffffff&fg=36312d&s=0&c=20201002\\\" alt=\\\"S\\\" title=\\\"S\\\" class=\\\"latex\\\" \\/p class=\\\"has-text-align-center\\\"img src=\\\"https:\\/\\/s0.wp.com\\/latex.php?latex=%5Cdisplaystyle+%5Csum_%7Bi+%5Cin+T%7D+%5Ctextup%7BMember%7D_%7BS%2C+i%7D+%5Cgeq+l+%5Ccdot+%5Ctextup%7BIsHit%7D_%7BT%2C+S%7D&bg=ffffff&fg=36312d&s=0&c=20201002\\\" alt=\\\"\\\\displaystyle \\\\sum_{i \\\\in T} \\\\textup{Member}_{S, i} \\\\geq l \\\\cdot \\\\textup{IsHit}_{T, S}\\\" title=\\\"\\\\displaystyle \\\\sum_{i \\\\in T} \\\\textup{Member}_{S, i} \\\\geq l \\\\cdot \\\\textup{IsHit}_{T, S}\\\" class=\\\"latex\\\" \\/img src=\\\"https:\\/\\/s0.wp.com\\/latex.php?latex=l+%3D+%7CT%7C&bg=ffffff&fg=36312d&s=0&c=20201002\\\" alt=\\\"l = |T|\\\" title=\\\"l = |T|\\\" class=\\\"latex\\\" \\/img src=\\\"https:\\/\\/s0.wp.com\\/latex.php?latex=%5Ctextup%7BIsHit%7D_%7BT%2C+S%7D&bg=ffffff&fg=36312d&s=0&c=20201002\\\" alt=\\\"\\\\textup{IsHit}_{T, S}\\\" title=\\\"\\\\textup{IsHit}_{T, S}\\\" class=\\\"latex\\\" \\/img src=\\\"https:\\/\\/s0.wp.com\\/latex.php?latex=%5Ctextup%7BIsHit%7D_%7BT%2C+S%7D+%3D+1&bg=ffffff&fg=36312d&s=0&c=20201002\\\" alt=\\\"\\\\textup{IsHit}_{T, S} = 1\\\" title=\\\"\\\\textup{IsHit}_{T, S} = 1\\\" class=\\\"latex\\\" \\/img src=\\\"https:\\/\\/s0.wp.com\\/latex.php?latex=l+%3D+%7CT%7C&bg=ffffff&fg=36312d&s=0&c=20201002\\\" alt=\\\"l = |T|\\\" title=\\\"l = |T|\\\" class=\\\"latex\\\" \\/a href=\\\"https:\\/\\/github.com\\/j2kun\\/subset-cover\\/blob\\/main\\/subset_cover_ilp.py\\\"\\/apre class=\\\"brush: plain; title: ; notranslate\\\"\",\"1873\":\"In my post about how to interpret p-values, I emphasize that p-values are not an error rate. The number one misinterpretation of p-values is that they are the probability of the null hypothesis being correct. The correct interpretation is that p-values indicate the probability of observing your sample data, or more extreme, when you assume [\\u2026] The post P-Values, Error Rates, and False Positives appeared first on Statistics By Jim.\",\"4277\":\"In this episode of Machine Learning Street Talk Dr. Tim Scarfe, Yannic Kilcher and Connor Shorten spoke with Marie-Anne Lachaux, Baptiste Roziere and Dr. Guillaume Lample from Facebook Research (FAIR) in Paris. They recently released the paper \\\"Unsupervised Translation of Programming Languages\\\" which was an exciting new approach to learned translation of programming languages (learned transcoder) using an unsupervised encoder trained on individual monolingual corpora i.e. no parallel language data needed. The trick they used what that there is significant token overlap when using word-piece embeddings. It was incredible to talk with this talented group of researchers and I hope you enjoy the conversation too. Yannic's video on this got watched over 120K times! Check it out too https:\\/\\/www.youtube.com\\/watch?v=xTzFJIknh7E Paper https:\\/\\/arxiv.org\\/abs\\/2006.03511; Marie-Anne Lachaux, Baptiste Roziere, Lowik Chanussot, Guillaume Lample Abstract; \\\"A transcompiler, also known as source-to-source translator, is a system that converts source code from a high-level programming language (such as C++ or Python) to another. Transcompilers are primarily used for interoperability, and to port codebases written in an obsolete or deprecated language (e.g. COBOL, Python 2) to a modern one. They typically rely on handcrafted rewrite rules, applied to the source code abstract syntax tree. Unfortunately, the resulting translations often lack readability, fail to respect the target language conventions, and require manual modifications in order to work properly. The overall translation process is timeconsuming and requires expertise in both the source and target languages, making code-translation projects expensive. Although neural models significantly outperform their rule-based counterparts in the context of natural language translation, their applications to transcompilation have been limited due to the scarcity of parallel data in this domain. In this paper, we propose to leverage recent approaches in unsupervised machine translation to train a fully unsupervised neural transcompiler. We train our model on source code from open source GitHub projects, and show that it can translate functions between C++, Java, and Python with high accuracy. Our method relies exclusively on monolingual source code, requires no expertise in the source or target languages, and can easily be generalized to other programming languages. We also build and release a test set composed of 852 parallel functions, along with unit tests to check the correctness of translations. We show that our model outperforms rule-based commercial baselines by a significant margin.\\\"\",\"1613\":\"Data structures are important for storing data in efficient ways. In this article, we will discuss the Graph Data Structure: definition, types and examples. Read the full story\",\"5684\":\"I have a multi-class classification problem in which one of the classes I am trying to predict is very sensitive to variations of the input data. Additionally, the input data has a very wide range of values. For simplicity we can assume I have one feature X that ranges from 0 to 500, and three classes, A, B, and C. Samples of class A are mostly located in 0\",\"1147\":\"Featuring Tom Crawford. More links & stuff in full description below \\u2193\\u2193\\u2193 Tom Crawford website: https:\\/\\/tomrocksmaths.com\\/ More of our videos with Tom at: http:\\/\\/bit.ly\\/Crawford_Videos Atom bombs on Periodic Videos: https:\\/\\/youtu.be\\/QLZMzsRB86E Numberphile is supported by the Mathematical Sciences Research Institute (MSRI): http:\\/\\/bit.ly\\/MSRINumberphile We are also supported by Science Sandbox, a Simons Foundation initiative dedicated to engaging everyone with the process of science. https:\\/\\/www.simonsfoundation.org\\/outreach\\/science-sandbox\\/ And support from Math For America - https:\\/\\/www.mathforamerica.org\\/ NUMBERPHILE Website: http:\\/\\/www.numberphile.com\\/ Numberphile on Facebook: http:\\/\\/www.facebook.com\\/numberphile Numberphile tweets: https:\\/\\/twitter.com\\/numberphile Subscribe: http:\\/\\/bit.ly\\/Numberphile_Sub Videos by Brady Haran Animation by Pete McPartlan Patreon: http:\\/\\/www.patreon.com\\/numberphile Numberphile T-Shirts and Merch: https:\\/\\/teespring.com\\/stores\\/numberphile Brady's videos subreddit: http:\\/\\/www.reddit.com\\/r\\/BradyHaran\\/ Brady's latest videos across all channels: http:\\/\\/www.bradyharanblog.com\\/ Sign up for (occasional) emails: http:\\/\\/eepurl.com\\/YdjL9\",\"3754\":\"(PhD student here) I've been resubmitting the same paper over the past 2 years (since January 2019) to 3 separate conferences and have got 3 rejections in a row so far - each time I address the reviews and add a new set of experiments the reviewers find some reason to reject the paper. I'm kind of burned out by this project and want to quit but am unsure to keep chugging along or to abandon the project. [link] [comments]\",\"4282\":\"In this episode of Machine Learning Street Talk, Tim Scarfe, Connor Shorten and Yannic Kilcher react to Yoshua Bengio\\u2019s ICLR 2020 Keynote \\u201cDeep Learning Priors Associated with Conscious Processing\\u201d. Bengio takes on many future directions for research in Deep Learning such as the role of attention in consciousness, sparse factor graphs and causality, and the study of systematic generalization. Bengio also presents big ideas in Intelligence that border on the line of philosophy and practical machine learning. This includes ideas such as consciousness in machines and System 1 and System 2 thinking, as described in Daniel Kahneman\\u2019s book \\u201cThinking Fast and Slow\\u201d. Similar to Yann LeCun\\u2019s half of the 2020 ICLR keynote, this talk takes on many challenging ideas and hopefully this video helps you get a better understanding of some of them! Thanks for watching! Please Subscribe for more videos! Paper Links: Link to Talk: https:\\/\\/iclr.cc\\/virtual_2020\\/speaker_7.html The Consciousness Prior: https:\\/\\/arxiv.org\\/abs\\/1709.08568 Thinking Fast and Slow: https:\\/\\/www.amazon.com\\/Thinking-Fast-Slow-Daniel-Kahneman\\/dp\\/0374533555 Systematic Generalization: https:\\/\\/arxiv.org\\/abs\\/1811.12889 CLOSURE: Assessing Systematic Generalization of CLEVR Models: https:\\/\\/arxiv.org\\/abs\\/1912.05783 Neural Module Networks: https:\\/\\/arxiv.org\\/abs\\/1511.02799 Experience Grounds Language: https:\\/\\/arxiv.org\\/pdf\\/2004.10151.pdf Benchmarking Graph Neural Networks: https:\\/\\/arxiv.org\\/pdf\\/2003.00982.pdf On the Measure of Intelligence: https:\\/\\/arxiv.org\\/abs\\/1911.01547 Please check out our individual channels as well! Machine Learning Dojo with Tim Scarfe: https:\\/\\/www.youtube.com\\/channel\\/UCXvHuBMbgJw67i5vrMBBobA Yannic Kilcher: https:\\/\\/www.youtube.com\\/channel\\/UCZHmQk67mSJgfCCTn7xBfe Henry AI Labs: https:\\/\\/www.youtube.com\\/channel\\/UCHB9VepY6kYvZjj0Bgxnpbw 00:00:00 Tim and Yannics takes 00:01:37 Intro to Bengio 00:03:13 System 2, language and Chomsky 00:05:58 Cristof Koch on conciousness 00:07:25 Francois Chollet on intelligence and consciousness 00:09:29 Meditation and Sam Harris on consciousness 00:11:35 Connor Intro 00:13:20 Show Main Intro 00:17:55 Priors associated with Conscious Processing 00:26:25 System 1 \\/ System 2 00:42:47 Implicit and Verbalized Knowledge [DONT MISS THIS!] 01:08:24 Inductive Priors for DL 2.0 01:27:20 Systematic Generalization 01:37:53 Contrast with the Symbolic AI Program 01:54:55 Attention 02:00:25 From Attention to Consciousness 02:05:31 Thoughts, Consciousness, Language 02:06:55 Sparse Factor graph 02:10:52 Sparse Change in Abstract Latent Space 02:15:10 Discovering Cause and Effect 02:20:00 Factorize the joint distribution 02:22:30 RIMS: Modular Computation 02:24:30 Conclusion #machinelearning #deeplearning\",\"4492\":\"For multiclass classification tasks, Scikit-learn's documentation for the AUROC score states that there are 2 versions of the AUROC score, the One-vs-Rest (OvR) and One-vs-One (OvO) version, controlled by the parameter multi_class: https:\\/\\/scikit-learn.org\\/stable\\/modules\\/generated\\/sklearn.metrics.roc_auc_score.html Does anyone know in what particular cases we would use OvR as opposed to OvO? In the general, is there a preference given to one? [link] [comments]\",\"1537\":\"Whitney Cummings is a stand-up comedian, actor, producer, writer, director, and the host of a new podcast called Good for You. Her most recent Netflix special called \\u201cCan I Touch It?\\u201d features in part a robot, she affectionately named Bearclaw, that is designed to be visually a replica of Whitney. It\\u2019s exciting for me to see one of my favorite comedians explore the social aspects of robotics and AI in our society. She also has some fascinating ideas about human behavior, psychology, and neurology, some of which she explores in her book called \\u201cI\\u2019m Fine\\u2026And Other Lies.\\u201d This conversation is\",\"5837\":\"[link] [comments]\",\"4294\":\"This week Dr. Tim Scarfe, Alex Stenlake and Yannic Kilcher speak with AGI and AI alignment specialist Connor Leahy a machine learning engineer from Aleph Alpha and founder of EleutherAI. Connor believes that AI alignment is philosophy with a deadline and that we are on the precipice, the stakes are astronomical. AI is important, and it will go wrong by default. Connor thinks that the singularity or intelligence explosion is near. Connor says that AGI is like climate change but worse, even harder problems, even shorter deadline and even worse consequences for the future. These problems are hard, and nobody knows what to do about them. 00:00:00 Introduction to AI alignment and AGI fire alarm 00:15:16 Main Show Intro 00:18:38 Different schools of thought on AI safety 00:24:03 What is intelligence? 00:25:48 AI Alignment 00:27:39 Humans dont have a coherent utility function 00:28:13 Newcomb's paradox and advanced decision problems 00:34:01 Incentives and behavioural economics 00:37:19 Prisoner's dilemma 00:40:24 Ayn Rand and game theory in politics and business 00:44:04 Instrumental convergence and orthogonality thesis 00:46:14 Utility functions and the Stop button problem 00:55:24 AI corrigibality - self alignment 00:56:16 Decision theory and stability \\/ wireheading \\/ robust delegation 00:59:30 Stop button problem 01:00:40 Making the world a better place 01:03:43 Is intelligence a search problem? 01:04:39 Mesa optimisation \\/ humans are misaligned AI 01:06:04 Inner vs outer alignment \\/ faulty reward functions 01:07:31 Large corporations are intelligent and have no stop function 01:10:21 Dutch booking \\/ what is rationality \\/ decision theory 01:16:32 Understanding very powerful AIs 01:18:03 Kolmogorov complexity 01:19:52 GPT-3 - is it intelligent, are humans even intelligent? 01:28:40 Scaling hypothesis 01:29:30 Connor thought DL was dead in 2017 01:37:54 Why is GPT-3 as intelligent as a human 01:44:43 Jeff Hawkins on intelligence as compression and the great lookup table 01:50:28 AI ethics related to AI alignment? 01:53:26 Interpretability 01:56:27 Regulation 01:57:54 Intelligence explosion Discord: https:\\/\\/discord.com\\/invite\\/vtRgjbM EleutherAI: https:\\/\\/www.eleuther.ai Twitter: https:\\/\\/twitter.com\\/npcollapse LinkedIn: https:\\/\\/www.linkedin.com\\/in\\/connor-j-leahy\\/\",\"749\":\"Because deep learning is so empirical, success in it is to a large extent proportional to raw experimental throughput - the ability to babysit a large number of experiments at once, staring at plots and tweaking\\/re-launching what works. This is necessary, but not sufficient.\",\"1149\":\"Dr Sabetta Matsumoto discusses some of the mathematics of colour in nature, including butterfly wings and soap films. More links & stuff in full description below \\u2193\\u2193\\u2193 Dr Matsumoto's group at Georgia Tech: http:\\/\\/matsumoto.gatech.edu And her Twitter: https:\\/\\/twitter.com\\/sabetta_ Previous video with Sabetta on Numberphile: https:\\/\\/youtu.be\\/0e5vzTsLh2s Brady looks at butterflies on Objectivity: https:\\/\\/youtu.be\\/lJWHppdupbI Numberphile is supported by the Mathematical Sciences Research Institute (MSRI): http:\\/\\/bit.ly\\/MSRINumberphile We are also supported by Science Sandbox, a Simons Foundation initiative dedicated to engaging everyone with the process of science. https:\\/\\/www.simonsfoundation.org\\/outreach\\/science-sandbox\\/ And support from Math For America - https:\\/\\/www.mathforamerica.org\\/ NUMBERPHILE Website: http:\\/\\/www.numberphile.com\\/ Numberphile on Facebook: http:\\/\\/www.facebook.com\\/numberphile Numberphile tweets: https:\\/\\/twitter.com\\/numberphile Subscribe: http:\\/\\/bit.ly\\/Numberphile_Sub Videos by Brady Haran Patreon: http:\\/\\/www.patreon.com\\/numberphile Numberphile T-Shirts and Merch: https:\\/\\/teespring.com\\/stores\\/numberphile Brady's videos subreddit: http:\\/\\/www.reddit.com\\/r\\/BradyHaran\\/ Brady's latest videos across all channels: http:\\/\\/www.bradyharanblog.com\\/ Sign up for (occasional) emails: http:\\/\\/eepurl.com\\/YdjL9\",\"1516\":\"Rajat Monga is an Engineering Director at Google, leading the TensorFlow team. If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman on Twitter, LinkedIn, Facebook, Medium, or YouTube where you can watch the video versions of these conversations.\",\"1143\":\"Professor Brian Butterworth is a neuroscientist who specialises in numbers and mathematics. More from this interview at https:\\/\\/youtu.be\\/FCS4b3OjVJM and earlier videos with him at: https:\\/\\/bit.ly\\/Brian_Butterworth More links & stuff in full description below \\u2193\\u2193\\u2193 Brian's website: https:\\/\\/www.mathematicalbrain.com Brian Butterworth playlist: https:\\/\\/bit.ly\\/Brian_Butterworth Discussed papers... Auditory midbrain neurons that count: https:\\/\\/pubmed.ncbi.nlm.nih.gov\\/12219094\\/ Mapping human temporal and parietal neuronal population activity and functional coupling during mathematical cognition: https:\\/\\/www.pnas.org\\/content\\/113\\/46\\/E7277 Numberphile is supported by the Mathematical Sciences Research Institute (MSRI): http:\\/\\/bit.ly\\/MSRINumberphile We are also supported by Science Sandbox, a Simons Foundation initiative dedicated to engaging everyone with the process of science. https:\\/\\/www.simonsfoundation.org\\/outreach\\/science-sandbox\\/ And support from Math For America - https:\\/\\/www.mathforamerica.org\\/ NUMBERPHILE Website: http:\\/\\/www.numberphile.com\\/ Numberphile on Facebook: http:\\/\\/www.facebook.com\\/numberphile Numberphile tweets: https:\\/\\/twitter.com\\/numberphile Subscribe: http:\\/\\/bit.ly\\/Numberphile_Sub Videos by Brady Haran Animation by Pete McPartlan Patreon: http:\\/\\/www.patreon.com\\/numberphile Numberphile T-Shirts and Merch: https:\\/\\/teespring.com\\/stores\\/numberphile Brady's videos subreddit: http:\\/\\/www.reddit.com\\/r\\/BradyHaran\\/ Brady's latest videos across all channels: http:\\/\\/www.bradyharanblog.com\\/ Sign up for (occasional) emails: http:\\/\\/eepurl.com\\/YdjL9\",\"2329\":\"Hi! I am annotating pictures in VGG VIA which works great. But it is difficult to double check my annotations. In other softwares there are color codes etc to make it easier, but in VIA there is not. Is there a software to annotate that i could upload my work (CSV) from VIA to do review more easily? (I have 18,000 pictures to review) Thanks! [link] [comments]\",\"1655\":\"Read the full story\",\"1514\":\"Jitendra Malik is a professor at Berkeley and one of the seminal figures in the field of computer vision, the kind before the deep learning revolution, and the kind after. He has been cited over 180,000 times and has mentored many world-class researchers in computer science. Support this podcast by supporting our sponsors: \\u2013 BetterHelp: http:\\/\\/betterhelp.com\\/lex \\u2013 ExpressVPN: https:\\/\\/www.expressvpn.com\\/lexpod If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman on Twitter, LinkedIn, Facebook, Medium, or YouTube where you can watch the video versions of these conversations. If you enjoy the podcast, please\",\"5683\":\"I tried to trace back the history of dot-product attention today, but did not successfully find its origin. My current understanding is that Bahdanau et al. proposed the original attention mechanism in Neural Machine Translation by Jointly Learning to Align and Translate. When the famous paper Attention is All You Need came out, it seemed that dot-product attention was already the standard, since they cited multiple prior works using the mechanism. However, those papers did seem to advertise dot-product attention as their original contribution, but also did not seem to cite someone else who proposed it. (Maybe I missed something since I was just skimming through the papers.) Does anyone know who originally proposed dot-product attention? [link] [comments]\",\"207\":\"How one Kaggler took top marks across multiple Covid-related challenges. Photo by Markus Spiske on Unsplash Today we interview Daniel, whose notebooks earned him top marks in Kaggle\\u2019s CORD-19 challenges. Kaggle hosted multiple challenges that worked with the Kaggle CORD-19 dataset, and Daniel won 1st place three times, including by a huge margin in the TREC-COVID challenge. (He had a score of 0.9, 2nd place overall had a score of 0.75, and 2nd place on Kaggle had a score of 0.6.) Let\\u2019s meet Daniel!Daniel, tell us a bit about yourself. Daniel: I\\u2019m Daniel Wolffram, a graduate student in mathematics and a data science student assistant at Karlsruhe Institute of Technology (KIT), in Germany. My research interests include probabilistic forecasting, causal inference and machine learning. As part of the Kaggle CORD-19 challenge I developed discovid.ai \\u2014 a search engine for COVID-19 literature. Right now, I\\u2019m working on the German COVID-19 forecast hub and writing my master thesis about building and evaluating forecast ensembles for COVID-19 death counts. Well, it\\u2019s no surprise you took top marks in the CORD-19 Challenge! That\\u2019s quite relevant! Daniel: Indeed. I\\u2019m also a student assistant where I\\u2019ve worked on several data science projects for the last 3 years and had the opportunity to work with real world data from different companies in highly diverse domains \\u2014 from predicting the waste in a sawmill to analyzing flaws in the process of surface galvanization and testing the efficiency of a marketing campaign. During my time as a student assistant, we\\u2019ve also consulted a company that works with a lot of text data \\u2014 that\\u2019s where I gained my first experience in NLP and also came across the idea of finding similar documents with the help of a topic model. At that time, our client wanted to stick with another approach, so I never really got to try out the LDA approach, but it always stayed in the back of my mind. How did you get started competing on Kaggle? During my undergraduate studies I joined a university group where we taught ourselves the basics of data science \\u2014 mostly by working on Kaggle projects such as the Titanic or Instacart challenge. That\\u2019s also how I got my job as a student assistant, because I met one of my now-colleagues there. What made you decide to enter this particular competition? A friend of mine showed me this competition and I was excited right away. I remembered the LDA approach and just wanted to try it out. Moreover, when the competition was launched, Covid cases were climbing in Germany, where I live. The first protective measures to flatten the curve were taken here \\u2014 all restaurants, shops (except supermarkets and drugstores) and leisure facilities were closed. My university was closed and all exams got cancelled. More shocking were the numbers from Italy and elsewhere. It was a very intimidating and uncertain atmosphere, so this challenge was actually a way to gain back some control by facing the crisis head on by simply using my skills for the best. I was aware that it might not have the biggest impact, but what kept me going was the thought that if even one medical researcher uses my model and stumbles upon something useful, my efforts were already worth it. Let\\u2019s get technicalWhat preprocessing and feature engineering did you do? To normalize the documents I removed stop words and performed tokenization and lemmatization. This last step was rather critical here, since the CORD-19 dataset contains highly technical papers with scientific language that can\\u2019t be processed successfully by standard packages. It was important to use scispacy, which is a package that is specialized on processing biomedical, scientific or clinical text and thus could also normalize technical terms (such as chemical elements, drug names, etc.). For the topic model to work properly, it was also necessary to perform language detection and remove non-English documents. All the details can be found in my preprocessing notebook: https:\\/\\/www.kaggle.com\\/danielwolffram\\/cord-19-create-dataframe. To further augment the data, I also searched each article for clinical trial ids to link the document to the WHO International Clinical Trials Registry Platform (ICTRP), which required hand crafting several regular expressions \\u2014 the details can be found in https:\\/\\/www.kaggle.com\\/danielwolffram\\/cord-19-match-clinical-trials. What machine learning methods did you use? I used Latent Dirichlet Allocation (LDA), which is an unsupervised topic model that learns hidden semantic relationships within the corpus. Initially, this was used to find relevant articles for each task of the CORD-19 challenge. But as we moved the approach to our website, we implemented a more common search engine with Whoosh, that allows for classical keyword searches or more complex boolean queries. On discovid.ai the topic model is now used to find related articles \\u2014 the idea is that each article is composed of a set of underlying topics and if we find articles with a similar topic mixture or an overlap in topics, they might be interesting for the reader and could spark new insights. Topic mixture of a selected paper and of a related articleClick through for interactivity: https:\\/\\/dwolffram.github.io\\/cord19_lda_topics\\/ Here you can explore 50 topics that our model found within the corpus \\u2014 each topic is a distribution over words and each document can then be seen as a mixture of these topics. What was your most important insight into the data? Before removing the non-English articles from the corpus, interestingly, the following topics had been discovered by our topic model: Topic #46: der die und bei mit von eine ist werden zu f\\u00fcr sind oder einer des den nicht das als nach zur auf durch auch ein Topic #40: de les des en une est dans du par un ou sont pour plus au que avec chez sur d\\u2019une qui cas \\u00eatre pas ces Topic #32: de en el los que se con las por un es para pacientes como m\\u00e1s virus son tratamiento su infecci\\u00f3n puede ha casos enfermedad entre Topic #7: un che con sono nel alla pi\\u00f9 ha tra gli degli come rischio ed pazienti nella nei osteonecrosis ad essere stato studio salute anche have As you can see, there was one for German, French, Spanish and Italian. To me this was very encouraging, because it demonstrates how powerful LDA is in learning hidden structures and that it actually learns something meaningful. Were you surprised by any of your findings? When people first tried out our search engine, it became clear that they only search for a few keywords \\u2014 unlike the tasks on Kaggle, that were composed of much more text. This was quite a problem, because the queries were simply too short to infer topics in a useful manner. That\\u2019s when I decided to implement a more common search engine with Whoosh as an initial search (https:\\/\\/www.kaggle.com\\/danielwolffram\\/whoosh-search). The topic model is now only used to find related articles that are composed of similar topics, which enables users to easily browse the corpus and discover new insights. How did you spend your time on this competition? As so often, most of my efforts went into data preparation and cleaning, especially in the beginning there were many changes in the data structure which required a lot of adjustments. I\\u2019ve also read a lot in the forum and talked to some people with medical background to identify needs of the community. That\\u2019s why we are also extracting methodological keywords as a first quality indicator and add cross references to clinical trials that are mentioned in the papers. I\\u2019ve also spent a good amount of time learning and figuring out new things, such as language detection or building a custom search engine with Whoosh, which I\\u2019ve never done before. What was the run time for both training and prediction of your winning solution? Transforming the documents and training the topic model takes roughly a day. TeamworkHow did your team form? I started out on my own and built some widgets in a Kaggle notebook to easily explore the CORD-19 dataset. But with the good feedback and increasing interest in my approach, I wanted to make it more user-friendly, so it could also be used without a technical background. That\\u2019s when I got in touch with one of my colleagues, who didn\\u2019t hesitate to assist me and who assembled a small team to build our website discovid.ai. How did your team work together? Two of my colleagues were working on the backend and frontend, another one got it up and running on the server and my girlfriend came up with the great design and also animated our introduction video. https:\\/\\/medium.com\\/media\\/a3bafdc8ddb3721e6d80e83cd060a088\\/hrefHow did competing on a team help you succeed? It definitely helped me to build a more well-rounded solution that is user-friendly and accessible by anyone. Just for funWhat is your dream job? I\\u2019m really drawn to data science in the medical field, because I wish to use my analytical skills in a meaningful project that helps others. I think that\\u2019s also what kept me going throughout the CORD-19 challenge \\u2014 it was never about winning, but more about using my strengths for the best and doing my part in this global crisis. Words of wisdomWhat have you taken away from this competition? It was a very meaningful project to me and along the way I got to know many interesting and inspiring people from all over the world. It was great to see how researchers from all around the globe rushed together to search answers to this global pandemic that affects each one of us in different ways and paradoxically unites us all. Do you have any advice for those just getting started in data science? Just get started! I think it\\u2019s important to get practical experience and learn how to handle different kinds of data, so you can easily transform it to a format you can work with. But as a math student, I also have to say that you shouldn\\u2019t neglect the fundamentals such as probability theory and statistics, because after all data science is a science, so it\\u2019s important to get an intuition about uncertainty and the limitations of different approaches. Also, I think it\\u2019s always important to first get a clear understanding of the problem you are trying to solve, before throwing the most complex machine learning models on it. \\u2026 You can find Daniel\\u2019s winning submission for CORD-19 here: https:\\/\\/www.kaggle.com\\/danielwolffram\\/discovid-ai-a-search-and-recommendation-engine Gaining a sense of control over the COVID-19 pandemic | A Winner\\u2019s Interview with Daniel Wolffram was originally published in Kaggle Blog on Medium, where people are continuing the conversation by highlighting and responding to this story.\",\"1145\":\"Eric Lander discusses how quasiperfect numbers gave him a start... More links & stuff in full description below \\u2193\\u2193\\u2193 This interview was filmed in 2015 but remained unedited and unpublished until now... The other main interview from that day (about \\\"Basic Research\\\") can be found here: https:\\/\\/youtu.be\\/6gnsQjPCC78 The Westinghouse Science Talent Search has since become the Regeneron Science Talent Search: https:\\/\\/en.wikipedia.org\\/wiki\\/Regeneron_Science_Talent_Search Numberphile is supported by the Mathematical Sciences Research Institute (MSRI): http:\\/\\/bit.ly\\/MSRINumberphile We are also supported by Science Sandbox, a Simons Foundation initiative dedicated to engaging everyone with the process of science. https:\\/\\/www.simonsfoundation.org\\/outreach\\/science-sandbox\\/ And support from Math For America - https:\\/\\/www.mathforamerica.org\\/ NUMBERPHILE Website: http:\\/\\/www.numberphile.com\\/ Numberphile on Facebook: http:\\/\\/www.facebook.com\\/numberphile Numberphile tweets: https:\\/\\/twitter.com\\/numberphile Subscribe: http:\\/\\/bit.ly\\/Numberphile_Sub Videos by Brady Haran Patreon: http:\\/\\/www.patreon.com\\/numberphile Numberphile T-Shirts and Merch: https:\\/\\/teespring.com\\/stores\\/numberphile Brady's videos subreddit: http:\\/\\/www.reddit.com\\/r\\/BradyHaran\\/ Brady's latest videos across all channels: http:\\/\\/www.bradyharanblog.com\\/ Sign up for (occasional) emails: http:\\/\\/eepurl.com\\/YdjL9\",\"2333\":\"link to paper If anyone is interested in the paper and\\/or code, please don't hesitate to message me. \\u200b Accurate and realistic simulation of high-dimensional medical images has become an important research area relevant to many AI-enabled healthcare applications. However, current state-of-the-art approaches lack the ability to produce satisfactory high-resolution and accurate subject-specific images. In this work, we present a deep learning framework, namely 4D-Degenerative Adversarial NeuroImage Net (4D-DANI-Net), to generate high-resolution, longitudinal MRI scans that mimic subject-specific neurodegeneration in ageing and dementia. 4D-DANI-Net is a modular framework based on adversarial training and a set of novel spatiotemporal, biologically-informed constraints. To ensure efficient training and overcome memory limitations affecting such high-dimensional problems, we rely on three key technological advances: i) a new 3D training consistency mechanism called Profile Weight Functions (PWFs), ii) a 3D super-resolution module and iii) a transfer learning strategy to fine-tune the system for a given individual. To evaluate our approach, we trained the framework on 9852 T1-weighted MRI scans from 876 participants in the Alzheimer's Disease Neuroimaging Initiative dataset and held out a separate test set of 1283 MRI scans from 170 participants for quantitative and qualitative assessment of the personalised time series of synthetic images. We performed three evaluations: i) image quality assessment; ii) quantifying the accuracy of regional brain volumes over and above benchmark models; and iii) quantifying visual perception of the synthetic images by medical experts. Overall, both quantitative and qualitative results show that 4D-DANI-Net produces realistic, low-artefact, personalised time series of synthetic T1 MRI that outperforms benchmark models. [link] [comments]\",\"1642\":\"From traditional sales to bounties to Digital Inversion, learn how to extract value from your data assets via Nevermined\\u2019s numerous commercialization models. Read the full story\",\"4298\":\"This week Dr. Tim Scarfe, Dr. Keith Duggar, Yannic \\\"Lightspeed\\\" Kilcher have a conversation with Microsoft Senior Software Engineer Sachin Kundu. We speak about programming languages including which our favourites are and functional programming vs OOP. Next we speak about software engineering and the intersection of software engineering and machine learning. We also talk about applications of ML and finally what makes an exceptional software engineer and tech lead. Sachin is an expert in this field so we hope you enjoy the conversation! Spoiler alert, how many of you have read the Mythical Man-Month by Frederick P. Brooks?! 00:00:00 Introduction 00:06:37 Programming Languages 00:53:41 Applications of ML 01:55:59 What makes an exceptional SE and tech lead 01:22:08 Outro\",\"1234\":\"Today we're going to talk about Bootstrapping. This is one of those fancy sounding things that is actually quite simple and crazy powerful. Today we will walk through how Bootstrapping can be used to test hypotheses with p-values. For more information about using Bootstrapping to calculate p-values, see: https:\\/\\/web.stanford.edu\\/class\\/archive\\/cs\\/cs109\\/cs109.1178\\/lectureHandouts\\/170-samples.pdf ...and... http:\\/\\/www.stat.ucla.edu\\/~rgould\\/110as02\\/bshypothesis.pdf And to learn more about the minimum number of measurements required for Bootstrapping, see: https:\\/\\/stats.stackexchange.com\\/questions\\/33300\\/determining-sample-size-necessary-for-bootstrap-method-proposed-method For a complete index of all the StatQuest videos, check out: https:\\/\\/statquest.org\\/video-index\\/ If you'd like to support StatQuest, please consider... Patreon: https:\\/\\/www.patreon.com\\/statquest ...or... YouTube Membership: https:\\/\\/www.youtube.com\\/channel\\/UCtYLUTtgS3k1Fg4y5tAhLbw\\/join ...buying a StatQuest Study Guide... https:\\/\\/statquest.org\\/studyguides\\/ ...or a cool StatQuest t-shirt or sweatshirt (USA\\/Europe): https:\\/\\/teespring.com\\/stores\\/statquest (everywhere): https:\\/\\/www.redbubble.com\\/people\\/starmer\\/works\\/40421224-statquest-double-bam?asc=u&p=t-shirt ...buying one or two of my songs (or go large and get a whole album!) https:\\/\\/joshuastarmer.bandcamp.com\\/ ...or just donating to StatQuest! https:\\/\\/www.paypal.me\\/statquest Lastly, if you want to keep up with me as I research and create new StatQuests, follow me on twitter: https:\\/\\/twitter.com\\/joshuastarmer\",\"1230\":\"This video describes two excellent websites for accessing U.S. Census data, along with a competition for $5k in prizes from the \\\"Let's Make it Count\\\" initiative. The effort is funded by the National Science Foundation's West Big Data Innovation Hub, and open for U.S. high school students and teachers, ending January 1st, 2021. Join the competition now at: https:\\/\\/bit.ly\\/letsmakeitcount2020 There are lots of links in the video and here they are: https:\\/\\/censusreporter.org\\/ https:\\/\\/data.census.gov\\/ https:\\/\\/www.letsmakeitcount.org\\/ ...and, last but not least... https:\\/\\/statquest.org\\/ \\u2b50 NOTE: When I code, I use Kite, a free AI-powered coding assistant that will help you code faster and smarter. The Kite plugin integrates with all the top editors and IDEs to give you smart completions and documentation while you\\u2019re typing. I love it! https:\\/\\/www.kite.com\\/get-kite\\/?utm_medium=referral&utm_source=youtube&utm_campaign=statquest&utm_content=description-only For a complete index of all the StatQuest videos, check out: https:\\/\\/statquest.org\\/video-index\\/ If you'd like to support StatQuest, please consider... Patreon: https:\\/\\/www.patreon.com\\/statquest ...or... YouTube Membership: https:\\/\\/www.youtube.com\\/channel\\/UCtYLUTtgS3k1Fg4y5tAhLbw\\/join ...a cool StatQuest t-shirt or sweatshirt (USA\\/Europe): https:\\/\\/teespring.com\\/stores\\/statquest (everywhere): https:\\/\\/www.redbubble.com\\/people\\/starmer\\/works\\/40421224-statquest-double-bam?asc=u&p=t-shirt ...buying one or two of my songs (or go large and get a whole album!) https:\\/\\/joshuastarmer.bandcamp.com\\/ ...or just donating to StatQuest! https:\\/\\/www.paypal.me\\/statquest Lastly, if you want to keep up with me as I research and create new StatQuests, follow me on twitter: https:\\/\\/twitter.com\\/joshuastarmer 0:00 Awesome song and introduction 1:13 censusreporter.org 2:56 data.census.gov 4:57 submitting our story to the competition #StatQuest #USCensus #Letsmakeitcount\",\"2328\":\"After chasing an Out Of Index bug in our Glue Evaluation script, we found out that the caching behaviour in run_glue.py is not what you would expect. Huggingface downloads glue data, tokenizes and featurizes the text using the supplied tokenizer, and caches the result. The problem is that the cache treats all tokenizers of the same class as identical. See this issue. This is a major problem if you (like us) are evaluating models using different tokenizers of the same type: the glue task data will be featurized using whichever tokenizer you happened to use first. In the worst case scenario means that the glue evaluation scores are based on nonsensical data (from the perspective of your pre-trained model). In the best case scenario it means that you got a cryptic Out Of Index bug. It seems like this has been fixed recently, as the run_glue script was updated to use the datasets library. But from what I can gather, this bug has existed for quite a while. TL;DR: If you've used the huggingface\\u00b4s run_glue script to evaluate models that happen to use different tokenizers of the same class, your results are probably invalid. [link] [comments]\",\"1352\":\"#ai #technology #poker This paper does for Poker what AlphaZero has done for Chess & Go. The combination of Self-Play Reinforcement Learning and Tree Search has had tremendous success in perfect-information games, but transferring such techniques to imperfect information games is a hard problem. Not only does ReBeL solve this problem, but it provably converges to a Nash Equilibrium and delivers a superhuman Heads Up No-Limit Hold'em bot with very little domain knowledge. OUTLINE: 0:00 - Intro & Overview 3:20 - Rock, Paper, and Double Scissor 10:00 - AlphaZero Tree Search 18:30 - Notation Setup: Infostates & Nash Equilibria 31:45 - One Card Poker: Introducing Belief Representations 45:00 - Solving Games in Belief Representation 55:20 - The ReBeL Algorithm 1:04:00 - Theory & Experiment Results 1:07:00 - Broader Impact 1:10:20 - High-Level Summary Paper: https:\\/\\/arxiv.org\\/abs\\/2007.13544 Code: https:\\/\\/github.com\\/facebookresearch\\/rebel Blog: https:\\/\\/ai.facebook.com\\/blog\\/rebel-a-general-game-playing-ai-bot-that-excels-at-poker-and-more\\/ ERRATA: As someone last video pointed out: This is not the best Poker algorithm, but the best one that uses very little expert knowledge. Abstract: The combination of deep reinforcement learning and search at both training and test time is a powerful paradigm that has led to a number of successes in single-agent settings and perfect-information games, best exemplified by AlphaZero. However, prior algorithms of this form cannot cope with imperfect-information games. This paper presents ReBeL, a general framework for self-play reinforcement learning and search that provably converges to a Nash equilibrium in any two-player zero-sum game. In the simpler setting of perfect-information games, ReBeL reduces to an algorithm similar to AlphaZero. Results in two different imperfect-information games show ReBeL converges to an approximate Nash equilibrium. We also show ReBeL achieves superhuman performance in heads-up no-limit Texas hold'em poker, while using far less domain knowledge than any prior poker AI. Authors: Noam Brown, Anton Bakhtin, Adam Lerer, Qucheng Gong Links: YouTube: https:\\/\\/www.youtube.com\\/c\\/yannickilcher Twitter: https:\\/\\/twitter.com\\/ykilcher Discord: https:\\/\\/discord.gg\\/4H8xxDF BitChute: https:\\/\\/www.bitchute.com\\/channel\\/yannic-kilcher Minds: https:\\/\\/www.minds.com\\/ykilcher Parler: https:\\/\\/parler.com\\/profile\\/YannicKilcher LinkedIn: https:\\/\\/www.linkedin.com\\/in\\/yannic-kilcher-488534136\\/ If you want to support me, the best thing to do is to share out the content :) If you want to support me financially (completely optional and voluntary, but a lot of people have asked for this): SubscribeStar: https:\\/\\/www.subscribestar.com\\/yannickilcher Patreon: https:\\/\\/www.patreon.com\\/yannickilcher Bitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq Ethereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2 Litecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m Monero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\",\"1344\":\"#memes #science #ai Part 2 of Antonio and me examining the latest and greatest of deep learning memes. Music: Sunshower - LATASH\\u00c1 Papov - Yung Logos Sunny Days - Anno Domini Beats Trinity - Jeremy Blake More memes: facebook.com\\/convolutionalmemes Links: YouTube: https:\\/\\/www.youtube.com\\/c\\/yannickilcher Twitter: https:\\/\\/twitter.com\\/ykilcher Discord: https:\\/\\/discord.gg\\/4H8xxDF BitChute: https:\\/\\/www.bitchute.com\\/channel\\/yannic-kilcher Minds: https:\\/\\/www.minds.com\\/ykilcher Parler: https:\\/\\/parler.com\\/profile\\/YannicKilcher LinkedIn: https:\\/\\/www.linkedin.com\\/in\\/yannic-kilcher-488534136\\/ BiliBili: https:\\/\\/space.bilibili.com\\/1824646584 If you want to support me, the best thing to do is to share out the content :) If you want to support me financially (completely optional and voluntary, but a lot of people have asked for this): SubscribeStar: https:\\/\\/www.subscribestar.com\\/yannickilcher Patreon: https:\\/\\/www.patreon.com\\/yannickilcher Bitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq Ethereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2 Litecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m Monero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\",\"1656\":\"While the number of product management roles in the US has grown by more than 30% in two years, according to LinkedIn, the responsibilities of the job are morphing. Read the full story\",\"3767\":\"I have images of metallic objects with superficial damage which are being labelled. The goal is to identify the defects with a Deep Learning model. The majority of the images are JPG, and a few are BMP. I asked to get the images in a sane format (e.g., PNG), but this wasn't possible. So I'm stuck with, say, 80% JPG and 20% BMP. Even if the semantic meaning is the same, the frequency content of JPG images and BMP images is very different, thus I'm not sure it's a good idea to train a model on a mixture of image formats. Since, for the JPGs, I don't have access to the original (uncompressed) images, should I convert the remaining BMP to JPG? Of course I don't know if the compression ratio will be the same for all JPG images. However, given that the dataset is such a mess, it's possible that not even the images which are already JPG, were all converted with the same compression ratio. Your advice? [link] [comments]\",\"1867\":\"We\\u2019re ironically searching for counterexamples to the Riemann Hypothesis. Setting up Pytest Adding a Database Search strategies In the last article, we improved our naive search from \\u201ctry all positive integers\\u201d to enumerate a subset of integers (superabundant numbers), which RH counterexamples are guaranteed to be among. These numbers grow large, fast, and we quickly reached the limit of what 64 bit integers can store. Unbounded integer arithmetic is possible on computers, but it requires a special software implementation. In brief, you represent numbers in base-N for some large N (say, ), and then use a 32-bit integer for each digit. Arithmetic on such quantities emulates a ripple-carry adder, which naturally requires linear time in the number of digits of each operand. Artem Golubin has a nice explanation of how Python does it internally. So Python can handle unbounded integer arithmetic, but neither numba nor our database engine do. Those both crash when exceeding 64-bit integers This is a problem because we won\\u2019t be able to store the results of our search without being able to put it in a database. This leaves us with a classic software engineering problem. What\\u2019s the path forward? Exploring Alternatives The impulse answer is to do as little as possible to make the damn thing work. In a situation where the software you\\u2019re writing is a prototype, and you expect it to be rewritten from scratch in the future, this is an acceptable attitude. That said, experienced engineers would caution you that, all too often, such \\u201cprototypes\\u201d are copy-pasted to become janky mission-critical systems for years. In pretending this is the \\u201creal thing,\\u201d let\\u2019s do what real engineers would do and scope out some alternatives before diving in. The two aspects are our database and the use of numba for performance. Let\\u2019s start with the database. A quick and dirty option: store all numbers as text strings in the database. There\\u2019s no limit on the size of the number in that case. The benefit: we don\\u2019t need to use a different database engine, and most of our code stays the same. The cost: we can\\u2019t use numeric operations in database queries, which would make further analysis and fetching awkward. In particular, we can\\u2019t even apply sorting operations, since text strings are sorted lexicographically (e.g., 100, 25) while numbers are sorted by magnitude (25, 100). Note, we applied this \\u201cnumbers as text\\u201d idea to the problem of serializing the search state, and it was hacky there, too. A second option is to find a database engine with direct support for unbounded-integer arithmetic. The benefit: fast database queries and the confidence that it will support future use cases well. The cost: if our existing sqlite-based interface doesn\\u2019t work with the new database engine, we\\u2019d have to write another implementation of our database interface. For numba, we have at least three options. First, fall back to native python arithmetic, which is slow. Second, implement arbitrary-precision arithmetic in Python in a way that numba can compile it. Third, find (or implement) a C-implementation of arbitrary precision integer arithmetic, provide Python bindings, and optionally see if it can work with (or replace) numba. As I write this I haven\\u2019t yet tried any of these options. My intuition tells me the best way to go would be to find \\u201cproper\\u201d support for arbitrary precision integers. For the database, I recall that the Postgres database engine supports various extensions, for example this extension that adds support for geographic objects. Postgres\\u2019s extension framework demonstrates an important software engineering principle that many of the best projects follow: \\u201cclosed for modification, open for extension.\\u201d That is, Postgres is designed so that others can contribute new features to Postgres without requiring the Postgres team to do anything special\\u2014specifically, they don\\u2019t have to change Postgres to accommodate it. The name for this sometimes goes by extensions, or plug-ins, hooks, or (at a lower level) callbacks. Github Actions is a good example of this. Geographic objects are almost certainly more complicated than arbitrary precision integers, so chances are good a Postgres extension exists for the latter. Incorporating it would involve migrating to Postgres, finding and installing that extension, and then converting the C library representation above to whatever representation Postgres accepts in a query. A good route will also ensure that we need not change our tests too much, since all we\\u2019re doing here is modifying implementations. We\\u2019ll see how well that holds up. gmp and pgmp After some digging, I found GMP (GNU Multiple Precision), a C library written by Torbj\\u00f6rn Granlund. It has a Python bindings library called gmpy that allows Python to use an \\u201cmpz\\u201d (\\u201cMultiple Precision \\u201c) type as a drop-in replacement for Python integers. And I found a PostgreSQL extension called pgmp. The standard Python library for Postgres is psycopg2, which was written by the same person who wrote pgmp, Daniele Varrazzo. To start, I ran a timing test of gmpy, which proves to be as fast as numba. This pull request has the details. It took a small bit of kicking to get pgmp to install, but then I made a test database that uses the new column type mpz and stores the value . postgres=# create database pgmp_test; CREATE DATABASE postgres=# \\\\connect pgmp_test; You are now connected to database \\\"pgmp_test\\\" as user \\\"jeremy\\\". pgmp_test=# CREATE EXTENSION pgmp; CREATE EXTENSION pgmp_test=# create table test_table (id int4, value mpz); CREATE TABLE pgmp_test=# insert into test_table pgmp_test-# values (1, 2::mpz ^ 513); INSERT 0 1 pgmp_test=# select * from test_table; id | value ----+------------------------------------------------------------------------------------------------------------------------------------------------------------- 1 | 26815615859885194199148049996411692254958731641184786755447122887443528060147093953603748596333806855380063716372972101707507765623893139892867298012168192 (1 row) Now I\\u2019m pretty confident this approach will work. This pull request includes the necessary commits to add a postgres implementation of our database interface, add tests (which is a minor nuisance). Then this pull request converts the main divisor computation functions to use gmpy, and this final commit converts the main program to use the postgres database. This exposed one bug, that I wasn\\u2019t converting the new mpz types properly in the postgres sql query. This commit fixes it, and this commit adds a regression test to catch that specific error going forward. Results and next steps With all that work, I ran the counterexample search for a few hours. When I stopped it, it had checked all possibly-superabundant numbers whose prime factorizations have at most 75 prime factors, including multiplicity. Since all possible counterexamples to the RH must be superabundant, and all superabundant numbers have the aforementioned special prime factorization, we can say it more simply. I ruled out all positive integers whose prime factorization has at most 75 factors. The top 10 are: divisor=# select n, witness_value from RiemannDivisorSums where witness_value\",\"3752\":\"There has recently been a lot of interest on applying transformers, and related ideas, to NLP and computer vision. As a theoretically-minded person (master student), I was wondering how well-understood these ideas are, beyond the intuitive justifications for the attention mechanism? More generally, what work has been done on theoretical guarantees for sequence-to-sequence (or image-to-sequence, or sequence-to-image) tasks? I've only heard about such guarantees for relatively basic classification or regression tasks (with vector inputs and outputs). By \\\"guarantees\\\" I mean things like PAC-learnability, sample complexity bounds etc. (In fact, does it even make sense to use those kinds of statistical tools for NLP or CV? Possibly the performance of e.g transformers in those fields is due to the specific structure of the data...) Sorry if this is obvious to anyone in the field, but I'm having trouble navigating the literature -- googling \\\"transformers\\\" or \\\"attention mechanism\\\" or even \\\"sequence-to-sequence\\\" keeps returning papers proposing new methods or extensions. [link] [comments]\",\"1681\":\"Read the full story\",\"1511\":\"Dileep George is a researcher at the intersection of neuroscience and artificial intelligence, co-founder of Vicarious, formerly co-founder of Numenta. From the early work on Hierarchical temporal memory to Recursive Cortical Networks to today, Dileep\\u2019s always sought to engineer intelligence that is closely inspired by the human brain. Support this channel by supporting our sponsors. Click links, get discount: \\u2013 Babbel: https:\\/\\/babbel.com and use code LEX \\u2013 MasterClass: https:\\/\\/masterclass.com\\/lex \\u2013 Raycon: https:\\/\\/buyraycon.com\\/lex If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman on Twitter, LinkedIn, Facebook, Medium, or YouTube where you can\",\"4286\":\"Dr. Eray Ozkural is an AGI researcher from Turkey, he is the founder of Celestial Intellect Cybernetics. Eray is extremely critical of Max Tegmark, Nick Bostrom and MIRI founder Elizier Yodokovsky and their views on AI safety. Eray thinks that these views represent a form of neoludditism and they are capturing valuable research budgets with doomsday fear-mongering and effectively want to prevent AI from being developed by those they don't agree with. Eray is also sceptical of the intelligence explosion hypothesis and the argument from simulation. Panel -- Dr. Keith Duggar, Dr. Tim Scarfe, Yannic Kilcher 00:00:00 Show teaser intro with added nuggets and commentary 00:48:39 Main Show Introduction 00:53:14 Doomsaying to Control 00:56:39 Fear the Basilisk! 01:08:00 Intelligence Explosion Ethics 01:09:45 Fear the Automous Drone! ... or spam 01:11:25 Infinity Point Hypothesis 01:15:26 Meat Level Intelligence 01:21:25 Defining Intelligence ... Yet Again 01:27:34 We'll make brains and then shoot them 01:31:00 The Universe likes deep learning 01:33:16 NNs are glorified hash tables 01:38:44 Radical behaviorists 01:41:29 Omega Architecture, possible AGI? 01:53:33 Simulation hypothesis 02:09:44 No one cometh unto Simulation, but by Jesus Christ 02:16:47 Agendas, Motivations, and Mind Projections 02:23:38 A computable Universe of Bulk Automata 02:30:31 Self-Organized Post-Show Coda 02:31:29 Investigating Intelligent Agency is Science 02:36:56 Goodbye and cheers! https:\\/\\/www.youtube.com\\/watch?v=pZsHZDA9TJU\",\"1685\":\"Read the full story\",\"1492\":\"Marcus Hutter is a senior research scientist at DeepMind and professor at Australian National University. Throughout his career of research, including with J\\u00fcrgen Schmidhuber and Shane Legg, he has proposed a lot of interesting ideas in and around the field of artificial general intelligence, including the development of the AIXI model which is a mathematical approach to AGI that incorporates ideas of Kolmogorov complexity, Solomonoff induction, and reinforcement learning. EPISODE LINKS: Hutter Prize: http:\\/\\/prize.hutter1.net Marcus web: http:\\/\\/www.hutter1.net Books mentioned: \\u2013 Universal AI: https:\\/\\/amzn.to\\/2waIAuw \\u2013 AI: A Modern Approach: https:\\/\\/amzn.to\\/3camxnY \\u2013 Reinforcement Learning: https:\\/\\/amzn.to\\/2PoANj9 \\u2013 Theory of Knowledge: https:\\/\\/amzn.to\\/3a6Vp7x This conversation\",\"1631\":\"Read the full story\",\"1868\":\"In the last article we set up pytest for a simple application that computes divisor sums and tries to disprove the Riemann Hypothesis. In this post we\\u2019ll show how to extend the application as we add a database dependency. The database stores the computed sums so we can analyze them after our application finishes. As in the previous post, I\\u2019ll link to specific git commits in the final code repository to show how the project evolves. You can browse or checkout the repository at each commit to see how it works. Interface before implementation The approach we\\u2019ll take is one that highlights the principle of good testing and good software design: separate components by thin interfaces so that the implementations of those interfaces can change later without needing to update lots of client code. We\\u2019ll take this to the extreme by implementing and testing the logic for our application before we ever decide what sort of database we plan to use! In other words, the choice of database will be our last choice, making it inherently flexible to change. That is, first we iron out a minimal interface that our application needs, and then choose the right database based on those needs. This is useful because software engineers often don\\u2019t understand how the choice of a dependency (especially a database dependency) will work out long term, particularly as a prototype starts to scale and hit application-specific bottlenecks. Couple this with the industry\\u2019s trend of chasing hot new fads, and eventually you realize no choice is sacred. Interface separation is the software engineer\\u2019s only defense, and their most potent tool for flexibility. As a side note, Tom Gamon summarizes this attitude well in a recent article, borrowing the analogy from a 1975 investment essay The Winner\\u2019s Game by Charles Ellis. Some of his other articles reinforce the idea that important decisions should be made as late as possible, since that is the only time you know enough to make those decisions well. Our application has two parts so far: adding new divisor sums to the database, and loading divisor sums for analysis. Since we\\u2019ll be adding to this database over time, it may also be prudent to summarize the contents of the database, e.g. to say what\\u2019s the largest computed integer. This suggests the following first-pass interface, implemented in this commit. class DivisorDb(ABC): @abstractmethod def load() -\",\"1503\":\"Dava Newman is the Apollo Program professor of AeroAstro at MIT and the former Deputy Administrator of NASA and has been a principal investigator on four spaceflight missions. Her research interests are in aerospace biomedical engineering, investigating human performance in varying gravity environments. She has developed a space activity suit, namely the BioSuit, which would provide pressure through compression directly on the skin via the suit\\u2019s textile weave, patterning, and materials rather than with pressurized gas. This conversation is part of the Artificial Intelligence podcast. If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect\",\"1583\":\"Gilbert Strang is a professor of mathematics at MIT and perhaps one of the most famous and impactful teachers of math in the world. His MIT OpenCourseWare lectures on linear algebra have been viewed millions of times. This conversation is part of the Artificial Intelligence podcast. If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman on Twitter, LinkedIn, Facebook, Medium, or YouTube where you can watch the video versions of these conversations. If you enjoy the podcast, please rate it 5 stars on Apple Podcasts or support it on Patreon. This episode\",\"1466\":\"Jeff Atwood is a co-founder of Stack Overflow and Stack Exchange, websites that are visited by millions of people every day. Much like with Wikipedia, it is difficult to understate the impact on global knowledge and productivity that these network of sites have created. Jeff is also the author of the famed Coding Horror blog, and the founder of Discourse, and open-source software project that seeks to improve the quality of our online community discussions. Video version is available on YouTube. If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman on\",\"1593\":\"It feels like Amazon removing Parler from AWS created a more dangerous world, not a less dangerous one. I may be wrong on this, so I'm listening, learning, and thinking. I hope we all are.\",\"1880\":\"Moderna has announced encouraging preliminary results for their COVID-19 vaccine. In this post, I assess the available data and explain what the vaccine\\u2019s effectiveness really means. I also look at Moderna\\u2019s experimental design and examine how it incorporates statistical procedures and concepts that I discuss throughout my blog posts and books. These concepts include experimental [\\u2026] The post Assessing a COVID-19 Vaccination Experiment and Its Results appeared first on Statistics By Jim.\",\"3758\":\"Let's talk about datasets for machine learning that change over time. In real-life projects, datasets are rarely static. They grow, change, and evolve over time. But this fact is not reflected in how most datasets are maintained. Taking inspiration from software dev, where codebases are managed using Git, we can create living Git repositories for our datasets as well. This means the dataset becomes easily manageable, and sharing, collaborating, and updating downstream consumers of changes to the data can be done similar to how we manage PIP or NPM packages. I wrote a blog about such a project, showcasing how to transform a dataset into a living-dataset, and use it in a machine learning project. https:\\/\\/dagshub.com\\/blog\\/datasets-should-behave-like-git-repositories\\/ Example project: The living dataset: https:\\/\\/dagshub.com\\/Simon\\/baby-yoda-segmentation-dataset A project using the living dataset as a dependency: https:\\/\\/dagshub.com\\/Simon\\/baby-yoda-segmentor Would love to hear your thoughts. \\u200b https:\\/\\/preview.redd.it\\/cvpu2j7ovac61.png?width=588&format=png&auto=webp&s=15d1fe9cfacf282427e4394b3c729082710d2b99 [link] [comments]\",\"643\":\"Michael Littman is a computer scientist at Brown University. Please support this podcast by checking out our sponsors: - SimpliSafe: https:\\/\\/simplisafe.com\\/lex and use code LEX to get a free security camera - ExpressVPN: https:\\/\\/expressvpn.com\\/lexpod and use code LexPod to get 3 months free - MasterClass: https:\\/\\/masterclass.com\\/lex to get 2 for price of 1 - BetterHelp: https:\\/\\/betterhelp.com\\/lex to get 10% off EPISODE LINKS: Michael's Twitter: https:\\/\\/twitter.com\\/mlittmancs Michael's Website: https:\\/\\/www.littmania.com\\/ Michael's YouTube: https:\\/\\/www.youtube.com\\/user\\/mlittman PODCAST INFO: Podcast website: https:\\/\\/lexfridman.com\\/podcast Apple Podcasts: https:\\/\\/apple.co\\/2lwqZIr Spotify: https:\\/\\/spoti.fi\\/2nEwCF8 RSS: https:\\/\\/lexfridman.com\\/feed\\/podcast\\/ Full episodes playlist: https:\\/\\/www.youtube.com\\/playlist?list=PLrAXtmErZgOdP_8GztsuKi9nrraNbKKp4 Clips playlist: https:\\/\\/www.youtube.com\\/playlist?list=PLrAXtmErZgOeciFP3CBCIEElOJeitOr41 OUTLINE: 0:00 - Introduction 2:30 - Robot and Frank 4:50 - Music 8:01 - Starring in a TurboTax commercial 18:14 - Existential risks of AI 36:36 - Reinforcement learning 1:02:24 - AlphaGo and David Silver 1:12:03 - Will neural networks achieve AGI? 1:24:30 - Bitter Lesson 1:37:20 - Does driving require a theory of mind? 1:46:46 - Book Recommendations 1:52:08 - Meaning of life CONNECT: - Subscribe to this YouTube channel - Twitter: https:\\/\\/twitter.com\\/lexfridman - LinkedIn: https:\\/\\/www.linkedin.com\\/in\\/lexfridman - Facebook: https:\\/\\/www.facebook.com\\/LexFridmanPage - Instagram: https:\\/\\/www.instagram.com\\/lexfridman - Medium: https:\\/\\/medium.com\\/@lexfridman - Support on Patreon: https:\\/\\/www.patreon.com\\/lexfridman\",\"258\":\"\\ud83d\\udd25New Video\\ud83d\\udd25 EVERYBODY is talking about @OpenAI's new DALL\\u00b7E model \\ud83d\\udc40 It takes any piece of text and turns it into an image, absolutely crazy \\ud83d\\ude31 Watch the video to learn more\\ud83d\\udcaa invidious.snopyta.org\\/j4xgkjWlfL4 #DALLE @ilyasut @_jongwook_kim @MikhailPavlov5 @gabeeegoooh @scottgray76\",\"1351\":\"#ai #research #machinelearning Deep Learning models are often overparameterized and have many degrees of freedom, which leads to many local minima that all perform equally well on the test set. But it turns out that even though they all generalize in-distribution, the performance of these models can be drastically different when tested out-of-distribution. Notably, in many cases, a good model can actually be found among all these candidates, but it seems impossible to select it. This paper describes this problem, which it calls underspecification, and gives several theoretical and practical examples. OUTLINE: 0:00 - Into & Overview 2:00 - Underspecification of ML Pipelines 11:15 - Stress Tests 12:40 - Epidemiological Example 20:45 - Theoretical Model 26:55 - Example from Medical Genomics 34:00 - ImageNet-C Example 36:50 - BERT Models 56:55 - Conclusion & Comments Paper: https:\\/\\/arxiv.org\\/abs\\/2011.03395 Abstract: ML models often exhibit unexpectedly poor behavior when they are deployed in real-world domains. We identify underspecification as a key reason for these failures. An ML pipeline is underspecified when it can return many predictors with equivalently strong held-out performance in the training domain. Underspecification is common in modern ML pipelines, such as those based on deep learning. Predictors returned by underspecified pipelines are often treated as equivalent based on their training domain performance, but we show here that such predictors can behave very differently in deployment domains. This ambiguity can lead to instability and poor model behavior in practice, and is a distinct failure mode from previously identified issues arising from structural mismatch between training and deployment domains. We show that this problem appears in a wide variety of practical ML pipelines, using examples from computer vision, medical imaging, natural language processing, clinical risk prediction based on electronic health records, and medical genomics. Our results show the need to explicitly account for underspecification in modeling pipelines that are intended for real-world deployment in any domain. Authors: Alexander D'Amour, Katherine Heller, Dan Moldovan, Ben Adlam, Babak Alipanahi, Alex Beutel, Christina Chen, Jonathan Deaton, Jacob Eisenstein, Matthew D. Hoffman, Farhad Hormozdiari, Neil Houlsby, Shaobo Hou, Ghassen Jerfel, Alan Karthikesalingam, Mario Lucic, Yian Ma, Cory McLean, Diana Mincu, Akinori Mitani, Andrea Montanari, Zachary Nado, Vivek Natarajan, Christopher Nielson, Thomas F. Osborne, Rajiv Raman, Kim Ramasamy, Rory Sayres, Jessica Schrouff, Martin Seneviratne, Shannon Sequeira, Harini Suresh, Victor Veitch, Max Vladymyrov, Xuezhi Wang, Kellie Webster, Steve Yadlowsky, Taedong Yun, Xiaohua Zhai, D. Sculley Links: YouTube: https:\\/\\/www.youtube.com\\/c\\/yannickilcher Twitter: https:\\/\\/twitter.com\\/ykilcher Discord: https:\\/\\/discord.gg\\/4H8xxDF BitChute: https:\\/\\/www.bitchute.com\\/channel\\/yannic-kilcher Minds: https:\\/\\/www.minds.com\\/ykilcher Parler: https:\\/\\/parler.com\\/profile\\/YannicKilcher LinkedIn: https:\\/\\/www.linkedin.com\\/in\\/yannic-kilcher-488534136\\/ If you want to support me, the best thing to do is to share out the content :) If you want to support me financially (completely optional and voluntary, but a lot of people have asked for this): SubscribeStar: https:\\/\\/www.subscribestar.com\\/yannickilcher Patreon: https:\\/\\/www.patreon.com\\/yannickilcher Bitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq Ethereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2 Litecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m Monero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\",\"3140\":\"The current global pandemic crisis presents various challenges to businesses in all industries, including financial services institutions, who are monitoring and dealing with the effects of COVID-19 across the world. At a time of a pandemic, it is important that teams get together to share their insights and experience, with the goal of inspiring and helping to contain and mitigate the negative impact. With this in mind, H2O.ai hosted a virtual fireside chat with Bill.com, where business leaders from both companies discussed the latest in AI, key factors to building and recruiting an AI-first engineering team, the impact of COVID-19 for SMBs, and the lessons learned from the pandemic in the finance industry. Check out the key takeaways from Sri Ambati\\u2019s conversation with Vinay Pai, SVP of Engineering at Bill.com, Manasa Murthy, VP of Engineering at Bill.com, and Prakruthi Narasimha, Senior Accountant at H2O.ai, on how to thrive during challenging times and the role of AI in shaping the future for the best. 1. The State of SMB Sri Ambati kicked off the fireside chat by offering a deeper look into the state of Small and Medium Business (SMB) with data gathered from a Salesforce Research report that analyzed the responses of more than 2,300 small and medium business (SMB) owners and leaders around the world to determine the impact of the COVID-19 pandemic, the role of digital transformation in terms of enhancing business resiliency, and how SMB leaders are planning for recovery and growth post-pandemic. According to the report, the world of SMBs radically changed in early 2020 due to the COVID-19 pandemic. With a significant reduction in revenue, most SMB leaders say they are struggling to keep their businesses afloat, and businesses that the consumed-focused and micro-businesses were the most affected. Also, besides the challenge of getting access to capital and cash-flow, SMBs are seeing a reduced customer demand, supply chain disruptions, and challenges with public health mandates. 2. Bringing teams together in times of COVID-19 Vinay Pai: One thing that I feel really lucky about is that being in tech we have the kind of work that we can do remotely. So we took the whole company remote at the beginning of March. And we started with four principles: stay healthy and safe, stay connected to your teammates, keep bill.com running for customers, and then focus on outcomes. We actually did a week-long innovation week, which was a hackathon, so we\\u2019re making this work, we\\u2019re growing the business. And even since March, we probably hired about 30 engineers. Manasa Murthy: I think a lot of the challenges all of us feel going remote with all the uncertainties of 2020 that are presented to us, it\\u2019s a lot about embracing change. We need to look at it wholeheartedly. We bring our whole selves to work. We all have kids running around during conference calls. We all have to tend to probably person things during the day. I think it\\u2019s really about listening to what\\u2019s working and what\\u2019s not. 3. Building a gender-balanced team Vinay Pai: Our CEO, Rene Lacerte, believes in diversity and inclusion and it\\u2019s reflected in the culture. If our teams reflect the same composition as our customers, you end up making better products. Creating a more diverse and inclusive culture is not something that you solve right away. It\\u2019s a long game. 4. The AI journey at Bill.com Vinay Pai: This goes back to three years ago when I joined, during that second generation of AI at Bill.com. My engineering team had already invested in building some of our own models and, as you know, we get a lot of invoices that our customers upload. So the challenge we had was: how do we extract data from invoices, bring them into the system to minimize data entry? We also do risk and fraud. I started using this term \\u201cdemocratizing data science\\u201d, and it\\u2019s like, how do you take the data we have and make it accessible to everyone in the company, whether you\\u2019re in product management or marketing or sales or analytics, where there\\u2019s so much we can learn from the data? So this was kind of a two or three-step process. One is, we started building our own ML models around invoices, risk, and fraud. We ran into H2O.ai at Money2020, we looked at you guys and one of your competitors, and we did a bake-off internally, you guys won. And then we decided to get access to some early adopters and product management that were very data-driven and get them trained along with our engineers, on Driverless AI. And let\\u2019s expand that out at the same time. I wanted to build our own AI capabilities. Part of that was hiring Manasa to lead the team and the next day hiring several data scientists. It\\u2019s been a very rich collaboration. Then, with the toolset, we brought H2O to a bunch of folks in the company that they\\u2019re using it in their day-to-day lives, which is very powerful. Manasa Murthy: If you think about a product like ours, where there\\u2019s so much opportunity to apply AI use cases all across the board, I feel like bringing H2O was really useful in accelerating our business use cases. Model building went from months to days, which is a very fast development. We\\u2019ve also been able to expand to so many new use cases. Early on when our team was smaller, it was incredibly useful to have a tool like this to expand, and while we build expertise. Sri Ambati: Our core vision is to democratize AI. But of course, we pride ourselves in trying to make our customers the AI companies. 5. AI and Bill.com helping customers and end-users in their day-to-day responsibilities Prakruthi Narasimha: I\\u2019ve been using Bill.com for about two to three years now, very much so on a regular basis. The whole AI functionality was such a game-changer from an end-user perspective. The fact that most of the information was coded and the bill that was being uploaded, it could be a PDF, or word, or an Excel file, or it could be just something that I took a picture from my phone. It would still capture 90% of the important information that I need. In terms of data entry, I\\u2019m doing little to no data entry. I\\u2019m just verifying information. That gives me a lot of confidence in the accuracy of the bills being entered. But there are also a lot of other things that Bill.com offers that set it apart from a lot of AP companies. Such as keeping the whole end-to-end AP on the cloud, and giving us the ability to do the job remotely, without having the need to be in a physical space. All of this without compromising on things like segregation of duties, and having a good approval integrated workflow. 6. Recommendations on the business value of AI Vinay Pai: I think AI is one of these things that gets a lot of bad press. Because there\\u2019s a lot of use cases where it\\u2019s being misused or used inappropriately and those are the things that get on the news. But then if you look at AI and the problems we\\u2019re solving. So I feel like in the pandemic and the recession, the fact that we can help businesses manage their business remotely, continue to pay their vendors, continue to get paid by their customers, it\\u2019s a great mission where we can help them manage their business and actually grow their business. So it\\u2019s coming from a place of solving real customer problems. Manasa Murthy: Where I have seen success is when you start from the customer problem. Then it becomes a real issue that you can see the benefit very clearly. Otherwise, AI is just a buzzword; it\\u2019s a tech play, \\u201cWhy do I care?\\u201d And AI, especially having such a high initial cost, you really need to start thinking about how do you actually balance that startup boosted cost versus the benefit that you\\u2019re going to get overtime. My recommendation would be to focus on value and focus on gathering the data and foundation first. 7. Building an AI-first engineering team Vinay Pai: I think the biggest challenge of any leader is building the team that\\u2019s going to, first of all, deliver what you have today, what you need to deliver today, as well as build for the future, and help grow and scale. So you\\u2019re always trying to bring in people smarter than you because you want to average upon the IQ of the team and bring in the skill sets. But at the same time, you want to bring along the team that\\u2019s there, because they have the scars and the battle stories, and they know the customer base. So it\\u2019s a fun engineering problem. So that\\u2019s really the factors on how we recruit and build. 8. The Future of AI Sri Ambati: In the next few years of innovation, thanks to AI, now being part of discovery itself, we\\u2019ll see such dramatic acceleration in innovation because the time to do an experiment and the cost to do an experiment has been dramatically reduced. When we say democratize AI, we\\u2019re really meaning faster, cheaper, easier ways to do experiments and making the experience so inexpensive that it\\u2019s almost natural to fail and learn from the failures and then quickly reinvent. But in general, I think being human is being redefined because AI is just making things much more intelligent, and as things become intelligent we can focus on being more human. In general, I think you will probably see a lot more empathy and gratitude. Click here to watch the recording of the panel. The post AI in the Financial Industry: 8 Key Takeaways from the Bill.com + H2O.ai Fireside Chat appeared first on Open Source Leader in AI and ML.\",\"1650\":\"Prior to analyzing large chunks of data, enterprises must homogenize them in a way that makes them available and accessible to decision-makers. Presently, data comes from many sources, and every particular source can define similar data points in different ways. Say for example, the state field in a source system may exhibit \\u201cIllinois\\u201d but the destination keeps it is as \\u201cIL\\u201d. Read the full story\",\"1638\":\"Read the full story\",\"1625\":\"Read the full story\",\"245\":\"wow nitter.net\\/goodwish916\\/status\\/1329234124394041345#m\",\"1689\":\"Read the full story\",\"1144\":\"Tom Crawford speaks about domes, curves, and catenaries. Check out http:\\/\\/KiwiCo.com\\/Numberphile for 50% off your first month of any subscription. More links & stuff in full description below \\u2193\\u2193\\u2193 Tom Crawford: https:\\/\\/tomrocksmaths.com\\/ More videos with Tom: http:\\/\\/bit.ly\\/Crawford_Videos Numberphile is supported by the Mathematical Sciences Research Institute (MSRI): http:\\/\\/bit.ly\\/MSRINumberphile We are also supported by Science Sandbox, a Simons Foundation initiative dedicated to engaging everyone with the process of science. https:\\/\\/www.simonsfoundation.org\\/outreach\\/science-sandbox\\/ And support from Math For America - https:\\/\\/www.mathforamerica.org\\/ NUMBERPHILE Website: http:\\/\\/www.numberphile.com\\/ Numberphile on Facebook: http:\\/\\/www.facebook.com\\/numberphile Numberphile tweets: https:\\/\\/twitter.com\\/numberphile Subscribe: http:\\/\\/bit.ly\\/Numberphile_Sub Videos by Brady Haran Patreon: http:\\/\\/www.patreon.com\\/numberphile Numberphile T-Shirts and Merch: https:\\/\\/teespring.com\\/stores\\/numberphile Brady's videos subreddit: http:\\/\\/www.reddit.com\\/r\\/BradyHaran\\/ Brady's latest videos across all channels: http:\\/\\/www.bradyharanblog.com\\/ Sign up for (occasional) emails: http:\\/\\/eepurl.com\\/YdjL9\",\"1555\":\"Paola Arlotta is a professor of stem cell and regenerative biology at Harvard University. She is interested in understanding the molecular laws that govern the birth, differentiation and assembly of the human brain\\u2019s cerebral cortex. She explores the complexity of the brain by studying and engineering elements of how the brain develops. This conversation is part of the Artificial Intelligence podcast. If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman on Twitter, LinkedIn, Facebook, Medium, or YouTube where you can watch the video versions of these conversations. If you enjoy the podcast,\",\"3873\":\"Why would anyone choose to clone and continuously maintain a perfect clone of all data on Github? Debricked has the answer. Read the full story\",\"4674\":\"I want to build a sparse convolutional autoencoder to do some image classification but i do not understand how to extract the features from the encoder output. Since it is a convolutional autoencoder, the outputs are feature maps, but for the classification i need a full connected layer. How can i transform these feature maps in a vector in a way that makes sense? Of course i could pool one value from each feature map or i could just flatten the maps and put them on a fc layer, but does this makes sense? Another question is how to ensure the sparsity during the training of the convolutional autoencoder? I only see examples using the kullback-leibler divergence on dense layers outputs, i could not find an example using sparsity on feature maps. Anyone have experience in this? [link] [comments]\",\"2339\":\"This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read. Please try to provide some insight from your understanding and please don't post things which are present in wiki. Preferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links. Previous weeks : 1-10 11-20 21-30 31-40 41-50 51-60 61-70 71-80 81-90 91-100 101-110 Week 1 Week 11 Week 21 Week 31 Week 41 Week 51 Week 61 Week 71 Week 81 Week 91 Week 101 Week 2 Week 12 Week 22 Week 32 Week 42 Week 52 Week 62 Week 72 Week 82 Week 92 Week 102 Week 3 Week 13 Week 23 Week 33 Week 43 Week 53 Week 63 Week 73 Week 83 Week 93 Week 103 Week 4 Week 14 Week 24 Week 34 Week 44 Week 54 Week 64 Week 74 Week 84 Week 94 Week 5 Week 15 Week 25 Week 35 Week 45 Week 55 Week 65 Week 75 Week 85 Week 95 Week 6 Week 16 Week 26 Week 36 Week 46 Week 56 Week 66 Week 76 Week 86 Week 96 Week 7 Week 17 Week 27 Week 37 Week 47 Week 57 Week 67 Week 77 Week 87 Week 97 Week 8 Week 18 Week 28 Week 38 Week 48 Week 58 Week 68 Week 78 Week 88 Week 98 Week 9 Week 19 Week 29 Week 39 Week 49 Week 59 Week 69 Week 79 Week 89 Week 99 Week 10 Week 20 Week 30 Week 40 Week 50 Week 60 Week 70 Week 80 Week 90 Week 100 Most upvoted papers two weeks ago: \\/u\\/Big_Temporary_3449: here \\/u\\/ArminBazzaa: pdf link \\/u\\/Captain_Flashheart: Machine Learning Design Patterns Besides that, there are no rules, have fun. [link] [comments]\",\"1872\":\"Having independent and identically distributed (IID) data is a common assumption for statistical procedures and hypothesis tests. But what does that mouthful of words actually mean? That\\u2019s the topic of this post! And, I\\u2019ll provide helpful tips for determining whether your data are IID. Let\\u2019s break the components down one-by-one. We talk about independent and [\\u2026] The post Independent and Identically Distributed Data (IID) appeared first on Statistics By Jim.\",\"1435\":\"Jim Keller is a legendary microprocessor engineer, having worked at AMD, Apple, Tesla, and now Intel. He\\u2019s known for his work on the AMD K7, K8, K12 and Zen microarchitectures, Apple A4, A5 processors, and co-author of the specifications for the x86-64 instruction set and HyperTransport interconnect. This conversation is part of the Artificial Intelligence podcast. If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman on Twitter, LinkedIn, Facebook, Medium, or YouTube where you can watch the video versions of these conversations. If you enjoy the podcast, please rate it 5 stars\",\"3756\":\"Hi r\\/MachineLearning, The hidden states of Transformers are an interesting place to seek clues for how the various layers process an input token and collectively select an output token. In this post, I demonstrate three ways of visualizing the hidden state. I build on awesome previous work in the space. What I find most exciting is that this post is only a demo for the open-source package creating these visualizations (Ecco). I'm sure other people will find more interesting ways to probe these models using this tool. I hope you find it useful! TL;DR: The output tokens tend to be selected by the latter half of model layers. But there are cases where even layer 0 is certain of the output token (look for solid dark pink columns in the visuals). https:\\/\\/jalammar.github.io\\/hidden-states\\/ [link] [comments]\",\"4312\":\"This week Dr. Tim Scarfe, Dr. Keith Duggar and Yannic \\\"Lightspeed\\\" Kilcher respond to the \\\"Algoshambles\\\" exam fiasco in the UK where the government were forced to step in to standardise the grades which were grossly inflated by the schools. The schools and teachers are all paid on metrics related to the grades received by students, what could possibly go wrong?! The result is that we end up with grades which have lost all their value and students are coached for the exams and don't actually learn the subject. We also cover the second Francois Chollet interview on the Lex Fridman podcast. We cover GPT-3, Neuralink, and discussion of intelligence. 00:00:00 Algoshambles 00:45:40 Lex Fridman\\/Chollet: Intro 00:55:21 Lex Fridman\\/Chollet: Neuralink 01:06:28 Lex Fridman\\/Chollet: GPT-3 01:23:43 Lex Fridman\\/Chollet: Intelligence discussion\",\"1615\":\"Imposter syndrome is a common experience for data scientists. But there are ways to tackle it and succeed despite it. Read the full story\",\"4900\":\"Asking for a friend nitter.net\\/veritasium\\/status\\/1352724375036825600#m\",\"1591\":\"\\\"Dwell on the beauty of life. Watch the stars, and see yourself running with them.\\\" - Marcus Aurelius\",\"1665\":\"Learn to use the Rpy2 library as a way to interface R using python for bioinformatics purposes Read the full story\",\"1050\":\"Get free access to over 2500 documentaries on CuriosityStream: http:\\/\\/go.thoughtleaders.io\\/1622720200820 (use promo code \\\"zachstar\\\" at sign up) STEMerch Store: https:\\/\\/stemerch.com\\/ Support the Channel: https:\\/\\/www.patreon.com\\/zachstar PayPal(one time donation): https:\\/\\/www.paypal.me\\/ZachStarYT \\u25baFollow me Instagram: https:\\/\\/www.instagram.com\\/zachstar\\/ Twitter: https:\\/\\/twitter.com\\/ImZachStar 3b1b Fractal Video: https:\\/\\/www.youtube.com\\/watch?v=gB9n2gHsHN4 Numberphile Infinity: https:\\/\\/www.youtube.com\\/watch?v=elvOZm0d4H0 Vsauce, Banach-Tarski Paradox: https:\\/\\/www.youtube.com\\/watch?v=s86-Z-CbaHA Epic Song: Release the Fire by FormantX Animations: Brainup Studios ( http:\\/\\/brainup.in\\/ ) \\u25baMy Setup: Space Pictures: https:\\/\\/amzn.to\\/2CC4Kqj Magnetic Floating Globe: https:\\/\\/amzn.to\\/2VgPdn0 Camera: https:\\/\\/amzn.to\\/2RivYu5 Mic: https:\\/\\/amzn.to\\/35bKiri Tripod: https:\\/\\/amzn.to\\/2RgMTNL Equilibrium Tube: https:\\/\\/amzn.to\\/2SowDrh \\u25baCheck out the my Amazon Store: https:\\/\\/www.amazon.com\\/shop\\/zachstar\",\"5682\":\"Hello Redditors, I wanted recommendations on some startups doing really cool ML. I keep finding very generic content on the web and wanted to know about any startups doing solid value creation via the application of ML. I know a lot about the work of big companies but wanted to understand the niches where startups are applying ML. Thanks. [link] [comments]\",\"256\":\"I haven't seen this much BS in legal statements by Google & co since they fired @timnitGebru . Attacks on free speech are wrong no matter by whom and to whom. Very dangerous if done by monopolies. nitter.net\\/Not_the_Bee\\/status\\/1348081057309077506#m\",\"1436\":\"Dmitry Korkin is a professor of bioinformatics and computational biology at WPI. Please support this podcast by checking out our sponsors: \\u2013 Brave: https:\\/\\/brave.com\\/lex \\u2013 NetSuite: http:\\/\\/netsuite.com\\/lex to get free product tour \\u2013 Magic Spoon: https:\\/\\/magicspoon.com\\/lex and use code LEX to get $5 off \\u2013 Eight Sleep: https:\\/\\/www.eightsleep.com\\/lex and use code LEX to get special savings EPISODE LINKS: Dmitry\\u2019s Website: http:\\/\\/korkinlab.org\\/ Dmitry\\u2019s Twitter: https:\\/\\/twitter.com\\/dmkorkin PODCAST INFO: Podcast website: https:\\/\\/lexfridman.com\\/podcast Apple Podcasts: https:\\/\\/apple.co\\/2lwqZIr Spotify: https:\\/\\/spoti.fi\\/2nEwCF8 RSS: https:\\/\\/lexfridman.com\\/feed\\/podcast\\/ YouTube Full Episodes: https:\\/\\/youtube.com\\/lexfridman YouTube Clips: https:\\/\\/youtube.com\\/lexclips SUPPORT & CONNECT: \\u2013 Check out the sponsors above, it\\u2019s the best way to support this podcast \\u2013 Support on Patreon: https:\\/\\/www.patreon.com\\/lexfridman \\u2013 Twitter: https:\\/\\/twitter.com\\/lexfridman \\u2013 Instagram: https:\\/\\/www.instagram.com\\/lexfridman \\u2013 LinkedIn: https:\\/\\/www.linkedin.com\\/in\\/lexfridman \\u2013 Facebook: https:\\/\\/www.facebook.com\\/LexFridmanPage \\u2013 Medium: https:\\/\\/medium.com\\/@lexfridman OUTLINE: Here\\u2019s the timestamps for the episode. On some podcast players you should be able to click the timestamp to jump to that time. (00:00) \\u2013 Introduction (07:21) \\u2013 Proteins and the building blocks of life (14:23) \\u2013 Spike protein (21:11) \\u2013 Coronavirus biological structure explained (26:09) \\u2013 Virus mutations (32:39) \\u2013 Evolution of proteins (42:25) \\u2013 Self-replicating computer programs (50:02) \\u2013 Origin of life (57:34) \\u2013 Extraterrestrial life in our solar system (59:31) \\u2013 Joshua Lederberg (1:05:30) \\u2013 Dendral (1:08:24) \\u2013 Why did expert systems fail? (1:10:35) \\u2013 AlphaFold 2 (1:32:13) \\u2013 Will AI revolutionize art and music? (1:39:12) \\u2013 Multi-protein folding (1:43:39) \\u2013 Will AlphaFold 2 result in a Nobel Prize? (1:46:10) \\u2013 Will AI be used to engineer deadly viruses? (2:01:17) \\u2013 Book recommendations (2:11:00) \\u2013 Family (2:13:38) \\u2013 A poem in Russian\",\"641\":\"Diana Walsh Pasulka is a professor of philosophy and religion at UNCW and author of American Cosmic: UFOs, Religion, and Technology. Please support this podcast by checking out our sponsors: - LMNT: https:\\/\\/drinkLMNT.com\\/lex to get free shipping - Grammarly: https:\\/\\/grammarly.com\\/lex to get 20% off premium - Business Wars: https:\\/\\/wondery.com\\/business-wars\\/ - Cash App: https:\\/\\/cash.app\\/ and use code LexPodcast to get $10 \\\"How many alien civilizations are out there?\\\" video: https:\\/\\/www.youtube.com\\/watch?v=JTmxA2MvEqk EPISODE LINKS: Diana's Website: https:\\/\\/uncw.edu\\/par\\/faculty\\/faculty-pasulka.html American Cosmic (book): https:\\/\\/amzn.to\\/3aK2kaj PODCAST INFO: Podcast website: https:\\/\\/lexfridman.com\\/podcast Apple Podcasts: https:\\/\\/apple.co\\/2lwqZIr Spotify: https:\\/\\/spoti.fi\\/2nEwCF8 RSS: https:\\/\\/lexfridman.com\\/feed\\/podcast\\/ Full episodes playlist: https:\\/\\/www.youtube.com\\/playlist?list=PLrAXtmErZgOdP_8GztsuKi9nrraNbKKp4 Clips playlist: https:\\/\\/www.youtube.com\\/playlist?list=PLrAXtmErZgOeciFP3CBCIEElOJeitOr41 OUTLINE: 0:00 - Introduction 2:03 - What is real? 7:32 - Can beliefs become reality? 12:34 - Donald Hoffman 16:33 - Immanuel Kant's Critique of Pure Reason 20:02 - Ayn Rand 27:00 - How do religions start? 42:13 - Religion is an evolutionary advantage 47:34 - Religion used in propaganda 52:07 - What did Nietzsche mean by \\\"God is Dead\\\"? 57:34 - American Cosmic 1:01:20 - What do aliens look like? 1:10:03 - History of space programs 1:13:06 - Jacques Vallee 1:22:31 - Artificial intelligence 1:28:00 - Ufology community 1:39:13 - Psychedelics 1:43:10 - Tic Tac UFO 1:51:44 - Roswell UFO incident 2:02:49 - Bob Lazar 2:06:25 - Monoliths in the desert 2:17:14 - Humans will co-evolve with AI 2:20:33 - Neuralink 2:25:23 - Singularity 2:35:14 - Books: Nietzsche 2:40:20 - Books: Hannah Arendt 2:45:18 - Fear of death 2:49:46 - Meaning of life CONNECT: - Subscribe to this YouTube channel - Twitter: https:\\/\\/twitter.com\\/lexfridman - LinkedIn: https:\\/\\/www.linkedin.com\\/in\\/lexfridman - Facebook: https:\\/\\/www.facebook.com\\/LexFridmanPage - Instagram: https:\\/\\/www.instagram.com\\/lexfridman - Medium: https:\\/\\/medium.com\\/@lexfridman - Support on Patreon: https:\\/\\/www.patreon.com\\/lexfridman\",\"1687\":\"Planning in a startup can feel like an exercise in futility \\u2014 especially when it comes to data \\u2014 especially when your data team is small and scrappy. Read the full story\",\"3195\":\"Regression refers to predictive modeling problems that involve predicting a numeric value. It is different from classification that involves predicting a class label. Unlike classification, you cannot use classification accuracy to evaluate the predictions made by a regression model. Instead, you must use error metrics specifically designed for evaluating predictions made on regression problems. In this tutorial, you will discover how to calculate error metrics for regression predictive modeling projects. After completing this tutorial, you will know: Regression predictive modeling are those problems that involve predicting a numeric value. Metrics for regression involve calculating an error score to summarize the predictive skill of a model. How to calculate and report mean squared error, root mean squared error, and mean absolute error. Let\\u2019s get started. Regression Metrics for Machine Learning Photo by Gael Varoquaux, some rights reserved. Tutorial Overview This tutorial is divided into three parts; they are: Regression Predictive Modeling Evaluating Regression Models Metrics for Regression Mean Squared Error Root Mean Squared Error Mean Absolute Error Regression Predictive Modeling Predictive modeling is the problem of developing a model using historical data to make a prediction on new data where we do not have the answer. Predictive modeling can be described as the mathematical problem of approximating a mapping function (f) from input variables (X) to output variables (y). This is called the problem of function approximation. The job of the modeling algorithm is to find the best mapping function we can given the time and resources available. For more on approximating functions in applied machine learning, see the post: How Machine Learning Algorithms Work Regression predictive modeling is the task of approximating a mapping function (f) from input variables (X) to a continuous output variable (y). Regression is different from classification, which involves predicting a category or class label. For more on the difference between classification and regression, see the tutorial: Difference Between Classification and Regression in Machine Learning A continuous output variable is a real-value, such as an integer or floating point value. These are often quantities, such as amounts and sizes. For example, a house may be predicted to sell for a specific dollar value, perhaps in the range of $100,000 to $200,000. A regression problem requires the prediction of a quantity. A regression can have real-valued or discrete input variables. A problem with multiple input variables is often called a multivariate regression problem. A regression problem where input variables are ordered by time is called a time series forecasting problem. Now that we are familiar with regression predictive modeling, let\\u2019s look at how we might evaluate a regression model. Evaluating Regression Models A common question by beginners to regression predictive modeling projects is: How do I calculate accuracy for my regression model? Accuracy (e.g. classification accuracy) is a measure for classification, not regression. We cannot calculate accuracy for a regression model. The skill or performance of a regression model must be reported as an error in those predictions. This makes sense if you think about it. If you are predicting a numeric value like a height or a dollar amount, you don\\u2019t want to know if the model predicted the value exactly (this might be intractably difficult in practice); instead, we want to know how close the predictions were to the expected values. Error addresses exactly this and summarizes on average how close predictions were to their expected values. There are three error metrics that are commonly used for evaluating and reporting the performance of a regression model; they are: Mean Squared Error (MSE). Root Mean Squared Error (RMSE). Mean Absolute Error (MAE) There are many other metrics for regression, although these are the most commonly used. You can see the full list of regression metrics supported by the scikit-learn Python machine learning library here: Scikit-Learn API: Regression Metrics. In the next section, let\\u2019s take a closer look at each in turn. Metrics for Regression In this section, we will take a closer look at the popular metrics for regression models and how to calculate them for your predictive modeling project. Mean Squared Error Mean Squared Error, or MSE for short, is a popular error metric for regression problems. It is also an important loss function for algorithms fit or optimized using the least squares framing of a regression problem. Here \\u201cleast squares\\u201d refers to minimizing the mean squared error between predictions and expected values. The MSE is calculated as the mean or average of the squared differences between predicted and expected target values in a dataset. MSE = 1 \\/ N * sum for i to N (y_i \\u2013 yhat_i)^2 Where y_i is the i\\u2019th expected value in the dataset and yhat_i is the i\\u2019th predicted value. The difference between these two values is squared, which has the effect of removing the sign, resulting in a positive error value. The squaring also has the effect of inflating or magnifying large errors. That is, the larger the difference between the predicted and expected values, the larger the resulting squared positive error. This has the effect of \\u201cpunishing\\u201d models more for larger errors when MSE is used as a loss function. It also has the effect of \\u201cpunishing\\u201d models by inflating the average error score when used as a metric. We can create a plot to get a feeling for how the change in prediction error impacts the squared error. The example below gives a small contrived dataset of all 1.0 values and predictions that range from perfect (1.0) to wrong (0.0) by 0.1 increments. The squared error between each prediction and expected value is calculated and plotted to show the quadratic increase in squared error. ... # calculate error err = (expected[i] - predicted[i])**2 The complete example is listed below. # example of increase in mean squared error from matplotlib import pyplot from sklearn.metrics import mean_squared_error # real value expected = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0] # predicted value predicted = [1.0, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0.0] # calculate errors errors = list() for i in range(len(expected)): # calculate error err = (expected[i] - predicted[i])**2 # store error errors.append(err) # report error print('\\/prepre class=\\\"urvanov-syntax-highlighter-plain-tag\\\"\",\"247\":\"When the @OpenAI CLIP openai.com\\/blog\\/clip\\/ came out I got curious if it has adversarial examples. It turns out that it does, they are easy to find & generalize to semantically related adversary classes. Blog post: stanislavfort.github.io\\/2021\\u2026 & @GoogleColab: github.com\\/stanislavfort\\/Ope\\u2026\",\"4467\":\"\\\"Every moment is a fresh beginning.\\\" \\u2013 T.S. Eliot\",\"1525\":\"Garry Kasparov is considered by many to be the greatest chess player of all time. From 1986 until his retirement in 2005, he dominated the chess world, ranking world number 1 for most of those 19 years. While he has many historic matches against human chess players, in the long arc of history he may be remembered for his match again a machine, IBM\\u2019s Deep Blue. His initial victories and eventual loss to Deep Blue captivated the imagination of the world of what role Artificial Intelligence systems may play in our civilization\\u2019s future. That excitement inspired an entire generation of\",\"2330\":\"Here is a quick article on creating a recipe recommendation bot that can also recognize your ingredients with a picture! Small tip: Telegram API is super useful for small projects or POC! Your frontend takes only a day to be ready. [link] [comments]\",\"714\":\"Day 2 | November 18, 2020 Theme: Building a Pipeline from Research to Impact Donna Auguste, Auguste Research Group The Accessible Computer Science Education Fall Workshop was hosted by Microsoft, University of Washington CREATE, and University of Colorado\\u2019s Coleman Institute. It took place November 17-19, 2020 and consisted of three half-days of talks, discussions, and planning for new research dedicated to making Computer Science education learning experiences more accessible for people with disabilities. More information on this workshop can be found at https:\\/\\/www.microsoft.com\\/en-us\\/research\\/event\\/accessible-cs-education-fall-workshop\\/\",\"2322\":\"Hi all, I organise a slack group for a small team of ML enthusiasts including students and researchers from academia and industry. We originally got together due to our common interest in Graph Representation Learning (we hold an online weekly journal club on GRL) but all of us also work on a number of other ML specialisations including NLP and Computer Vision. We are a friendly group that try to help each other and we thought that others might like to join. If you feel like this is something you want to be involved in, feel free to join by clicking here. Cheers! [link] [comments]\",\"3748\":\"Hi everyone, I work at a financial services company and am responsible for the release of model output data to our clients and the quality assurance thereof, particular on the DS side of things. A key part of our process is comparing the data our models have generated for publicly traded companies for consecutive quarters, and in particular identifying if there is any significant change in the distribution of the model outputs or large amounts of outliers between quarters for said outputs. Currently, our process investigating the output data involves a great deal of data visualization, and displaying tables showing outliers in terms of quarter-on-quarter change for particular output metrics. However, in many cases, large changes in outputs can often be explained by other factors in the dataset (such as a noted change in methodology from a data vendor, or a substantial increase in revenue for a particular company, all of which are also part of the dataset being QA'd). As such, I was wondering if people could point me in the direction of an anomaly detection approach which (ideally) learns in an unsupervised manner, but doesn't attempt to find anomalies with respect to all features in the dataset, instead looking for anomalies in individual features which cannot be explained by changes in the remaining features of the dataset. I have looked into some anomaly detection techniques, but for the most part it seems that they look for outliers across all features, which does not fit my use case, as we are particularly interested in observing whether there are bugs in our modelling code causing undues shifts in the output values our models generate. I imagine a supervised way of solving this problem would be to build a regression model that is trained to predict the values of certain output metrics given the input data, and then identifying outliers by taking the delta of the predicted and real values. However, due to the changing nature of the inputs and outputs of our models, and the number of models we develop, I would prefer to use an unsupervised approach if possible. TLDR: Do you know of any anomaly detection approaches which consider the totality of the high-dimensional data fed into them, but exclusively look for anomalies on the basis of idiosyncratic values in individual features? Thanks in advance! [link] [comments]\",\"645\":\"Dan Gable is one of the greatest Olympic athletes and wrestling coaches of all time. Please support this podcast by checking out our sponsors: - Tryolabs: https:\\/\\/tryolabs.com\\/lex - ExpressVPN: https:\\/\\/expressvpn.com\\/lexpod and use code LexPod to get 3 months free - Grammarly: https:\\/\\/grammarly.com\\/lex to get 20% off premium - SimpliSafe: https:\\/\\/simplisafe.com\\/lex and use code LEX to get a free security camera EPISODE LINKS: Dan's Twitter: https:\\/\\/twitter.com\\/dannygable Dan's Website: https:\\/\\/dangable.com\\/ Dan's Books: https:\\/\\/amzn.to\\/2VK5nbn PODCAST INFO: Podcast website: https:\\/\\/lexfridman.com\\/podcast Apple Podcasts: https:\\/\\/apple.co\\/2lwqZIr Spotify: https:\\/\\/spoti.fi\\/2nEwCF8 RSS: https:\\/\\/lexfridman.com\\/feed\\/podcast\\/ Full episodes playlist: https:\\/\\/www.youtube.com\\/playlist?list=PLrAXtmErZgOdP_8GztsuKi9nrraNbKKp4 Clips playlist: https:\\/\\/www.youtube.com\\/playlist?list=PLrAXtmErZgOeciFP3CBCIEElOJeitOr41 OUTLINE: 0:00 - Introduction 2:56 - Russian wrestling 4:34 - Coaching the science, art, and toughness of wrestling 11:30 - The pain of defeat and the tattoo of a hawk clawing out the heart 14:29 - Roger Bannister and the 4 minute mile 17:35 - The dream of becoming an Olympic champion 20:03 - The day in 1972 of the Olympic final 23:35 - Sauna story 25:05 - Match against the Russian 30:38 - The role of fear in wrestling 35:40 - The line between physical wrestling and anger 40:18 - Tragic loss of Dan's sister 47:46 - The role of family in wrestling 53:08 - Wrestling being voted out of the Olympics 57:52 - To beat the best you must study the best 1:03:05 - The role of luck (Old Man and the Sea) CONNECT: - Subscribe to this YouTube channel - Twitter: https:\\/\\/twitter.com\\/lexfridman - LinkedIn: https:\\/\\/www.linkedin.com\\/in\\/lexfridman - Facebook: https:\\/\\/www.facebook.com\\/LexFridmanPage - Instagram: https:\\/\\/www.instagram.com\\/lexfridman - Medium: https:\\/\\/medium.com\\/@lexfridman - Support on Patreon: https:\\/\\/www.patreon.com\\/lexfridman\",\"1708\":\"Visualize Insights and Discover Driving Features in Lending Credit Risk Model for Loan Defaults Read the full story\",\"1629\":\"Read the full story\",\"1571\":\"George Hotz is the founder of Comma.ai, a machine learning based vehicle automation company. He is an outspoken personality in the field of AI and technology in general. He first gained recognition for being the first person to carrier-unlock an iPhone, and since then has done quite a few interesting things at the intersection of hardware and software. This conversation is part of the Artificial Intelligence podcast. If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman on Twitter, LinkedIn, Facebook, Medium, or YouTube where you can watch the video versions of these\",\"1146\":\"Featuring Daniel Litt. Check out Brilliant (get 20% off their premium service): https:\\/\\/brilliant.org\\/numberphile (sponsor) More links & stuff in full description below \\u2193\\u2193\\u2193 Daniel Litt: https:\\/\\/www.daniellitt.com\\/ See Daniel previously on Numberphile: https:\\/\\/youtu.be\\/eYfpSAxGakI Numberphile is supported by the Mathematical Sciences Research Institute (MSRI): http:\\/\\/bit.ly\\/MSRINumberphile We are also supported by Science Sandbox, a Simons Foundation initiative dedicated to engaging everyone with the process of science. https:\\/\\/www.simonsfoundation.org\\/outreach\\/science-sandbox\\/ And support from Math For America - https:\\/\\/www.mathforamerica.org\\/ NUMBERPHILE Website: http:\\/\\/www.numberphile.com\\/ Numberphile on Facebook: http:\\/\\/www.facebook.com\\/numberphile Numberphile tweets: https:\\/\\/twitter.com\\/numberphile Subscribe: http:\\/\\/bit.ly\\/Numberphile_Sub Video by Brady Haran and Pete McPartlan Patreon: http:\\/\\/www.patreon.com\\/numberphile Numberphile T-Shirts and Merch: https:\\/\\/teespring.com\\/stores\\/numberphile Brady's videos subreddit: http:\\/\\/www.reddit.com\\/r\\/BradyHaran\\/ Brady's latest videos across all channels: http:\\/\\/www.bradyharanblog.com\\/ Sign up for (occasional) emails: http:\\/\\/eepurl.com\\/YdjL9\",\"1521\":\"David Eagleman is a neuroscientist at Stanford. Support this podcast by supporting our sponsors: \\u2013 Athletic Greens: https:\\/\\/athleticgreens.com\\/lex \\u2013 BetterHelp: https:\\/\\/betterhelp.com\\/lex \\u2013 Cash App: download app & use code \\u201cLexPodcast\\u201d Episode links: David\\u2019s Website: https:\\/\\/www.eagleman.com\\/ David\\u2019s Twitter: https:\\/\\/twitter.com\\/davideagleman Livewired (book): https:\\/\\/amzn.to\\/3ba4ezv If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/podcast or connect with @lexfridman on Twitter, LinkedIn, Facebook, Medium, or YouTube where you can watch the video versions of these conversations. If you enjoy the podcast, please rate it 5 stars on Apple Podcasts, follow on Spotify, or support it on Patreon. Here\\u2019s the outline of\",\"3855\":\"This was definitely awesome! @kenneth0stanley's tutorial at ICML was the kickoff that made me put more work into my channel and it's a great honor to talk to him. nitter.net\\/MLStreetTalk\\/status\\/1351709386612363267#m\",\"1482\":\"Sheldon Solomon is a social psychologist, a philosopher, co-developer of Terror Management Theory, co-author of The Worm at the Core. Please support this channel by supporting our sponsors: \\u2013 Blinkist: https:\\/\\/blinkist.com\\/lex \\u2013 ExpressVPN at https:\\/\\/www.expressvpn.com\\/lexpod \\u2013 Cash App: download app & use code \\u201cLexPodcast\\u201d Episode links: Sheldon\\u2019s Website: https:\\/\\/www.skidmore.edu\\/psychology\\/faculty\\/solomon.php The Worm at the Core (book): https:\\/\\/amzn.to\\/31hQAXH Denial of Death (book): https:\\/\\/amzn.to\\/329Zxl4 If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/podcast or connect with @lexfridman on Twitter, LinkedIn, Facebook, Medium, or YouTube where you can watch the video versions of these conversations. If you enjoy the podcast, please\",\"4093\":\"I was recently working on a project and found out that that a good chunk of our dataset might be useless or not as useful as we originally thought (40-60%). Luckily we have a ton of data, so we should still be able to do something with it, but I wanted to know how common this is [link] [comments]\",\"702\":\"Day 1 | December 1, 2020 Neel Joshi, Principal Research Manager at Microsoft, and Gonzalo Ramos, Principal Researcher at Microsoft, give a warm welcome to the PhD students in attendance at the third Microsoft PhD Summit. Following the welcome is a Fireside Chat with Johannes Gehrke, Technical Fellow & Managing Director of Research at Microsoft Redmond. Microsoft's third PhD Summit was a two-day virtual workshop. It was an opportunity for top PhD students to enhance their skills, build a network, and discuss research within a community of peers and notable Microsoft researchers. Speakers: Neel Joshi, Principal Research Manager, Microsoft Gonzalo Ramos, Principal Researcher, Microsoft Johannes Gehrke, Technical Fellow & Managing Director, Research at Redmond More information on this workshop can be found at https:\\/\\/www.microsoft.com\\/en-us\\/research\\/event\\/phd-summit-2020\\/\",\"1549\":\"Gustav Soderstrom is the Chief Research & Development Officer at Spotify, leading Product, Design, Data, Technology & Engineering teams. This conversation is part of the Artificial Intelligence podcast. If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman on Twitter, LinkedIn, Facebook, Medium, or YouTube where you can watch the video versions of these conversations. If you enjoy the podcast, please rate it 5 stars on iTunes or support it on Patreon.\",\"1148\":\"Neil Sloane from the OEIS discusses the Choix de Bruxelles. Check out Brilliant (get 20% off their premium service): https:\\/\\/brilliant.org\\/numberphile (sponsor) More links & stuff in full description below \\u2193\\u2193\\u2193 Neil Sloane founded the runs the OEIS: https:\\/\\/oeis.org\\/ Brussels Choice on the OEIS: https:\\/\\/oeis.org\\/A323454 Neil Sloane playlist on Numberphile: http:\\/\\/bit.ly\\/Sloane_Numberphile Neil Sloane on the Numberphile podcast: https:\\/\\/youtu.be\\/mNk_MfFKnuY Numberphile is supported by the Mathematical Sciences Research Institute (MSRI): http:\\/\\/bit.ly\\/MSRINumberphile We are also supported by Science Sandbox, a Simons Foundation initiative dedicated to engaging everyone with the process of science. https:\\/\\/www.simonsfoundation.org\\/outreach\\/science-sandbox\\/ And support from Math For America - https:\\/\\/www.mathforamerica.org\\/ NUMBERPHILE Website: http:\\/\\/www.numberphile.com\\/ Numberphile on Facebook: http:\\/\\/www.facebook.com\\/numberphile Numberphile tweets: https:\\/\\/twitter.com\\/numberphile Subscribe: http:\\/\\/bit.ly\\/Numberphile_Sub Video by Brady Haran and Pete McPartlan Patreon: http:\\/\\/www.patreon.com\\/numberphile Numberphile T-Shirts and Merch: https:\\/\\/teespring.com\\/stores\\/numberphile Brady's videos subreddit: http:\\/\\/www.reddit.com\\/r\\/BradyHaran\\/ Brady's latest videos across all channels: http:\\/\\/www.bradyharanblog.com\\/ Sign up for (occasional) emails: http:\\/\\/eepurl.com\\/YdjL9\",\"1478\":\"Chris Lattner is a senior director at Google working on several projects including CPU, GPU, TPU accelerators for TensorFlow, Swift for TensorFlow, and all kinds of machine learning compiler magic going on behind the scenes. He is one of the top experts in the world on compiler technologies, which means he deeply understands the intricacies of how hardware and software come together to create efficient code. He created the LLVM compiler infrastructure project and the CLang compiler. He led major engineering efforts at Apple, including the creation of the Swift programming language. He also briefly spent time at Tesla as\",\"1699\":\"Read the full story\",\"1548\":\"Karl Friston is one of the greatest neuroscientists in history, cited over 245,000 times, known for many influential ideas in brain imaging, neuroscience, and theoretical neurobiology, including the fascinating idea of the free-energy principle for action and perception. Support this podcast by signing up with these sponsors: \\u2013 Cash App \\u2013 use code \\u201cLexPodcast\\u201d and download: \\u2013 Cash App (App Store): https:\\/\\/apple.co\\/2sPrUHe \\u2013 Cash App (Google Play): https:\\/\\/bit.ly\\/2MlvP5w EPISODE LINKS: Karl\\u2019s Website: https:\\/\\/www.fil.ion.ucl.ac.uk\\/~karl\\/ Karl\\u2019s Wiki: https:\\/\\/en.wikipedia.org\\/wiki\\/Karl_J._Friston This conversation is part of the Artificial Intelligence podcast. If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman\",\"1586\":\"David Patterson is a Turing award winner and professor of computer science at Berkeley. He is known for pioneering contributions to RISC processor architecture used by 99% of new chips today and for co-creating RAID storage. The impact that these two lines of research and development have had on our world is immeasurable. He is also one of the great educators of computer science in the world. His book with John Hennessy \\u201cComputer Architecture: A Quantitative Approach\\u201d is how I first learned about and was humbled by the inner workings of machines at the lowest level. Support this podcast by\",\"2332\":\"Hi fellas. I don't know if I am under a wrong impression, but it didn't succeed in finding a systematic way for benchmarking GPUs. Here is what I'd need: A way to benchmark the raw computational power of the cards, in each field (vision, nlp, tabular) leveraging both dense & sparse matrices. A way to benchmark how good the cards are communicating with each other *and* with the CPU. \\u200b Can you offer any suggestion? Thanks. [link] [comments]\",\"3143\":\"There is a new minor release of H2O that introduces two useful improvements to our XGBoost integration: interaction constraints and feature interactions. Interaction Constraints Feature interaction constraints allow users to decide which variables are allowed to interact and which are not. Potential benefits: Better predictive performance from focusing on interactions that work \\u2013 whether through domain-specific knowledge or algorithms that rank interactions Less noise in predictions; better generalization More control given to the user on what the model can fit. For example, the user may want to exclude some interactions even if they perform well due to regulatory constraints (Source: https:\\/\\/xgboost.readthedocs.io\\/en\\/latest\\/tutorials\\/feature_interaction_constraint.html) The H2O documentation is available here. XGBFI-like Tool for Revealing Feature Interactions We have implemented ranks of features and feature interactions by various measures in XGBFI style. Thanks to this tool, H2O provides insights into higher-order interactions between features in trees all in a user-friendly manner. Additionally, leaf statistics and split value histograms are provided. The measures used are either one of: Gain implies the relative contribution of the corresponding feature to the model calculated by taking each feature\\u2019s contribution for each tree in the model. A higher value of this metric when compared to another feature implies it is more important for generating a prediction. Cover is a metric to measure the number of observations affected by the split. Counted over the specific feature it measures the relative quantity of observations concerned by a feature. Frequency (FScore) is the number of times a feature is used in all generated trees. Please note that it does not take the tree-depth nor tree-index of splits a feature occurs into consideration, neither the amount of possible splits of a feature. Hence, it is often suboptimal measure for importance or their averaged \\/ weighed \\/ ranked alternatives. The H2O documentation is available here. Example The Jupyter notebook demo with all example codes presented below is available here. Train XGBoostEstimator with interaction_constraints parameter: # start h2o import h2o h2o.init() from h2o.estimators.xgboost import * # check if the H2O XGBoostEstimator is available assert H2OXGBoostEstimator.available() is True # import data data = h2o.import_file(path = \\\"..\\/..\\/smalldata\\/logreg\\/prostate.csv\\\") x = list(range(1, data.ncol-2)) y = data.names[len(data.names) - 1] ntree = 5 h2o_params = { 'eta': 0.3, 'max_depth': 3, 'ntrees': ntree, 'tree_method': 'hist' } # define interactions as a list of list of names of colums # the lists defines allowed interaction # the interactions of each column with itself are always allowed # so you cannot specified list with one column e.g. [\\\"PSA\\\"] h2o_params[\\\"interaction_constraints\\\"] = [[\\\"CAPSULE\\\", \\\"AGE\\\"], [\\\"PSA\\\", \\\"DPROS\\\"]] # train h2o XGBoost model h2o_model = H2OXGBoostEstimator(**h2o_params) h2o_model.train(x=x, y=y, training_frame=data) The result: Display feature interactions: # calculate multi-level feature interactions h2o_model.feature_interaction() Credits This new H2O release is brought to you by Veronika Maurerova, Zuzana Olajcova, and Hannah Tillman. How to Get Started? Download H2O-3 from here and follow the steps in this example notebook. You can also check out our training center for both self-paced tutorials and instructor-led courses. The post New Improvements in H2O 3.32.0.2 appeared first on Open Source Leader in AI and ML.\",\"1487\":\"Ben Goertzel is one of the most interesting minds in the artificial intelligence community. He is the founder of SingularityNET, designer of OpenCog AI framework, formerly a director of the Machine Intelligence Research Institute, Chief Scientist of Hanson Robotics, the company that created the Sophia Robot. He has been a central figure in the AGI community for many years, including in the Conference on Artificial General Intelligence. Support this podcast by supporting these sponsors: \\u2013 Jordan Harbinger Show: https:\\/\\/jordanharbinger.com\\/lex\\/ \\u2013 MasterClass: https:\\/\\/masterclass.com\\/lex This conversation is part of the Artificial Intelligence podcast. If you would like to get more information about this\",\"1707\":\"Read the full story\",\"1443\":\"Daphne Koller is a professor of computer science at Stanford University, a co-founder of Coursera with Andrew Ng and Founder and CEO of insitro, a company at the intersection of machine learning and biomedicine. Support this podcast by signing up with these sponsors: \\u2013 Cash App \\u2013 use code \\u201cLexPodcast\\u201d and download: \\u2013 Cash App (App Store): https:\\/\\/apple.co\\/2sPrUHe \\u2013 Cash App (Google Play): https:\\/\\/bit.ly\\/2MlvP5w EPISODE LINKS: Daphne\\u2019s Twitter: https:\\/\\/twitter.com\\/daphnekoller Daphne\\u2019s Website: https:\\/\\/ai.stanford.edu\\/users\\/koller\\/index.html Insitro: http:\\/\\/insitro.com This conversation is part of the Artificial Intelligence podcast. If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman on Twitter,\",\"1614\":\"Read the full story\",\"1618\":\"This blog post is a refresh of a talk that James and I gave at Strata back in 2017. Why recap a 3-year-old conference talk? Well, the core ideas have aged well, we\\u2019ve never actually put them into writing before, and we\\u2019ve learned some new things in the meantime. Enjoy! Read the full story\",\"1709\":\"Read the full story\",\"971\":\"Banned from all social media, Trump has found a way to communicate with me through duolingo\",\"2327\":\"Hi guys, recently I came along to read this bizarre (to me) scientific article published in Nature: https:\\/\\/www.nature.com\\/articles\\/s41598-020-79310-1 It claims that a machine learning model is capable of predicting people's political orientation from their facial expressions. What's even worse the images are collected from Facebook and dating apps. So, how does the face of a conservativist or liberal look like? Political views are not the natural way in which humans evolved since time immemorial, thus it must be not possible to be present on someone's face. This really reminds me of the Clever Hans effect and detecting criminals from images, while only detecting a presence or absence of a smile: https:\\/\\/www.callingbullshit.org\\/case_studies\\/case_study_criminal_machine_learning.html So, society is polarized, so let's make it even more :) What are your thoughts on this? [link] [comments]\",\"3138\":\"In conversation with G\\u00e1bor Fodor: A Data Scientist at H2O.ai and a Kaggle Competitions\\u2019 Grandmaster. In this series of interviews, I present the stories of established Data Scientists and Kaggle Grandmasters at H2O.ai, who share their journey, inspirations, and accomplishments. These interviews are intended to motivate and encourage others who want to understand what it takes to be a Kaggle Grandmaster. In this interview, I shall be sharing my interaction with G\\u00e1bor Fodor, better known as Beluga in Kaggle world. He is a Kaggle Competitions Grandmaster and a Data Scientist at H2O.ai. Gabor, who hails from Hungary, holds a master\\u2019s degree in Mathematics as well as Computer Engineering and has around ten years of experience in the Data Science domain. He joined Kaggle nine years ago and since then has made quite a mark there. His best global rank is 4th for competitions and 7th for notebooks. Here is also a link to G\\u00e1bor\\u2019s recent interview at CTDS.show where he discusses his 10th place solo gold in Cornell Birdcall competition on Kaggle Here is an excerpt from my conversation with G\\u00e1bor: Q: You have a background in Mathematics. How did the transition from academia to industry happen? G\\u00e1bor: Doing a Master\\u2019s in mathematics with a stochastics major certainly provided a strong background (discrete math, probability theory, statistics, stochastic processes, etc.), although the courses mainly focused on theory. Fortunately, I was free to take some additional courses, and as a result, I got to learn about programming & data mining as well. During my final year, I had a chance to intern as a Data Mining trainee in the telco industry. It was quite interesting to retrain and improve the old drifted churn models. However, the most valuable part was that I had direct access to their data warehouse, and I could learn and practice SQL with real-world data and business problems. After the internship, I stayed at the company and became a full-time data analyst. Since then, I have had a chance to work in different industries working on varied types of business problems. Q: How did you get interested in Machine Learning? Kaggle Competition Tutorial being presented by Gabor during Kaggle Days China, 2020 G\\u00e1bor: I immensely enjoyed my data mining courses. My first data mining competition was in 2009, and it was quite fun. Then I found Kaggle and got addicted forever. At that time, I already had a full-time job and just started a new master\\u2019s in computer science, so finding time for new Kaggle challenges was not always easy. But the learning opportunity was enormous, and I could not resist trying to solve those unique data-driven problems. Q: How hard is it to become a Kaggle Grandmaster? What initially attracted you to Kaggle, and When did the first win come your way? G\\u00e1bor\\u2019s Kaggle Profile G\\u00e1bor: Reaching a Grandmaster\\u2019s status in competitions is undoubtedly demanding. One needs five gold medals in different competitions, and at least one has to be a solo gold. It requires a lot of effort and hard work to earn gold in every competition, for instance, him. My first competition win came in 2013. It was a small research competition with 81 teams. The task was to recognize bird species in audio recordings. We only had a few hundred audio files for training at that time, and we did not have all the comfortable deep learning tools. I was able to win the competition with template matching on the spectrograms and using random forests only. The competitions became a bit more difficult since the good old days as the Kaggle community grew. Nowadays, it is hard to find competitions with less than a thousand teams. Q: As a Data Scientist at H2O.ai, what are your roles, and in which specific areas do you work? G\\u00e1bor along with some of the GrandMasters at H2O.ai G\\u00e1bor: I just joined H2O.ai in August, and I like the flexibility to work on different projects here. Besides helping customers using H2O Driverless AI during POCs, I also create H2O Wave apps and test new Driverless AI features. Q: What are some of the best things you have learned via Kaggle that you apply in your professional work at H2O.ai? G\\u00e1bor: I hear way too often that in Kaggle competitions, participants fight over the 4th decimals on the leaderboard, and the differences are not significant. Well, there are much bigger victories (e.g., in the recently finished Lyft Motion Prediction competition where Philipp and his team won by 8% improvement over the second team). Even if the race is much closer, you have to turn all the rocks and squeeze every possible gain from your features and models. In my experience, that also teaches you how to get a robust baseline model fast. The other criticism that I hear is that the competitions reward overfitting and data leaks. While I agree that data leaks could be a significant issue, and I did have to exploit them to win competitions, overfitting is not rewarded at Kaggle. Quite the opposite! During the competition, you don\\u2019t receive feedback about the final test set. I saw (and have experienced) quite brutal shake-ups where only the best validation strategies and most stable models survived. Data leaks are quite common in the real world too. When you see a \\u2014 too good to be true- AUC result, you should start to think immediately about the cause. Seeing all the possible data leaks in previous Kaggle challenges helps to debug the machine learning pipeline quicker. Q: If you were to team up with grandmasters at H2O.ai, who would they be and why? G\\u00e1bor: Good question . I recently created the membership network of the Kaggle team at H2O.ai. While we are mostly in a largely connected ecosystem, I did not team up directly with anyone before. I can\\u2019t pick a single person as we have so many talented kagglers but probably will team up with some of them in 2021. Q: The Data Science domain is rapidly evolving. How do you manage to keep up with all the latest developments? Gabor presenting during Kaggle Days Paris in 2019. G\\u00e1bor: I think it is impossible to keep up with everything. Besides the fun, I like Kaggle competitions because they show what tools work the best for specific problems. You can learn a lot just by reading the competition winning solutions. But trying to apply those tips and tricks in the next competition will teach you a lot more. There is quite a few stuff to catch up for me regarding Natural Language Processing or Reinforcement Learning. Fortunately, the team at H2O.ai has experts in every field. On the other hand, it also means that the tools are getting better. In the recent Cornell Birdcall Competition, I could train models with a few hundred code lines with PyTorch. Or look at Driverless AI; with a few clicks, you could solve all sorts of supervised machine learning problems. Q: A word of advice for the Data Science aspirants who have just started or wish to start their Data Science journey? G\\u00e1bor: Don\\u2019t be afraid to start and prepare for the long run. The community is enormous and willing to share. If you already learned the basics and want to get your hands dirty, I can only recommend participating in Kaggle competitions. There are personally a lot of takeaways from this interaction. Firstly, Data Science is an area where one needs to be self-motivated and eager to learn at every stage. Secondly, there is always so much to learn from every machine learning competition, even if you perform well or not. The important thing is to identify your weak points and work on them while leveraging your strengths. In the end, the community around you is always ready to help, and the flourishing Kaggle community is a testimony to that fact. Originally published here. The post Grandmaster Series: The inspiring journey of the \\u2018Beluga\\u2019 of Kaggle World \\ud83d\\udc0b appeared first on Open Source Leader in AI and ML.\",\"4289\":\"This week Dr. Tim Scarfe, Sayak Paul and Yannic Kilcher speak with Dr. Simon Kornblith from Google Brain (Ph.D from MIT). Simon is trying to understand how neural nets do what they do. Simon was the second author on the seminal Google AI SimCLR paper. We also cover \\\"Do Wide and Deep Networks learn the same things?\\\", \\\"Whats in a Loss function for Image Classification?\\\", and \\\"Big Self-supervised models are strong semi-supervised learners\\\". Simon used to be a neuroscientist and also gives us the story of his unique journey into ML. 00:00:00 Show Teaser \\/ or \\\"short version\\\" 00:18:34 Show intro 00:22:11 Relationship between neuroscience and machine learning 00:29:28 Similarity analysis and evolution of representations in Neural Networks 00:39:55 Expressability of NNs 00:42:33 Whats in a loss function for image classification 00:46:52 Loss function implications for transfer learning 00:50:44 SimCLR paper 01:00:19 Contrast SimCLR to BYOL 01:01:43 Data augmentation 01:06:35 Universality of image representations 01:09:25 Universality of augmentations 01:23:04 GPT-3 01:25:09 GANs for data augmentation?? 01:26:50 Julia language @skornblith https:\\/\\/www.linkedin.com\\/in\\/simon-kornblith-54b2033a\\/ https:\\/\\/arxiv.org\\/abs\\/2010.15327 Do Wide and Deep Networks Learn the Same Things? Uncovering How Neural Network Representations Vary with Width and Depth https:\\/\\/arxiv.org\\/abs\\/2010.16402 What's in a Loss Function for Image Classification? https:\\/\\/arxiv.org\\/abs\\/2002.05709 A Simple Framework for Contrastive Learning of Visual Representations https:\\/\\/arxiv.org\\/abs\\/2006.10029 Big Self-Supervised Models are Strong Semi-Supervised Learners\",\"1515\":\"Peter Norvig is a research director at Google and the co-author with Stuart Russell of the book Artificial Intelligence: A Modern Approach that educated and inspired a whole generation of researchers including myself to get into the field. This conversation is part of the Artificial Intelligence podcast. If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman on Twitter, LinkedIn, Facebook, Medium, or YouTube where you can watch the video versions of these conversations. If you enjoy the podcast, please rate it 5 stars on iTunes or support it on Patreon. Here\\u2019s the outline\",\"1694\":\"Like lists comprehensions and lambda functions python one line codes can save a lot of time and space so how you can master them? Read the full story\",\"1499\":\"Judea Pearl is a professor at UCLA and a winner of the Turing Award, that\\u2019s generally recognized as the Nobel Prize of computing. He is one of the seminal figures in the field of artificial intelligence, computer science, and statistics. He has developed and championed probabilistic approaches to AI, including Bayesian Networks and profound ideas in causality in general. These ideas are important not just for AI, but to our understanding and practice of science. But in the field of AI, the idea of causality, cause and effect, to many, lies at the core of what is currently missing and\",\"1455\":\"Paul Krugman is a Nobel Prize winner in economics, professor at CUNY, and columnist at the New York Times. His academic work centers around international economics, economic geography, liquidity traps, and currency crises. This conversation is part of the Artificial Intelligence podcast. If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman on Twitter, LinkedIn, Facebook, Medium, or YouTube where you can watch the video versions of these conversations. If you enjoy the podcast, please rate it 5 stars on Apple Podcasts, follow on Spotify, or support it on Patreon. This episode is\",\"264\":\"\\ud83d\\udd25Deep Learning MEME REVIEW Episode 2 is here\\ud83d\\udd25 Antonio and I go over the State of the Art memes of 2020 \\ud83d\\udcaa Check it out! invidious.snopyta.org\\/7DGlElSVYGo\",\"1488\":\"The coronavirus pandemic is a global tragedy, but it is also a moment that unites us, that reveals the strength of our community, the human capacity to be compassionate to each other and to work hard in the face of danger. In this episode I describe what, to me, might be 7 levels of attack on our society and how we can fight back. For each level, I describe our pain, our challenge, and our hope for a positive future on the other side. Fill out this one-question survey on whether you want to see more solo episodes like these:\",\"640\":\"Matthew W. Johnson is a professor and psychedelics researcher at Johns Hopkins. Please support this podcast by checking out our sponsors: - Brave: https:\\/\\/brave.com\\/lex - Neuro: https:\\/\\/www.getneuro.com and use code LEX to get 15% off - Four Sigmatic: https:\\/\\/foursigmatic.com\\/lex and use code LexPod to get up to 60% off - Cash App: https:\\/\\/cash.app\\/ and use code LexPodcast to get $10 EPISODE LINKS: Matt's Twitter: https:\\/\\/twitter.com\\/Drug_Researcher Matt's Website: https:\\/\\/www.hopkinsmedicine.org\\/profiles\\/results\\/directory\\/profile\\/0800020\\/matthew-johnson Study Website: https:\\/\\/hopkinspsychedelic.org\\/ PODCAST INFO: Podcast website: https:\\/\\/lexfridman.com\\/podcast Apple Podcasts: https:\\/\\/apple.co\\/2lwqZIr Spotify: https:\\/\\/spoti.fi\\/2nEwCF8 RSS: https:\\/\\/lexfridman.com\\/feed\\/podcast\\/ Full episodes playlist: https:\\/\\/www.youtube.com\\/playlist?list=PLrAXtmErZgOdP_8GztsuKi9nrraNbKKp4 Clips playlist: https:\\/\\/www.youtube.com\\/playlist?list=PLrAXtmErZgOeciFP3CBCIEElOJeitOr41 OUTLINE: 0:00 - Introduction 2:02 - Introduction to psychedelics 18:04 - Psychedelics expand the mind 21:16 - The priors we bring to the psychedelic experience 25:11 - Elon Musk and first principles thinking 35:41 - DMT 47:03 - Joe Rogan and DMT 53:11 - The nature of drug addiction 1:07:00 - The economics of drug pricing 1:13:15 - Should we legalize all drugs? 1:25:18 - What is the most dangerous drug? 1:27:52 - Does drug prohibition work? 1:31:46 - Cocaine and sex 1:38:46 - Risky sexual decisions 1:49:43 - Psilocybin helping people quit smoking 1:56:01 - Young Jamie 2:18:09 - Participating in a study 2:25:28 - Psychedelics and the human mind 2:32:51 - The future of psychedelics 2:35:32 - Neuralink 2:45:05 - Consciousness 2:57:46 - Panpsychism 3:07:51 - Aliens and DMT 3:17:55 - Mortality 3:27:44 - Meaning of life CONNECT: - Subscribe to this YouTube channel - Twitter: https:\\/\\/twitter.com\\/lexfridman - LinkedIn: https:\\/\\/www.linkedin.com\\/in\\/lexfridman - Facebook: https:\\/\\/www.facebook.com\\/LexFridmanPage - Instagram: https:\\/\\/www.instagram.com\\/lexfridman - Medium: https:\\/\\/medium.com\\/@lexfridman - Support on Patreon: https:\\/\\/www.patreon.com\\/lexfridman\",\"1520\":\"Simon Sinek is an author of several books including Start With Why, Leaders Eat Last, and his latest The Infinite Game. He is one of the best communicators of what it takes to be a good leader, to inspire, and to build businesses that solve big difficult challenges. Support this podcast by signing up with these sponsors: \\u2013 MasterClass: https:\\/\\/masterclass.com\\/lex \\u2013 Cash App \\u2013 use code \\u201cLexPodcast\\u201d and download: \\u2013 Cash App (App Store): https:\\/\\/apple.co\\/2sPrUHe \\u2013 Cash App (Google Play): https:\\/\\/bit.ly\\/2MlvP5w EPISODE LINKS: Simon twitter: https:\\/\\/twitter.com\\/simonsinek Simon facebook: https:\\/\\/www.facebook.com\\/simonsinek Simon website: https:\\/\\/simonsinek.com\\/ Books: \\u2013 Infinite Game: https:\\/\\/amzn.to\\/2WxBH1i \\u2013 Leaders Eat\",\"1639\":\"For those looking to analyze crime rates or trends over a specific area or time period, we have compiled a list of the 16 best crime datasets made available for public use. Read the full story\",\"1472\":\"William MacAskill is a philosopher, ethicist, and one of the originators of the effective altruism movement. His research focuses on the fundamentals of effective altruism \\u2013 the use of evidence and reason to help others by as much as possible with our time and money, with a particular concentration on how to act given moral uncertainty. He is the author of Doing Good Better \\u2013 Effective Altruism and a Radical New Way to Make a Difference. He is a co-founder and the President of the Centre for Effective Altruism (CEA) that encourages people to commit to donate at least 10%\",\"1688\":\"Read the full story\",\"1226\":\"The Chain Rule is a method for finding complex derivatives and is used all the time in Statistics and Machine Learning. This video breaks it down into its two simple pieces and shows you how they easily come together. We then use the Chain Rule to solve a common Machine Learning problem - optimizing the Residual Squared Loss Function. \\u2b50 NOTE: When I code, I use Kite, a free AI-powered coding assistant that will help you code faster and smarter. The Kite plugin integrates with all the top editors and IDEs to give you smart completions and documentation while you\\u2019re typing. I love it! https:\\/\\/www.kite.com\\/get-kite\\/?utm_medium=referral&utm_source=youtube&utm_campaign=statquest&utm_content=description-only For a complete index of all the StatQuest videos, check out: https:\\/\\/statquest.org\\/video-index\\/ If you'd like to support StatQuest, please consider... Patreon: https:\\/\\/www.patreon.com\\/statquest ...or... YouTube Membership: https:\\/\\/www.youtube.com\\/channel\\/UCtYLUTtgS3k1Fg4y5tAhLbw\\/join ...a cool StatQuest t-shirt or sweatshirt (USA\\/Europe): https:\\/\\/teespring.com\\/stores\\/statquest (everywhere): https:\\/\\/www.redbubble.com\\/people\\/starmer\\/works\\/40421224-statquest-double-bam?asc=u&p=t-shirt ...buying one or two of my songs (or go large and get a whole album!) https:\\/\\/joshuastarmer.bandcamp.com\\/ ...or just donating to StatQuest! https:\\/\\/www.paypal.me\\/statquest Lastly, if you want to keep up with me as I research and create new StatQuests, follow me on twitter: https:\\/\\/twitter.com\\/joshuastarmer 0:00 Awesome song and introduction 2:02 A super simple example 6:32 A slightly more complicated example 9:16 The Chain Rule when the relationship is not obvious 11:47 The Chain Rule for the Residual Sum of Squares #StatQuest #TheChainRule\",\"3191\":\"Function optimization involves finding the input that results in the optimal value from an objective function. Optimization algorithms navigate the search space of input variables in order to locate the optima, and both the shape of the objective function and behavior of the algorithm in the search space are opaque on real-world problems. As such, it is common to study optimization algorithms using simple low-dimensional functions that can be easily visualized directly. Additionally, the samples in the input space of these simple functions made by an optimization algorithm can be visualized with their appropriate context. Visualization of lower-dimensional functions and algorithm behavior on those functions can help to develop the intuitions that can carry over to more complex higher-dimensional function optimization problems later. In this tutorial, you will discover how to create visualizations for function optimization in Python. After completing this tutorial, you will know: Visualization is an important tool when studying function optimization algorithms. How to visualize one-dimensional functions and samples using line plots. How to visualize two-dimensional functions and samples using contour and surface plots. Let\\u2019s get started. Visualization for Function Optimization in Python Photo by Nathalie, some rights reserved. Tutorial Overview This tutorial is divided into three parts; they are: Visualization for Function Optimization Visualize 1D Function Optimization Test Function Sample Test Function Line Plot of Test Function Scatter Plot of Test Function Line Plot with Marked Optima Line Plot with Samples Visualize 2D Function Optimization Test Function Sample Test Function Contour Plot of Test Function Filled Contour Plot of Test Function Filled Contour Plot of Test Function with Samples Surface Plot of Test Function Visualization for Function Optimization Function optimization is a field of mathematics concerned with finding the inputs to a function that result in the optimal output for the function, typically a minimum or maximum value. Optimization may be straightforward for simple differential functions where the solution can be calculated analytically. However, most functions we\\u2019re interested in solving in applied machine learning may or may not be well behaved and may be complex, nonlinear, multivariate, and non-differentiable. As such, it is important to have an understanding of a wide range of different algorithms that can be used to address function optimization problems. An important aspect of studying function optimization is understanding the objective function that is being optimized and understanding the behavior of an optimization algorithm over time. Visualization plays an important role when getting started with function optimization. We can select simple and well-understood test functions to study optimization algorithms. These simple functions can be plotted to understand the relationship between the input to the objective function and the output of the objective function and highlighting hills, valleys, and optima. In addition, the samples selected from the search space by an optimization algorithm can also be plotted on top of plots of the objective function. These plots of algorithm behavior can provide insight and intuition into how specific optimization algorithms work and navigate a search space that can generalize to new problems in the future. Typically, one-dimensional or two-dimensional functions are chosen to study optimization algorithms as they are easy to visualize using standard plots, like line plots and surface plots. We will explore both in this tutorial. First, let\\u2019s explore how we might visualize a one-dimensional function optimization. Visualize 1D Function Optimization A one-dimensional function takes a single input variable and outputs the evaluation of that input variable. Input variables are typically continuous, represented by a real-valued floating-point value. Often, the input domain is unconstrained, although for test problems we impose a domain of interest. Test Function In this case we will explore function visualization with a simple x^2 objective function: f(x) = x^2 This has an optimal value with an input of x=0.0, which equals 0.0. The example below implements this objective function and evaluates a single input. # example of a 1d objective function # objective function def objective(x): return x**2.0 # evaluate inputs to the objective function x = 4.0 result = objective(x) print('f(%.3f) = %.3f' % (x, result)) Running the example evaluates the value 4.0 with the objective function, which equals 16.0. f(4.000) = 16.000 Sample the Test Function The first thing we might want to do with a new function is define an input range of interest and sample the domain of interest using a uniform grid. This sample will provide the basis for generating a plot later. In this case, we will define a domain of interest around the optima of x=0.0 from x=-5.0 to x=5.0 and sample a grid of values in this range with 0.1 increments, such as -5.0, -4.9, -4.8, etc. ... # define range for input r_min, r_max = -5.0, 5.0 # sample input range uniformly at 0.1 increments inputs = arange(r_min, r_max, 0.1) # summarize some of the input domain print(inputs[:5]) We can then evaluate each of the x values in our sample. ... # compute targets results = objective(inputs) # summarize some of the results print(results[:5]) Finally, we can check some of the input and their corresponding outputs. ... # create a mapping of some inputs to some results for i in range(5): print('f(%.3f) = %.3f' % (inputs[i], results[i])) Tying this together, the complete example of sampling the input space and evaluating all points in the sample is listed below. # sample 1d objective function from numpy import arange # objective function def objective(x): return x**2.0 # define range for input r_min, r_max = -5.0, 5.0 # sample input range uniformly at 0.1 increments inputs = arange(r_min, r_max, 0.1) # summarize some of the input domain print(inputs[:5]) # compute targets results = objective(inputs) # summarize some of the results print(results[:5]) # create a mapping of some inputs to some results for i in range(5): print('f(%.3f) = %.3f' % (inputs[i], results[i])) Running the example first generates a uniform sample of input points as we expected. The input points are then evaluated using the objective function and finally, we can see a simple mapping of inputs to outputs of the objective function. [-5. -4.9 -4.8 -4.7 -4.6] [25. 24.01 23.04 22.09 21.16] f(-5.000) = 25.000 f(-4.900) = 24.010 f(-4.800) = 23.040 f(-4.700) = 22.090 f(-4.600) = 21.160 Now that we have some confidence in generating a sample of inputs and evaluating them with the objective function, we can look at generating plots of the function. Line Plot of Test Function We could sample the input space randomly, but the benefit of a uniform line or grid of points is that it can be used to generate a smooth plot. It is smooth because the points in the input space are ordered from smallest to largest. This ordering is important as we expect (hope) that the output of the objective function has a similar smooth relationship between values, e.g. small changes in input result in locally consistent (smooth) changes in the output of the function. In this case, we can use the samples to generate a line plot of the objective function with the input points (x) on the x-axis of the plot and the objective function output (results) on the y-axis of the plot. ... # create a line plot of input vs result pyplot.plot(inputs, results) # show the plot pyplot.show() Tying this together, the complete example is listed below. # line plot of input vs result for a 1d objective function from numpy import arange from matplotlib import pyplot # objective function def objective(x): return x**2.0 # define range for input r_min, r_max = -5.0, 5.0 # sample input range uniformly at 0.1 increments inputs = arange(r_min, r_max, 0.1) # compute targets results = objective(inputs) # create a line plot of input vs result pyplot.plot(inputs, results) # show the plot pyplot.show() Running the example creates a line plot of the objective function. We can see that the function has a large U-shape, called a parabola. This is a common shape when studying curves, e.g. the study of calculus. Line Plot of a One-Dimensional Function Scatter Plot of Test Function The line is a construct. It is not really the function, just a smooth summary of the function. Always keep this in mind. Recall that we, in fact, generated a sample of points in the input space and corresponding evaluation of those points. As such, it would be more accurate to create a scatter plot of points; for example: # scatter plot of input vs result for a 1d objective function from numpy import arange from matplotlib import pyplot # objective function def objective(x): return x**2.0 # define range for input r_min, r_max = -5.0, 5.0 # sample input range uniformly at 0.1 increments inputs = arange(r_min, r_max, 0.1) # compute targets results = objective(inputs) # create a scatter plot of input vs result pyplot.scatter(inputs, results) # show the plot pyplot.show() Running the example creates a scatter plot of the objective function. We can see the familiar shape of the function, but we don\\u2019t gain anything from plotting the points directly. The line and the smooth interpolation between the points it provides are more useful as we can draw other points on top of the line, such as the location of the optima or the points sampled by an optimization algorithm. Scatter Plot of a One-Dimensional Function Line Plot with Marked Optima Next, let\\u2019s draw the line plot again and this time draw a point where the known optima of the function is located. This can be helpful when studying an optimization algorithm as we might want to see how close an optimization algorithm can get to the optima. First, we must define the input for the optima, then evaluate that point to give the x-axis and y-axis values for plotting. ... # define the known function optima optima_x = 0.0 optima_y = objective(optima_x) We can then plot this point with any shape or color we like, in this case, a red square. ... # draw the function optima as a red square pyplot.plot([optima_x], [optima_y], 's', color='r') Tying this together, the complete example of creating a line plot of the function with the optima highlighted by a point is listed below. # line plot of input vs result for a 1d objective function and show optima from numpy import arange from matplotlib import pyplot # objective function def objective(x): return x**2.0 # define range for input r_min, r_max = -5.0, 5.0 # sample input range uniformly at 0.1 increments inputs = arange(r_min, r_max, 0.1) # compute targets results = objective(inputs) # create a line plot of input vs result pyplot.plot(inputs, results) # define the known function optima optima_x = 0.0 optima_y = objective(optima_x) # draw the function optima as a red square pyplot.plot([optima_x], [optima_y], 's', color='r') # show the plot pyplot.show() Running the example creates the familiar line plot of the function, and this time, the optima of the function, e.g. the input that results in the minimum output of the function, is marked with a red square. Line Plot of a One-Dimensional Function With Optima Marked by a Red Square This is a very simple function and the red square for the optima is easy to see. Sometimes the function might be more complex, with lots of hills and valleys, and we might want to make the optima more visible. In this case, we can draw a vertical line across the whole plot. ... # draw a vertical line at the optimal input pyplot.axvline(x=optima_x, ls='--', color='red') Tying this together, the complete example is listed below. # line plot of input vs result for a 1d objective function and show optima as line from numpy import arange from matplotlib import pyplot # objective function def objective(x): return x**2.0 # define range for input r_min, r_max = -5.0, 5.0 # sample input range uniformly at 0.1 increments inputs = arange(r_min, r_max, 0.1) # compute targets results = objective(inputs) # create a line plot of input vs result pyplot.plot(inputs, results) # define the known function optima optima_x = 0.0 # draw a vertical line at the optimal input pyplot.axvline(x=optima_x, ls='--', color='red') # show the plot pyplot.show() Running the example creates the same plot and this time draws a red line clearly marking the point in the input space that marks the optima. Line Plot of a One-Dimensional Function With Optima Marked by a Red Line Line Plot with Samples Finally, we might want to draw the samples of the input space selected by an optimization algorithm. We will simulate these samples with random points drawn from the input domain. ... # simulate a sample made by an optimization algorithm seed(1) sample = r_min + rand(10) * (r_max - r_min) # evaluate the sample sample_eval = objective(sample) We can then plot this sample, in this case using small black circles. ... # plot the sample as black circles pyplot.plot(sample, sample_eval, 'o', color='black') The complete example of creating a line plot of a function with the optima marked by a red line and an algorithm sample drawn with small black dots is listed below. # line plot of domain for a 1d function with optima and algorithm sample from numpy import arange from numpy.random import seed from numpy.random import rand from matplotlib import pyplot # objective function def objective(x): return x**2.0 # define range for input r_min, r_max = -5.0, 5.0 # sample input range uniformly at 0.1 increments inputs = arange(r_min, r_max, 0.1) # compute targets results = objective(inputs) # simulate a sample made by an optimization algorithm seed(1) sample = r_min + rand(10) * (r_max - r_min) # evaluate the sample sample_eval = objective(sample) # create a line plot of input vs result pyplot.plot(inputs, results) # define the known function optima optima_x = 0.0 # draw a vertical line at the optimal input pyplot.axvline(x=optima_x, ls='--', color='red') # plot the sample as black circles pyplot.plot(sample, sample_eval, 'o', color='black') # show the plot pyplot.show() Running the example creates the line plot of the domain and marks the optima with a red line as before. This time, the sample from the domain selected by an algorithm (really a random sample of points) is drawn with black dots. We can imagine that a real optimization algorithm will show points narrowing in on the domain as it searches down-hill from a starting point. Line Plot of a One-Dimensional Function With Optima Marked by a Red Line and Samples Shown with Black Dots Next, let\\u2019s look at how we might perform similar visualizations for the optimization of a two-dimensional function. Visualize 2D Function Optimization A two-dimensional function is a function that takes two input variables, e.g. x and y. Test Function We can use the same x^2 function and scale it up to be a two-dimensional function; for example: f(x, y) = x^2 + y^2 This has an optimal value with an input of [x=0.0, y=0.0], which equals 0.0. The example below implements this objective function and evaluates a single input. # example of a 2d objective function # objective function def objective(x, y): return x**2.0 + y**2.0 # evaluate inputs to the objective function x = 4.0 y = 4.0 result = objective(x, y) print('f(%.3f, %.3f) = %.3f' % (x, y, result)) Running the example evaluates the point [x=4, y=4], which equals 32. f(4.000, 4.000) = 32.000 Next, we need a way to sample the domain so that we can, in turn, sample the objective function. Sample Test Function A common way for sampling a two-dimensional function is to first generate a uniform sample along each variable, x and y, then use these two uniform samples to create a grid of samples, called a mesh grid. This is not a two-dimensional array across the input space; instead, it is two two-dimensional arrays that, when used together, define a grid across the two input variables. This is achieved by duplicating the entire x sample array for each y sample point and similarly duplicating the entire y sample array for each x sample point. This can be achieved using the meshgrid() NumPy function; for example: ... # define range for input r_min, r_max = -5.0, 5.0 # sample input range uniformly at 0.1 increments xaxis = arange(r_min, r_max, 0.1) yaxis = arange(r_min, r_max, 0.1) # create a mesh from the axis x, y = meshgrid(xaxis, yaxis) # summarize some of the input domain print(x[:5, :5]) We can then evaluate each pair of points using our objective function. ... # compute targets results = objective(x, y) # summarize some of the results print(results[:5, :5]) Finally, we can review the mapping of some of the inputs to their corresponding output values. ... # create a mapping of some inputs to some results for i in range(5): print('f(%.3f, %.3f) = %.3f' % (x[i,0], y[i,0], results[i,0])) The example below demonstrates how we can create a uniform sample grid across the two-dimensional input space and objective function. # sample 2d objective function from numpy import arange from numpy import meshgrid # objective function def objective(x, y): return x**2.0 + y**2.0 # define range for input r_min, r_max = -5.0, 5.0 # sample input range uniformly at 0.1 increments xaxis = arange(r_min, r_max, 0.1) yaxis = arange(r_min, r_max, 0.1) # create a mesh from the axis x, y = meshgrid(xaxis, yaxis) # summarize some of the input domain print(x[:5, :5]) # compute targets results = objective(x, y) # summarize some of the results print(results[:5, :5]) # create a mapping of some inputs to some results for i in range(5): print('f(%.3f, %.3f) = %.3f' % (x[i,0], y[i,0], results[i,0])) Running the example first summarizes some points in the mesh grid, then the objective function evaluation for some points. Finally, we enumerate coordinates in the two-dimensional input space and their corresponding function evaluation. [[-5. -4.9 -4.8 -4.7 -4.6] [-5. -4.9 -4.8 -4.7 -4.6] [-5. -4.9 -4.8 -4.7 -4.6] [-5. -4.9 -4.8 -4.7 -4.6] [-5. -4.9 -4.8 -4.7 -4.6]] [[50. 49.01 48.04 47.09 46.16] [49.01 48.02 47.05 46.1 45.17] [48.04 47.05 46.08 45.13 44.2 ] [47.09 46.1 45.13 44.18 43.25] [46.16 45.17 44.2 43.25 42.32]] f(-5.000, -5.000) = 50.000 f(-5.000, -4.900) = 49.010 f(-5.000, -4.800) = 48.040 f(-5.000, -4.700) = 47.090 f(-5.000, -4.600) = 46.160 Now that we are familiar with how to sample the input space and evaluate points, let\\u2019s look at how we might plot the function. Contour Plot of Test Function A popular plot for two-dimensional functions is a contour plot. This plot creates a flat representation of the objective function outputs for each x and y coordinate where the color and contour lines indicate the relative value or height of the output of the objective function. This is just like a contour map of a landscape where mountains can be distinguished from valleys. This can be achieved using the contour() Matplotlib function that takes the mesh grid and the evaluation of the mesh grid as input directly. We can then specify the number of levels to draw on the contour and the color scheme to use. In this case, we will use 50 levels and a popular \\u201cjet\\u201d color scheme where low-levels use a cold color scheme (blue) and high-levels use a hot color scheme (red). ... # create a contour plot with 50 levels and jet color scheme pyplot.contour(x, y, results, 50, alpha=1.0, cmap='jet') # show the plot pyplot.show() Tying this together, the complete example of creating a contour plot of the two-dimensional objective function is listed below. # create a contour plot with 50 levels and jet color scheme pyplot.contour(x, y, results, 50, alpha=1.0, cmap='jet') # show the plot pyplot.show() Tying this together, the complete example of creating a contour plot of the two-dimensional objective function is listed below. # contour plot for 2d objective function from numpy import arange from numpy import meshgrid from matplotlib import pyplot # objective function def objective(x, y): return x**2.0 + y**2.0 # define range for input r_min, r_max = -5.0, 5.0 # sample input range uniformly at 0.1 increments xaxis = arange(r_min, r_max, 0.1) yaxis = arange(r_min, r_max, 0.1) # create a mesh from the axis x, y = meshgrid(xaxis, yaxis) # compute targets results = objective(x, y) # create a contour plot with 50 levels and jet color scheme pyplot.contour(x, y, results, 50, alpha=1.0, cmap='jet') # show the plot pyplot.show() Running the example creates the contour plot. We can see that the more curved parts of the surface around the edges have more contours to show the detail, and the less curved parts of the surface in the middle have fewer contours. We can see that the lowest part of the domain is the middle, as expected. Contour Plot of a Two-Dimensional Objective Function Filled Contour Plot of Test Function It is also helpful to color the plot between the contours to show a more complete surface. Again, the colors are just a simple linear interpolation, not the true function evaluation. This must be kept in mind on more complex functions where fine detail will not be shown. We can fill the contour plot using the contourf() version of the function that takes the same arguments. ... # create a filled contour plot with 50 levels and jet color scheme pyplot.contourf(x, y, results, levels=50, cmap='jet') We can also show the optima on the plot, in this case as a white star that will stand out against the blue background color of the lowest part of the plot. ... # define the known function optima optima_x = [0.0, 0.0] # draw the function optima as a white star pyplot.plot([optima_x[0]], [optima_x[1]], '*', color='white') Tying this together, the complete example of a filled contour plot with the optima marked is listed below. # filled contour plot for 2d objective function and show the optima from numpy import arange from numpy import meshgrid from matplotlib import pyplot # objective function def objective(x, y): return x**2.0 + y**2.0 # define range for input r_min, r_max = -5.0, 5.0 # sample input range uniformly at 0.1 increments xaxis = arange(r_min, r_max, 0.1) yaxis = arange(r_min, r_max, 0.1) # create a mesh from the axis x, y = meshgrid(xaxis, yaxis) # compute targets results = objective(x, y) # create a filled contour plot with 50 levels and jet color scheme pyplot.contourf(x, y, results, levels=50, cmap='jet') # define the known function optima optima_x = [0.0, 0.0] # draw the function optima as a white star pyplot.plot([optima_x[0]], [optima_x[1]], '*', color='white') # show the plot pyplot.show() Running the example creates the filled contour plot that gives a better idea of the shape of the objective function. The optima at [x=0, y=0] is then marked clearly with a white star. Filled Contour Plot of a Two-Dimensional Objective Function With Optima Marked by a White Star Filled Contour Plot of Test Function with Samples We may want to show the progress of an optimization algorithm to get an idea of its behavior in the context of the shape of the objective function. In this case, we can simulate the points chosen by an optimization algorithm with random coordinates in the input space. ... # simulate a sample made by an optimization algorithm seed(1) sample_x = r_min + rand(10) * (r_max - r_min) sample_y = r_min + rand(10) * (r_max - r_min) These points can then be plotted directly as black circles and their context color can give an idea of their relative quality. ... # plot the sample as black circles pyplot.plot(sample_x, sample_y, 'o', color='black') Tying this together, the complete example of a filled contour plot with optimal and input sample plotted is listed below. # filled contour plot for 2d objective function and show the optima and sample from numpy import arange from numpy import meshgrid from numpy.random import seed from numpy.random import rand from matplotlib import pyplot # objective function def objective(x, y): return x**2.0 + y**2.0 # define range for input r_min, r_max = -5.0, 5.0 # sample input range uniformly at 0.1 increments xaxis = arange(r_min, r_max, 0.1) yaxis = arange(r_min, r_max, 0.1) # create a mesh from the axis x, y = meshgrid(xaxis, yaxis) # compute targets results = objective(x, y) # simulate a sample made by an optimization algorithm seed(1) sample_x = r_min + rand(10) * (r_max - r_min) sample_y = r_min + rand(10) * (r_max - r_min) # create a filled contour plot with 50 levels and jet color scheme pyplot.contourf(x, y, results, levels=50, cmap='jet') # define the known function optima optima_x = [0.0, 0.0] # draw the function optima as a white star pyplot.plot([optima_x[0]], [optima_x[1]], '*', color='white') # plot the sample as black circles pyplot.plot(sample_x, sample_y, 'o', color='black') # show the plot pyplot.show() Running the example, we can see the filled contour plot as before with the optima marked. We can now see the sample drawn as black dots and their surrounding color and relative distance to the optima gives an idea of how close the algorithm (random points in this case) got to solving the problem. Filled Contour Plot of a Two-Dimensional Objective Function With Optima and Input Sample Marked Surface Plot of Test Function Finally, we may want to create a three-dimensional plot of the objective function to get a fuller idea of the curvature of the function. This can be achieved using the plot_surface() Matplotlib function, that, like the contour plot, takes the mesh grid and function evaluation directly. ... # create a surface plot with the jet color scheme figure = pyplot.figure() axis = figure.gca(projection='3d') axis.plot_surface(x, y, results, cmap='jet') The complete example of creating a surface plot is listed below. # surface plot for 2d objective function from numpy import arange from numpy import meshgrid from matplotlib import pyplot from mpl_toolkits.mplot3d import Axes3D # objective function def objective(x, y): return x**2.0 + y**2.0 # define range for input r_min, r_max = -5.0, 5.0 # sample input range uniformly at 0.1 increments xaxis = arange(r_min, r_max, 0.1) yaxis = arange(r_min, r_max, 0.1) # create a mesh from the axis x, y = meshgrid(xaxis, yaxis) # compute targets results = objective(x, y) # create a surface plot with the jet color scheme figure = pyplot.figure() axis = figure.gca(projection='3d') axis.plot_surface(x, y, results, cmap='jet') # show the plot pyplot.show() Running the example creates a three-dimensional surface plot of the objective function. Surface Plot of a Two-Dimensional Objective Function Additionally, the plot is interactive, meaning that you can use the mouse to drag the perspective on the surface around and view it from different angles. Surface Plot From a Different Angle of a Two-Dimensional Objective Function Further Reading This section provides more resources on the topic if you are looking to go deeper. APIs Optimization and root finding (scipy.optimize) Optimization (scipy.optimize) numpy.meshgrid API. matplotlib.pyplot.contour API. matplotlib.pyplot.contourf API. mpl_toolkits.mplot3d.Axes3D.plot_surface API. Articles Mathematical optimization, Wikipedia. Parabola, Wikipedia. Summary In this tutorial, you discovered how to create visualizations for function optimization in Python. Specifically, you learned: Visualization is an important tool when studying function optimization algorithms. How to visualize one-dimensional functions and samples using line plots. How to visualize two-dimensional functions and samples using contour and surface plots. Do you have any questions? Ask your questions in the comments below and I will do my best to answer. The post Visualization for Function Optimization in Python appeared first on Machine Learning Mastery.\",\"2337\":\"I've heard it mentioned several times (such as in this paper) that a lot of companies are working on next gen ML hardware. There are big companies like Intel and IBM working on neurochips, and I've heard about Cerberas and Graphcore, but there should be a lot more than these. I trying to assemble a list with the intention of applying to these companies afterwards. Recently, I've finished a new version of the Spiral programming language and I want to show it to them. It would be good if we could make a list in this thread. I think it would be of interest of more than just myself to know what kind of hardware is coming down the pipe. [link] [comments]\",\"713\":\"Day 2 | December 2, 2020 Meredith Ringel Morris welcomes back attendees of the Microsoft PhD Summit to Day 2 and Mary L. Gray, Sr. Principal Researcher at Microsoft, gives a talk on big data, research ethics, and human rights. Microsoft's third PhD Summit was a two-day virtual workshop. It was an opportunity for top PhD students to enhance their skills, build a network, and discuss research within a community of peers and notable Microsoft researchers. Speakers: Meredith Ringel Morris, Sr. Principal Researcher & Research Area Manager, Microsoft Mary L. Gray, Sr. Principal Researcher, Microsoft More information on this workshop can be found at https:\\/\\/www.microsoft.com\\/en-us\\/research\\/event\\/phd-summit-2020\\/\",\"1693\":\"Read the full story\",\"478\":\"Back to the basics with logarithms. Full playlist: https:\\/\\/www.youtube.com\\/playlist?list=PLZHQObOWTQDP5CVelJJ1bNDouqrAhVPev Home page: https:\\/\\/www.3blue1brown.com Brought to you by you: https:\\/\\/3b1b.co\\/ldm-thanks Triangle of power: https:\\/\\/youtu.be\\/sULa9Lc4pck Beautiful pictorial summary by @ThuyNganVu: https:\\/\\/twitter.com\\/ThuyNganVu\\/status\\/1258222677573001219 ------------------ The live question setup with stats on-screen is powered by Itempool. https:\\/\\/itempool.com\\/ Curious about other animations? https:\\/\\/www.3blue1brown.com\\/faq#manim Music by Vincent Rubinetti. Download the music on Bandcamp: https:\\/\\/vincerubinetti.bandcamp.com\\/album\\/the-music-of-3blue1brown Stream the music on Spotify: https:\\/\\/open.spotify.com\\/album\\/1dVyjwS8FBqXhRunaG5W5u If you want to contribute translated subtitles or to help review those that have already been made by others and need approval, you can click the gear icon in the video and go to subtitles\\/cc, then \\\"add subtitles\\/cc\\\". I really appreciate those who do this, as it helps make the lessons accessible to more people. ------------------ 3blue1brown is a channel about animating math, in all senses of the word animate. And you know the drill with YouTube, if you want to stay posted on new videos, subscribe: http:\\/\\/3b1b.co\\/subscribe Various social media stuffs: Website: https:\\/\\/www.3blue1brown.com Twitter: https:\\/\\/twitter.com\\/3blue1brown Reddit: https:\\/\\/www.reddit.com\\/r\\/3blue1brown Instagram: https:\\/\\/www.instagram.com\\/3blue1brown_animations\\/ Patreon: https:\\/\\/patreon.com\\/3blue1brown Facebook: https:\\/\\/www.facebook.com\\/3blue1brown\",\"1568\":\"Vijay Kumar is one of the top roboticists in the world, professor at the University of Pennsylvania, Dean of Penn Engineering, former director of GRASP lab, or the General Robotics, Automation, Sensing and Perception Laboratory at Penn that was established back in 1979, 40 years ago. Vijay is perhaps best known for his work in multi-robot systems (or robot swarms) and micro aerial vehicles, robots that elegantly cooperate in flight under all the uncertainty and challenges that real-world conditions present. This conversation is part of the Artificial Intelligence podcast. If you would like to get more information about this podcast go\",\"653\":\"John Clarke is a BJJ black belt and MMA coach. Please support this podcast by checking out our sponsors: - Theragun: https:\\/\\/theragun.com\\/lex to get 30 day trial - Magic Spoon: https:\\/\\/magicspoon.com\\/lex and use code LEX to get free shipping - Eight Sleep: https:\\/\\/www.eightsleep.com\\/lex and use code LEX to get $200 off - Cash App: https:\\/\\/cash.app\\/ and use code LexPodcast to get $10 EPISODE LINKS: Broadway Jiu Jitsu website: https:\\/\\/www.broadwayjiujitsu.com Broadway Jiu Jitsu instagram: https:\\/\\/www.instagram.com\\/broadwayjiujitsu Please, Allow Me podcast: https:\\/\\/podcasts.apple.com\\/us\\/podcast\\/please-allow-me\\/id1531735873 Please, Allow Me instagram: https:\\/\\/www.instagram.com\\/please_allow_me_podcast\\/ PODCAST INFO: Podcast website: https:\\/\\/lexfridman.com\\/podcast Apple Podcasts: https:\\/\\/apple.co\\/2lwqZIr Spotify: https:\\/\\/spoti.fi\\/2nEwCF8 RSS: https:\\/\\/lexfridman.com\\/feed\\/podcast\\/ Full episodes playlist: https:\\/\\/www.youtube.com\\/playlist?list=PLrAXtmErZgOdP_8GztsuKi9nrraNbKKp4 Clips playlist: https:\\/\\/www.youtube.com\\/playlist?list=PLrAXtmErZgOeciFP3CBCIEElOJeitOr41 OUTLINE: 0:00 - Introduction 2:43 - The great American road trip 20:13 - Martial arts and philosophy 23:13 - Real vs fake success on Instagram 33:58 - The brutal honesty of Mike Tyson 38:44 - Breaking your opponent in wrestling 46:51 - Genghis Khan 57:57 - It's okay to change your mind 1:02:34 - Why do politicians become inauthentic 1:09:11 - Greatness requires sacrifice 1:11:54 - Whiplash 1:20:02 - Relationships 1:25:39 - Greatest fighters of all time 1:33:20 - Greatest fight of all time 1:47:43 - Khabib Nurmagomedov 1:49:31 - Can Conor McGregor beat Khabib Nurmagomedov? 2:03:47 - Conor vs Khabib 2 2:10:23 - Will there always be war? 2:11:59 - Future of civilization 2:14:10 - Kids 2:20:55 - The meaning of a \\\"like\\\" on social media 2:29:52 - Starting a podcast 2:48:34 - Book recommendations 2:52:04 - Keeping the independence of solitude CONNECT: - Subscribe to this YouTube channel - Twitter: https:\\/\\/twitter.com\\/lexfridman - LinkedIn: https:\\/\\/www.linkedin.com\\/in\\/lexfridman - Facebook: https:\\/\\/www.facebook.com\\/LexFridmanPage - Instagram: https:\\/\\/www.instagram.com\\/lexfridman - Medium: https:\\/\\/medium.com\\/@lexfridman - Support on Patreon: https:\\/\\/www.patreon.com\\/lexfridman\",\"1569\":\"Kyle Vogt is the President and CTO of Cruise Automation, leading an effort in trying to solve one of the biggest robotics challenges of our time: vehicle autonomy. He is the co-founder of 2 successful companies (Cruise and Twitch) that were each acquired for 1 billion dollars. Video version is available on YouTube. If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman on Twitter, LinkedIn, Facebook, or YouTube where you can watch the video versions of these conversations.\",\"1696\":\"Read the full story\",\"204\":\"I graduated on Warsaw University of Technology with master thesis about text mining topic (intelligent web crawling methods). I work for Polish IT consulting company (Sollers Consulting), where I develop and design various insurance industry related stuff, (one of them is insurance fraud detection platform). From time to time I try to compete in data mining contests (Netflix, competitions on Kaggle and tunedit.org) \\u2014 from my perspective it is a very good way to get real data mining experience. What I tried As far as I remember, the basis of the solution I defined at the very beginning: to create separate predictors for each individual loop and time interval. So my solution required me to build 61x10=610 regression models. I was playing with various regression algorithms, but quickly chose linear regression \\u2014 because the results were good and the computation time was short. I think the key to get quite good result (especially on public RMSE \\ud83d\\ude42 ) was the set of attributes used. I used the following attributes for the linear regression for each individual loop&time interval: \\u2014 number of minutes from 0:00 hours up to current moment (\\u201cnow\\u201d) \\u2014 average drive time for given loop&interval \\u2014 loop times for current moment and some number of historical moments before (the number of time points and the loop varied between the methods) \\u2014 differences between \\u201cneighboring\\u201d time moments for the above data: just differences or differences transformed with logistic function (1\\/1+e^-difference). Use of logistic function gave a jump from public RMSE at about 198 to 189. The idea to use of sigmoid function here was just my intuition inspired by differences distribution. \\u2014 \\u201csaturations\\u201d for for each loop (except the 2 first loops at both directions ). I introduced the simple (and very naive) model of traffic growth: If the speed at given loop is up to 40 km\\/h \\u2014 the saturation is 1; If the difference between the previous loop and the given loop is more than 5 km\\/h: it is assumed that this road part is partially saturated: there is segment that is moving at 30 km\\/h and second segment with the same speed as in the loop that is before given loop. The saturation is derived as the proportion of first segment to the whole road part. Each loop detector has its minimal value in RTAData file \\u2014 after the regression this minimal value was used if predicted value was less than minimum. I did not use historical data at all \\u2014 I found them useless during the initial tests (maybe too hastily). The only source of data for learning and testing was RTAData and lengths files (also no weekends, holidays, weather conditions). What ended up working For each of 610 regression models the following 3 models were competing. Models were being trained with all data availabe in RTAData file: Model 1: For all (61) loops: current + 5 times moments before and 5 simple differences \\u2014 675 attributes, Model 2: For 10 before, current and next 9 loops (if available or less): current + 9 times moments before and 9 simple differences, saturations (for current time moment only) \\u2014 204 to 404 atrributes, Model 3: For 10 before, current and next 9 loops (if available or less): current + 9 times moments before and 9 sigmoided differences, saturations (for current time moment only) 204 to 404 atrributes, Model with least RMSE computed on the train file was selected for particular loop. It is not a very good strategy, however I thought that generally linear regression was resistant to overfitting (it is not true \\u2014 as the number of variable grows, the more variance can be explained \\u2014 this is what I have learnt). This strategy gave me public RMSE 189.3 I added also 4th model, that I just used for 15, 30 minutes predictions arbitrarily: Model 4: For all (61) loops: current + 5 times moments before and 5 sigmoided differences, saturations (for current time moment only) \\u2014 614 attributes. This turn gave mi 188.6 public result. What is interesting, the best private solution (however not selected by me since I relied to much on public results) was 190.819 (public 197.979) , it was just the model 3 described above combined with model 5 (model 5 was used for 15,30,45,60,90 minutes predictions arbitrarily, rest model 3): Model 5: like model 3 but also loop times are \\u201csigmoided\\u201d not only differences. What tools I used My solution is written as Java application with Weka linked as library (as always when I try to compete in data mining contests). Since linear regression requires to solve matrix equation (in this case quite huge), the memory allocated by the program was becoming more and more important issue (3,5GB for one thread) \\u2014 at the of the competition i was using computer with 4 processors and 12 GB of RAM \\u2014 with 3 separate threads building and testing the models. The whole computation for my last attempts took about 48 hours of computations. Originally published at blog.kaggle.com on February 17, 2011. Marcin Pionnier on finishing 5th in the RTA competition was originally published in Kaggle Blog on Medium, where people are continuing the conversation by highlighting and responding to this story.\",\"651\":\"Michael Malice is a political thinker, podcaster, and author. Please support this podcast by checking out our sponsors: - NetSuite: http:\\/\\/netsuite.com\\/strategy to get free product tour - Athletic Greens: https:\\/\\/athleticgreens.com\\/lex and use code LEX to get 1 month of fish oil - Sun Basket: https:\\/\\/sunbasket.com\\/lex and use code LEX to get $35 off - Cash App: https:\\/\\/cash.app\\/ and use code LexPodcast to get $10 EPISODE LINKS: Michael's Twitter: https:\\/\\/twitter.com\\/michaelmalice Michael's Community: https:\\/\\/malice.locals.com\\/ Michael's YouTube: https:\\/\\/www.youtube.com\\/channel\\/UC5tj5QCpJKIl-KIa4Gib5Xw Michael's Website: http:\\/\\/michaelmalice.com\\/about\\/ Your Welcome podcast: https:\\/\\/bit.ly\\/30q8oz1 The New Right (book): https:\\/\\/amzn.to\\/34gxLo3 Dear Reader (book): https:\\/\\/amzn.to\\/2HPPlHS Podcast (Round 1): https:\\/\\/www.youtube.com\\/watch?v=BIk1zUy8ehU PODCAST INFO: Podcast website: https:\\/\\/lexfridman.com\\/podcast Apple Podcasts: https:\\/\\/apple.co\\/2lwqZIr Spotify: https:\\/\\/spoti.fi\\/2nEwCF8 RSS: https:\\/\\/lexfridman.com\\/feed\\/podcast\\/ Full episodes playlist: https:\\/\\/www.youtube.com\\/playlist?list=PLrAXtmErZgOdP_8GztsuKi9nrraNbKKp4 Clips playlist: https:\\/\\/www.youtube.com\\/playlist?list=PLrAXtmErZgOeciFP3CBCIEElOJeitOr41 OUTLINE: 0:00 - Introduction 3:25 - Conversation with Alex Jones and Tim Pool 12:10 - Michael's outfit 20:31 - Self-publishing a book 30:19 - The white pill 41:43 - What did the volcano say to his true love? 43:06 - Myth of Sisyphus 46:47 - Journalism failed to stop Stalin and Hitler 54:31 - Good Germans 58:27 - Richard Wolff 1:01:58 - Could United States have stayed out of World War II 1:04:50 - Trump Derangement Syndrome 1:06:36 - Nazism and Antisemitism 1:09:18 - Knock knock 1:15:58 - Putin 1:23:38 - The evil of Kim Jong-il and North Korea 1:32:10 - Dark humor 1:36:56 - Comedy is tragedy plus timing 1:44:12 - Interviewing difficult guests 1:53:44 - Curtis Yarvin (Mencius Moldbug) 2:10:02 - Violence under anarchism 2:25:36 - Ayn Rand 2:28:45 - Secession in United States 2:38:24 - Politics over next 4 years 2:45:52 - Mars 2:49:55 - UFOs 2:52:50 - Psychedelics 2:56:46 - What is love? CONNECT: - Subscribe to this YouTube channel - Twitter: https:\\/\\/twitter.com\\/lexfridman - LinkedIn: https:\\/\\/www.linkedin.com\\/in\\/lexfridman - Facebook: https:\\/\\/www.facebook.com\\/LexFridmanPage - Instagram: https:\\/\\/www.instagram.com\\/lexfridman - Medium: https:\\/\\/medium.com\\/@lexfridman - Support on Patreon: https:\\/\\/www.patreon.com\\/lexfridman\",\"974\":\"\\u23f0Video PSA\\u23f0End your week with a healthy dose of new DEEP LEARNING MEMES \\ud83e\\uddd9 Collaboration with the infamous @orvieto_antonio, I think this is the best one so far \\ud83c\\udf89 Watch and enjoy. invidious.snopyta.org\\/hHZSA9z_abE #science #ai #memes #machinelearning #technology\",\"1440\":\"Diana Walsh Pasulka is a professor of philosophy and religion at UNCW and author of American Cosmic: UFOs, Religion, and Technology. Please support this podcast by checking out our sponsors: \\u2013 LMNT: https:\\/\\/drinkLMNT.com\\/lex to get free shipping \\u2013 Grammarly: https:\\/\\/grammarly.com\\/lex to get 20% off premium \\u2013 Business Wars: https:\\/\\/wondery.com\\/business-wars\\/ \\u2013 Cash App: https:\\/\\/cash.app\\/ and use code LexPodcast to get $10 EPISODE LINKS: Diana\\u2019s Website: https:\\/\\/uncw.edu\\/par\\/faculty\\/faculty-pasulka.html American Cosmic (book): https:\\/\\/amzn.to\\/3aK2kaj PODCAST INFO: Podcast website: https:\\/\\/lexfridman.com\\/podcast Apple Podcasts: https:\\/\\/apple.co\\/2lwqZIr Spotify: https:\\/\\/spoti.fi\\/2nEwCF8 RSS: https:\\/\\/lexfridman.com\\/feed\\/podcast\\/ YouTube Full Episodes: https:\\/\\/youtube.com\\/lexfridman YouTube Clips: https:\\/\\/youtube.com\\/lexclips SUPPORT & CONNECT: \\u2013 Check out the sponsors above, it\\u2019s the best way to support this podcast \\u2013 Support on Patreon: https:\\/\\/www.patreon.com\\/lexfridman \\u2013 Twitter: https:\\/\\/twitter.com\\/lexfridman \\u2013 Instagram: https:\\/\\/www.instagram.com\\/lexfridman \\u2013 LinkedIn: https:\\/\\/www.linkedin.com\\/in\\/lexfridman \\u2013 Facebook: https:\\/\\/www.facebook.com\\/LexFridmanPage \\u2013 Medium: https:\\/\\/medium.com\\/@lexfridman OUTLINE: Here\\u2019s the timestamps for the episode. On some podcast players you should be able to click the timestamp to jump to that time. (00:00) \\u2013 Introduction (08:28) \\u2013 What is real? (13:56) \\u2013 Can beliefs become reality? (18:59) \\u2013 Donald Hoffman (22:57) \\u2013 Immanuel Kant\\u2019s Critique of Pure Reason (26:27) \\u2013 Ayn Rand (33:25) \\u2013 How do religions start? (48:38) \\u2013 Religion is an evolutionary advantage (53:59) \\u2013 Religion used in propaganda (58:32) \\u2013 What did Nietzsche mean by \\u201cGod is Dead\\u201d? (1:03:59) \\u2013 American Cosmic (1:07:45) \\u2013 What do aliens look like? (1:16:28) \\u2013 History of space programs (1:19:30) \\u2013 Jacques Vallee (1:28:55) \\u2013 Artificial intelligence (1:34:25) \\u2013 Ufology community (1:45:38) \\u2013 Psychedelics (1:49:35) \\u2013 Tic Tac UFO (1:58:09) \\u2013 Roswell UFO incident (2:09:14) \\u2013 Bob Lazar (2:12:50) \\u2013 Monoliths in the desert (2:23:39) \\u2013 Humans will co-evolve with AI (2:26:58) \\u2013 Neuralink (2:31:48) \\u2013 Singularity (2:41:39) \\u2013 Books: Nietzsche (2:46:44) \\u2013 Books: Hannah Arendt (2:51:43) \\u2013 Fear of death (2:56:11) \\u2013 Meaning of life\",\"3135\":\"Liqui.do is a technological and innovative company developing a platform for leasing equipment for small and medium enterprises. As part of its business to provide a variety of credit options for companies that want to finance capital purchases, Liqui.do needs to rapidly and accurately assess the credit risk and scoring of a customer in order to offer the most attractive credit options. Expanding access to credit by reducing previous lending biases helps to drive both freedom and flexibility for people and businesses as well as business growth. This is why Liqui.do decided to use Machine Learning to meet their needs. H2O.ai provided their AutoML technology, Driverless AI, that enabled Liqui.do to provide deep insights into the model to help the team minimize past biases. Now they are able to provide 20% more applicants with credit and offer the right financial products to meet the needs of their customers. The solution helps Liqui.do improve its credit risk predictions, improving lending fairness, and customer experience. Using Driverless AI, in less than three months, the company was able to develop an algorithm that boosted its model performance by 20%, giving Liqui.do the ability to increase its credit approvals without compromising the company credit losses. Liqui.do also aims to add AI to more processes, using H2O.ai technology, improving efficiency and efficacy. \\u201cH2O.ai has been a terrific partner in helping Liqui.do utilize artificial intelligence to enhance its business and increase responsiveness to customers,\\u201d said Sergio Nunes, Liqui.do CEO. \\u201cOur processes are much more efficient thanks to Driverless AI, the solution helps us make the right decision at the right time and in a fair way. We are excited by the initial results H2O.ai\\u2019s offerings have provided and looking forward to expanding the usage of AI within the company.\\u201d H2O.ai is a leading open-source AI platform, and its Driverless AI is a leading automatic machine learning (AutoML) platform that helps customers easily build and deploy machine learning models. H2O Driverless AI automates time-consuming machine learning workflows with automatic feature engineering, model tuning, and model selection to achieve the highest predictive accuracy within the shortest amount of time. \\u201cLiqui.do is using H2O Driverless AI to personalize credit towards equipment rentals for small-medium enterprises.\\u201d said Sri Ambati, CEO & Founder of H2O.ai. \\u201cH2O\\u2019s autoML is helping Liqui.do transform the future of equipment renting.\\u201d To learn more about Liqui.do, visit https:\\/\\/liqui.do The post Liqui.do Speeds Credit Scoring for Fair Lending with H2O.ai appeared first on Open Source Leader in AI and ML.\",\"3141\":\"Todas as revolu\\u00e7\\u00f5es que tivemos at\\u00e9 hoje, tanto as tecnol\\u00f3gicas quanto industriais, possuem uma semelhan\\u00e7a: elas est\\u00e3o ligadas \\u00e0 forma como os seres humanos lidam com as m\\u00e1quinas. Antes, os processos eram feitos de forma muito manual e, com o tempo, acabaram sofrendo uma evolu\\u00e7\\u00e3o natural voltada para a automa\\u00e7\\u00e3o. Com o aprendizado de m\\u00e1quinas n\\u00e3o \\u00e9 diferente. No in\\u00edcio, muitas tarefas relacionadas ao aprendizado de m\\u00e1quina eram manuais, justamente por estarem em uma fase de experimenta\\u00e7\\u00e3o. Por\\u00e9m, quando falarmos em aprendizado de m\\u00e1quina automatizado, popularmente conhecido como AutoML, estamos falando de um processo de evolu\\u00e7\\u00e3o do aprendizado de m\\u00e1quina. O programador, quando bastante sagaz, vai dar um jeito de automatizar as tarefas repetitivas e manuais para otimizar o seu tempo. Normalmente, isso \\u00e9 feito atrav\\u00e9s da cria\\u00e7\\u00e3o de scripts, templates ou at\\u00e9 mesmo bibliotecas que re\\u00fanem um compilado de todas as principais fun\\u00e7\\u00f5es que ele necessita para poder desenvolver uma determinada tarefa e, consequentemente, facilitar a sua vida. \\u00c9 assim que funciona com o AutoML. Como qualquer framework ou linguagem de programa\\u00e7\\u00e3o, \\u00e9 uma ferramenta que ser\\u00e1 utilizada em benef\\u00edcio do usu\\u00e1rio que est\\u00e1 desenvolvendo o modelo. Com o intuito de desmistificar algumas quest\\u00f5es relacionadas ao AutoML, destacamos abaixo alguns pontos importantes e que costumam gerar muitas d\\u00favidas. Inclusive, vale lembrar que a desmistifica\\u00e7\\u00e3o do AutoML tamb\\u00e9m foi pauta da live que gravamos juntos no Linkedin e no Twitter da H2O.ai m\\u00eas passado (voc\\u00eas podem conferir o v\\u00eddeo na \\u00edntegra ao fim deste post). AutoML: amea\\u00e7a ou aliado? Criou-se um mito, uma cren\\u00e7a equivocada n\\u00e3o s\\u00f3 do AutoML, como tamb\\u00e9m do aprendizado de m\\u00e1quina em geral e da Intelig\\u00eancia Artificial de que eles iriam, em algum momento, substituir o usu\\u00e1rio. Na realidade n\\u00e3o \\u00e9 isso que acontece. O AutoML \\u00e9 apenas mais uma ferramenta que veio para auxiliar e facilitar o trabalho do desenvolvedor de modelos de aprendizado de m\\u00e1quina, com o objetivo de otimizar o seu tempo, e n\\u00e3o para substitu\\u00ed-lo. Ao utilizar uma ferramenta de AutoML, muitas das tarefas com as quais o cientista de dados teria que se preocupar (como feature engineering, selecionar o melhor transformador para extra\\u00e7\\u00e3o de caracter\\u00edsticas a partir de um determinado conjunto de dados, selecionar o melhor modelo, etc.), acabam sendo definidas pela ferramenta e, com isso, ele acaba tendo mais tempo para focar no que realmente interessa para o neg\\u00f3cio, utilizando os seus conhecimentos matem\\u00e1ticos e estat\\u00edsticos de an\\u00e1lise para gerar melhores insights para a empresa. Alguns desafios Um dos principais desafios da utiliza\\u00e7\\u00e3o do AutoML \\u00e9 cultural. Com a falsa impress\\u00e3o de que o AutoML representa uma amea\\u00e7a, os usu\\u00e1rios acabaram se tornando resistentes \\u00e0 ideia da automa\\u00e7\\u00e3o. Outro ponto \\u00e9 que, muitas vezes, a ferramenta de AutoML mais engessa o cientista de dados do que lhe d\\u00e1 liberdade. Por isso, \\u00e9 fundamental que o usu\\u00e1rio considere uma boa ferramenta de AutoML e a possibilidade que ele tem de poder estender e expandir essa plataforma para adequ\\u00e1-la \\u00e0s necessidades com o seu caso de uso real. Por exemplo, muitas ferramentas de AutoML d\\u00e3o um determinado conjunto de modelos \\u201cx\\u201d e um conjunto de feature engineering \\u201cy\\u201d. O cientista de dados ent\\u00e3o tem que adaptar o seu caso de uso para ser aplicado na ferramenta e n\\u00e3o \\u00e9 assim que deveria funcionar. A ferramenta precisa ser flex\\u00edvel e extens\\u00edvel a ponto de se adaptar \\u00e0s necessidades do usu\\u00e1rio e n\\u00e3o o contr\\u00e1rio. Al\\u00e9m disso, existem algumas outras limitantes, como o fato de que os modelos precisam ser supervisionados, ou seja, a cria\\u00e7\\u00e3o de modelos a partir de um conjunto de dados que possui uma vari\\u00e1vel-alvo. Mas isso \\u00e9 algo que tem evolu\\u00eddo e \\u00e9 poss\\u00edvel que em breve tenhamos a oportunidade de trabalhar com modelos n\\u00e3o supervisionados dentro de uma plataforma de AutoML tamb\\u00e9m. Inclusive, esse \\u00e9 um campo de estudo cont\\u00ednuo e j\\u00e1 existem aplica\\u00e7\\u00f5es de t\\u00e9cnicas com a finalidade de alcan\\u00e7ar esse objetivo . Exemplos de sucesso Um caso bem interessante que a H2O.ai est\\u00e1 trabalhando e que podemos citar \\u00e9 o de um modelo de classifica\\u00e7\\u00e3o de imagens para detec\\u00e7\\u00e3o de corros\\u00e3o. Os cientistas de dados da empresa que nos procurou estava trabalhando nesse conjunto de imagens h\\u00e1 uns 2 meses, 2 meses e meio nessa atividade, e com o Driverless AI trazendo o dataset correto, o nosso time na H2O.ai conseguiu resolver o problema em praticamente um dia. E \\u00e9 gratificante ver a rea\\u00e7\\u00e3o das pessoas quando recebem os resultados muitas vezes mais precisos e gerados em um tempo significativamente mais curto. Para concluir, \\u00e9 fundamental enxergarmos o AutoML como pe\\u00e7a-chave no processo de democratiza\\u00e7\\u00e3o da Intelig\\u00eancia Artificial porque ele empodera todo o tipo de usu\\u00e1rio \\u2013 desde o analista de neg\\u00f3cios, que necessita utilizar os conhecimentos do aprendizado de m\\u00e1quina e intelig\\u00eancia artificial para gerar novos insights para seus neg\\u00f3cios, at\\u00e9 o cientista de dados mais conhecimento mais avan\\u00e7ado. Se interessou e quer assistir \\u00e0 grava\\u00e7\\u00e3o da live na \\u00edntegra? Clique aqui. The post Mitos e verdades sobre o AutoML appeared first on Open Source Leader in AI and ML.\",\"1523\":\"Stephen Wolfram is a computer scientist, mathematician, and theoretical physicist. This is our second conversation on the podcast. Please check out our sponsors to get a discount and to support this podcast: \\u2013 SimpliSafe: https:\\/\\/simplisafe.com\\/lex \\u2013 Sun Basket, use code LEX: https:\\/\\/sunbasket.com\\/lex \\u2013 MasterClass: https:\\/\\/masterclass.com\\/lex If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/podcast or connect with @lexfridman on Twitter, LinkedIn, Facebook, Medium, or YouTube where you can watch the video versions of these conversations. If you enjoy the podcast, please rate it 5 stars on Apple Podcasts, follow on Spotify, or support it on Patreon.\",\"2805\":\"\\\"It is easy to denounce the evildoer. It is difficult to understand him.\\\" - Dostoevsky\",\"1609\":\"My NPC character is chaotic good. I now understand & accept my pre-programmed storyline and ethical alignment. Amor fati.\",\"263\":\"Pretty sure \\\"the guy\\\" just took a couple of pictures of his dad in a wig \\ud83d\\ude02 nitter.net\\/Not_the_Bee\\/status\\/1349035604160487429#m\",\"1139\":\"Featuring Professor Sylvain Cappell from NYU. Extra footage at: https:\\/\\/youtu.be\\/NV3EeagyU0Y More links & stuff in full description below \\u2193\\u2193\\u2193 Merch based on this video: https:\\/\\/teespring.com\\/numberphile-knots And here: https:\\/\\/teespring.com\\/numberphile-figure-eight Numberphile is supported by the Mathematical Sciences Research Institute (MSRI): http:\\/\\/bit.ly\\/MSRINumberphile We are also supported by Science Sandbox, a Simons Foundation initiative dedicated to engaging everyone with the process of science. https:\\/\\/www.simonsfoundation.org\\/outreach\\/science-sandbox\\/ And support from Math For America - https:\\/\\/www.mathforamerica.org\\/ NUMBERPHILE Website: http:\\/\\/www.numberphile.com\\/ Numberphile on Facebook: http:\\/\\/www.facebook.com\\/numberphile Numberphile tweets: https:\\/\\/twitter.com\\/numberphile Subscribe: http:\\/\\/bit.ly\\/Numberphile_Sub Video by Brady Haran and Pete McPartlan Patreon: http:\\/\\/www.patreon.com\\/numberphile Numberphile T-Shirts and Merch: https:\\/\\/teespring.com\\/stores\\/numberphile Brady's videos subreddit: http:\\/\\/www.reddit.com\\/r\\/BradyHaran\\/ Brady's latest videos across all channels: http:\\/\\/www.bradyharanblog.com\\/ Sign up for (occasional) emails: http:\\/\\/eepurl.com\\/YdjL9\",\"504\":\"In this video, we learn how byte pair encoding works. We look at the motivation and then see how character level byte pair encoding works and we also touch byte-level BPE and wordpiece tokenization. #BPE #BytePairEncoding #NLP Please subscribe and like the video to help me keep motivated to make awesome videos like this one. :) To buy my book, Approaching (Almost) Any Machine Learning problem, please visit: https:\\/\\/bit.ly\\/buyaaml Follow me on: Twitter: https:\\/\\/twitter.com\\/abhi1thakur LinkedIn: https:\\/\\/www.linkedin.com\\/in\\/abhi1thakur\\/ Kaggle: https:\\/\\/kaggle.com\\/abhishek Instagram: https:\\/\\/instagram.com\\/abhi4ml\",\"1480\":\"Fran\\u00e7ois Chollet is the creator of Keras, which is an open source deep learning library that is designed to enable fast, user-friendly experimentation with deep neural networks. It serves as an interface to several deep learning libraries, most popular of which is TensorFlow, and it was integrated into TensorFlow main codebase a while back. Aside from creating an exceptionally useful and popular library, Fran\\u00e7ois is also a world-class AI researcher and software engineer at Google, and is definitely an outspoken, if not controversial, personality in the AI world, especially in the realm of ideas around the future of artificial intelligence.\",\"4288\":\"In this episode of Machine Learning Street Talk, Tim Scarfe, Yannic Kilcher and Connor Shorten discuss their takeaways from OpenAI\\u2019s GPT-3 language model. With the help of Microsoft\\u2019s ZeRO-2 \\/ DeepSpeed optimiser, OpenAI trained an 175 BILLION parameter autoregressive language model. The paper demonstrates how self-supervised language modelling at this scale can perform many downstream tasks without fine-tuning. 00:00:00 Intro 00:00:54 ZeRO1+2 (model + Data parallelism) (Connor) 00:03:17 Recent history of NLP (Tim) 00:06:04 Yannic \\\"Light-speed\\\" Kilcher's brief overview of GPT-3 00:14:25 Reviewing Yannic's YT comments on his GPT-3 video (Tim) 00:20:26 Main show intro 00:23:03 Is GPT-3 reasoning? 00:28:15 Architecture discussion and autoregressive (GPT*) vs denoising autoencoder (BERT) 00:36:18 Utility of GPT-3 in industry 00:43:03 Can GPT-3 do math? (reasoning\\/system 1\\/system 2) 00:51:03 Generalisation 00:56:48 Esoterics of language models 00:58:46 Architectural trade-offs 01:07:37 Memorization machines and intepretability 01:17:16 Nearest neighbour probes \\/ watermarks 01:20:03 YouTube comments on GPT-3 video 01:21:50 GPT-3 news article generation issue 01:27:36 Sampling data for language models \\/ bias \\/ fairness \\/ politics 01:51:12 Outro These paradigms of task adaptation are divided into zero, one, and few shot learning. Zero-shot learning is a very extreme case where we expect a language model to perform a task such as sentiment classification or extractive question answering, without any additional supervision. One and Few-shot learning provide some examples to the model. However, GPT-3s definition of this diverges a bit from the conventional literature. GPT-3 provides one and few-shot examples in the form of \\u201cIn-Context Learning\\u201d. Instead of fine-tuning the model on a few examples, the model has to use the input to infer the downstream task. For example, the GPT-3 transformer has an input sequence of 2048 tokens, so demonstrations of a task such as yelp sentiment reviews, would have to fit in this input sequence as well as the new review. Thanks for watching! Please Subscribe! Paper Links: GPT-3: https:\\/\\/arxiv.org\\/abs\\/2005.14165 ZeRO: https:\\/\\/arxiv.org\\/abs\\/1910.02054 ZeRO (Blog Post): https:\\/\\/www.microsoft.com\\/en-us\\/research\\/blog\\/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters\\/ ZeRO-2 (Blog Post): https:\\/\\/www.microsoft.com\\/en-us\\/research\\/blog\\/zero-2-deepspeed-shattering-barriers-of-deep-learning-speed-scale\\/?OCID=msr_blog_deepspeed2_build_tw #machinelearning #naturallanguageprocessing #deeplearning #gpt3\",\"254\":\"God bless ML reddit \\ud83d\\ude02\",\"262\":\"Thank God Machine Learning has #12. None of the others, but at least #12 \\ud83d\\ude43 nitter.net\\/eigenbros\\/status\\/1348269634240327682#m\",\"1546\":\"Ryan Hall is a jiu jitsu black belt, UFC fighter, and a philosopher of the martial arts. Please check out our sponsors to get a discount and to support this podcast: \\u2013 PowerDot, use code LEX: https:\\/\\/powerdot.com\\/lex \\u2013 Babbel: https:\\/\\/babbel.com and use code LEX \\u2013 Cash App: download app & use code \\u201cLexPodcast\\u201d If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/podcast or connect with @lexfridman on Twitter, LinkedIn, Facebook, Medium, or YouTube where you can watch the video versions of these conversations. If you enjoy the podcast, please rate it 5 stars on Apple Podcasts,\",\"1527\":\"Cristos Goodrow is VP of Engineering at Google and head of Search and Discovery at YouTube (aka YouTube Algorithm). This conversation is part of the Artificial Intelligence podcast. If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman on Twitter, LinkedIn, Facebook, Medium, or YouTube where you can watch the video versions of these conversations. If you enjoy the podcast, please rate it 5 stars on Apple Podcasts, follow on Spotify, or support it on Patreon. This episode is presented by Cash App. Download it (App Store, Google Play), use code \\u201cLexPodcast\\u201d. Here\\u2019s\",\"205\":\"Here at Kaggle we\\u2019re excited to showcase the work of our Grandmasters. This post was written by Vladimir Iglovikov, and is filled with advice that he wishes someone had shared when he was active on Kaggle. The original post can be found on Vlad\\u2019s Ternaus Blog. Introduction I participated in machine learning (ML) competitions at Kaggle and other platforms to build machine learning muscles. I was 19th in the global rating, got Kaggle Grandmaster title. Every ML challenge ended with new knowledge, code, and model weights. I loved new learnings but ignored the value that old ML pipelines could bring. Code stayed in private GitHub repositories. Weights were scattered all over the hard drive. In the end, all of them were deleted. The situation is not unique to Kaggle. Same story in academia. The student trains a model, writes a paper. After it is accepted to the conference, pipelines are abandoned, training artifacts deleted and student moves on. This article will talk about small steps that you can do after the end of every ML challenge. These steps will: Boost technical knowledge. Build a personal brand. Improve career opportunities. Make the world a better place :) As an example, I will use the repository https:\\/\\/github.com\\/ternaus\\/retinaface It was not a part of a Kaggle challenge but was created to illustrate the story. I. +5 min: Release code to the Public GitHub repository Most likely, code is already at GitHub, but in a private repo. What will you lose if you will make it public? There are situations when private should stay private, but in your pet project, your Kaggle solution, or your paper, it may not be the case. The most common obstacle that I have seen: people assume that all public code should be perfect and that they will be judged if it is not the case. In reality, no one cares. Just do it. Release it as is, without any polishing. Making code public is an important psychological step. Releasing non-perfect code is a confident, bold move. Besides, all later steps are based on this one. Example: https:\\/\\/github.com\\/ternaus\\/retinaface II. +20 min: Improve readability You can improve the readability of your python code by adding syntax formatters and checkers. It is not hard and not time-consuming. Checkers and formatters will not transform bad code into good, but the readability will go up. Think about fixing syntax as about basic hygiene. It is like brushing your teeth, but for the code. I wrote a blog post on the topic called Nine Simple Steps for Better Looking python code. Feel free to check it out. Step 1: configuration files Add these files to the root of your repository. setup.cfg \\u2014 configuration for flake8 and mypy. pyproject.toml \\u2014 configuration for black. Step 2: requirements Install the required libraries with pip install black flake8 mypyStep 3: black There are 100500 ways to format the code. Formatters like black or yapf modify the code to satisfy a pre-defined set of rules. It is easier to read codebase that has some standards. When you work on the code for hours and need to switch a context between different coding styles, it drains \\u201cwillpower energy\\u201d \\u2014 no need to do it without a good reason. Running black . will reformat all python files to follow the set of rules by black. Step 4: flake8 Running flake8 will not modify the code, but will check code for syntax issues and output them to the screen. Fix them. Step 5: mypy Python does not have mandatory static typization, but it is recommended to add types to the function arguments and return types. For example: class MyModel(nn.Module): ....def forward(x: torch.Tensor) -\\/preollili\\/olpre\\/preh4\\/h4strong\\/strongstrong\\/strongstrong\\/strongstrong\\/stronga href=\\\"https:\\/\\/github.com\\/ternaus\\/retinaface\\/blob\\/master\\/.pre-commit-config.yaml\\\"\\/apre\\/prepre\\/preh4\\/h4a href=\\\"https:\\/\\/github.com\\/ternaus\\/retinaface\\/blob\\/master\\/.github\\/workflows\\/ci.yml\\\"\\/apre\\/prea href=\\\"https:\\/\\/amzn.to\\/32fIaiO\\\"\\/ah3\\/h3ullili\\/ulullililili\\/ulstrong\\/stronga href=\\\"https:\\/\\/github.com\\/ternaus\\/retinaface\\/blob\\/master\\/retinaface\\/predict_single.py\\\"\\/a\",\"1630\":\"Read the full story\",\"1675\":\"Read the full story\",\"1540\":\"Dmitri Dolgov is the CTO of Waymo, an autonomous vehicle company. Please support this podcast by checking out our sponsors: \\u2013 Tryolabs: https:\\/\\/tryolabs.com\\/lex \\u2013 Blinkist: https:\\/\\/blinkist.com\\/lex and use code LEX to get 25% off premium \\u2013 BetterHelp: https:\\/\\/betterhelp.com\\/lex to get 10% off \\u2013 Cash App: https:\\/\\/cash.app\\/ and use code LexPodcast to get $10 EPISODE LINKS: Waymo\\u2019s Twitter: https:\\/\\/twitter.com\\/waymo Waymo\\u2019s Website: https:\\/\\/waymo.com PODCAST INFO: Podcast website: https:\\/\\/lexfridman.com\\/podcast Apple Podcasts: https:\\/\\/apple.co\\/2lwqZIr Spotify: https:\\/\\/spoti.fi\\/2nEwCF8 RSS: https:\\/\\/lexfridman.com\\/feed\\/podcast\\/ YouTube Full Episodes: https:\\/\\/youtube.com\\/lexfridman YouTube Clips: https:\\/\\/youtube.com\\/lexclips SUPPORT & CONNECT: \\u2013 Check out the sponsors above, it\\u2019s the best way to support this podcast \\u2013 Support on Patreon: https:\\/\\/www.patreon.com\\/lexfridman \\u2013 Twitter: https:\\/\\/twitter.com\\/lexfridman \\u2013 Instagram: https:\\/\\/www.instagram.com\\/lexfridman \\u2013 LinkedIn: https:\\/\\/www.linkedin.com\\/in\\/lexfridman \\u2013 Facebook: https:\\/\\/www.facebook.com\\/LexFridmanPage \\u2013 Medium: https:\\/\\/medium.com\\/@lexfridman OUTLINE: Here\\u2019s the timestamps for the episode. On some podcast players you should be able to click the timestamp to jump to that time. (00:00) \\u2013 Introduction (07:46) \\u2013 Computer games (12:52) \\u2013 Childhood (15:24) \\u2013 Robotics (16:14) \\u2013 Moscow Institute of Physics and Technology (18:26) \\u2013 DARPA Urban Challenge (28:46) \\u2013 Waymo origin story (44:27) \\u2013 Waymo self-driving hardware (53:00) \\u2013 Connected cars (58:53) \\u2013 Waymo fully driverless service in Phoenix (1:03:14) \\u2013 Getting feedback from riders (1:11:28) \\u2013 Creating a product that people love (1:17:18) \\u2013 Do self-driving cars need to break the rules like humans do? (1:24:03) \\u2013 Waymo Trucks (1:29:41) \\u2013 Future of Waymo (1:42:53) \\u2013 Role of lidar in autonomous driving (1:55:53) \\u2013 Machine learning is essential for autonomous driving (1:59:55) \\u2013 Pedestrians (2:06:32) \\u2013 Trolley problem (2:11:00) \\u2013 Book recommendations (2:22:26) \\u2013 Meaning of life\",\"1433\":\"Dawn Song is a professor of computer science at UC Berkeley with research interests in security, most recently with a focus on the intersection between computer security and machine learning. Support this podcast by signing up with these sponsors: \\u2013 Cash App \\u2013 use code \\u201cLexPodcast\\u201d and download: \\u2013 Cash App (App Store): https:\\/\\/apple.co\\/2sPrUHe \\u2013 Cash App (Google Play): https:\\/\\/bit.ly\\/2MlvP5w EPISODE LINKS: Dawn\\u2019s Twitter: https:\\/\\/twitter.com\\/dawnsongtweets Dawn\\u2019s Website: https:\\/\\/people.eecs.berkeley.edu\\/~dawnsong\\/ Oasis Labs: https:\\/\\/www.oasislabs.com This conversation is part of the Artificial Intelligence podcast. If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman on Twitter, LinkedIn, Facebook, Medium,\",\"5690\":\"Hello Guys, We (IEEE Soft Robotics Podcast) would like to thank you for the great questions you sent to John, he is really a humble person and a great roboticist, we enjoyed this discussion and hopefully, that would be helpful for students to acknowledge what are the open problems, challenges and also interesting anecdotes by John, I hope you enjoy listening. ( We apologize if the post sounds self-promotional, feel free to close it if it against this subreddit rules) \\u200b The episode (audio podcast) Souncloud: https:\\/\\/soundcloud.com\\/ieeeras-softrobotics\\/john-leonard-marine-robotics-simultaneous-localization-and-mapping-slam Spotify: https:\\/\\/open.spotify.com\\/show\\/3f19OvcbN05f9r9DUY15tk ITunes: https:\\/\\/podcasts.apple.com\\/us\\/podcast\\/id1475793741 \\u200b The video format : https:\\/\\/youtu.be\\/JuQSUXF3LaM [link] [comments]\",\"1465\":\"Sara Seager is a planetary scientist at MIT, known for her work on the search for exoplanets. Support this podcast by supporting our sponsors. Click links, get discount: \\u2013 Public Goods at https:\\/\\/publicgoods.com\\/lex and use code LEX \\u2013 PowerDot: https:\\/\\/powerdot.com\\/lex and use code LEX \\u2013 Cash App \\u2013 use code \\u201cLexPodcast\\u201d and download: \\u2013 Cash App (App Store): https:\\/\\/apple.co\\/2sPrUHe \\u2013 Cash App (Google Play): https:\\/\\/bit.ly\\/2MlvP5w Episode links: Sara\\u2019s Twitter: https:\\/\\/twitter.com\\/profsaraseager Sara\\u2019s Website: https:\\/\\/www.saraseager.com\\/ The Smallest Lights in the Universe (book): https:\\/\\/amzn.to\\/3g3LfHA If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman on\",\"1664\":\"Read the full story\",\"1442\":\"Greg Brockman is the Co-Founder and CTO of OpenAI, a research organization developing ideas in AI that lead eventually to a safe & friendly artificial general intelligence that benefits and empowers humanity. Video version is available on YouTube. If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman on Twitter, LinkedIn, Facebook, Medium, or YouTube where you can watch the video versions of these conversations.\",\"489\":\"A fun puzzle stemming from repeated exponentiation. Full playlist: https:\\/\\/www.youtube.com\\/playlist?list=PLZHQObOWTQDP5CVelJJ1bNDouqrAhVPev Home page: https:\\/\\/www.3blue1brown.com Brought to you by you: https:\\/\\/3b1b.co\\/ldm-thanks Notes by Ng\\u00e2n V\\u0169: https:\\/\\/twitter.com\\/ThuyNganVu\\/status\\/1261014161464516608?s=20 Play along on Desmos: https:\\/\\/www.desmos.com\\/calculator\\/nul32eaaa9 Related videos. Calculus series: https:\\/\\/www.youtube.com\\/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr In particular look at: https:\\/\\/youtu.be\\/CfW845LNObM Numberphile on Grahm's constant: https:\\/\\/youtu.be\\/XTeJ64KD5cg ------------------ Video timeline (thanks to user \\\"noonesperfect\\\") 0:36 Question 1 1:13 Answer 1 1:29 Introduction to tetration 3:37 How exponentiation works in tetration 6:10 Python program for power tower iterations 8:40 Question 2 9:32 Python Program regarding question 2 10:37 Answer 2 and explanation 13:18 Power tower for infinite size converges or not? (Thumbnail question) 15:21 Question 3 16:28 Footage of Grant's setup arrangement problem due to construction-work back at home. 16:49 Answer 3 and explanation 17:40 Checking logic behind 2 different problems of power towers whose answer converges to the same value (Is it even possible?) 19:42 Checking same logic using Desmos graph tool i.e. Cobweb Graph (Desmos graph link in description) 28:12 Question 4 28:51 Questions from audience tweets 29:15 Knuth's Up Arrow Notation and Graham's Number (Check Numberphile video in description) 32:32 Answer 4 and explanation 37:29 Homework\\/Challenge Puzzle 39:20 Thumbnail question power tower logic 40:55 Audience questions from twitter 41:45 Power tower for complex numbers\\/Fractal set 45:19 Brainteaser 48:06 More questions from tweets 53:17 Notes for lock-down series in Grant's Tweeter ------------------ The live question setup with stats on-screen is powered by Itempool. https:\\/\\/itempool.com\\/ Curious about other animations? https:\\/\\/www.3blue1brown.com\\/faq#manim Music by Vincent Rubinetti. Download the music on Bandcamp: https:\\/\\/vincerubinetti.bandcamp.com\\/album\\/the-music-of-3blue1brown Stream the music on Spotify: https:\\/\\/open.spotify.com\\/album\\/1dVyjwS8FBqXhRunaG5W5u If you want to contribute translated subtitles or to help review those that have already been made by others and need approval, you can click the gear icon in the video and go to subtitles\\/cc, then \\\"add subtitles\\/cc\\\". I really appreciate those who do this, as it helps make the lessons accessible to more people. ------------------ 3blue1brown is a channel about animating math, in all senses of the word animate. And you know the drill with YouTube, if you want to stay posted on new videos, subscribe: http:\\/\\/3b1b.co\\/subscribe Various social media stuffs: Website: https:\\/\\/www.3blue1brown.com Twitter: https:\\/\\/twitter.com\\/3blue1brown Reddit: https:\\/\\/www.reddit.com\\/r\\/3blue1brown Instagram: https:\\/\\/www.instagram.com\\/3blue1brown_animations\\/ Patreon: https:\\/\\/patreon.com\\/3blue1brown Facebook: https:\\/\\/www.facebook.com\\/3blue1brown\",\"4896\":\"Go catch @ykilcher latest deep learning video. nitter.net\\/ykilcher\\/status\\/1352704316080091139#m\",\"1877\":\"My background includes working on scientific projects as the data guy. In these positions, I was responsible for establishing valid data collection procedures, collecting usable data, and statistically analyzing and presenting the results. In this post, I describe the excitement of being a statistician helping expand the limits of human knowledge, what I learned about [\\u2026] The post Using Applied Statistics to Expand Human Knowledge appeared first on Statistics By Jim.\",\"237\":\"eg tonight this random walk around the markets of Cairo, Egypt has been a nice background track to some late night email invidious.snopyta.org\\/watch?v=YVYDz_co\\u2026\",\"1651\":\"Read the full story\",\"1484\":\"Ian Goodfellow is the author of the popular textbook on deep learning (simply titled \\u201cDeep Learning\\u201d). He coined the term Generative Adversarial Networks (GANs) and with his 2014 paper is responsible for launching the incredible growth of research on GANs. Video version is available on YouTube. If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman on Twitter, LinkedIn, Facebook, Medium, or YouTube where you can watch the video versions of these conversations.\",\"1637\":\"A detailed plan for going from not being able to write code to being a deep learning expert. Advice based on personal experience. Read the full story\",\"1451\":\"Sean Carroll is a theoretical physicist at Caltech and Santa Fe Institute specializing in quantum mechanics, arrow of time, cosmology, and gravitation. He is the author of Something Deeply Hidden and several popular books and he is the host of a great podcast called Mindscape. This is the second time Sean has been on the podcast. You can watch the first time on YouTube or listen to the first time on its episode page. This conversation is part of the Artificial Intelligence podcast. If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman\",\"1692\":\"This article will help our readers to identify and understand the challenges faced by the AI development companies to market the AI & ML products. Read the full story\",\"976\":\"how.rl.works\\/\",\"1223\":\"This StatQuest picks up right here Part 1 left off, and this time we're going to go totally bonkers with The Chain Rule and optimize every single parameter in this simple Neural Network. BAM!!! \\u2b50 NOTE: When I code, I use Kite, a free AI-powered coding assistant that will help you code faster and smarter. The Kite plugin integrates with all the top editors and IDEs to give you smart completions and documentation while you\\u2019re typing. I love it! https:\\/\\/www.kite.com\\/get-kite\\/?utm_medium=referral&utm_source=youtube&utm_campaign=statquest&utm_content=description-only NOTE: This StatQuest assumes that you already know the main ideas behind Backpropagation: https:\\/\\/youtu.be\\/IN2XmBhILt4 ...and that also means you should be familiar with... Neural Networks: https:\\/\\/youtu.be\\/CqOfi41LfDw The Chain Rule: https:\\/\\/youtu.be\\/wl1myxrtQHQ Gradient Descent: https:\\/\\/youtu.be\\/sDv4f4s2SB8 LAST NOTE: When I was researching this 'Quest, I found this page by Sebastian Raschka to be helpful: https:\\/\\/sebastianraschka.com\\/faq\\/docs\\/backprop-arbitrary.html For a complete index of all the StatQuest videos, check out: https:\\/\\/statquest.org\\/video-index\\/ If you'd like to support StatQuest, please consider... Patreon: https:\\/\\/www.patreon.com\\/statquest ...or... YouTube Membership: https:\\/\\/www.youtube.com\\/channel\\/UCtYLUTtgS3k1Fg4y5tAhLbw\\/join ...a cool StatQuest t-shirt or sweatshirt (USA\\/Europe): https:\\/\\/teespring.com\\/stores\\/statquest (everywhere): https:\\/\\/www.redbubble.com\\/people\\/starmer\\/works\\/40421224-statquest-double-bam?asc=u&p=t-shirt ...buying one or two of my songs (or go large and get a whole album!) https:\\/\\/joshuastarmer.bandcamp.com\\/ ...or just donating to StatQuest! https:\\/\\/www.paypal.me\\/statquest Lastly, if you want to keep up with me as I research and create new StatQuests, follow me on twitter: https:\\/\\/twitter.com\\/joshuastarmer 0:00 Awesome song and introduction 1:28 The derivative of the weight W1 5:58 The derivative of the bias b1 7:39 The derivatives of W2 and b2 9:21 Gradient Descent for all parameters 11:18 Fancy Gradient Descent Animation #StatQuest #NeuralNetworks #Backpropagation\",\"972\":\"This was hilarious and has a lot of truth to it. @ykilcher\",\"1619\":\"Enterprise players across all industries are eager for optimization and improvement of their business processes: administration, customer service, marketing, sales, recruiting, and others. Today AI-driven software can cover the most common Enterprise needs like data security, data processing, resource optimization, and brand awareness. Forrester has reported that AI is also able to improve customer service and quality of existing products, increase revenue streams, and customer lifetime value. Read the full story\",\"253\":\"nitter.net\\/ykilcher\\/status\\/1349006655439179781#m\",\"1677\":\"Read the full story\",\"1705\":\"There are many articles on analyzing Spotify data and many applications as well. Some are a one-time analysis on individual's music library and some are an app for a specific purpose. This app is different in that it does not do one thing. It is meant to grow and provide a place to add more analysis. This article is about how the audio features time series was created. Read the full story\",\"2325\":\"Hi, Today CVPR reviews are released. Got weak accept, weak accept and a borderline. Any thoughts? [link] [comments]\",\"1596\":\"I switched to Signal for texting. Now my introversion & loneliness is more secure.\",\"4087\":\"Hello everyone, I am looking for recommendations for the papers covering different ml biases and some suggestive approaches to mitigate these biases and what are the pros and cons. I have very limited knowledge on this topic and would like to learn more about it and couldn't find any suitable thread for my query. I appreciate your responses. [link] [comments]\",\"1679\":\"Python is trending as the second most popular programming language in the world and grabbed its position edging out Java. Read the full story\",\"1526\":\"Erik Brynjolfsson is an economist at Stanford. Please support this podcast by checking out our sponsors: \\u2013 Vincero: https:\\/\\/vincerowatches.com\\/lex to get up to 25% off + free shipping \\u2013 Four Sigmatic: https:\\/\\/foursigmatic.com\\/lex and use code LexPod to get up to 60% off \\u2013 ExpressVPN: https:\\/\\/expressvpn.com\\/lexpod and use code LexPod to get 3 months free \\u2013 Cash App: https:\\/\\/cash.app\\/ and use code LexPodcast to get $10 EPISODE LINKS: Erik\\u2019s Twitter: https:\\/\\/twitter.com\\/erikbryn Erik\\u2019s Website: https:\\/\\/www.brynjolfsson.com\\/ The Second Machine Age (book): https:\\/\\/amzn.to\\/33f1Pk2 Machine, Platform, Crowd (book): https:\\/\\/amzn.to\\/3miJZ76 PODCAST INFO: Podcast website: https:\\/\\/lexfridman.com\\/podcast Apple Podcasts: https:\\/\\/apple.co\\/2lwqZIr Spotify: https:\\/\\/spoti.fi\\/2nEwCF8 RSS: https:\\/\\/lexfridman.com\\/feed\\/podcast\\/ YouTube Full Episodes: https:\\/\\/youtube.com\\/lexfridman\",\"1356\":\"#ai #research #attention Transformers have huge memory and compute requirements because they construct an Attention matrix, which grows quadratically in the size of the input. The Performer is a model that uses random positive orthogonal features to construct an unbiased estimator to the Attention matrix and obtains an arbitrarily good approximation in linear time! The method generalizes beyond attention and opens the door to the next generation of deep learning architectures. OUTLINE: 0:00 - Intro & Outline 6:15 - Quadratic Bottleneck in Attention Mechanisms 10:00 - Decomposing the Attention Matrix 15:30 - Approximating the Softmax Kernel 24:45 - Different Choices, Different Kernels 28:00 - Why the Naive Approach does not work! 31:30 - Better Approximation via Positive Features 36:55 - Positive Features are Infinitely Better 40:10 - Orthogonal Features are Even Better 43:25 - Experiments 49:20 - Broader Impact Statement 50:00 - Causal Attention via Prefix Sums 52:10 - Code 53:50 - Final Remarks & Conclusion Paper: https:\\/\\/arxiv.org\\/abs\\/2009.14794 Code: https:\\/\\/github.com\\/google-research\\/google-research\\/tree\\/master\\/performer Blog: https:\\/\\/ai.googleblog.com\\/2020\\/10\\/rethinking-attention-with-performers.html Kernels on ML Street Talk: https:\\/\\/www.youtube.com\\/watch?v=y_RjsDHl5Y4 My Video on Linformer: https:\\/\\/www.youtube.com\\/watch?v=-_2AF9Lhweo My Video on Reformer: https:\\/\\/www.youtube.com\\/watch?v=i4H0kjxrias My Video on Attention: https:\\/\\/www.youtube.com\\/watch?v=iDulhoQ2pro Abstract: We introduce Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using only linear (as opposed to quadratic) space and time complexity, without relying on any priors such as sparsity or low-rankness. To approximate softmax attention-kernels, Performers use a novel Fast Attention Via positive Orthogonal Random features approach (FAVOR+), which may be of independent interest for scalable kernel methods. FAVOR+ can be also used to efficiently model kernelizable attention mechanisms beyond softmax. This representational power is crucial to accurately compare softmax with other kernels for the first time on large-scale tasks, beyond the reach of regular Transformers, and investigate optimal attention-kernels. Performers are linear architectures fully compatible with regular Transformers and with strong theoretical guarantees: unbiased or nearly-unbiased estimation of the attention matrix, uniform convergence and low estimation variance. We tested Performers on a rich set of tasks stretching from pixel-prediction through text models to protein sequence modeling. We demonstrate competitive results with other examined efficient sparse and dense attention methods, showcasing effectiveness of the novel attention-learning paradigm leveraged by Performers. Authors: Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, Adrian Weller Links: YouTube: https:\\/\\/www.youtube.com\\/c\\/yannickilcher Twitter: https:\\/\\/twitter.com\\/ykilcher Discord: https:\\/\\/discord.gg\\/4H8xxDF BitChute: https:\\/\\/www.bitchute.com\\/channel\\/yannic-kilcher Minds: https:\\/\\/www.minds.com\\/ykilcher Parler: https:\\/\\/parler.com\\/profile\\/YannicKilcher LinkedIn: https:\\/\\/www.linkedin.com\\/in\\/yannic-kilcher-488534136\\/ If you want to support me, the best thing to do is to share out the content :) If you want to support me financially (completely optional and voluntary, but a lot of people have asked for this): SubscribeStar: https:\\/\\/www.subscribestar.com\\/yannickilcher Patreon: https:\\/\\/www.patreon.com\\/yannickilcher Bitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq Ethereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2 Litecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m Monero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\",\"1052\":\"Sign up with brilliant and get 20% off your annual subscription: https:\\/\\/brilliant.org\\/ZachStar\\/ Support the Channel: https:\\/\\/www.patreon.com\\/zachstar PayPal(one time donation): https:\\/\\/www.paypal.me\\/ZachStarYT \\u25baFollow me Instagram: https:\\/\\/www.instagram.com\\/zachstar\\/ Twitter: https:\\/\\/twitter.com\\/ImZachStar Previous Video: https:\\/\\/youtu.be\\/jj5lDmaQTuo Curved Spaces Program: http:\\/\\/www.geometrygames.org\\/CurvedSpaces\\/index.html.en Animations: Brainup Studios ( http:\\/\\/brainup.in\\/ ) \\u25baMy Setup: Space Pictures: https:\\/\\/amzn.to\\/2CC4Kqj Magnetic Floating Globe: https:\\/\\/amzn.to\\/2VgPdn0 Camera: https:\\/\\/amzn.to\\/2RivYu5 Mic: https:\\/\\/amzn.to\\/35bKiri Tripod: https:\\/\\/amzn.to\\/2RgMTNL Equilibrium Tube: https:\\/\\/amzn.to\\/2SowDrh \\u25baCheck out the my Amazon Store: https:\\/\\/www.amazon.com\\/shop\\/zachstar\",\"3875\":\"On the occasion of receiving the most influential test-of-time paper award for his POPL 2011 paper (which describes the technology behind the popular Flash Fill feature in Excel), Sumit shares stories related to his inspiration for the problem definition in the paper, the solution strategy, and the impact that came after. These stories span a decade before and after this paper, with this paper being the most important turning point in his research philosophy and career. These stories speak to his learnings that are as much cultural as technical. Most Influential Test of Time paper: https:\\/\\/www.microsoft.com\\/en-us\\/research\\/publication\\/automating-string-processing-spreadsheets-using-input-output-examples\\/ The PROSE research and engineering team: https:\\/\\/www.microsoft.com\\/en-us\\/research\\/group\\/prose\\/\",\"1588\":\"Michael Mina is an immunologist, epidemiologist, and physician at Harvard. Please support this podcast by checking out our sponsors: \\u2013 Brave: https:\\/\\/brave.com\\/lex \\u2013 Athletic Greens: https:\\/\\/athleticgreens.com\\/lex and use code LEX to get 1 month of fish oil \\u2013 ExpressVPN: https:\\/\\/expressvpn.com\\/lexpod and use code LexPod to get 3 months free \\u2013 Cash App: https:\\/\\/cash.app\\/ and use code LexPodcast to get $10 EPISODE LINKS: Michael\\u2019s Twitter: https:\\/\\/twitter.com\\/michaelmina_lab Michael\\u2019s Time article: https:\\/\\/time.com\\/5912705\\/covid-19-stop-spread-christmas\\/ Rapid Tests: https:\\/\\/www.rapidtests.org\\/ PODCAST INFO: Podcast website: https:\\/\\/lexfridman.com\\/podcast Apple Podcasts: https:\\/\\/apple.co\\/2lwqZIr Spotify: https:\\/\\/spoti.fi\\/2nEwCF8 RSS: https:\\/\\/lexfridman.com\\/feed\\/podcast\\/ YouTube Full Episodes: https:\\/\\/youtube.com\\/lexfridman YouTube Clips: https:\\/\\/youtube.com\\/lexclips SUPPORT & CONNECT: \\u2013 Check out the sponsors above, it\\u2019s the best way to support this podcast \\u2013 Support on Patreon: https:\\/\\/www.patreon.com\\/lexfridman \\u2013 Twitter: https:\\/\\/twitter.com\\/lexfridman \\u2013 Instagram: https:\\/\\/www.instagram.com\\/lexfridman \\u2013 LinkedIn: https:\\/\\/www.linkedin.com\\/in\\/lexfridman \\u2013 Facebook: https:\\/\\/www.facebook.com\\/LexFridmanPage \\u2013 Medium: https:\\/\\/medium.com\\/@lexfridman OUTLINE: Here\\u2019s the timestamps for the episode. On some podcast players you should be able to click the timestamp to jump to that time. (00:00) \\u2013 Introduction (07:28) \\u2013 Interacting between viruses and bacteria (11:42) \\u2013 Deadlier viruses (15:13) \\u2013 Will COVID-19 mutate? (16:47) \\u2013 Rapid testing (34:11) \\u2013 PCR vs rapid antigen tests (43:55) \\u2013 Medical industrial complex (47:47) \\u2013 Lex takes COVID test (54:32) \\u2013 FDA and cheap tests (57:17) \\u2013 Explanation of Elon Musk\\u2019s positive COVID tests (1:04:25) \\u2013 Role of testing during vaccine deployment (1:07:54) \\u2013 Public health policy (1:17:34) \\u2013 A weather system for viruses (1:34:26) \\u2013 Can a virus kill all humans? (1:40:05) \\u2013 Engineering a deadly virus (1:44:47) \\u2013 AlphaFold 2 and viruses (1:50:42) \\u2013 Advice for young people (1:58:50) \\u2013 Time as a Buddhist monk (2:04:54) \\u2013 Meditation (2:12:32) \\u2013 Meaning of life\",\"1508\":\"Vladimir Vapnik is the co-inventor of support vector machines, support vector clustering, VC theory, and many foundational ideas in statistical learning. His work has been cited over 170,000 times. He has some very interesting ideas about artificial intelligence and the nature of learning, especially on the limits of our current approaches and the open problems in the field. Video version is available on YouTube. If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman on Twitter, LinkedIn, Facebook, or YouTube where you can watch the video versions of these conversations.\",\"3759\":\"I am working with a bunch of ROS bags and I was hoping there would be an easy way to convert the data into a format that is accepted by machine learning frameworks without having to write a bunch of custom data loaders. I was considering modifying nuScenes to support my custom data, but it means I have to change all the labels and modify a bunch of stuff because my data is for autonomous boats (not autonomous vehicles). Before I go down this rabbit hole, I was wondering if anyone knew of any existing tools that could easily convert data (PCD, images, transformations, imu, sonar, etc), to different formats. I'd also appreciate if anyone knew of any universal data formats. If there isn't one, it seems like a good project. [link] [comments]\",\"1467\":\"Vitalik Buterin is co-creator of Ethereum and ether, which is a cryptocurrency that is currently the second-largest digital currency after bitcoin. Ethereum has a lot of interesting technical ideas that are defining the future of blockchain technology, and Vitalik is one of the most brilliant people innovating this space today. Support this podcast by supporting the sponsors with a special code: \\u2013 Get ExpressVPN at https:\\/\\/www.expressvpn.com\\/lexpod \\u2013 Sign up to MasterClass at https:\\/\\/masterclass.com\\/lex EPISODE LINKS: Vitalik blog: https:\\/\\/vitalik.ca Ethereum whitepaper: http:\\/\\/bit.ly\\/3cVDTpj Casper FFG (paper): http:\\/\\/bit.ly\\/2U6j7dJ Quadratic funding (paper): http:\\/\\/bit.ly\\/3aUZ8Wd Bitcoin whitepaper: https:\\/\\/bitcoin.org\\/bitcoin.pdf Mastering Ethereum (book): https:\\/\\/amzn.to\\/2xEjWmE This conversation is part\",\"1233\":\"This StatQuest is sponsored by JADBIO. Just Add Data, and their automatic machine learning algorithms will do all of the work for you. For more details, see: https:\\/\\/bit.ly\\/3bxtheb AutoML is all the rage these days. In this StatQuest, I interview CEO and Co-Founder of Gnosis Data Analysis to find out the latest info. \\u2b50 NOTE: When I code, I use Kite, a free AI-powered coding assistant that will help you code faster and smarter. The Kite plugin integrates with all the top editors and IDEs to give you smart completions and documentation while you\\u2019re typing. I love it! https:\\/\\/www.kite.com\\/get-kite\\/?utm_medium=referral&utm_source=youtube&utm_campaign=statquest&utm_content=description-only For a complete index of all the StatQuest videos, check out: https:\\/\\/statquest.org\\/video-index\\/ If you'd like to support StatQuest, please consider... Patreon: https:\\/\\/www.patreon.com\\/statquest ...or... YouTube Membership: https:\\/\\/www.youtube.com\\/channel\\/UCtYLUTtgS3k1Fg4y5tAhLbw\\/join ...a cool StatQuest t-shirt or sweatshirt (USA\\/Europe): https:\\/\\/teespring.com\\/stores\\/statquest (everywhere): https:\\/\\/www.redbubble.com\\/people\\/starmer\\/works\\/40421224-statquest-double-bam?asc=u&p=t-shirt ...buying one or two of my songs (or go large and get a whole album!) https:\\/\\/joshuastarmer.bandcamp.com\\/ ...or just donating to StatQuest! https:\\/\\/www.paypal.me\\/statquest Lastly, if you want to keep up with me as I research and create new StatQuests, follow me on twitter: https:\\/\\/twitter.com\\/joshuastarmer 0:00 Awesome song and introduction 0:43 What is AutoML? 3:04 Who is AutoML for? 5:04 What do we need to know about AutoML? 7:38 Manual ML vs AutoML 10:46 Will AutoML replace Data Science jobs? 12:28 What is the future for AutoML? #StatQuest #AutoML\",\"1562\":\"Sergey Levine is a professor at Berkeley and a world-class researcher in deep learning, reinforcement learning, robotics, and computer vision, including the development of algorithms for end-to-end training of neural network policies that combine perception and control, scalable algorithms for inverse reinforcement learning, and deep RL algorithms. Support this podcast by supporting these sponsors: \\u2013 ExpressVPN: https:\\/\\/www.expressvpn.com\\/lexpod \\u2013 Cash App \\u2013 use code \\u201cLexPodcast\\u201d and download: \\u2013 Cash App (App Store): https:\\/\\/apple.co\\/2sPrUHe \\u2013 Cash App (Google Play): https:\\/\\/bit.ly\\/2MlvP5w If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman on Twitter, LinkedIn, Facebook,\",\"1517\":\"Brian Kernighan is a professor of computer science at Princeton University. He co-authored the C Programming Language with Dennis Ritchie (creator of C) and has written a lot of books on programming, computers, and life including the Practice of Programming, the Go Programming Language, his latest UNIX: A History and a Memoir. He co-created AWK, the text processing language used by Linux folks like myself. He co-designed AMPL, an algebraic modeling language for large-scale optimization. Support this podcast by supporting our sponsors: \\u2013 Eight Sleep: https:\\/\\/eightsleep.com\\/lex \\u2013 Raycon: http:\\/\\/buyraycon.com\\/lex If you would like to get more information about this podcast\",\"1448\":\"Lisa Feldman Barrett is a neuroscientist, psychologist, and author. Please support this podcast by checking out our sponsors: \\u2013 Athletic Greens: https:\\/\\/athleticgreens.com\\/lex and use code LEX to get 1 month of fish oil \\u2013 Eight Sleep: https:\\/\\/www.eightsleep.com\\/lex and use code LEX to get $200 off \\u2013 MasterClass: https:\\/\\/masterclass.com\\/lex to get 15% off annual sub \\u2013 BetterHelp: https:\\/\\/betterhelp.com\\/lex to get 10% off EPISODE LINKS: Seven and a Half Lessons About the Brain (book): https:\\/\\/amzn.to\\/2Sp5ar9 How Emotions Are Made (book): https:\\/\\/amzn.to\\/2GwAFg6 Lisa\\u2019s Twitter: https:\\/\\/twitter.com\\/LFeldmanBarrett Lisa\\u2019s Website: https:\\/\\/lisafeldmanbarrett.com\\/ PODCAST INFO: Podcast website: https:\\/\\/lexfridman.com\\/podcast Apple Podcasts: https:\\/\\/apple.co\\/2lwqZIr Spotify: https:\\/\\/spoti.fi\\/2nEwCF8 RSS: https:\\/\\/lexfridman.com\\/feed\\/podcast\\/ YouTube Full Episodes:\",\"2321\":\"From https:\\/\\/twitter.com\\/advadnoun\\/status\\/1351038053033406468: The Big Sleep Here's the notebook for generating images by using CLIP to guide BigGAN. It's very much unstable and a prototype, but it's also a fair place to start. I'll likely update it as time goes on. colab.research.google.com\\/drive\\/1NCceX2mbiKOSlAd_o7IU7nA9UskKN5WR?usp=sharing Change the text in the above notebook in the Parameters section to your desired text. The first output image usually seems to be a very poor match to the desired text, but the second output image often seems to be a decent match to the desired text. This system is non-deterministic. In other words, different runs using the same inputs can (and seemingly usually do) result in different outputs. Steps to follow if you want to generate a different image with the same Colab instance: Click menu item Runtime-lilili\",\"1663\":\"Russian doomer neural network creates paintings and music videos. Tutorial. Stylegan2 was trained on thousands of images of soviet architecture. Read the full story\",\"1560\":\"Guido van Rossum is the creator of Python, one of the most popular and impactful programming languages in the world. Video version is available on YouTube. If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman on Twitter, LinkedIn, Facebook, or YouTube where you can watch the video versions of these conversations.\",\"1543\":\"Chris Urmson was the CTO of the Google Self-Driving Car team, a key engineer and leader behind the Carnegie Mellon autonomous vehicle entries in the DARPA grand challenges and the winner of the DARPA urban challenge. Today he is the CEO of Aurora Innovation, an autonomous vehicle software company he started with Sterling Anderson, who was the former director of Tesla Autopilot, and Drew Bagnell, Uber\\u2019s former autonomy and perception lead. This conversation is part of the Artificial Intelligence podcast. If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman on Twitter, LinkedIn,\",\"1623\":\"Read the full story\",\"1550\":\"Bjarne Stroustrup is the creator of C++, a programming language that after 40 years is still one of the most popular and powerful languages in the world. Its focus on fast, stable, robust code underlies many of the biggest systems in the world that we have come to rely on as a society. If you\\u2019re watching this on YouTube, many of the critical back-end component of YouTube are written in C++. Same goes for Google, Facebook, Amazon, Twitter, most Microsoft applications, Adobe applications, most database systems, and most physical systems that operate in the real-world like cars, robots, rockets that\",\"644\":\"Avi Loeb is an astrophysicist at Harvard. Please support this podcast by checking out our sponsors: - Zero Fasting: https:\\/\\/go.zerofasting.com\\/s\\/lex-promo to get 30% off annual subscription - LMNT: https:\\/\\/drinkLMNT.com\\/lex to get free sample pack - Sun Basket: https:\\/\\/sunbasket.com\\/lex and use code LEX to get $35 off - Pessimists Archive: https:\\/\\/pessimists.co\\/ EPISODE LINKS: Extraterrestrial (book): https:\\/\\/amzn.to\\/39xdnkT Avi's Website: https:\\/\\/astronomy.fas.harvard.edu\\/people\\/avi-loeb PODCAST INFO: Podcast website: https:\\/\\/lexfridman.com\\/podcast Apple Podcasts: https:\\/\\/apple.co\\/2lwqZIr Spotify: https:\\/\\/spoti.fi\\/2nEwCF8 RSS: https:\\/\\/lexfridman.com\\/feed\\/podcast\\/ Full episodes playlist: https:\\/\\/www.youtube.com\\/playlist?list=PLrAXtmErZgOdP_8GztsuKi9nrraNbKKp4 Clips playlist: https:\\/\\/www.youtube.com\\/playlist?list=PLrAXtmErZgOeciFP3CBCIEElOJeitOr41 OUTLINE: 0:00 - Introduction 2:31 - Are we alone in the universe? 6:46 - Consciousness 11:23 - Sending digital copies of humans to space 16:01 - Oumuamua 38:04 - Alien space junk 42:03 - What do aliens look like? 59:21 - Drake equation 1:00:23 - Industrial polution from aliens 1:12:15 - UFO sightings 1:20:11 - How long will human civilization last? 1:22:51 - Radio signal from Proxima Centauri 1:26:12 - Breakthrough Starshot project 1:29:11 - Space race 1:34:22 - Human space exploration 1:39:38 - Social media is a threat to society 1:44:26 - Are humans ready for discovering an alien civilization? 1:48:38 - Mayans used astrology to wage war 1:49:53 - Black holes 2:08:43 - Stephen Hawking 2:12:21 - Grigori Perelman 2:16:46 - Theory of everything 2:23:45 - Dark matter 2:26:28 - Advice for young people 2:29:32 - Memories of my father and mother 2:34:01 - Existentialism 2:36:15 - Mortality 2:38:49 - Meaning of life CONNECT: - Subscribe to this YouTube channel - Twitter: https:\\/\\/twitter.com\\/lexfridman - LinkedIn: https:\\/\\/www.linkedin.com\\/in\\/lexfridman - Facebook: https:\\/\\/www.facebook.com\\/LexFridmanPage - Instagram: https:\\/\\/www.instagram.com\\/lexfridman - Medium: https:\\/\\/medium.com\\/@lexfridman - Support on Patreon: https:\\/\\/www.patreon.com\\/lexfridman\",\"973\":\"OpenAI DALL\\u00b7E: Fighter Jet For The Mind! \\u2708\\ufe0f \\u25b6\\ufe0fFull video (ours): invidious.snopyta.org\\/C7D5EzkhT6A \\ud83d\\udcdcSource post: openai.com\\/blog\\/dall-e\\/ #dalle #openai #gpt3 #ai #deeplearning #machinelearning #twominutepapers #whatatimetobealive\",\"1533\":\"Eugenia Kuyda co-founder of Replika, an AI companion. Please check out our sponsors to get a discount and to support this podcast: \\u2013 Dollar Shave Club: https:\\/\\/dollarshaveclub.com\\/lex \\u2013 DoorDash: download app & use code LEX \\u2013 Cash App: download app & use code \\u201cLexPodcast\\u201d Episode links: Eugenia\\u2019s Twitter: https:\\/\\/twitter.com\\/ekuyda Replika\\u2019s Twitter: https:\\/\\/twitter.com\\/myreplika Replika\\u2019s Website: https:\\/\\/replika.ai If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/podcast or connect with @lexfridman on Twitter, LinkedIn, Facebook, Medium, or YouTube where you can watch the video versions of these conversations. If you enjoy the podcast, please rate it 5 stars on\",\"1572\":\"Michael Littman is a computer scientist at Brown University. Please support this podcast by checking out our sponsors: \\u2013 SimpliSafe: https:\\/\\/simplisafe.com\\/lex and use code LEX to get a free security camera \\u2013 ExpressVPN: https:\\/\\/expressvpn.com\\/lexpod and use code LexPod to get 3 months free \\u2013 MasterClass: https:\\/\\/masterclass.com\\/lex to get 2 for price of 1 \\u2013 BetterHelp: https:\\/\\/betterhelp.com\\/lex to get 10% off EPISODE LINKS: Michael\\u2019s Twitter: https:\\/\\/twitter.com\\/mlittmancs Michael\\u2019s Website: https:\\/\\/www.littmania.com\\/ Michael\\u2019s YouTube: https:\\/\\/www.youtube.com\\/user\\/mlittman PODCAST INFO: Podcast website: https:\\/\\/lexfridman.com\\/podcast Apple Podcasts: https:\\/\\/apple.co\\/2lwqZIr Spotify: https:\\/\\/spoti.fi\\/2nEwCF8 RSS: https:\\/\\/lexfridman.com\\/feed\\/podcast\\/ YouTube Full Episodes: https:\\/\\/youtube.com\\/lexfridman YouTube Clips: https:\\/\\/youtube.com\\/lexclips SUPPORT & CONNECT: \\u2013 Check out the sponsors above, it\\u2019s\",\"1469\":\"Joscha Bach is the VP of Research at the AI Foundation, previously doing research at MIT and Harvard. Joscha work explores the workings of the human mind, intelligence, consciousness, life on Earth, and the possibly-simulated fabric of our universe. Support this podcast by supporting these sponsors: \\u2013 ExpressVPN at https:\\/\\/www.expressvpn.com\\/lexpod \\u2013 Cash App \\u2013 use code \\u201cLexPodcast\\u201d and download: \\u2013 Cash App (App Store): https:\\/\\/apple.co\\/2sPrUHe \\u2013 Cash App (Google Play): https:\\/\\/bit.ly\\/2MlvP5w This conversation is part of the Artificial Intelligence podcast. If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman on Twitter, LinkedIn,\",\"1229\":\"NOTE: You can support StatQuest by purchasing the Jupyter Notebook and Python code seen in this video here: https:\\/\\/statquest.org\\/product\\/jupyter-notebook-classification-trees-in-python-from-start-to-finish\\/ \\u2b50 NOTE: When I code, I use Kite, a free AI-powered coding assistant that will help you code faster and smarter. The Kite plugin integrates with all the top editors and IDEs to give you smart completions and documentation while you\\u2019re typing. I love it! https:\\/\\/www.kite.com\\/get-kite\\/?utm_medium=referral&utm_source=youtube&utm_campaign=statquest&utm_content=description-only This webinar was recorded 20200528 at 11:00am (New York time). NOTE: This StatQuest assumes are already familiar with: Decision Trees: https:\\/\\/youtu.be\\/7VeUPuFGJHk Cross Validation: https:\\/\\/youtu.be\\/fSytzGwwBVw Confusion Matrices: https:\\/\\/youtu.be\\/Kdsp6soqA7o Cost Complexity Pruning: https:\\/\\/youtu.be\\/D0efHEJsfHo Bias and Variance and Overfitting: https:\\/\\/youtu.be\\/EuBBz3bI-aA For a complete index of all the StatQuest videos, check out: https:\\/\\/statquest.org\\/video-index\\/ If you'd like to support StatQuest, please consider... Patreon: https:\\/\\/www.patreon.com\\/statquest ...or... YouTube Membership: https:\\/\\/www.youtube.com\\/channel\\/UCtYLUTtgS3k1Fg4y5tAhLbw\\/join ...a cool StatQuest t-shirt or sweatshirt (USA\\/Europe): https:\\/\\/teespring.com\\/stores\\/statquest (everywhere): https:\\/\\/www.redbubble.com\\/people\\/starmer\\/works\\/40421224-statquest-double-bam?asc=u&p=t-shirt ...buying one or two of my songs (or go large and get a whole album!) https:\\/\\/joshuastarmer.bandcamp.com\\/ ...or just donating to StatQuest! https:\\/\\/www.paypal.me\\/statquest Lastly, if you want to keep up with me as I research and create new StatQuests, follow me on twitter: https:\\/\\/twitter.com\\/joshuastarmer 0:00 Awesome song and introduction 5:23 Import Modules 7:40 Import Data 11:18 Missing Data Part 1: Identifying 15:57 Missing Data Part 2: Dealing with it 21:16 Format Data Part 1: X and y 23:33 Format Data Part 2: One-Hot Encoding 37:29 Build Preliminary Tree 46:31 Pruning Part 1: Visualize Alpha 51:22 Pruning Part 2: Cross Validation 56:46 Build and Draw Final Tree #StatQuest #ML #ClassificationTrees\",\"1231\":\"One of the most basic concepts in statistics is hypothesis testing and something called The Null Hypothesis. This video breaks these concepts down into easy to understand pieces so that you can understand their motivation and their uses. By the time you're done with this video, Hypothesis Testing and the Null Hypothesis will be Clearly Explained!!! NOTE: If you'd like to learn about The Alternative Hypothesis, check out... And if you'd like to learn about p-values, check out... p-values: What they are and how to interpret them: https:\\/\\/youtu.be\\/vemZtEM63GY How to Calculate p-values: https:\\/\\/youtu.be\\/JQc3yx0-Q9E \\u2b50 NOTE: When I code, I use Kite, a free AI-powered coding assistant that will help you code faster and smarter. The Kite plugin integrates with all the top editors and IDEs to give you smart completions and documentation while you\\u2019re typing. I love it! https:\\/\\/www.kite.com\\/get-kite\\/?utm_medium=referral&utm_source=youtube&utm_campaign=statquest&utm_content=description-only For a complete index of all the StatQuest videos, check out: https:\\/\\/statquest.org\\/video-index\\/ If you'd like to support StatQuest, please consider... Patreon: https:\\/\\/www.patreon.com\\/statquest ...or... YouTube Membership: https:\\/\\/www.youtube.com\\/channel\\/UCtYLUTtgS3k1Fg4y5tAhLbw\\/join ...a cool StatQuest t-shirt or sweatshirt (USA\\/Europe): https:\\/\\/teespring.com\\/stores\\/statquest (everywhere): https:\\/\\/www.redbubble.com\\/people\\/starmer\\/works\\/40421224-statquest-double-bam?asc=u&p=t-shirt ...buying one or two of my songs (or go large and get a whole album!) https:\\/\\/joshuastarmer.bandcamp.com\\/ ...or just donating to StatQuest! https:\\/\\/www.paypal.me\\/statquest Lastly, if you want to keep up with me as I research and create new StatQuests, follow me on twitter: https:\\/\\/twitter.com\\/joshuastarmer 0:00 Awesome song and introduction 0:22 Background 2:32 First hypothesis 4:09 Rejecting a hypothesis 4:40 Second hypothesis 7:14 Failing to reject a hypothesis 8:21 Rejecting vs Failing to Reject 9:01 Motivation for the Null Hypothesis 9:51 The Null Hypothesis 13:44 The next steps #StatQuest #NullHypothesis\",\"1559\":\"Andrew Ng is one of the most impactful educators, researchers, innovators, and leaders in artificial intelligence and technology space in general. He co-founded Coursera and Google Brain, launched deeplearning.ai, Landing.ai, and the AI fund, and was the Chief Scientist at Baidu. As a Stanford professor, and with Coursera and deeplearning.ai, he has helped educate and inspire millions of students including me. EPISODE LINKS: Andrew Twitter: https:\\/\\/twitter.com\\/AndrewYNg Andrew Facebook: https:\\/\\/www.facebook.com\\/andrew.ng.96 Andrew LinkedIn: https:\\/\\/www.linkedin.com\\/in\\/andrewyng\\/ deeplearning.ai: https:\\/\\/www.deeplearning.ai landing.ai: https:\\/\\/landing.ai AI Fund: https:\\/\\/aifund.ai\\/ AI for Everyone: https:\\/\\/www.coursera.org\\/learn\\/ai-for-everyone The Batch newsletter: https:\\/\\/www.deeplearning.ai\\/thebatch\\/ This conversation is part of the Artificial Intelligence podcast. If you would like to\",\"1224\":\"Backpropagation is the method we use to optimize parameters in a Neural Network. The ideas behind backpropagation are quite simple, but there are tons of details. This StatQuest focuses on explaining the main ideas in a way that is easy to understand. \\u2b50 NOTE: When I code, I use Kite, a free AI-powered coding assistant that will help you code faster and smarter. The Kite plugin integrates with all the top editors and IDEs to give you smart completions and documentation while you\\u2019re typing. I love it! https:\\/\\/www.kite.com\\/get-kite\\/?utm_medium=referral&utm_source=youtube&utm_campaign=statquest&utm_content=description-only NOTE: This StatQuest assumes that you already know the main ideas behind... Neural Networks: https:\\/\\/youtu.be\\/CqOfi41LfDw The Chain Rule: https:\\/\\/youtu.be\\/wl1myxrtQHQ Gradient Descent: https:\\/\\/youtu.be\\/sDv4f4s2SB8 LAST NOTE: When I was researching this 'Quest, I found this page by Sebastian Raschka to be helpful: https:\\/\\/sebastianraschka.com\\/faq\\/docs\\/backprop-arbitrary.html For a complete index of all the StatQuest videos, check out: https:\\/\\/statquest.org\\/video-index\\/ If you'd like to support StatQuest, please consider... Patreon: https:\\/\\/www.patreon.com\\/statquest ...or... YouTube Membership: https:\\/\\/www.youtube.com\\/channel\\/UCtYLUTtgS3k1Fg4y5tAhLbw\\/join ...a cool StatQuest t-shirt or sweatshirt (USA\\/Europe): https:\\/\\/teespring.com\\/stores\\/statquest (everywhere): https:\\/\\/www.redbubble.com\\/people\\/starmer\\/works\\/40421224-statquest-double-bam?asc=u&p=t-shirt ...buying one or two of my songs (or go large and get a whole album!) https:\\/\\/joshuastarmer.bandcamp.com\\/ ...or just donating to StatQuest! https:\\/\\/www.paypal.me\\/statquest Lastly, if you want to keep up with me as I research and create new StatQuests, follow me on twitter: https:\\/\\/twitter.com\\/joshuastarmer 0:00 Awesome song and introduction 3:55 Fitting the Neural Network to the data 6:04 The Sum of the Squared Residuals 7:23 Testing different values for a parameter 8:38 Using the Chain Rule to calculate a derivative 13:28 Using Gradient Descent 16:05 Summary #StatQuest #NeuralNetworks #Backpropagation\",\"1497\":\"Steven Pinker is a professor at Harvard and before that was a professor at MIT. He is the author of many books, several of which have had a big impact on the way I see the world for the better. In particular, The Better Angels of Our Nature and Enlightenment Now have instilled in me a sense of optimism grounded in data, science, and reason. Video version is available on YouTube. If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman on Twitter, LinkedIn, Facebook, or YouTube where you can watch the video\",\"2334\":\"Hi, while I learned quite a bit about machine learning at university I never really actually deployed machine learning generated code, which I want to change. So here is the machine learning part of my first project: I have a collection of around 5k square images with sizes a href=\\\"https:\\/\\/github.com\\/lucidrains\\/stylegan2-pytorch\\\"\\/a!-- SC_ON --a href=\\\"https:\\/\\/teddit.net\\/r\\/MachineLearning\\/comments\\/kzseha\\/p_stylegan_on_5k_images\\/\\\"\\/aa href=\\\"https:\\/\\/teddit.net\\/r\\/MachineLearning\\/comments\\/kzseha\\/p_stylegan_on_5k_images\\/\\\"\\/a\",\"1676\":\"The original AI Dungeon was made just over a year ago, the result of a curious gamer, a hackathon, and the GPT-2 text transformer. Fast forward to the present day, and AI Dungeon has expanded into a unique example of creative AI technology. The game now boasts 1.5 million players, multiple genres for stories, and even multiplayer adventures. Read the full story\",\"228\":\"Finding it increasingly hard to keep up with all of the activity in deep learning right now, as # tabs -\",\"198\":\"I\\u2019m a PhD student of the Machine Learning Group in the University of Waikato, Hamilton, New Zealand. I\\u2019m also a part-time software developer for 11ants analytics. My PhD research focuses on meta-learning and the full model selection problem. In 2009 and 2010, I participated the UCSD\\/FICO data mining contests. What I tried and What ended up working I tried many different algorithms (mainly weka and matlab implementations) and feature sets in nearly 80 submissions. This report will briefly introduce two approaches that worked for this competition. Each of them will be discussed sequentially in the order of submissions. After the first 10 testing submissions, I realised that there was a concept drift happening between 2007 and 2008. The success rates decline gradually from 2007. Also, on the information page of the contest, it states that \\u201cIn Australia, success rates have fallen to 20\\u201325 per cent\\u2026\\u201d. To me, this probably means, the decision rules for grant applications were somehow changed during 2007 and 2008. Here are some consequences that I could think of, including but not limited to: The overall success rates will continue to drop Successful applications in 2005\\/2006 would be declined in 2007\\/2008, so for 2009\\/2010 Success patterns becoming to be \\u201cmore\\u201d random Decision rules for year 2009\\/2010 will be close to that for 2007\\/2008, compared with rules for year 2006 and prior. Based on the information and assumptions above, I decided to mainly use data points from 2007 and 2008 for training my classifiers, which turns out to be a reasonable choice. Approach A: Ensemble Selection with transformed feature set (used in the first 20 submissions) Data engineering\\/transformation part Start.date to numeric, year, month, day in numbers RFCD.Code.X (X=1 to 5) to nominal Person.ID.X (X=1 to 15) to nominal Number.of.Grant.X (X=1 to 15) Total number of successful\\/unsuccessful grants per application Publications AA, A, B, C Total number of AA, A, B, C publications per application Role.X Total number of CHIEF_INVESTIGATORs, PRINCIPAL_SUPERVISORs, DELEGATED_RESEARCHER, EXT_CHIEF_INVESTIGATORs per application Country.of.Birth.X Total number of Asia_Pacific born, Australia, Great_Britain, Western_Europe, Eastern_Europe, North_America, New_Zealand, Middle_East_and_Africa per application With.PHD Total number of PhDs per application Years.IN.UNI Total number of people who has been in the University for more than 5 years After all those transformations are done, I also had a java program to transform all nominal attributes to its corresponding frequency. The frequency counting is based on all the available data points. So, the final feature set consists of the original features, transformed features and frequency. Modeling part My main method is called Ensemble Selection, originally proposed by Rich Caruana and co-authors of Cornell University (http:\\/\\/portal.acm.org\\/citation.cfm?id=1015432). The following pseudocode demonstrates the basic idea of Ensemble Selection: 0. Split the data into two parts: The build set and the hillclimb set 1. Start with the empty ensemble. 2. Add to the ensemble the model (trained on \\u201cbuild\\u201d set) in the library that maximizes the ensemble\\u2019s performance to the error metric (AUC for this contest) on a \\u201chillclimb\\u201d (validation) set. 3. Repeat Step 2 for a \\ufb01xed number of iterations or until all the models have been used. 4. Return the ensemble from the nested set of ensembles that has maximum performance on the hillclimb (validation) set. Model library used for my Ensemble Selection system: AdaBoost, LogitBoost, RealAdaBoost, DecisionTable, RotationForest, BayesNet, NaiveBayes, 7 algorithms with different parameters, in total 28 base classifiers. Building set and hillclimb set for Ensemble Selection: Data points from year 2007 are used as the \\u201cbuild set\\u201d Data points from year 2008 are used as the \\u201chillclimb set\\u201d Or Data points from year 2007\\/01\\/01 to 2008\\/04\\/30 are used as the \\u201cbuild set\\u201d Data points after year 2008\\/04\\/30 are used as the \\u201chillclimb set\\u201d Both setups worked well for the Ensemble Selection approach. In summary, the final system for Approach A consists of three main components: Data points from 2007 for training and 2008 for hillclimbing. Ensemble Selection, num of bags: 10, hillclimb iterations = size of the model library. In total 352 features. Learderboard AUC: 0.956X, Best final test set AUC: 0.961X From submission 20 to the end of the competition, the following features are added to Approach A feature set: Number of missing values Number of non-missing values Missing value rate Transform \\u201cContract.Value.Band\\u201d to numeric values Average contract value RFCD.CODE mean, sum, max, min, standard deviation per application based RFCD.PCT mean, sum, max, min, std per application based SEO.CODE mean, sum, max, min, std per application based SEO.PCT mean, sum, max, min, std per application based Successful.grant mean, sum, max, min, std per application based Unsuccessful.grant mean, sum, max, min, std per application based Successful.grant mean average per application based Successful.grant sum average per application based All the above features for the first three applicants All the above features for Unsuccessful.grant Success rate of applicant 1, applicant 2, and applicant 3 per application based Success rate of all applicants per application based Mean, max, std success rates of all applicants per application based Number of publications mean, sum, max, min, std per application based Except the frequency counting described in Approach A, only \\u201crow-based (per-application-based)\\u201d statistical features were gradually introduced to my system during the competition, because I thought that, compared with \\u201ctime based\\/column based features\\u201d, \\u201crow-based\\u201d statistical features would reduce the chance of overfitting. Also, the following algorithms (with different\\/diverse parameter settings) were gradually added to the model library while the competition: RandomForest RacedIncrementalLogitBoost Bagging with trees ADTree Linear Regression RandomCommittee with Random Trees Dagging J48 Approach B: Rotation Forest with the feature set from Approach A I tried using only Rotation forest (http:\\/\\/www.computer.org\\/portal\\/web\\/csdl\\/doi\\/10.1109\\/TPAMI.2006.211) with the following setup: Base classifier: M5P model tree (weka default is J48) Rotation method: Random Projection with Gaussian distribution (weka default is PCA) The Rotation forest classifier was trained on data points from 2007 and 2008 with the feature set from Approach A. Here are the results: Leaderboard AUC: 0.947X, Final test set AUC: 0.962X Averaging the two approaches could improve the final test set AUC to 0.963X. What tools I used Software\\/Tools used for modelling and data analysis: Weka 3.7.1 is used for modelling (with my own improved version of the Ensemble Selection algorithm) Matlab and SAS are used for data visualization and statistical analysis Java is used as the main programming language for this project Most experiments were done on my home PC: AMD 6-core, 16G ram on Windows system. Originally published at blog.kaggle.com on February 22, 2011. Quan Sun on finishing in second place in Predict Grant Applications was originally published in Kaggle Blog on Medium, where people are continuing the conversation by highlighting and responding to this story.\",\"1686\":\"Read the full story\",\"1058\":\"Get free access to over 2500 documentaries on CuriosityStream: https:\\/\\/curiositystream.thld.co\\/zachstaroct16 (use code \\\"zachstar\\\" at sign up) STEMerch Store: https:\\/\\/stemerch.com\\/ (book recommendation list now available) Support the Channel: https:\\/\\/www.patreon.com\\/zachstar PayPal(one time donation): https:\\/\\/www.paypal.me\\/ZachStarYT Join this channel to get access to perks: https:\\/\\/www.youtube.com\\/channel\\/UCpCSAcbqs-sjEVfk_hMfY9w\\/join \\u25baFollow me Instagram: https:\\/\\/www.instagram.com\\/zachstar\\/ Twitter: https:\\/\\/twitter.com\\/ImZachStar \\u25baPuzzle Books (affiliates) Martin Gardener Book of Puzzles: https:\\/\\/amzn.to\\/3jfEdAU Peter Winkler Mathematical Puzzles: https:\\/\\/amzn.to\\/34cUy54 Peter Winkler Video (he mentions this problem at 1:05:10): https:\\/\\/www.youtube.com\\/watch?v=bGQKgIEX1Do Animations: Brainup Studios ( http:\\/\\/brainup.in\\/ ) Check out my Spanish channel here: https:\\/\\/www.youtube.com\\/channel\\/UCnkNu2xQBLASpj6cKC8vtpA \\u25baMy Setup: Space Pictures: https:\\/\\/amzn.to\\/2CC4Kqj Magnetic Floating Globe: https:\\/\\/amzn.to\\/2VgPdn0 Camera: https:\\/\\/amzn.to\\/2RivYu5 Mic: https:\\/\\/amzn.to\\/35bKiri Tripod: https:\\/\\/amzn.to\\/2RgMTNL Equilibrium Tube: https:\\/\\/amzn.to\\/2SowDrh \\u25baCheck out my Amazon Store: https:\\/\\/www.amazon.com\\/shop\\/zachstar\",\"1621\":\"Data is a central piece of the climate change debate. With the climate change datasets on this list, many data scientists have created visualizations and models to measure and track the change in surface temperatures, sea ice levels, and more. Many of these datasets have been made public to allow people to contribute and add valuable insight into the way the climate is changing and its causes. Read the full story\",\"488\":\"Part 1: https:\\/\\/youtu.be\\/X8jsijhllIA Watch Ben Eater's video: https:\\/\\/youtu.be\\/h0jloehRKas Viewer-supported: https:\\/\\/3b1b.co\\/hamming-thanks A cleaner perspective on Hamming error correction codes. You can read Hamming's own perspective on his discovery of these codes in chapter 12 of \\\"The Art of Doing Science and Engineering\\\". https:\\/\\/amzn.to\\/3lwcnmh Heavily related is the chessboard puzzle I did with Matt Parker: https:\\/\\/youtu.be\\/as7Gkm7Y7h4 If you're curious to learn a bit about Shannon, the father of information theory, take a look at this documentary. https:\\/\\/amzn.to\\/2RHK5HL ------------------ These animations are largely made using manim, a scrappy open-source python library: https:\\/\\/github.com\\/3b1b\\/manim If you want to check it out, I feel compelled to warn you that it's not the most well-documented tool, and it has many other quirks you might expect in a library someone wrote with only their own use in mind. Music by Vincent Rubinetti. Download the music on Bandcamp: https:\\/\\/vincerubinetti.bandcamp.com\\/album\\/the-music-of-3blue1brown Stream the music on Spotify: https:\\/\\/open.spotify.com\\/album\\/1dVyjwS8FBqXhRunaG5W5u ------------------ 3blue1brown is a channel about animating math, in all senses of the word animate. And you know the drill with YouTube, if you want to stay posted on new videos, subscribe: http:\\/\\/3b1b.co\\/subscribe Various social media links: Website: https:\\/\\/www.3blue1brown.com Twitter: https:\\/\\/twitter.com\\/3blue1brown Reddit: https:\\/\\/www.reddit.com\\/r\\/3blue1brown Instagram: https:\\/\\/www.instagram.com\\/3blue1brown_animations\\/ Patreon: https:\\/\\/patreon.com\\/3blue1brown Facebook: https:\\/\\/www.facebook.com\\/3blue1brown\",\"1670\":\"Read the full story\",\"242\":\"just binge watching Journey to the Microcosmos invidious.snopyta.org\\/microcosmos\\/vi\\u2026\",\"4302\":\"Connor Tan is a physicist and senior data scientist working for a multinational energy company where he co-founded and leads a data science team. He holds a first-class degree in experimental and theoretical physics from Cambridge university. With a master's in particle astrophysics. He specializes in the application of machine learning models and Bayesian methods. Today we explore the history, pratical utility, and unique capabilities of Bayesian methods. We also discuss the computational difficulties inherent in Bayesian methods along with modern methods for approximate solutions such as Markov Chain Monte Carlo. Finally, we discuss how Bayesian optimization in the context of automl may one day put Data Scientists like Connor out of work. Panel: Dr. Keith Duggar, Alex Stenlake, Dr. Tim Scarfe 00:00:00 Duggars philisophical ramblings on Bayesianism 00:05:10 Introduction 00:07:30 small datasets and prior scientific knowledge 00:10:37 Bayesian methods are probability theory 00:14:00 Bayesian methods demand hard computations 00:15:46 uncertainty can matter more than estimators 00:19:29 updating or combining knowledge is a key feature 00:25:39 Frequency or Reasonable Expectation as the Primary Concept 00:30:02 Gambling and coin flips 00:37:32 Rev. Thomas Bayes's pool table 00:40:37 ignorance priors are beautiful yet hard 00:43:49 connections between common distributions 00:49:13 A curious Universe, Benford's Law 00:55:17 choosing priors, a tale of two factories 01:02:19 integration, the computational Achilles heel 01:35:25 Bayesian social context in the ML community 01:10:24 frequentist methods as a first approximation 01:13:13 driven to Bayesian methods by small sample size 01:18:46 Bayesian optimization with automl, a job killer? 01:25:28 different approaches to hyper-parameter optimization 01:30:18 advice for aspiring Bayesians 01:33:59 who would connor interview next? Connor Tann: https:\\/\\/www.linkedin.com\\/in\\/connor-tann-a92906a1\\/ https:\\/\\/twitter.com\\/connossor\",\"3874\":\"Reinforcement Learning Day 2021, hosted by Microsoft Research, was designed to bring together the RL community to build on the latest knowledge. Hear from Professor Doina Precup, McGill University, as she discussed \\\"New Advances in Hierarchical Reinforcement Learning\\\". In addition, listen in as Professor Yoshua Bengio, Mila (Quebec AI Institute), and Dr. John Langford, Microsoft Research, NYC, engaged in a lively debate on \\\"Reinforcement Learning: The State of RL and The Theory-Practice Divide.\\\" 4:50: \\u201cNew Advances in Hierarchical Reinforcement Learning\\u201d Keynote presented by Professor Doina Precup, McGill University, moderated by Ida Momennejad, Microsoft Research, NYC 54:56: \\u201cReinforcement Learning Day Debate: RL and the Theory-Practice Divide\\u201d featuring Professor Yoshua Bengio, Mila (Quebec AI Institute), and Dr. John Langford, Microsoft Research, NYC, moderated by Katja Hofmann, Microsoft Research, Cambridge, UK Learn more about Reinforcement Learning Day 2021: https:\\/\\/www.microsoft.com\\/en-us\\/research\\/event\\/reinforcement-learning-day-2021\\/\",\"1704\":\"Read the full story\",\"4284\":\"We cover Francois Chollet's recent paper. Abstract; To make deliberate progress towards more intelligent and more human-like artificial systems, we need to be following an appropriate feedback signal: we need to be able to define and evaluate intelligence in a way that enables comparisons between two systems, as well as comparisons with humans. Over the past hundred years, there has been an abundance of attempts to define and measure intelligence, across both the fields of psychology and AI. We summarize and critically assess these definitions and evaluation approaches, while making apparent the two historical conceptions of intelligence that have implicitly guided them. We note that in practice, the contemporary AI community still gravitates towards benchmarking intelligence by comparing the skill exhibited by AIs and humans at specific tasks such as board games and video games. We argue that solely measuring skill at any given task falls short of measuring intelligence, because skill is heavily modulated by prior knowledge and experience: unlimited priors or unlimited training data allow experimenters to \\\"buy\\\" arbitrary levels of skills for a system, in a way that masks the system's own generalization power. We then articulate a new formal definition of intelligence based on Algorithmic Information Theory, describing intelligence as skill-acquisition efficiency and highlighting the concepts of scope, generalization difficulty, priors, and experience. Using this definition, we propose a set of guidelines for what a general AI benchmark should look like. Finally, we present a benchmark closely following these guidelines, the Abstraction and Reasoning Corpus (ARC), built upon an explicit set of priors designed to be as close as possible to innate human priors. We argue that ARC can be used to measure a human-like form of general fluid intelligence and that it enables fair general intelligence comparisons between AI systems and humans.\",\"1458\":\"Regina Barzilay is a professor at MIT and a world-class researcher in natural language processing and applications of deep learning to chemistry and oncology, or the use of deep learning for early diagnosis, prevention and treatment of cancer. She has also been recognized for her teaching of several successful AI-related courses at MIT, including the popular Introduction to Machine Learning course. This conversation is part of the Artificial Intelligence podcast. If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman on Twitter, LinkedIn, Facebook, Medium, or YouTube where you can watch the video\",\"1876\":\"Variance Inflation Factors (VIFs) measure the correlation among independent variables in least squares regression models. Statisticians refer to this type of correlation as multicollinearity. Excessive multicollinearity can cause problems for regression models. In this post, I focus on VIFs and how they detect multicollinearity, why they\\u2019re better than pairwise correlations, how to calculate VIFs yourself, [\\u2026] The post Variance Inflation Factors (VIFs) appeared first on Statistics By Jim.\",\"3753\":\"So I've got a pet project I've been working on which is using GPT-2 for integer factoring. My current model and dataset loader is a minimal GPT-2 trained on simple sequences of random 24-bit integers multiplied together with their product; the dataloader generates sequences of multiplicand, multiplier, and product, and with the multiplicand and multiplier masked as the goal is to have the trained model consume just the product during inference and then produce the multiplicand and multiplier. During training I can get down to ~0.8 loss but eventually training gets stuck and fails to converge, and with training and test loss being very close to each other. Any thoughts on ideal hyperparams for GPT-2 being used for mathematical function approximation? Number of layers, heads, embeddings, warmup tokens, final tokens etc? tia! [link] [comments]\",\"1475\":\"Max Tegmark is a physicist and AI researcher at MIT. Please support this podcast by checking out our sponsors: \\u2013 The Jordan Harbinger Show: https:\\/\\/www.jordanharbinger.com\\/lex\\/ \\u2013 Four Sigmatic: https:\\/\\/foursigmatic.com\\/lex and use code LexPod to get up to 60% off \\u2013 BetterHelp: https:\\/\\/betterhelp.com\\/lex to get 10% off \\u2013 ExpressVPN: https:\\/\\/expressvpn.com\\/lexpod and use code LexPod to get 3 months free EPISODE LINKS: News Project Explainer Video: https:\\/\\/www.youtube.com\\/watch?v=PRLF17Pb6vo News Project Website: https:\\/\\/www.improvethenews.org\\/ Max\\u2019s Twitter: https:\\/\\/twitter.com\\/tegmark Max\\u2019s Website: https:\\/\\/space.mit.edu\\/home\\/tegmark\\/ Future of Life Institute: https:\\/\\/futureoflife.org\\/ Lex Fridman Podcast #1: https:\\/\\/www.youtube.com\\/watch?v=Gi8LUnhP5yU PODCAST INFO: Podcast website: https:\\/\\/lexfridman.com\\/podcast Apple Podcasts: https:\\/\\/apple.co\\/2lwqZIr Spotify: https:\\/\\/spoti.fi\\/2nEwCF8 RSS: https:\\/\\/lexfridman.com\\/feed\\/podcast\\/ YouTube Full Episodes: https:\\/\\/youtube.com\\/lexfridman YouTube Clips: https:\\/\\/youtube.com\\/lexclips SUPPORT & CONNECT: \\u2013 Check out the sponsors above, it\\u2019s the best way to support this podcast \\u2013 Support on Patreon: https:\\/\\/www.patreon.com\\/lexfridman \\u2013 Twitter: https:\\/\\/twitter.com\\/lexfridman \\u2013 Instagram: https:\\/\\/www.instagram.com\\/lexfridman \\u2013 LinkedIn: https:\\/\\/www.linkedin.com\\/in\\/lexfridman \\u2013 Facebook: https:\\/\\/www.facebook.com\\/LexFridmanPage \\u2013 Medium: https:\\/\\/medium.com\\/@lexfridman OUTLINE: Here\\u2019s the timestamps for the episode. On some podcast players you should be able to click the timestamp to jump to that time. (00:00) \\u2013 Introduction (08:15) \\u2013 AI and physics (21:32) \\u2013 Can AI discover new laws of physics? (30:22) \\u2013 AI safety (47:59) \\u2013 Extinction of human species (58:57) \\u2013 How to fix fake news and misinformation (1:20:30) \\u2013 Autonomous weapons (1:35:54) \\u2013 The man who prevented nuclear war (1:46:02) \\u2013 Elon Musk and AI (1:59:39) \\u2013 AI alignment (2:05:42) \\u2013 Consciousness (2:14:45) \\u2013 Richard Feynman (2:18:56) \\u2013 Machine learning and computational physics (2:29:53) \\u2013 AI and creativity (2:41:08) \\u2013 Aliens (2:56:51) \\u2013 Mortality\",\"1504\":\"Michio Kaku is a theoretical physicist, futurist, and professor at the City College of New York. He is the author of many fascinating books on the nature of our reality and the future of our civilization. This conversation is part of the Artificial Intelligence podcast. If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman on Twitter, LinkedIn, Facebook, Medium, or YouTube where you can watch the video versions of these conversations. If you enjoy the podcast, please rate it 5 stars on Apple Podcasts or support it on Patreon. Here\\u2019s the outline\",\"4297\":\"This week we spoke with Sayak Paul, who is extremely active in the machine learning community. We discussed the AI landscape in India, unsupervised representation learning, data augmentation and contrastive learning, explainability, abstract scene representations and finally pruning and the recent super positions paper. I really enjoyed this conversation and I hope you folks do too! 00:00:00 Intro to Sayak 00:17:50 AI landscape in India 00:24:20 Unsupervised representation learning 00:26:11 DATA AUGMENTATION\\/Contrastive learning 00:59:20 EXPLAINABILITY 01:12:10 ABSTRACT SCENE REPRESENTATIONS 01:14:50 PRUNING and super position paper\",\"1622\":\"Read the full story\",\"3854\":\"Epic special edition. @kenneth0stanley on why greatness cannot be planned, abandoning objectives and open-endedness. @joelbot3000 @jeffclune @ykilcher invidious.snopyta.org\\/watch?v=lhYGXYeM\\u2026\",\"1563\":\"Stephen Wolfram is a computer scientist, mathematician, and theoretical physicist who is the founder and CEO of Wolfram Research, a company behind Mathematica, Wolfram Alpha, Wolfram Language, and the new Wolfram Physics project. He is the author of several books including A New Kind of Science, which on a personal note was one of the most influential books in my journey in computer science and artificial intelligence. Support this podcast by signing up with these sponsors: \\u2013 ExpressVPN at https:\\/\\/www.expressvpn.com\\/lexpod \\u2013 Cash App \\u2013 use code \\u201cLexPodcast\\u201d and download: \\u2013 Cash App (App Store): https:\\/\\/apple.co\\/2sPrUHe \\u2013 Cash App (Google Play):\",\"701\":\"Day 2 | November 18, 2020 Theme: Building a Pipeline from Research to Impact Andreas Stefik, University of Nevada, Las Vegas The Accessible Computer Science Education Fall Workshop was hosted by Microsoft, University of Washington CREATE, and University of Colorado\\u2019s Coleman Institute. It took place November 17-19, 2020 and consisted of three half-days of talks, discussions, and planning for new research dedicated to making Computer Science education learning experiences more accessible for people with disabilities. More information on this workshop can be found at https:\\/\\/www.microsoft.com\\/en-us\\/research\\/event\\/accessible-cs-education-fall-workshop\\/\",\"4085\":\"[link] [comments]\",\"1646\":\"Want to train machine learning models on your Mac\\u2019s integrated AMD GPU or an external graphics card? Look no further than PlaidML. Read the full story\",\"4276\":\"We speak with Robert Lange! Robert is a PhD student at the Technical University Berlin. His research combines Deep Multi-Agent Reinforcement Learning and Cognitive Science to study the learning dynamics of large collectives. He has a brilliant blog where he distils and explains cutting edge ML research. We spoke about his story, economics, multi-agent RL, intelligence and AGI, and his recent article summarising the state of the art in neural network pruning. Robert's article on pruning in NNs https:\\/\\/roberttlange.github.io\\/posts\\/2020\\/06\\/lottery-ticket-hypothesis\\/ 00:00:00 Intro 00:04:17 Show start and intro to Robert 00:11:39 Economics background 00:27:20 Intrinsic motivation 00:33:22 Intelligence\\/consciousness 00:48:16 Lottery ticket\\/pruning article discussion 01:43:21 Robert's advice for younger self and state of deep learning Robert's LinkedIn: https:\\/\\/www.linkedin.com\\/in\\/robert-tjarko-lange-19539a12a\\/ @RobertTLange #machinelearning #deeplearning\",\"1493\":\"Sebastian Thrun is one of the greatest roboticists, computer scientists, and educators of our time. He led development of the autonomous vehicles at Stanford that won the 2005 DARPA Grand Challenge and placed second in the 2007 DARPA Urban Challenge. He then led the Google self-driving car program which launched the self-driving revolution. He taught the popular Stanford course on Artificial Intelligence in 2011 which was one of the first MOOCs. That experience led him to co-found Udacity, an online education platform. He is also the CEO of Kitty Hawk, a company working on building flying cars or more technically\",\"477\":\"Intro to trig with a lurking mystery about cos(x)^2 Full playlist: https:\\/\\/www.youtube.com\\/playlist?list=PLZHQObOWTQDP5CVelJJ1bNDouqrAhVPev Home page: https:\\/\\/www.3blue1brown.com Brought to you by you: https:\\/\\/3b1b.co\\/ldm-thanks ----------------- Contents: Introduction - 0:00 Q1 Graph of (cos \\u03b8)\\u00b2 - 2:14 Q2 Translations of cos \\u03b8 to (cos \\u03b8)\\u00b2 - 5:34 Q3 f(2x) = f(x)\\u00b2 - 10:54 Intro to Trig - 13:14 Q4 sin(3) & cos(3) - 16:44 SohCahToa - 20:44 Q5 Leaning Tower - 22:29 How to Compute Trig Functions? - 30:59 Q6 sin(\\u03c0\\/6) - 33:34 Q7 cos(\\u03c0\\/6) - 36:19 Q8 Trig of -\\u03b8 - 43:34 Computing Trig Functions - 47:44 Q9 cos(\\u03c0\\/12) - 0:49:54 Adv Trig Functions - 0:56:44 Q10 Graph of tan(\\u03b8) - 1:00:30 JSON comment - 1:02:24 \\u201cThe most exciting part of the lecture\\u201d \\u20131:05:34 ------------------ The live question setup with stats on-screen is powered by Itempool. https:\\/\\/itempool.com\\/ The graphing calculator used here is Desmos. https:\\/\\/www.desmos.com\\/ Curious about other animations? https:\\/\\/www.3blue1brown.com\\/faq#manim Music by Vincent Rubinetti. Download the music on Bandcamp: https:\\/\\/vincerubinetti.bandcamp.com\\/album\\/the-music-of-3blue1brown Stream the music on Spotify: https:\\/\\/open.spotify.com\\/album\\/1dVyjwS8FBqXhRunaG5W5u If you want to contribute translated subtitles or to help review those that have already been made by others and need approval, you can click the gear icon in the video and go to subtitles\\/cc, then \\\"add subtitles\\/cc\\\". I really appreciate those who do this, as it helps make the lessons accessible to more people. ------------------ 3blue1brown is a channel about animating math, in all senses of the word animate. And you know the drill with YouTube, if you want to stay posted on new videos, subscribe: http:\\/\\/3b1b.co\\/subscribe Various social media stuffs: Website: https:\\/\\/www.3blue1brown.com Twitter: https:\\/\\/twitter.com\\/3blue1brown Reddit: https:\\/\\/www.reddit.com\\/r\\/3blue1brown Instagram: https:\\/\\/www.instagram.com\\/3blue1brown_animations\\/ Patreon: https:\\/\\/patreon.com\\/3blue1brown Facebook: https:\\/\\/www.facebook.com\\/3blue1brown\",\"241\":\"(the classical robotics and computer graphics stacks are being re-written in neural net modules, typically building closely on classical algorithms but, whenever possible, swapping in differentiable versions so you can propagate gradients when it's plugged into the wider system)\",\"1345\":\"#ai #biology #neuroscience Backpropagation is the workhorse of modern deep learning and a core component of most frameworks, but it has long been known that it is not biologically plausible, driving a divide between neuroscience and machine learning. This paper shows that Predictive Coding, a much more biologically plausible algorithm, can approximate Backpropagation for any computation graph, which they verify experimentally by building and training CNNs and LSTMs using Predictive Coding. This suggests that the brain and deep neural networks could be much more similar than previously believed. OUTLINE: 0:00 - Intro & Overview 3:00 - Backpropagation & Biology 7:40 - Experimental Results 8:40 - Predictive Coding 29:00 - Pseudocode 32:10 - Predictive Coding approximates Backprop 35:00 - Hebbian Updates 36:35 - Code Walkthrough 46:30 - Conclusion & Comments Paper: https:\\/\\/arxiv.org\\/abs\\/2006.04182 Code: https:\\/\\/github.com\\/BerenMillidge\\/PredictiveCodingBackprop Abstract: Backpropagation of error (backprop) is a powerful algorithm for training machine learning architectures through end-to-end differentiation. However, backprop is often criticised for lacking biological plausibility. Recently, it has been shown that backprop in multilayer-perceptrons (MLPs) can be approximated using predictive coding, a biologically-plausible process theory of cortical computation which relies only on local and Hebbian updates. The power of backprop, however, lies not in its instantiation in MLPs, but rather in the concept of automatic differentiation which allows for the optimisation of any differentiable program expressed as a computation graph. Here, we demonstrate that predictive coding converges asymptotically (and in practice rapidly) to exact backprop gradients on arbitrary computation graphs using only local learning rules. We apply this result to develop a straightforward strategy to translate core machine learning architectures into their predictive coding equivalents. We construct predictive coding CNNs, RNNs, and the more complex LSTMs, which include a non-layer-like branching internal graph structure and multiplicative interactions. Our models perform equivalently to backprop on challenging machine learning benchmarks, while utilising only local and (mostly) Hebbian plasticity. Our method raises the potential that standard machine learning algorithms could in principle be directly implemented in neural circuitry, and may also contribute to the development of completely distributed neuromorphic architectures. Authors: Beren Millidge, Alexander Tschantz, Christopher L. Buckley Links: YouTube: https:\\/\\/www.youtube.com\\/c\\/yannickilcher Twitter: https:\\/\\/twitter.com\\/ykilcher Discord: https:\\/\\/discord.gg\\/4H8xxDF BitChute: https:\\/\\/www.bitchute.com\\/channel\\/yannic-kilcher Minds: https:\\/\\/www.minds.com\\/ykilcher Parler: https:\\/\\/parler.com\\/profile\\/YannicKilcher LinkedIn: https:\\/\\/www.linkedin.com\\/in\\/yannic-kilcher-488534136\\/ If you want to support me, the best thing to do is to share out the content :) If you want to support me financially (completely optional and voluntary, but a lot of people have asked for this): SubscribeStar: https:\\/\\/www.subscribestar.com\\/yannickilcher Patreon: https:\\/\\/www.patreon.com\\/yannickilcher Bitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq Ethereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2 Litecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m Monero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\",\"1491\":\"Harry Cliff is a particle physicist at the University of Cambridge working on the Large Hadron Collider beauty experiment that specializes in searching for hints of new particles and forces by studying a type of particle called the \\u201cbeauty quark\\u201d, or \\u201cb quark\\u201d. In this way, he is part of the group of physicists who are searching answers to some of the biggest questions in modern physics. He is also an exceptional communicator of science with some of the clearest and most captivating explanations of basic concepts in particle physics I\\u2019ve ever heard. Support this podcast by signing up with\",\"479\":\"Intro to the geometry complex numbers. Full playlist: https:\\/\\/www.youtube.com\\/playlist?list=PLZHQObOWTQDP5CVelJJ1bNDouqrAhVPev Home page: https:\\/\\/www.3blue1brown.com Brought to you by you: https:\\/\\/3b1b.co\\/ldm-thanks Beautiful pictorial summary by @ThuyNganVu: https:\\/\\/twitter.com\\/ThuyNganVu\\/status\\/1258219199769440257 Errors: - On the first sketch of a complex plane, there is a \\\"2i\\\" written instead of \\\"-2i\\\". - At the end, in writing the angle sum identity, the last term should be sin(beta) instead of sin(alpha). - During Q9, the terms in parentheses should include an i, (1\\/2 + sqrt(3)\\/2 i) ------------------ The live question setup with stats on-screen is powered by Itempool. https:\\/\\/itempool.com\\/ The graphing calculator used here is Desmos. https:\\/\\/www.desmos.com\\/ The \\\"Complex slide rule\\\" came from Geogebra, via Ben Sparks. https:\\/\\/www.geogebra.org\\/m\\/mbhbdvkr Curious about other animations? https:\\/\\/www.3blue1brown.com\\/faq#manim Music by Vincent Rubinetti. Download the music on Bandcamp: https:\\/\\/vincerubinetti.bandcamp.com\\/album\\/the-music-of-3blue1brown Stream the music on Spotify: https:\\/\\/open.spotify.com\\/album\\/1dVyjwS8FBqXhRunaG5W5u If you want to contribute translated subtitles or to help review those that have already been made by others and need approval, you can click the gear icon in the video and go to subtitles\\/cc, then \\\"add subtitles\\/cc\\\". I really appreciate those who do this, as it helps make the lessons accessible to more people. ------------------ Video Timeline (thanks to user \\\"Just TIEriffic\\\") 0:00:30 - W3 Results 0:01:00 - W4 Prompt 0:02:00 - Ask What would you call 'imaginary numbers'? 0:06:40 - Startingpoint & assumptions 0:10:25 - W4 Results 0:11:25 - Q1 Prompt 0:12:20 - Q1 Process 0:14:05 - RotatingCoordinates 0:16:40 - Q1 Result 0:17:40 - Q2 0:18:15 - Q3 Prompt 0:19:40 - Q3 Results 0:21:35 - RotationAnimation 0:22:35 - 3 facts about Multiplication 0:25:40 - Q4 Prompt 0:26:10 - Ask imaginary I vs physics i&j 0:28:15 - Q4 Result 0:31:00 - GeoGebraDemo 0:32:10 - Q5 Prompt 0:33:30 - Q5 Results 0:34:00 - Q5 Solution 0:35:55 - RotatingImages Example 0:37:10 - PythonExample 0:38:25 - PythonImage Rotation Example 0:40:35 - Ask Vectors & Matrices for rotation 0:42:40 - Q6 Prompt 0:46:55 - Q6 Results 0:47:25 - Q6 Solution 0:52:20 - RedefiningAngle Addition 0:57:20 - Q7 Prompt 0:57:55 - Ask Can we do without complex numbers? 1:00:10 - Q7 Results 1:00:55 - Q7 Solution 1:05:45 - Q8 Prompt 1:06:30 - Ask sum\\/difference of angles 1:09:25 - Q8 Results 1:10:25 - Q8 Solution 1:12:00 - DesmosExample 1:15:05 - Bringing it all together 1:16:25 - The \\\"cis\\\" shorthand explained 1:18:05 - Q9 Prompt 1:19:35 - Q9 Results 1:20:55 - ClosingRemarks ------------------ 3blue1brown is a channel about animating math, in all senses of the word animate. And you know the drill with YouTube, if you want to stay posted on new videos, subscribe: http:\\/\\/3b1b.co\\/subscribe Various social media stuffs: Website: https:\\/\\/www.3blue1brown.com Twitter: https:\\/\\/twitter.com\\/3blue1brown Reddit: https:\\/\\/www.reddit.com\\/r\\/3blue1brown Instagram: https:\\/\\/www.instagram.com\\/3blue1brown_animations\\/ Patreon: https:\\/\\/patreon.com\\/3blue1brown Facebook: https:\\/\\/www.facebook.com\\/3blue1brown\",\"244\":\"It looks like if you bombard Earth with photons for a while, it can emit a Roadster. hah\",\"1668\":\"Read the full story\",\"3142\":\"Create model documentation for Supervised learning models in H2O-3 and Scikit-Learn \\u2014 in minutes. The Federal Reserve\\u2019s 2011 guidelines state that without adequate documentation, model risk assessment and management would be ineffective. A similar requirement is put forward today by many regulatory and corporate governance bodies. Thus model documentation today is more of a necessity than a choice. However, there is still no denying the fact that it is one of the most time-consuming jobs for a data scientist. As opposed to building and validating machine learning models, describing how a model works in detail is tedious and takes a considerable amount of time and effort. There are also issues of consistency, clarity, and collaboration. What if there was a way to automate the entire documentation process? Well, this is precisely the issue that the H2O AutoDoc tries to address by creating comprehensive, high-quality model documentation in minutes. H2O AutoDoc frees up the user from the time-consuming task of documenting and summarizing their workflow while building machine learning models. Additionally, it also increases the consistency of model documentation by applying a standard template across all models, essential for model governance, reproducibility, and regulatory compliance. In a way, it is using AI to explain AI. Challenges in creating a robust documentation Model documentation includes how a model was created, training and test data characteristics, what alternatives were considered, how the model was evaluated, information on model performance, etc. Today, documenting the models is both necessary as a best practice and a vital requirement from the business point of view. Challenges associating with manually documenting models But creating good documentation isn\\u2019t a piece of cake, and at times, many teams struggle with it. The process is often tedious and time-consuming for the business because the data scientist could be using that time to build additional models and create more value. Additionally, inconsistent or inaccurate model documentation can be an issue for model validation, governance, and regulatory compliance. A better idea: Automate the documentation process itself with H2O AutoDoc. H2O AutoDoc Automated Model Documentation (H2O AutoDoc) is a new time-saving ML documentation product from H2O.ai. H2O AutoDoc can automatically generate model Documentation for supervised learning models created in H2O-3 and Scikit-Learn. Interestingly, automated documentation is already used in production in H2O Driverless AI. This industry-leading capability is now available as a new standalone commercial module. Key Features of H2O AutoDoc H2O AutoDoc is a Python package for creating automatic reports for supervised learning models. Distributed automatic document generation in Microsoft Word (.docx) and Markdown (.md) formats. Out-of-the-box documentation template included Customizable templates to fit unique business needs, internal best practices, and compliance requirements Support for a variety of supervised models generated in H2O-3 and Scikit-Learn Advantages of using H2O AutoDoc H2O AutoDoc provides various advantages over the traditional method of manual documentation: H2O AutoDoc ensures compliance and provides a consistent, accurate, and thorough approach to model documentation. It can be shared with production teams and other data scientists, thereby improving collaboration amongst teams. Saves time and money by automatically creating model documents instead of having valuable resources writing and editing documents H2O AutoDoc in Action We know that H2O AutoDoc can automatically generate model documentation for supervised learning models created in H2O-3 and Scikit-Learn. Let\\u2019s see some of the ways by which we can generate the automatic report: 1. H2O AutoDoc for models created in H2O-3 H2O-3 is a fully open-source, distributed in-memory machine learning platform with linear scalability. The speed, quality, ease-of-use, and model-deployment for the various cutting-edge algorithms make H2O a highly sought-after API for big data data science. H2O also has an industry-leading AutoML functionality that can be used for automating the machine learning workflow. The documentation can be generated in an editable word or a markdown format as follows: from h2o_autodoc import Config from h2o_autodoc import render_autodoc# get the H2O-3 model object required to create an H2O AutoDoc model = h2o.get_model(\\u201cmy_gbm_model\\u201d)# configure and render an AutoDoc Config = Config(output_path=\\u201dfull\\/path\\/AutoDoc_H2O3.docx\\u201d) render_autodoc(h2o, config, model) 2. H2O AutoDoc for models created in Scikit-learn Scikit-learn is an open-source software machine learning library for the Python programming language. It features various classification, regression, and clustering algorithms. The process to create automatic documentation for models created in scikit learn is also pretty similar to the ones created in H2O-3 and is as follows: from h2o_autodoc import Config from h2o_autodoc.scikit.autodoc import render_autodoc# build a logistic regression model model = LogisticRegression() model.fit(X_train, y_train)# configure and render an AutoDoc Config = Config(output_path=\\u201dfull\\/path\\/AutoDoc_ScikitLearn.docx\\u201d) render_autodoc(config, model, X_train, y_train) 3. Steam: H2O AutoDoc The H2O AutoDoc is currently available in Steam, another H2O.ai product that allows you to launch or connect to an H2O Cluster securely. This version of the H2O AutoDoc leverages the Steam Python API. The code follows the same structure as the H2O AutoDoc Python API, and the generated report is almost identical. One difference is that the Steam version only supports H2O-3 models. import h2osteam from h2osteam.clients import H2oClient# login to steam h2osteam.login(url=\\u201dhttps:\\/\\/steam.h2o.ai:9555\\\", username=\\u201duser01\\\", password=\\u201dtoken-here\\u201d, verify_ssl=True) cluster = H2oClient.get_cluster(\\u201ctest-cluster\\u201d)from h2osteam import AutoDocConfig# get H2O-3 objects using their keys model = h2o.get_model(\\u201cgbm_model\\u201d) train = h2o.get_frame(\\u201cCreditCard_TRAIN\\u201d)# use default configuration settings config = AutoDocConfig()# specify the path to the output file output_file_path = \\u201cautodoc_report.docx\\u201d# download an H2O AutoDoc cluster.download_autodoc(model, config, train, output_file_path) Documentation Features H2O AutoDoc generates an editable Word document based on an automated template that includes several features. Some of the important ones have been tabulated below: * For supported algorithms Try H2O AutoDocDo you want to get your hands dirty and experience the power that H2O AutoDoc brings to your machine learning project? We have made it easy for you. You can : Register for the trial license here and then try H2O AutoDoc in your environment. Our team will reach out, provide a 30-day trial license, and help you get up and running. Experiment and use it with your H2O-3 and scikit-learn models. Conclusion H2O AutoDoc automatically generates comprehensive model documentation in minutes using out-of-the-box or custom templates. H2O AutoDoc saves data science teams weeks of tedious work and increases data science productivity by allowing them to focus on model building. H2O AutoDoc increases the consistency of model documentation by applying a standard template across all models and teams, which is essential for model governance, reproducibility, and compliance with regulations. The post Automate your Model Documentation using H2O AutoDoc appeared first on Open Source Leader in AI and ML.\",\"1698\":\"\\u2014 All the images (plots) are generated and modified by the Author. Read the full story\",\"234\":\"8.5 years ago I was training restricted boltzmann machines in Matlab on CPU on my machine below the desk.\",\"4469\":\"I have hope for 2021. Let's make this year amazing. Let engineers, scientists, small businesses do what they do best: solve problems.\",\"1566\":\"Eric Weinstein is a mathematician with a bold and piercing intelligence, unafraid to explore the biggest questions in the universe and shine a light on the darkest corners of our society. He is the host of The Portal podcast, a part of which, he recently released his 2013 Oxford lecture on his theory of Geometric Unity that is at the center of his lifelong efforts in arriving at a theory of everything that unifies the fundamental laws of physics. Support this podcast by signing up with these sponsors: \\u2013 Cash App \\u2013 use code \\u201cLexPodcast\\u201d and download: \\u2013 Cash App\",\"4086\":\"Hi all, I'm working as a Business Head for a small company focused on developing bespoke AI and ML based solutions. We want to diversify into industrial monitoring via computer vision using drones. Currently we have a small team(\",\"232\":\"(the impressiveness of these are to be judged by how out of distribution a prompt\\/output is likely to be. E.g. \\\"a collection of glasses on table\\\" giving generic images is nice, but rendering arbitrary text from the prompt into textures, or rare\\/specific prompts are \\ud83d\\udca5)\",\"1513\":\"Manolis Kellis is a professor at MIT and head of the MIT Computational Biology Group. Please check out our sponsors to get a discount and to support this podcast: \\u2013 Public Goods: https:\\/\\/publicgoods.com\\/lex and use code LEX \\u2013 Magic Spoon: https:\\/\\/magicspoon.com\\/lex link & using code LEX at checkout \\u2013 ExpressVPN: https:\\/\\/www.expressvpn.com\\/lexpod Lex Fridman Podcast survey mentioned in the intro: https:\\/\\/www.surveygizmo.com\\/s3\\/5833660\\/Lex-Fridman-Podcast-Survey If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/podcast or connect with @lexfridman on Twitter, LinkedIn, Facebook, Medium, or YouTube where you can watch the video versions of these conversations. If you enjoy the podcast, please rate\",\"1654\":\"To help you build object recognition models, scene recognition models, and more, we\\u2019ve compiled a list of the best image classification datasets. These datasets vary in scope and magnitude and can suit a variety of use cases. Furthermore, the datasets have been divided into the following categories: medical imaging, agriculture & scene recognition, and others. Read the full story\",\"2682\":\"I read my review from CVPR 2021. I think that one of the reviewers is totally unqualified. He\\/she just strongly rejected my manuscript because he\\/she can't understand my method. He\\/she claimed that there is nothing attractive, and refused to evaluate the experiment results. He\\/she said I have no explanation for my algorithm---in fact I used a section to introduce it since it used some Interdisciplinary acknowledge. Perhaps he\\/she overlooked it deliberately just because he\\/she can't understand it? In fact the other two reviewers have understood my method clearly, and they also brought out good advice and critics as well. I don't know how to face such a reviewer. I decided to drawback the paper. I once think CVPR reviewers are very professional according to my past experience . But now, I think some reviewer is really not qualified for CVPR. [link] [comments]\",\"1702\":\"Read the full story\",\"231\":\"Retweeting this one more time because it is so excellent, describes nicely how the mRNA vaccines are a direct hacking of Life's assembly code for the Spike protein + all of its headers, metadata, + tweaking it to be more stable and likely to evade the immune system defenses \\ud83d\\udc4c\\ud83d\\udc4c nitter.net\\/PowerDNS_Bert\\/status\\/1342568484946010116#m\",\"750\":\"Nick Lane's books are So. Good. amazon.com\\/Vital-Question-Ev\\u2026\",\"250\":\"This week with @Connossor we explore the history, practical utility, and unique capabilities of Bayesian methods. We also discuss the computational difficulties inherent in Bayesian methods. invidious.snopyta.org\\/watch?v=RQ31MHw8\\u2026 with Tim @ecsquendor, Alex and Keith\",\"1141\":\"Dr James Grime is discussing a new prime-generating constant. Check out Brilliant (get 20% off their premium service): https:\\/\\/brilliant.org\\/numberphile (sponsor) More links & stuff in full description below \\u2193\\u2193\\u2193 Extra footage from this interview: https:\\/\\/youtu.be\\/yXPhq-36Eq4 More James Grime videos: http:\\/\\/bit.ly\\/grimevideos James Grime's website: https:\\/\\/www.singingbanana.com Mills' Constant video: https:\\/\\/youtu.be\\/6ltrPVPEwfo A Prime-Representing Constant by Dylan Fridman, Juli Garbulsky, Bruno Glecer, James Grime & Massi Tron Florentin: https:\\/\\/www.tandfonline.com\\/doi\\/abs\\/10.1080\\/00029890.2019.1530554 Numberphile is supported by the Mathematical Sciences Research Institute (MSRI): http:\\/\\/bit.ly\\/MSRINumberphile We are also supported by Science Sandbox, a Simons Foundation initiative dedicated to engaging everyone with the process of science. https:\\/\\/www.simonsfoundation.org\\/outreach\\/science-sandbox\\/ And support from Math For America - https:\\/\\/www.mathforamerica.org\\/ NUMBERPHILE Website: http:\\/\\/www.numberphile.com\\/ Numberphile on Facebook: http:\\/\\/www.facebook.com\\/numberphile Numberphile tweets: https:\\/\\/twitter.com\\/numberphile Subscribe: http:\\/\\/bit.ly\\/Numberphile_Sub Videos by Brady Haran Patreon: http:\\/\\/www.patreon.com\\/numberphile Numberphile T-Shirts and Merch: https:\\/\\/teespring.com\\/stores\\/numberphile Brady's videos subreddit: http:\\/\\/www.reddit.com\\/r\\/BradyHaran\\/ Brady's latest videos across all channels: http:\\/\\/www.bradyharanblog.com\\/ Sign up for (occasional) emails: http:\\/\\/eepurl.com\\/YdjL9\",\"647\":\"Dan Kokotov is VP of Engineering at Rev.ai, an automatic speech recognition company. Please support this podcast by checking out our sponsors: - Athletic Greens: https:\\/\\/athleticgreens.com\\/lex and use code LEX to get 1 month of fish oil - Blinkist: https:\\/\\/blinkist.com\\/lex and use code LEX to get 25% off premium - Business Wars: https:\\/\\/wondery.com\\/business-wars\\/ - Cash App: https:\\/\\/cash.app\\/ and use code LexPodcast to get $10 EPISODE LINKS: Rev: https:\\/\\/www.rev.com Rev.ai: https:\\/\\/rev.ai PODCAST INFO: Podcast website: https:\\/\\/lexfridman.com\\/podcast Apple Podcasts: https:\\/\\/apple.co\\/2lwqZIr Spotify: https:\\/\\/spoti.fi\\/2nEwCF8 RSS: https:\\/\\/lexfridman.com\\/feed\\/podcast\\/ Full episodes playlist: https:\\/\\/www.youtube.com\\/playlist?list=PLrAXtmErZgOdP_8GztsuKi9nrraNbKKp4 Clips playlist: https:\\/\\/www.youtube.com\\/playlist?list=PLrAXtmErZgOeciFP3CBCIEElOJeitOr41 OUTLINE: 0:00 - Introduction 3:23 - Dune 6:39 - Rev 12:39 - Translation 19:28 - Gig economy 28:08 - Automatic speech recognition 38:58 - Create products that people love 47:08 - The future of podcasts at Spotify 1:08:46 - Book recommendations 1:10:08 - Stories of our dystopian future 1:13:50 - Movies about Stalin and Hitler 1:19:05 - Interviewing Putin 1:25:02 - Meaning of life CONNECT: - Subscribe to this YouTube channel - Twitter: https:\\/\\/twitter.com\\/lexfridman - LinkedIn: https:\\/\\/www.linkedin.com\\/in\\/lexfridman - Facebook: https:\\/\\/www.facebook.com\\/LexFridmanPage - Instagram: https:\\/\\/www.instagram.com\\/lexfridman - Medium: https:\\/\\/medium.com\\/@lexfridman - Support on Patreon: https:\\/\\/www.patreon.com\\/lexfridman\",\"1489\":\"Kai-Fu Lee is the Chairman and CEO of Sinovation Ventures that manages a 2 billion dollar dual currency investment fund with a focus on developing the next generation of Chinese high-tech companies. He is the former President of Google China and the founder of what is now called Microsoft Research Asia, an institute that trained many of the AI leaders in China, including CTOs or AI execs at Baidu, Tencent, Alibaba, Lenovo, and Huawei. He was named one of the 100 most influential people in the world by TIME Magazine. He is the author of seven best-selling books in Chinese,\",\"1662\":\"With the exponential rise in applications of AI, Data Science, and Machine Learning these are the critical Ethical AI Libraries to know. Read the full story\",\"1570\":\"George Hotz (geohot) is a programmer, hacker, and the founder of Comma.ai. Please support this podcast by checking out our sponsors: \\u2013 Four Sigmatic: https:\\/\\/foursigmatic.com\\/lex and use code LexPod to get up to 40% & free shipping \\u2013 Decoding Digital: https:\\/\\/appdirect.com\\/decoding-digital \\u2013 ExpressVPN: https:\\/\\/expressvpn.com\\/lexpod and use code LexPod to get 3 months free EPISODE LINKS: Comma.ai\\u2019s Twitter: https:\\/\\/twitter.com\\/comma_ai Comma.ai\\u2019s Website: https:\\/\\/comma.ai\\/ George\\u2019s Instagram: https:\\/\\/www.instagram.com\\/georgehotz George\\u2019s Twitch: https:\\/\\/www.twitch.tv\\/georgehotz George\\u2019s Twitter: https:\\/\\/twitter.com\\/realgeorgehotz Comma.ai YouTube (unofficial): https:\\/\\/www.youtube.com\\/channel\\/UCwgKmJM4ZJQRJ-U5NjvR2dg PODCAST INFO: Podcast website: https:\\/\\/lexfridman.com\\/podcast Apple Podcasts: https:\\/\\/apple.co\\/2lwqZIr Spotify: https:\\/\\/spoti.fi\\/2nEwCF8 RSS: https:\\/\\/lexfridman.com\\/feed\\/podcast\\/ YouTube Full Episodes: https:\\/\\/youtube.com\\/lexfridman YouTube Clips: https:\\/\\/youtube.com\\/lexclips SUPPORT & CONNECT: \\u2013 Check out the\",\"1573\":\"Grant Sanderson is a math educator and creator of 3Blue1Brown. Support this podcast by supporting our sponsors: \\u2013 Dollar Shave Club: https:\\/\\/dollarshaveclub.com\\/lex \\u2013 DoorDash: download app & use code LEX \\u2013 Cash App: download app & use code \\u201cLexPodcast\\u201d Episode links: 3Blue1Brown: http:\\/\\/youtube.com\\/3blue1brown Grant\\u2019s Twitter: https:\\/\\/twitter.com\\/3blue1brown If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/podcast or connect with @lexfridman on Twitter, LinkedIn, Facebook, Medium, or YouTube where you can watch the video versions of these conversations. If you enjoy the podcast, please rate it 5 stars on Apple Podcasts, follow on Spotify, or support it on\",\"505\":\"In this video, we will build a deep learning model using #PyTorch to classify the different types of diseases in Cassava leaf images. This is a multi-class #ImageClassification problem and a #Kaggle competition too. Competition link: https:\\/\\/www.kaggle.com\\/c\\/cassava-leaf-disease-classification We will be using Tez, a small trainer library we developed in one of the previous videos: https:\\/\\/github.com\\/abhishekkrthakur\\/tez Tutorial Kernel: https:\\/\\/www.kaggle.com\\/abhishek\\/using-tez-in-leaf-disease-classification Training Kernel: https:\\/\\/www.kaggle.com\\/abhishek\\/tez-faster-and-easier-training-for-leaf-detection Inference kernel: https:\\/\\/www.kaggle.com\\/abhishek\\/leaf-disease-inference-using-tez Please subscribe and like the video to help me keep motivated to make awesome videos like this one. :) To buy my book, Approaching (Almost) Any Machine Learning problem, please visit: https:\\/\\/bit.ly\\/buyaaml Follow me on: Twitter: https:\\/\\/twitter.com\\/abhi1thakur LinkedIn: https:\\/\\/www.linkedin.com\\/in\\/abhi1thakur\\/ Kaggle: https:\\/\\/kaggle.com\\/abhishek Instagram: https:\\/\\/instagram.com\\/abhi4ml\",\"4089\":\"I'm finally in a position to buy a 3090 and am tossing up which brand to go with. Each manufacturer seems to have their own cooling system and its hard to rate each of these for ML use. Are there major differences between the different brands? As it's been some months since the RTX 3090 launch now, I'd love to hear people's experiences. Currently I'm looking at Asus or Zotac. [link] [comments]\",\"4083\":\"Hello Guys, We (IEEE RAS Robotics Podcast) are going to have John Leonard- MIT CSAIL, his research is focused on marine robotics and visual SLAM, let us know if you have any questions :) You can also send them directly here: https:\\/\\/docs.google.com\\/forms\\/d\\/e\\/1FAIpQLSfZVWPho7xpZ1YMfg1pZRw7007gztZ9-xP_EUJa4u44mYxH4A\\/viewform?vc=0&c=0&w=1&flr=0&gxids=7628 [link] [comments]\",\"1592\":\"This changes everything.\",\"203\":\"I am Yuanchen He, a senior engineer in McAfee lab. I have been working on large data analysis and classification modeling for network security problems. Method Many thanks to Kaggle for setting up this competition. And congratulations to the winners! I enjoyed it and learned a lot from working on this challenging data and reading the winners\\u2019 posts. I am sorry I didn\\u2019t find free time last week to write this report. The data came with a lot of categorical features with a high number of values. At the very beginning, I removed useless features (by weka.filters.unsupervised.attribute.RemoveUseless -M 99.0) and removed the features with almost 100% missing values. After that, I tried to transform the categorical features into a group of binary features with each is a yes or no on a specific value. I also generated 4 quarter features and 12 month features from startdate and generated binary indicator features for missing values. The binary features, date-based features, indicator features, as well as other numerical features, after simply filling missing values with mean, were fed into R randomForest classifier for RFE. With that I got 94.9x on the leaderboard. I kept tuning along this way but the accuracy cannot be improved further. Then I started to suspect there were some information loss during the process of feature transformation and feature selection. So I tried to build classifiers directly on the categorical features without transforming them into binary features. A simple frequency based pre-filtering was applied. For a raw categorical feature, all values presented less than 10 instances in the data were combined into a specific common value \\u201c-1\\u201d. However, R randomForest cannot accept a categorical feature with more than 32 values. So I had to split each categorical feature again into \\u201csub features\\u201d, with each has no more than 32 values. The way I split the values into different sub features was sorting the values with information gain first, and then top 31 values were assigned into sub feature 1, the next 31 values were assigned into sub feature 2, and so on. With this feature transformation strategy I got 94.6x on the leaderboard. The next one I tried was simply combining the top features from the above two methods. The randomForest classifiers on the combined feature sets can improve the leaderboard ROC to 95.1x-95.3x, depending on the instances used for training. The best classifiers were generated from training only on instances after 0606, only on instances after 0612, and only on instances after 0706. Finally, I observed the prediction results from these classifiers were different enough and hence it was worth to make a major voting from them, and I got my best leaderboard AUC 95.555, which generalized to the other 75% test instances with the final AUC 96.1051 Originally published at blog.kaggle.com on March 1, 2011. Yuanchen He on finishing third in the Melbourne University competition was originally published in Kaggle Blog on Medium, where people are continuing the conversation by highlighting and responding to this story.\",\"1494\":\"Eric Schmidt was the CEO of Google from 2001 to 2011, and its executive chairman from 2011 to 2017, guiding the company through a period of incredible growth and a series of world-changing innovations. Video version is available on YouTube. If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman on Twitter, LinkedIn, Facebook, or YouTube where you can watch the video versions of these conversations.\",\"257\":\"\\ud83c\\udf89New Video\\ud83c\\udf89 CLIP by @OpenAI is a huge step into connecting Images and Text! It beats fully supervised models ZERO-SHOT(!) on ImageNet\\ud83d\\udd25Trained on 400 Million scraped samples w\\/ huge potential applications\\ud83d\\udcaa invidious.snopyta.org\\/T9XSU0pKX2E @AlecRad @_jongwook_kim @ilyasut\",\"4269\":\"Tweet Share Share Optimization is a field of mathematics concerned with finding a good or best solution among many candidates. It is an important foundational topic required in machine learning as most machine learning algorithms are fit on historical data using an optimization algorithm. Additionally, broader problems, such as model selection and hyperparameter tuning, can also be framed as an optimization problem. Although having some background in optimization is critical for machine learning practitioners, it can be a daunting topic given that it is often described using highly mathematical language. In this post, you will discover top books on optimization that will be helpful to machine learning practitioners. Let\\u2019s get started. Books on Optimization for Machine Learning Photo by Patrick Alexander, some rights reserved. Overview The field of optimization is enormous as it touches many other fields of study. As such, there are hundreds of books on the topic, and most are textbooks filed with math and proofs. This is fair enough given that it is a highly mathematical subject. Nevertheless, there are books that provide a more approachable description of optimization algorithms. Not all optimization algorithms are relevant to machine learning; instead, it is useful to focus on a small subset of algorithms. Frankly, it is hard to group optimization algorithms as there are many concerns. Nevertheless, it is important to have some idea of the optimization that underlies simpler algorithms, such as linear regression and logistic regression (e.g. convex optimization, least squares, newton methods, etc.), and neural networks (first-order methods, gradient descent, etc.). These are foundational optimization algorithms covered in most optimization textbooks. Not all optimization problems in machine learning are well behaved, such as optimization used in AutoML and hyperparameter tuning. Therefore, knowledge of stochastic optimization algorithms is required (simulated annealing, genetic algorithms, particle swarm, etc.). Although these are optimization algorithms, they are also a type of learning algorithm referred to as biologically inspired computation or computational intelligence. Therefore, we will take a look at both books that cover classical optimization algorithms as well as books on alternate optimization algorithms. In fact, the first book we will look at covers both types of algorithms, and much more. Algorithms for Optimization This book was written by Mykel Kochenderfer and Tim Wheeler and was published in 2019. Algorithms for Optimization This book might be one of the very few textbooks that I\\u2019ve seen that broadly covers the field of optimization techniques relevant to modern machine learning. This book provides a broad introduction to optimization with a focus on practical algorithms for the design of engineering systems. We cover a wide variety of optimization topics, introducing the underlying mathematical problem formulations and the algorithms for solving them. Figures, examples, and exercises are provided to convey the intuition behind the various approaches. \\u2014 Page xiiix, Algorithms for Optimization, 2019. Importantly the algorithms range from univariate methods (bisection, line search, etc.) to first-order methods (gradient descent), second-order methods (Newton\\u2019s method), direct methods (pattern search), stochastic methods (simulated annealing), and population methods (genetic algorithms, particle swarm), and so much more. It includes both technical descriptions of algorithms with references and worked examples of algorithms in Julia. It\\u2019s a shame the examples are not in Python as this would make the book near perfect in my eyes. The complete table of contents for the book is listed below. Chapter 01: Introduction Chapter 02: Derivatives and Gradients Chapter 03: Bracketing Chapter 04: Local Descent Chapter 05: First-Order Methods Chapter 06: Second-Order Methods Chapter 07: Direct Methods Chapter 08: Stochastic Methods Chapter 09: Population Methods Chapter 10: Constraints Chapter 11: Linear Constrained Optimization Chapter 12: Multiobjective Optimization Chapter 13: Sampling Plans Chapter 14: Surrogate Models Chapter 15: Probabilistic Surrogate Models Chapter 16: Surrogate Optimization Chapter 17: Optimization under Uncertainty Chapter 18: Uncertainty Propagation Chapter 19: Discrete Optimization Chapter 20: Expression Optimization Chapter 21: Multidisciplinary Optimization I like this book a lot; it is full of valuable practical advice. I highly recommend it! Learn More: Algorithms for Optimization, 2019. Numerical Optimization This book was written by Jorge Nocedal and Stephen Wright and was published in 2006. Numerical Optimization This book is focused on the math and theory of the optimization algorithms presented and does cover many of the foundational techniques used by common machine learning algorithms. It may be a little too heavy for the average practitioner. The book is intended as a textbook for graduate students in mathematical subjects. We intend that this book will be used in graduate-level courses in optimization, as offered in engineering, operations research, computer science, and mathematics departments. \\u2014 Page xviii, Numerical Optimization, 2006. Even though it is highly mathematical, the descriptions of the algorithms are precise and may provide a useful alternative description to complement the other books listed. The complete table of contents for the book is listed below. Chapter 01: Introduction Chapter 02: Fundamentals of Unconstrained Optimization Chapter 03: Line Search Methods Chapter 04: Trust-Region Methods Chapter 05: Conjugate Gradient Methods Chapter 06: Quasi-Newton Methods Chapter 07: Large-Scale Unconstrained Optimization Chapter 08: Calculating Derivatives Chapter 09: Derivative-Free Optimization Chapter 10: Least-Squares Problems Chapter 11: Nonlinear Equations Chapter 12: Theory of Constrained Optimization Chapter 13: Linear Programming: The Simplex Method Chapter 14: Linear Programming: Interior-Point Methods Chapter 15: Fundamentals of Algorithms for Nonlinear Constrained Optimization Chapter 16: Quadratic Programming Chapter 17: Penalty and Augmented Lagrangian Methods Chapter 18: Sequential Quadratic Programming Chapter 19: Interior-Point Methods for Nonlinear Programming It\\u2019s a solid textbook on optimization. Learn More: Numerical Optimization, 2006. If you do prefer the theoretical approach to the subject, another widely used mathematical book on optimization is \\u201cConvex Optimization\\u201d written by Stephen Boyd and Lieven Vandenberghe and published in 2004. Computational Intelligence: An Introduction This book was written by Andries Engelbrecht and published in 2007. Computational Intelligence: An Introduction This book provides an excellent overview of the field of nature-inspired optimization algorithms, also referred to as computational intelligence. This includes fields such as evolutionary computation and swarm intelligence. This book is far less mathematical than the previous textbooks and is more focused on the metaphor of the inspired system and how to configure and use the specific algorithms with lots of pseudocode explanations. While the material is introductory in nature, it does not shy away from details, and does present the mathematical foundations to the interested reader. The intention of the book is not to provide thorough attention to all computational intelligence paradigms and algorithms, but to give an overview of the most popular and frequently used models. \\u2014 Page xxix, Computational Intelligence: An Introduction, 2007. Algorithms like genetic algorithms, genetic programming, evolutionary strategies, differential evolution, and particle swarm optimization are useful to know for machine learning model hyperparameter tuning and perhaps even model selection. They also form the core of many modern AutoML systems. The complete table of contents for the book is listed below. Part I Introduction Chapter 01: Introduction to Computational Intelligence Part II Artificial Neural Networks Chapter 02: The Artificial Neuron Chapter 03: Supervised Learning Neural Networks Chapter 04: Unsupervised Learning Neural Networks Chapter 05: Radial Basis Function Networks Chapter 06: Reinforcement Learning Chapter 07: Performance Issues (Supervised Learning) Part III Evolutionary Computation Chapter 08: Introduction to Evolutionary Computation Chapter 09: Genetic Algorithms Chapter 10: Genetic Programming Chapter 11: Evolutionary Programming Chapter 12: Evolution Strategies Chapter 13: Differential Evolution Chapter 14: Cultural Algorithms Chapter 15: Coevolution Part IV Computational Swarm Intelligence Chapter 16: Particle Swarm Optimization Chapter 17: Ant Algorithms Part V Artificial Immune Systems Chapter 18: Natural Immune System Chapter 19: Artificial Immune Models Part VI Fuzzy Systems Chapter 20: Fuzzy Sets Chapter 21: Fuzzy Logic and Reasoning I\\u2019m a fan of this book and recommend it. Learn More: Computational Intelligence: An Introduction, 2007. Summary In this post, you discovered books on optimization algorithms that are helpful to know for applied machine learning. Did I miss a good book on optimization? Let me know in the comments below. Have you read any of the books listed? Let me know what you think of it in the comments. Tweet Share Share The post 3 Books on Optimization for Machine Learning appeared first on Machine Learning Mastery.\",\"4088\":\"Using the Gradio interface with the Crowd Counting model. You can try out this interface yourself in the link below! Today's model will be based on the paper at this arXiv link: Distribution Matching for Crowd Counting. Repo here and interface shown above here. Crowd counting is a popular research problem with applications in journalism, human traffic management, and surveillance. The paper linked above proposes a novel approach to crowd counting. These are the existing methods of crowd counting: Detect-then-count method - This method detects every person in the image and then counts the number of individuals identified. This technique is not very accurate because it is very sensitive to occlusion and noise. Density map calculations - This method divides the image into regions and estimates the human density in each region. This method is more accurate, especially for larger crowds. The paper above uses a variation of existing density map methods. Existing methods take the annotated data (where every human is identified with a location on the image) and process it with a Gaussian to generate smoothened density maps. The loss between predicted output and annotated data is calculated as the difference between the density maps. DM-Count proposes an alternative method to calculate loss between predicted output and annotation. Named \\\"Optimal Transport\\\", this loss function takes the number of humans predicted at each location in the predicted output, and calculates the total amount of movement needed for each human human to go from the prediction to the annotation. This total movement is calculated as the loss function. The paper states that this method has tighter error bounds than previous methods of calculating loss, and reduces error compared to SOTA models by 16%. -------------------------------------- I've been working with a lot of newly researched models lately, and I wanted to share the most interesting models I've worked with here. This is part of a series where I post an interesting model along with a description of the research purpose and an interactive interface generated with Gradio. Previous post (Model of the Day #2) here. [link] [comments]\",\"711\":\"Day 1 | December 1, 2020 A Fireside chat with Susan Dumais, Technical Fellow & Managing Director of Microsoft Research New England, New York City, and Montreal. Microsoft's third PhD Summit was a two-day virtual workshop. It was an opportunity for top PhD students to enhance their skills, build a network, and discuss research within a community of peers and notable Microsoft researchers. More information on this workshop can be found at https:\\/\\/www.microsoft.com\\/en-us\\/research\\/event\\/phd-summit-2020\\/\",\"1565\":\"Anca Dragan is a professor at Berkeley, working on human-robot interaction \\u2014 algorithms that look beyond the robot\\u2019s function in isolation, and generate robot behavior that accounts for interaction and coordination with human beings. Support this podcast by supporting the sponsors and using the special code: \\u2013 Download Cash App on the App Store or Google Play & use code \\u201cLexPodcast\\u201d EPISODE LINKS: Anca\\u2019s Twitter: https:\\/\\/twitter.com\\/ancadianadragan Anca\\u2019s Website: https:\\/\\/people.eecs.berkeley.edu\\/~anca\\/ This conversation is part of the Artificial Intelligence podcast. If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman on Twitter, LinkedIn, Facebook, Medium,\",\"704\":\"Experiment design is hallmark of virtually all research disciplines. In many settings, one important challenge is how to automatically design experiments over large action\\/design spaces. Furthermore, it is also important for such a procedure to be adaptive, i.e., to adapt to the outcomes of previous experiments. In this talk, I will describe recent progress in using data-driven algorithmic techniques for adaptive experiment design, also known as active learning and Bayesian optimization in the machine learning community. Building upon the Gaussian process (GP) framework, I will describe case studies in personalized clinical therapy and nanophotonic structure design. Motivated by these applications, I will show how to incorporate real-world considerations such as safety, preference elicitation, and multi-fidelity experiment design into the GP framework, with new algorithms, theoretical guarantees, and empirical validation. Time permitting, I will also briefly overview a few other case studies as well. Learn more about the 2020-2021 Directions in ML: AutoML and Automating Algorithms virtual speaker series: https:\\/\\/aka.ms\\/diml\",\"1678\":\"The benefits that come with using Docker containers are well known: they provide consistent and isolated environments so that applications can be deployed anywhere - locally, in dev \\/ testing \\/ prod environments, across all cloud providers, and on-premise - in a repeatable way. Read the full story\",\"1342\":\"#openai #science #gpt3 OpenAI's newest model, DALL\\u00b7E, shows absolutely amazing abilities in generating high-quality images from arbitrary text descriptions. Like GPT-3, the range of applications and the diversity of outputs is astonishing, given that this is a single model, trained on a purely autoregressive task. This model is a significant step towards the combination of text and images in future AI applications. OUTLINE: 0:00 - Introduction 2:45 - Overview 4:20 - Dataset 5:35 - Comparison to GPT-3 7:00 - Model Architecture 13:20 - VQ-VAE 21:00 - Combining VQ-VAE with GPT-3 27:30 - Pre-Training with Relaxation 32:15 - Experimental Results 33:00 - My Hypothesis about DALL\\u00b7E's inner workings 36:15 - Sparse Attention Patterns 38:00 - DALL\\u00b7E can't count 39:35 - DALL\\u00b7E can't global order 40:10 - DALL\\u00b7E renders different views 41:10 - DALL\\u00b7E is very good at texture 41:40 - DALL\\u00b7E can complete a bust 43:30 - DALL\\u00b7E can do some reflections, but not others 44:15 - DALL\\u00b7E can do cross-sections of some objects 45:50 - DALL\\u00b7E is amazing at style 46:30 - DALL\\u00b7E can generate logos 47:40 - DALL\\u00b7E can generate bedrooms 48:35 - DALL\\u00b7E can combine unusual concepts 49:25 - DALL\\u00b7E can generate illustrations 50:15 - DALL\\u00b7E sometimes understands complicated prompts 50:55 - DALL\\u00b7E can pass part of an IQ test 51:40 - DALL\\u00b7E probably does not have geographical \\/ temporal knowledge 53:10 - Reranking dramatically improves quality 53:50 - Conclusions & Comments Blog: https:\\/\\/openai.com\\/blog\\/dall-e\\/ Links: TabNine Code Completion (Referral): http:\\/\\/bit.ly\\/tabnine-yannick YouTube: https:\\/\\/www.youtube.com\\/c\\/yannickilcher Twitter: https:\\/\\/twitter.com\\/ykilcher Discord: https:\\/\\/discord.gg\\/4H8xxDF BitChute: https:\\/\\/www.bitchute.com\\/channel\\/yannic-kilcher Minds: https:\\/\\/www.minds.com\\/ykilcher Parler: https:\\/\\/parler.com\\/profile\\/YannicKilcher LinkedIn: https:\\/\\/www.linkedin.com\\/in\\/yannic-kilcher-488534136\\/ If you want to support me, the best thing to do is to share out the content :) If you want to support me financially (completely optional and voluntary, but a lot of people have asked for this): SubscribeStar: https:\\/\\/www.subscribestar.com\\/yannickilcher Patreon: https:\\/\\/www.patreon.com\\/yannickilcher Bitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq Ethereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2 Litecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m Monero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\",\"1648\":\"The shutdowns brought an opportunity for my daughter to participate in virtual scouting events all over the United States. When the event registration form changed, I took the chance to try out some new web scraping skills while inspiring my daughter about the power of code for everyday tasks. Read the full story\",\"3761\":\"Hey guys, just wanted to check if anyone received the hackerrank link to do the coding assessment from the 2021 residency program. Just wanted to know if everyone that applied receives this or if you already have to go through a selection process to receive this invitation. Thanks and good luck to anyone here that also applied :) [link] [comments]\",\"1496\":\"Joe Rogan is a comedian, UFC commentator, and the host of the Joe Rogan Experience. Please check out our sponsors to get a discount and to support this podcast: \\u2013 Neuro: https:\\/\\/www.getneuro.com and use code LEX \\u2013 Eight Sleep: https:\\/\\/eightsleep.com\\/lex and use code LEX \\u2013 Dollar Shave Club: https:\\/\\/dollarshaveclub.com\\/lex Episode Links: Joe\\u2019s Instagram: https:\\/\\/www.instagram.com\\/joerogan Joe\\u2019s Twitter: http:\\/\\/twitter.com\\/joerogan JRE (Spotify): https:\\/\\/open.spotify.com\\/show\\/4rOoJ6Egrf8K2IrywzwOMk JRE (Apple): https:\\/\\/podcasts.apple.com\\/us\\/podcast\\/the-joe-rogan-experience\\/id360084272 JRE (YouTube): https:\\/\\/www.youtube.com\\/channel\\/UCzQUP1qoWDoEbmsQxvdjxgQ If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/podcast or connect with @lexfridman on Twitter, LinkedIn, Facebook, Medium, or YouTube where you can watch the video versions of these conversations.\",\"1582\":\"Stephen Kotkin is a professor of history at Princeton university and one of the great historians of our time, specializing in Russian and Soviet history. He has written many books on Stalin and the Soviet Union including the first 2 of a 3 volume work on Stalin, and he is currently working on volume 3. This conversation is part of the Artificial Intelligence podcast. If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman on Twitter, LinkedIn, Facebook, Medium, or YouTube where you can watch the video versions of these conversations. If you\",\"1048\":\"Get 25% off a year subscription to CuriosityStream, ends Jan 3rd 2021: (use code \\\"zachstar\\\" at sign up): https:\\/\\/curiositystream.thld.co\\/zachstardec2 STEMerch Store: https:\\/\\/stemerch.com\\/ Support the Channel: https:\\/\\/www.patreon.com\\/zachstar PayPal(one time donation): https:\\/\\/www.paypal.me\\/ZachStarYT Join this channel to get access to perks: https:\\/\\/www.youtube.com\\/channel\\/UCpCSAcbqs-sjEVfk_hMfY9w\\/join \\u25baFollow me Instagram: https:\\/\\/www.instagram.com\\/zachstar\\/ Twitter: https:\\/\\/twitter.com\\/ImZachStar Animations: Brainup Studios ( http:\\/\\/brainup.in\\/ ) Check out my Spanish channel here: https:\\/\\/www.youtube.com\\/channel\\/UCnkNu2xQBLASpj6cKC8vtpA \\u25baMy Setup: Space Pictures: https:\\/\\/amzn.to\\/2CC4Kqj Magnetic Floating Globe: https:\\/\\/amzn.to\\/2VgPdn0 Camera: https:\\/\\/amzn.to\\/2RivYu5 Mic: https:\\/\\/amzn.to\\/35bKiri Tripod: https:\\/\\/amzn.to\\/2RgMTNL Equilibrium Tube: https:\\/\\/amzn.to\\/2SowDrh \\u25baCheck out my Amazon Store: https:\\/\\/www.amazon.com\\/shop\\/zachstar\",\"1449\":\"New podcast name. New Russian hitman thumbnail. Everything else stays the same. AI is still my passion, but this gives me a bit more freedom to talk to interesting folks from all over. Thanks for the support & the love. If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/podcast or connect with @lexfridman on Twitter, LinkedIn, Facebook, Medium, or YouTube where you can watch the video versions of these conversations. If you enjoy the podcast, please rate it 5 stars on Apple Podcasts, follow on Spotify, or support it on Patreon.\",\"1879\":\"Moving averages can smooth time series data, reveal underlying trends, and identify components for use in statistical modeling. Smoothing is the process of removing random variations that appear as coarseness in a plot of raw time series data. It reduces the noise to emphasize the signal that can contain trends and cycles. Analysts also refer [\\u2026] The post Using Moving Averages to Smooth Time Series Data appeared first on Statistics By Jim.\",\"1620\":\"Read the full story\",\"4971\":\"ena Voita is a Ph.D. student at the University of Edinburgh and University of Amsterdam. Previously, She was a research scientist at Yandex Research and worked closely with the Yandex Translate team. She still teaches NLP at the Yandex School of Data Analysis. She has created an exciting new NLP course on her website lena-voita.github.io which you folks need to check out! She has one of the most well presented blogs we have ever seen, where she discusses her research in an easily digestable manner. Lena has been investigating many fascinating topics in machine learning and NLP. Today we are going to talk about three of her papers and corresponding blog articles; Source and Target Contributions to NMT Predictions -- Where she talks about the influential dichotomy between the source and the prefix of neural translation models. https:\\/\\/arxiv.org\\/pdf\\/2010.10907.pdf https:\\/\\/lena-voita.github.io\\/posts\\/source_target_contributions_to_nmt.html Information-Theoretic Probing with MDL -- Where Lena proposes a technique of evaluating a model using the minimum description length or Kolmogorov complexity of labels given representations rather than something basic like accuracy https:\\/\\/arxiv.org\\/pdf\\/2003.12298.pdf https:\\/\\/lena-voita.github.io\\/posts\\/mdl_probes.html Evolution of Representations in the Transformer - Lena investigates the evolution of representations of individual tokens in Transformers -- trained with different training objectives (MT, LM, MLM) https:\\/\\/arxiv.org\\/abs\\/1909.01380 https:\\/\\/lena-voita.github.io\\/posts\\/emnlp19_evolution.html Panel Dr. Tim Scarfe, Yannic Kilcher, Sayak Paul 00:00:00 Kenneth Stanley \\/ Greatness can not be planned house keeping 00:21:09 Kilcher intro 00:28:54 Hello Lena 00:29:21 Tim - Lenas NMT paper 00:35:26 Tim - Minimum Description Length \\/ Probe paper 00:40:12 Tim - Evolution of representations 00:46:40 Lenas NLP course 00:49:18 The peppermint tea situation 00:49:28 Main Show Kick Off 00:50:22 Hallucination vs exposure bias 00:53:04 Lenas focus on explaining the models not SOTA chasing 00:56:34 Probes paper and NLP intepretability 01:02:18 Why standard probing doesnt work 01:12:12 Evolutions of representations paper 01:23:53 BERTScore and BERT Rediscovers the Classical NLP Pipeline paper 01:25:10 Is the shifting encoding context because of BERT bidirectionality 01:26:43 Objective defines which information we lose on input 01:27:59 How influential is the dataset? 01:29:42 Where is the community going wrong? 01:31:55 Thoughts on GOFAI\\/Understanding in NLP? 01:36:38 Lena's NLP course 01:47:40 How to foster better learning \\/ understanding 01:52:17 Lena's toolset and languages 01:54:12 Mathematics is all you need 01:56:03 Programming languages https:\\/\\/lena-voita.github.io\\/ https:\\/\\/www.linkedin.com\\/in\\/elena-voita\\/ https:\\/\\/scholar.google.com\\/citations?user=EcN9o7kAAAAJ&hl=ja https:\\/\\/twitter.com\\/lena_voita\",\"1542\":\"Dmitry Korkin is a professor of bioinformatics and computational biology at Worcester Polytechnic Institute, where he specializes in bioinformatics of complex disease, computational genomics, systems biology, and biomedical data analytics. I came across Dmitry\\u2019s work when in February his group used the viral genome of the COVID-19 to reconstruct the 3D structure of its major viral proteins and their interactions with human proteins, in effect creating a structural genomics map of the coronavirus and making this data open and available to researchers everywhere. We talked about the biology of COVID-19, SARS, and viruses in general, and how computational methods can\",\"4898\":\"Great video summary of some of my recent work! Thanks @ykilcher! nitter.net\\/ykilcher\\/status\\/1352704316080091139#m\",\"4279\":\"Today Yannic Lightspeed Kilcher and I spoke with Alex Stenlake about Kernel Methods. What is a kernel? Do you remember those weird kernel things which everyone obsessed about before deep learning? What about Representer theorem and reproducible kernel hilbert spaces? SVMs and kernel ridge regression? Remember them?! Hope you enjoy the conversation! 00:00:00 Tim Intro 00:01:35 Yannic clever insight from this discussion 00:03:25 Street talk and Alex intro 00:05:06 How kernels are taught 00:09:20 Computational tractability 00:10:32 Maths 00:11:50 What is a kernel? 00:19:39 Kernel latent expansion 00:23:57 Overfitting 00:24:50 Hilbert spaces 00:30:20 Compare to DL 00:31:18 Back to hilbert spaces 00:45:19 Computational tractability 2 00:52:23 Curse of dimensionality 00:55:01 RBF: infinite taylor series 00:57:20 Margin\\/SVM 01:00:07 KRR\\/dual 01:03:26 Complexity compute kernels vs deep learning 01:05:03 Good for small problems? vs deep learning) 01:07:50 Whats special about the RBF kernel 01:11:06 Another DL comparison 01:14:01 Representer theorem 01:20:05 Relation to back prop 01:25:10 Connection with NLP\\/transformers 01:27:31 Where else kernels good 01:34:34 Deep learning vs dual kernel methods 01:33:29 Thoughts on AI 01:34:35 Outro\",\"249\":\"I'm really excited to share my latest survey paper on Deep Learning Applications for COVID-19!! \\ud83d\\udcdc This has been the product of a few months of research, and I really hope it helps people find research projects to impact the pandemic! journalofbigdata.springerope\\u2026 #100DaysOfMLCode\",\"1680\":\"Building a social graph \\u2014 knowledge graph \\u2014 to improve clinical trials' processes and reduce costs by providing better clarity and access to heterogeneous datasets. Read the full story\",\"5677\":\"[link] [comments]\",\"1599\":\"Photo of an earlier discarded version of the T-800. It was too buggy, lacked personality, kept talking about Dostoevksy. It was later repurposed as a human-like podcast host. Thanks to ethos_pictures on Instagram for the drawing: instagram.com\\/ethos_pictures\\u2026\",\"2341\":\"Hi everyone, Regarding monocular depth estimation, there seems to be lots of landmark papers and advances in the direction of unsupervised\\/self-supervised learning of depth. However, it occurred to me that such models do not regress metric depth maps, it's always in \\\"bananas\\\". And I'm wondering, how do they deal with the lack of a distance unit in autonomous vehicles? Surely it is useful to know the depth in meters to determine the velocity of other cars? Or how far is that pedestrian from us? I believe it is also easier to gather data indoor, since you can use RGB-D cameras, such as the Kinect, are there ImageNet-like dataset (bigger than NYU Depth) to train on? What is the current state-of-the-art in terms of accuracy? Thanks in advance, Best regards, Kostozard [link] [comments]\",\"1661\":\"What work does a data engineer actually do? Let me tell you one thing: it\\u2019s not what you think they should be doing, especially not the part where they are running around collecting data for you or building yet another one of those dashboards that will only be used for a few weeks. Read the full story\",\"1691\":\"Text classification datasets are used to categorize natural language texts according to content. For example, think classifying news articles by topic, or classifying book reviews based on a positive or negative response. Text classification is also helpful for language detection, organizing customer feedback, and fraud detection. Though time consuming when done manually, this process can be automated with machine learning models. The result saves companies time while also providing valuable data insights. Read the full story\",\"1053\":\"Get free access to over 2500 documentaries on CuriosityStream: https:\\/\\/curiositystream.thld.co\\/zachstardec16 (use code \\\"zachstar\\\" at sign up) STEMerch Store: https:\\/\\/stemerch.com\\/ Support the Channel: https:\\/\\/www.patreon.com\\/zachstar PayPal(one time donation): https:\\/\\/www.paypal.me\\/ZachStarYT Join this channel to get access to perks: https:\\/\\/www.youtube.com\\/channel\\/UCpCSAcbqs-sjEVfk_hMfY9w\\/join \\u25baFollow me Instagram: https:\\/\\/www.instagram.com\\/zachstar\\/ Twitter: https:\\/\\/twitter.com\\/ImZachStar 2D Graphing Software: https:\\/\\/www.desmos.com\\/calculator Animations: Brainup Studios ( http:\\/\\/brainup.in\\/ ) Check out my Spanish channel here: https:\\/\\/www.youtube.com\\/channel\\/UCnkNu2xQBLASpj6cKC8vtpA \\u25baMy Setup: Space Pictures: https:\\/\\/amzn.to\\/2CC4Kqj Magnetic Floating Globe: https:\\/\\/amzn.to\\/2VgPdn0 Camera: https:\\/\\/amzn.to\\/2RivYu5 Mic: https:\\/\\/amzn.to\\/35bKiri Tripod: https:\\/\\/amzn.to\\/2RgMTNL Equilibrium Tube: https:\\/\\/amzn.to\\/2SowDrh \\u25baCheck out my Amazon Store: https:\\/\\/www.amazon.com\\/shop\\/zachstar\",\"3139\":\"Some organizations have already identified the benefits that can be gained from Artificial Intelligence and Data Science, bringing in talented resources to enable them to build AI models and solutions. But more often than not, the business doesn\\u2019t understand the capabilities and huge potential of AI well enough, nor the investments that are required in tooling and people to make the benefits sustainable and long-term. This gap in understanding often leads to tactical exercises and proofs-of-concept that end up going nowhere. In this session, Andrew Burgess (Strategic Adviser on AI \\u2013 Founder and CEO of Greenhouse Intelligence) explained how the data scientists and the business executives can work together to maximize the value of AI. Check out some key takeaways from the session. How do you talk AI to a business person? 1. Explain AI in simple terms Business people usually don\\u2019t want to hear about algorithms or python libraries and it is highly unlikely that they will understand these aspects, but they will still want to understand what AI does and what it means for their business. It is important to explain the capabilities of AI in simple terms and focus on what AI can do, rather on how it works. To explain AI, Andrew uses a framework of 8 AI capabilities that are split into three main areas: capturing information, explaining what is happening, and finally explaining why it is happening. 2. Focus on the positives\\u2026 AI can bring huge value to an organization and it is important that the business understands the full range of benefits available. These include customer satisfaction, risk mitigation, cost reduction, loss mitigation, revenue leakage mitigation, and revenue generation. These terms will definitely resonate with the business people especially when they align with their strategic goals. 3. \\u2026without getting carried away\\u2026 Don\\u2019t over promise and under deliver. Understanding the business\\u2019s AI maturity is key in defining where the starting point is, and will help build a successful AI journey. A great way of assessing a business\\u2019s AI maturity is to talk to the different business lines and understand where they want to be in terms of implementing an AI strategy: their \\u2018AI ambition\\u2019. The gap between the current situation and their ambition is the scope for AI. Setting the expectations for what AI can do will avoid the teams getting over excitement and help achieve realistic goals. 4. \\u2026and without ignoring the risks There are a few risks specific to AI and it\\u2019s important to keep them in mind. Problems like Black Box models, bias, AI\\u2019s naivety, over-excitement\\u2026 can all happen. The business people have to understand that there are risks associated with an AI journey, but that there are also solutions to overcome them. 5. Get a strategy Getting an AI strategy in place is important to define what the business challenges are within the business, as well as the objectives, and how AI can help deliver them. It is the tool that brings the business and the data teams together. When creating an AI strategy, it is crucial to identify the opportunities for AI that align with the business\\u2019s objectives and \\/ or can solve their specific challenges. 6. Think big but act small Having a strategy and a vision is key, but to ensure you get there, it is important to start with things that are very practical and achievable in a short period of time. Try and identify the activities that are going to give the most value and are easiest to implement. Start with proofs-of-concept from the highest priority opportunities in the list, but also work on data cleansing, change management, and education. 7. Look at all the options Many business people become paralyzed when they look at the plethora of AI vendors out there, but vendors are only one option, and should only be used when the requirement exactly matches their standard capabilities. At the other end of the scale, where you have a unique data set and\\/or a major project, then you may need to code from scratch, but most opportunities will likely involve the use and configuration of data science platforms. 8. Keep measuring and checking How do you get continuous improvement and optimization on all the activities you are doing? As your program is starting to build momentum, make sure that you\\u2019ve got regular meetings with the program team to monitor progress. Demonstrate a clear return on investment and have simple dashboards that display what the benefits are. By following these eight points, data scientists, analysts, and developers can engage effectively with the business to deliver AI projects that are valuable and beneficial to the whole organization. Interested in watching the recording of the session and learning more? Click here. The post Maximizing your Value from AI appeared first on Open Source Leader in AI and ML.\",\"2335\":\"Hello! This is my first post on reddit. Actually, we lately released the code of our new paper \\\"Random Shadows and Highlights - A new data augmentation method for extreme lighting conditions\\\". The code is available here: http:\\/\\/bit.do\\/AugRSH Also, you can read the paper here: Random Shadows and Highlights Your comments are welcome. [link] [comments]\",\"4274\":\"Welcome to the Christmas special community edition of MLST! We discuss some recent and interesting papers from Pedro Domingos (are NNs kernel machines?), Deepmind (can NNs out-reason symbolic machines?), Anna Rodgers - When BERT Plays The Lottery, All Tickets Are Winning, Prof. Mark Bishop (even causal methods won't deliver understanding), We also cover our favourite bits from the recent Montreal AI event run by Prof. Gary Marcus (including Rich Sutton, Danny Kahneman and Christof Koch). We respond to a reader mail on Capsule networks. Then we do a deep dive into Type Theory and Lambda Calculus with community member Alex Mattick. In the final hour we discuss inductive priors and label information density with another one of our discord community members. Panel: Dr. Tim Scarfe, Yannic Kilcher, Alex Stenlake, Dr. Keith Duggar Enjoy the show and don't forget to subscribe! 00:00:00 Welcome to Christmas Special! 00:00:44 SoTa meme 00:01:30 Happy Christmas! 00:03:11 Paper -- DeepMind - Outperforming neuro-symbolic models with NNs (Ding et al) 00:08:57 What does it mean to understand? 00:17:37 Paper - Prof. Mark Bishop Artificial Intelligence is stupid and causal reasoning wont fix it 00:25:39 Paper -- Pedro Domingos - Every Model Learned by Gradient Descent Is Approximately a Kernel Machine 00:31:07 Paper - Bengio - Inductive Biases for Deep Learning of Higher-Level Cognition 00:32:54 Anna Rodgers - When BERT Plays The Lottery, All Tickets Are Winning 00:37:16 Montreal AI event - Gary Marcus on reasoning 00:40:37 Montreal AI event -- Rich Sutton on universal theory of AI 00:49:45 Montreal AI event -- Danny Kahneman, System 1 vs 2 and Generative Models ala free energy principle 01:02:57 Montreal AI event -- Christof Koch - Neuroscience is hard 01:10:55 Markus Carr -- reader letter on capsule networks 01:13:21 Alex response to Marcus Carr 01:22:06 Type theory segment -- with Alex Mattick from Discord 01:24:45 Type theory segment -- What is Type Theory 01:28:12 Type theory segment -- Difference between functional and OOP languages 01:29:03 Type theory segment -- Lambda calculus 01:30:46 Type theory segment -- Closures 01:35:05 Type theory segment -- Term rewriting (confluency and termination) 01:42:02 MType theory segment -- eta term rewritig system - Lambda Calculus 01:54:44 Type theory segment -- Types \\/ semantics 02:06:26 Type theory segment -- Calculus of constructions 02:09:27 Type theory segment -- Homotopy type theory 02:11:02 Type theory segment -- Deep learning link 02:17:27 Jan from Discord segment -- Chrome MRU skit 02:18:56 Jan from Discord segment -- Inductive priors (with XMaster96\\/Jan from Discord) 02:37:59 Jan from Discord segment -- Label information density (with XMaster96\\/Jan from Discord) 02:55:13 Outro\",\"5722\":\"[link] [comments]\",\"1871\":\"Note: this is a guest post by Alexander Moreno, a Computer Science PhD student at the Georgia Institute of Technology. He blogs at www.boostedml.com Survival analysis is an important subfield of statistics and biostatistics. These methods involve modeling the time to a first event such as death. In this post we give a brief tour [\\u2026] The post A Tour of Survival Analysis appeared first on Statistics By Jim.\",\"229\":\"berthub.eu\\/articles\\/posts\\/re\\u2026 does a great job breaking down what is in the Pfizer vaccine.\",\"1597\":\"Here's my conversation with Max Tegmark (@tegmark). Our first chat was episode #1 of this podcast. Now he's back! We talk about the intersection of machine learning and physics, and also about how to avoid near-term and long-term existential threats of AI. invidious.snopyta.org\\/watch?v=RL4j4KPw\\u2026\",\"1432\":\"Juergen Schmidhuber is the co-creator of long short-term memory networks (LSTMs) which are used in billions of devices today for speech recognition, translation, and much more. Over 30 years, he has proposed a lot of interesting, out-of-the-box ideas in artificial intelligence including a formal theory of creativity. Video version is available on YouTube. If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman on Twitter, LinkedIn, Facebook, or YouTube where you can watch the video versions of these conversations.\",\"2837\":\"Presented by the Calc Intelligence team: https:\\/\\/aka.ms\\/calcintel 0:00 \\u2013 Introduction, Andy Gordon 4:08 \\u2013 LAMBDA in action, Jack Williams 8:47 \\u2013 How we started the collaboration with Excel, Simon Peyton Jones 13:08 \\u2013 Elastic sheet-defined functions and gridlets, Advait Sarkar 18:57 \\u2013 Units and knowledge base categories as types, Carina Negreanu 22:41 \\u2013 Andy Gordon in conversation with Brian Jones, GPM at Excel Microsoft Research is hiring Programming Language folks worldwide: https:\\/\\/aka.ms\\/popl\",\"1576\":\"Michael Malice is a political thinker, podcaster, and author. Please support this podcast by checking out our sponsors: \\u2013 SEMrush: https:\\/\\/www.semrush.com\\/partner\\/lex\\/ to get a free month of Guru \\u2013 DoorDash: https:\\/\\/doordash.com\\/ and use code LEX to get $5 off \\u2013 MasterClass: https:\\/\\/masterclass.com\\/lex to get 15% off annual sub EPISODE LINKS: Michael\\u2019s Twitter: https:\\/\\/twitter.com\\/michaelmalice Michael\\u2019s Community: https:\\/\\/malice.locals.com\\/ Michael\\u2019s YouTube: https:\\/\\/www.youtube.com\\/channel\\/UC5tj5QCpJKIl-KIa4Gib5Xw Michael\\u2019s Website: http:\\/\\/michaelmalice.com\\/about\\/ Your Welcome podcast: https:\\/\\/bit.ly\\/30q8oz1 The New Right (book): https:\\/\\/amzn.to\\/34gxLo3 Dear Reader (book): https:\\/\\/amzn.to\\/2HPPlHS PODCAST INFO: Podcast website: https:\\/\\/lexfridman.com\\/podcast Apple Podcasts: https:\\/\\/apple.co\\/2lwqZIr Spotify: https:\\/\\/spoti.fi\\/2nEwCF8 RSS: https:\\/\\/lexfridman.com\\/feed\\/podcast\\/ YouTube Full Episodes: https:\\/\\/youtube.com\\/lexfridman YouTube Clips: https:\\/\\/lexclips SUPPORT & CONNECT: \\u2013 Check out\",\"3194\":\"Applied machine learning is typically focused on finding a single model that performs well or best on a given dataset. Effective use of the model will require appropriate preparation of the input data and hyperparameter tuning of the model. Collectively, the linear sequence of steps required to prepare the data, tune the model, and transform the predictions is called the modeling pipeline. Modern machine learning libraries like the scikit-learn Python library allow this sequence of steps to be defined and used correctly (without data leakage) and consistently (during evaluation and prediction). Nevertheless, working with modeling pipelines can be confusing to beginners as it requires a shift in perspective of the applied machine learning process. In this tutorial, you will discover modeling pipelines for applied machine learning. After completing this tutorial, you will know: Applied machine learning is concerned with more than finding a good performing model; it also requires finding an appropriate sequence of data preparation steps and steps for the post-processing of predictions. Collectively, the operations required to address a predictive modeling problem can be considered an atomic unit called a modeling pipeline. Approaching applied machine learning through the lens of modeling pipelines requires a change in thinking from evaluating specific model configurations to sequences of transforms and algorithms. Let\\u2019s get started. A Gentle Introduction to Machine Learning Modeling Pipelines Photo by Jay Huang, some rights reserved. Tutorial Overview This tutorial is divided into three parts; they are: Finding a Skillful Model Is Not Enough What Is a Modeling Pipeline? Implications of a Modeling Pipeline Finding a Skillful Model Is Not Enough Applied machine learning is the process of discovering the model that performs best for a given predictive modeling dataset. In fact, it\\u2019s more than this. In addition to discovering which model performs the best on your dataset, you must also discover: Data transforms that best expose the unknown underlying structure of the problem to the learning algorithms. Model hyperparameters that result in a good or best configuration of a chosen model. There may also be additional considerations such as techniques that transform the predictions made by the model, like threshold moving or model calibration for predicted probabilities. As such, it is common to think of applied machine learning as a large combinatorial search problem across data transforms, models, and model configurations. This can be quite challenging in practice as it requires that the sequence of one or more data preparation schemes, the model, the model configuration, and any prediction transform schemes must be evaluated consistently and correctly on a given test harness. Although tricky, it may be manageable with a simple train-test split but becomes quite unmanageable when using k-fold cross-validation or even repeated k-fold cross-validation. The solution is to use a modeling pipeline to keep everything straight. What Is a Modeling Pipeline? A pipeline is a linear sequence of data preparation options, modeling operations, and prediction transform operations. It allows the sequence of steps to be specified, evaluated, and used as an atomic unit. Pipeline: A linear sequence of data preparation and modeling steps that can be treated as an atomic unit. To make the idea clear, let\\u2019s look at two simple examples: The first example uses data normalization for the input variables and fits a logistic regression model: [Input], [Normalization], [Logistic Regression], [Predictions] The second example standardizes the input variables, applies RFE feature selection, and fits a support vector machine. [Input], [Standardization], [RFE], [SVM], [Predictions] You can imagine other examples of modeling pipelines. As an atomic unit, the pipeline can be evaluated using a preferred resampling scheme such as a train-test split or k-fold cross-validation. This is important for two main reasons: Avoid data leakage. Consistency and reproducibility. A modeling pipeline avoids the most common type of data leakage where data preparation techniques, such as scaling input values, are applied to the entire dataset. This is data leakage because it shares knowledge of the test dataset (such as observations that contribute to a mean or maximum known value) with the training dataset, and in turn, may result in overly optimistic model performance. Instead, data transforms must be prepared on the training dataset only, then applied to the training dataset, test dataset, validation dataset, and any other datasets that require the transform prior to being used with the model. A modeling pipeline ensures that the sequence of data preparation operations performed is reproducible. Without a modeling pipeline, the data preparation steps may be performed manually twice: once for evaluating the model and once for making predictions. Any changes to the sequence must be kept consistent in both cases, otherwise differences will impact the capability and skill of the model. A pipeline ensures that the sequence of operations is defined once and is consistent when used for model evaluation or making predictions. The Python scikit-learn machine learning library provides a machine learning modeling pipeline via the Pipeline class. You can learn more about how to use this Pipeline API in this tutorial: How to Avoid Data Leakage When Performing Data Preparation Implications of a Modeling Pipeline The modeling pipeline is an important tool for machine learning practitioners. Nevertheless, there are important implications that must be considered when using them. The main confusion for beginners when using pipelines comes in understanding what the pipeline has learned or the specific configuration discovered by the pipeline. For example, a pipeline may use a data transform that configures itself automatically, such as the RFECV technique for feature selection. When evaluating a pipeline that uses an automatically-configured data transform, what configuration does it choose? or When fitting this pipeline as a final model for making predictions, what configuration did it choose? The answer is, it doesn\\u2019t matter. Another example is the use of hyperparameter tuning as the final step of the pipeline. The grid search will be performed on the data provided by any prior transform steps in the pipeline and will then search for the best combination of hyperparameters for the model using that data, then fit a model with those hyperparameters on the data. When evaluating a pipeline that grid searches model hyperparameters, what configuration does it choose? or When fitting this pipeline as a final model for making predictions, what configuration did it choose? The answer again is, it doesn\\u2019t matter. The answer applies when using a threshold moving or probability calibration step at the end of the pipeline. The reason is the same reason that we are not concerned about the specific internal structure or coefficients of the chosen model. For example, when evaluating a logistic regression model, we don\\u2019t need to inspect the coefficients chosen on each k-fold cross-validation round in order to choose the model. Instead, we focus on its out-of-fold predictive skill Similarly, when using a logistic regression model as the final model for making predictions on new data, we do not need to inspect the coefficients chosen when fitting the model on the entire dataset before making predictions. We can inspect and discover the coefficients used by the model as an exercise in analysis, but it does not impact the selection and use of the model. This same answer generalizes when considering a modeling pipeline. We are not concerned about which features may have been automatically selected by a data transform in the pipeline. We are also not concerned about which hyperparameters were chosen for the model when using a grid search as the final step in the modeling pipeline. In all three cases: the single model, the pipeline with automatic feature selection, and the pipeline with a grid search, we are evaluating the \\u201cmodel\\u201d or \\u201cmodeling pipeline\\u201d as an atomic unit. The pipeline allows us as machine learning practitioners to move up one level of abstraction and be less concerned with the specific outcomes of the algorithms and more concerned with the capability of a sequence of procedures. As such, we can focus on evaluating the capability of the algorithms on the dataset, not the product of the algorithms, i.e. the model. Once we have an estimate of the pipeline, we can apply it and be confident that we will get similar performance, on average. It is a shift in thinking and may take some time to get used to. It is also the philosophy behind modern AutoML (automatic machine learning) techniques that treat applied machine learning as a large combinatorial search problem. Further Reading This section provides more resources on the topic if you are looking to go deeper. How to Avoid Data Leakage When Performing Data Preparation Summary In this tutorial, you discovered modeling pipelines for applied machine learning. Specifically, you learned: Applied machine learning is concerned with more than finding a good performing model; it also requires finding an appropriate sequence of data preparation steps and steps for the post-processing of predictions. Collectively, the operations required to address a predictive modeling problem can be considered an atomic unit called a modeling pipeline. Approaching applied machine learning through the lens of modeling pipelines requires a change in thinking from evaluating specific model configurations to sequences of transforms and algorithms. Do you have any questions? Ask your questions in the comments below and I will do my best to answer. The post A Gentle Introduction to Machine Learning Modeling Pipelines appeared first on Machine Learning Mastery.\",\"4673\":\"I\\u2019m trying to understand how precision and recall changes for different loss functions. I\\u2019m especially looking into Cross Entropy, Dice Loss and Focal Loss for object detection. So far I\\u2019ve researched the loss functions and I know that both Dice Loss and Focal loss are good when there is data imbalance and I know how they improve on Cross Entropy for loss calculation but I\\u2019m struggling to analyse how they would differ from each other in terms of Precision and Recall. I\\u2019d appreciate it if someone could explain me how using different loss functions affect Precision&Recall Cheers! [link] [comments]\",\"4296\":\"Today we had a fantastic conversation with Professor Max Welling, VP of Technology, Qualcomm Technologies Netherlands B.V. Max is a strong believer in the power of data and computation and its relevance to artificial intelligence. There is a fundamental blank slate paradgm in machine learning, experience and data alone currently rule the roost. Max wants to build a house of domain knowledge on top of that blank slate. Max thinks there are no predictions without assumptions, no generalization without inductive bias. The bias-variance tradeoff tells us that we need to use additional human knowledge when data is insufficient. Max Welling has pioneered many of the most sophistocated inductive priors in DL models developed in recent years, allowing us to use Deep Learning with non-euclidean data i.e. on graphs\\/topology (a field we now called \\\"geometric deep learning\\\") or allowing network architectures to recognise new symmetries in the data for example gauge or SE(3) equivariance. Max has also brought many other concepts from his physics playbook into ML, for example quantum and even Bayesian approaches. This is not an episode to miss, it might be our best yet! Panel: Dr. Tim Scarfe, Yannic Kilcher, Alex Stenlake 00:00:00 Show introduction 00:04:37 Protein Fold from DeepMind -- did it use SE(3) transformer? 00:09:58 How has machine learning progressed 00:19:57 Quantum Deformed Neural Networks paper 00:22:54 Probabilistic Numeric Convolutional Neural Networks paper 00:27:04 Ilia Karmanov from Qualcomm interview mini segment 00:32:04 Main Show Intro 00:35:21 How is Max known in the community? 00:36:35 How Max nurtures talent, freedom and relationship is key 00:40:30 Selecting research directions and guidance 00:43:42 Priors vs experience (bias\\/variance trade-off) 00:48:47 Generative models and GPT-3 00:51:57 Bias\\/variance trade off -- when do priors hurt us 00:54:48 Capsule networks 01:03:09 Which old ideas whould we revive 01:04:36 Hardware lottery paper 01:07:50 Greatness can't be planned (Kenneth Stanley reference) 01:09:10 A new sort of peer review and originality 01:11:57 Quantum Computing 01:14:25 Quantum deformed neural networks paper 01:21:57 Probabalistic numeric convolutional neural networks 01:26:35 Matrix exponential 01:28:44 Other ideas from physics i.e. chaos, holography, renormalisation 01:34:25 Reddit 01:37:19 Open review system in ML 01:41:43 Outro\",\"3764\":\"https:\\/\\/preview.redd.it\\/3o45ur0apdc61.png?width=600&format=png&auto=webp&s=8e589725ebf5b9f2c7aa279944c73688f85e4bc1 \\u200b TL;DR : It's a clean, easy to understand repo that allows one to make output of GAN to match input text. (uses CLIP as a transfer layer) https:\\/\\/github.com\\/cloneofsimo\\/clipping-CLIP-to-GAN Recently, openAI proposed CLIP : multimodal transformer based model that can perform incredible wide-domain zero shot tasks. You can read all about it from openAI's blog and it's paper. It i On the other hand, DALL-E, which is generative model, has also been released on the same date, but it is currently not-released and probably end up like GPT-3. More recently, Ryan Murdock proposed that good feature visualization should generate some image that matches the text : mainly, he used SIREN as a set of parameters to optimize over and used autograd to learn the best parameters that generates image that matches given set of images. In general, this could be done with any kind of deterministic generative model, such as GAN, AE , VQVAE, etc (I think VQVAE would be really good here because DALL-E used it, but the gradient ascent part is still something to implement). This repository contains test.py, that in general takes generative model and learnable latent vector to find image matching input text. [link] [comments]\",\"243\":\"New blog post: Is the Great Stagnation ending? What technologies am I watching in the decade ahead? Are we going to get life extension treatments soon? 5700+ words on all of this. Please read and share! elidourado.com\\/blog\\/notes-on\\u2026\",\"1464\":\"Grant Sanderson is a math educator and creator of 3Blue1Brown, a popular YouTube channel that uses programmatically-animated visualizations to explain concepts in linear algebra, calculus, and other fields of mathematics. This conversation is part of the Artificial Intelligence podcast. If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman on Twitter, LinkedIn, Facebook, Medium, or YouTube where you can watch the video versions of these conversations. If you enjoy the podcast, please rate it 5 stars on Apple Podcasts, follow on Spotify, or support it on Patreon. This episode is presented by Cash\",\"5675\":\"This stuff is completely over my head. They always ask this for data science and ML positions and while I feel like I know the principles of ML\\/stats I get shafted by this section. I have never taken a CS class, most of my programming is done in R\\/Julia and focuses on numerical computing. They almost never ask me to implement a GLM via gradient descent or IRLS which I would be very comfortable doing. Its always some data structure or general algo related question. Never statistical\\/ML algorithms. Why are these questions asked instead of asking you to analyze a dataset or something? What value do leetcode style questions have in this field? I have never seen these things come up in actual stats\\/ML work. Yet so many DS\\/ML coding challenges include them. The coding parts never ask me to demonstrate my actual knowledge of ML in code. I find these questions a lot harder than \\u201creal\\u201d ML. My mind doesn\\u2019t work this way and I didn\\u2019t go into CS for this reason. Its like they want to gatekeep stat\\/math\\/sciences people from getting into ML. [link] [comments]\",\"1578\":\"John Clarke is a BJJ black belt and MMA coach. Please support this podcast by checking out our sponsors: \\u2013 Theragun: https:\\/\\/theragun.com\\/lex to get 30 day trial \\u2013 Magic Spoon: https:\\/\\/magicspoon.com\\/lex and use code LEX to get free shipping \\u2013 Eight Sleep: https:\\/\\/www.eightsleep.com\\/lex and use code LEX to get $200 off \\u2013 Cash App: https:\\/\\/cash.app\\/ and use code LexPodcast to get $10 EPISODE LINKS: Broadway Jiu Jitsu website: https:\\/\\/www.broadwayjiujitsu.com Broadway Jiu Jitsu instagram: https:\\/\\/www.instagram.com\\/broadwayjiujitsu Please, Allow Me podcast: https:\\/\\/podcasts.apple.com\\/us\\/podcast\\/please-allow-me\\/id1531735873 Please, Allow Me instagram: https:\\/\\/www.instagram.com\\/please_allow_me_podcast\\/ PODCAST INFO: Podcast website: https:\\/\\/lexfridman.com\\/podcast Apple Podcasts: https:\\/\\/apple.co\\/2lwqZIr Spotify: https:\\/\\/spoti.fi\\/2nEwCF8 RSS: https:\\/\\/lexfridman.com\\/feed\\/podcast\\/ YouTube Full Episodes: https:\\/\\/youtube.com\\/lexfridman\",\"5667\":\"https:\\/\\/youtu.be\\/Q0kN_ZHHDQY [Tim spends first 20 minutes talking about Kenneth Stanley episode and Open Endedness] Lena Voita is a Ph.D. student at the University of Edinburgh and University of Amsterdam. Previously, She was a research scientist at Yandex Research and worked closely with the Yandex Translate team. She still teaches NLP at the Yandex School of Data Analysis. She has created an exciting new NLP course on her website lena-voita.github.io which you folks need to check out! She has one of the most well presented blogs we have ever seen, where she discusses her research in an easily digestable manner. Lena has been investigating many fascinating topics in machine learning and NLP. Today we are going to talk about three of her papers and corresponding blog articles; Source and Target Contributions to NMT Predictions -- Where she talks about the influential dichotomy between the source and the prefix of neural translation models. https:\\/\\/arxiv.org\\/pdf\\/2010.10907.pdf https:\\/\\/lena-voita.github.io\\/posts\\/source_target_contributions_to_nmt.html Information-Theoretic Probing with MDL -- Where Lena proposes a technique of evaluating a model using the minimum description length or Kolmogorov complexity of labels given representations rather than something basic like accuracy https:\\/\\/arxiv.org\\/pdf\\/2003.12298.pdf https:\\/\\/lena-voita.github.io\\/posts\\/mdl_probes.html Evolution of Representations in the Transformer - Lena investigates the evolution of representations of individual tokens in Transformers -- trained with different training objectives (MT, LM, MLM) https:\\/\\/arxiv.org\\/abs\\/1909.01380 https:\\/\\/lena-voita.github.io\\/posts\\/emnlp19_evolution.html Panel Dr. Tim Scarfe, Yannic Kilcher, Sayak Paul [link] [comments]\",\"201\":\"With sports (and everything else) cancelled, this data scientist decided to take on COVID-19 | A Winner\\u2019s Interview with David MezzettiWhen his hobbies went on hiatus, Kaggler David Mezzetti made fighting COVID-19 his mission.Photo by Clay Banks on UnsplashLet\\u2019s learn about David!https:\\/\\/www.kaggle.com\\/davidmezzetti David Mezzetti is the founder of NeuML, a data analytics and machine learning company that develops innovative products backed by machine learning. He previously co-founded and built Data Works into a 50+ person well-respected software services company. In August 2019, Data Works was acquired and Dave worked to ensure a successful transition. David, what can you tell us about your background? David: My technical background is in ETL, data extraction, data engineering and data analytics. I spent over a decade of my career developing large-scale data pipelines to transform both structured and unstructured data into formats that can be utilized in downstream systems. I also have experience in building large-scale distributed text search and Natural Language Processing (NLP) systems. Do you have any prior experience or domain knowledge that helped you succeed in this competition? I\\u2019ve worked in the data analytics space for 15+ years but did not have prior knowledge of medical documents or the medical industry. How did you get started competing on Kaggle? I\\u2019ve participated in a couple March Madness competitions. I was looking forward to the 2020 tournament and had a model I was very excited about. The way the season went was perfect for the strengths of the model but we\\u2019ll never know how it would have performed. What made you decide to enter this competition? When the 2020 March Madness competition was cancelled and COVID-19 was really starting to hit hard, I wanted to find a way to get involved and help. NeuML was working on a real-time sports event tracking application, neuspo but sports along with everything else was being shut down and there were no sports to track. With sports and life on a hiatus, I saw the Kaggle CORD-19 challenge and felt I had the background to be able to contribute. On top of everything going on, my Mom passed away in early March. She was a high school biology teacher and would have been happy to know I was involved. This effort was also a good distraction from everything going on and a way to feel like I could do my part to help beat COVID-19. Let\\u2019s get technicalTell us about the overall architecture or approach to the problem. The solution consisted of two main parts, a sentence embeddings based search index and a custom BERT QA model to extract column based answers, known as summary tables for a specific list of questions. For each query, an embeddings query identifies the list of best matching documents. Common fields including date, title, authors and the reference url are stored as search result columns. A custom BERT QA model was developed to add additional columns to the list of search results. For example, given a search of the CORD-19 dataset for \\u201chypertension\\u201d, an additional column for the question \\u201cWhat is the risk factor of developing severe symptoms for patients with hypertension?\\u201d is added as a separate column. Did any past research or previous competitions inform your approach? Much of the search logic was based on a prior project, codequestion (https:\\/\\/github.com\\/neuml\\/codequestion). codequestion builds a sentence embeddings index over coding questions to match developers questions with previously asked questions\\/answers. Given that I already had that code base, I took that approach when starting with the CORD-19 dataset and much of the code is still derived from codequestion today. What preprocessing and feature engineering did you do? The CORD-19 dataset has a metadata CSV file with the full list of documents along with full-text stored in separate JSON files. An ETL process was built to take the CSV, find the corresponding text articles and load the data into a SQLite database. The text is then broken down into sentences per document, and those sentences are mapped to sentence embeddings using a BM25 + fastText method described in this Medium article. What supervised learning methods did you use? All search and question-answering was unsupervised using fastText+BM25 and a BERT based model for QA. An important concept discovered early on was the importance of study design. All articles are not considered equal, and the medical community puts more weight behind different study types. For example, studies with a larger sample size (i.e. more patients) or systematic reviews are held in higher regard vs mathematical modeling\\/forecasting articles. A Random Forest classifier was built to analyze articles to determine the study design based on the word tokens and named entities within an article. What was your most important insight into the data? The CORD-19 dataset is dynamic and growing. I saw that almost everyone, including myself took the first approach of building a search index that allowed finding documents based on matching tokens. Additionally, summarization was seen as a way to also add value. The thought being to show researchers all data on a particular term or concept. Building on the previous point on study design, not all documents are of equal value. Labeling documents with a study design proved to be greatly beneficial in allowing researchers to review a document vs just showing documents with matching tokens. Additionally, where tokens show up in a document is important. Some articles reference a concept in the introduction or discussion sections but the article doesn\\u2019t cover that concept. Most medical articles have methods & results sections and matches in those sections are more important. Were you surprised by any of your findings? I had little to no expectations entering this competition, so I wouldn\\u2019t say I was surprised by anything. It was great to see so many smart and capable people all working together to try to help in whatever way they could. Which tools did you use? All of the work is driven by the Kaggle platform. The list of notebooks cover all the submissions for Round 1 and Round 2 of the CORD-19 challenge. All of the notebooks are in Python. Sentence Embeddings Notebook Round 1: What is known about transmission, incubation, and environmental stability? What do we know about COVID-19 risk factors? What do we know about virus genetics, origin, and evolution? What do we know about vaccines and therapeutics? What do we know about non-pharmaceutical interventions? What has been published about medical care? What do we know about diagnostics and surveillance? What has been published about information sharing and inter-sectoral collaboration? What has been published about ethical and social science considerations? Round 2: Task 1: Population Task 2: Relevant Factors Task 3: Patient Descriptions Task 4: Models and Open Questions Task 5: Materials Task 6: Diagnostics Task 7: Therapeutics Task 8: Risk Factors Full Task CSV Export List There is also a separate Python project on github, cord19q. cord19q has the logic for ETL, building the embeddings index and running the custom BERT QA model. How did you spend your time on this competition? The early days of the effort were spent on EDA and exchanging ideas with other members of the community. Before models could be built, gaining an understanding of the data, strengths and weaknesses of the dataset and what researchers are looking for out of the CORD-19 dataset was needed. I was fortunate enough to find like-minded data scientists who were willing to roll up their sleeves and write code to help discover what we want from the data. It wasn\\u2019t until 1\\u20132 months into the effort that machine learning models and feature engineering were even considered. Most of Round 1 was focused on data extraction, parsing, requirements analysis and building a system to search for documents. The work of Round 1 led to discovering that building summary tables with extracted answers to a series of questions, would be most beneficial to the medical community. Fortunately, a team of medical experts manually curated a dataset that could be used to help build machine learning models. In Round 2, a BERT based QA model was developed to be able to extract answers from medical documents. This required building a custom question-answer dataset to teach a model how to answer medical questions. In Round 2, the majority of time was spent on building this model. What does your hardware setup look like? All of the submissions were built on the Kaggle platform as CPU Notebooks. Development was done on a quad core laptop with a 8GB GPU and 32GB of RAM. The fastText embeddings, study design models, and custom BERT QA models were built offline using this laptop. What was the run time for both training and prediction of your winning solution? Given that the data is continually updated, there is a recurring job that runs each update (using kernelpipes). It takes about 6 hours to fully ETL, build the models and run all the solution notebooks on Kaggle. Words of wisdomWhat have you taken away from this competition? This challenge was unique for a number of reasons. First there was no known answer, this was a real-world problem like you would encounter in industry, where someone has a large dataset and they aren\\u2019t sure what to do with it. This approach requires an iterative process of exploring the data, sharing feedback with experts and building a workflow to solve the problem. The data scientists involved in this effort were extremely fortunate to be guided by Savanna Reid, an epidemiologist volunteering her time. We were also fortunate Kaggle was heavily involved with Paul Mooney and Anthony Goldbloom helping guide the effort. I was fortunate to be able to bounce ideas off other data scientists working the effort, specifically Ken Miller and Andy White. It was an honor to volunteer and while I\\u2019ll never know the true impact these contributions made, I like to think it did a small part to help. Looking back, what would you do differently now? Entering the competition, my first instinct was to use sentence embeddings since I had an existing similar project. If starting over, I would have explored different methods to search the documents to see if any other methods performed better. Do you have any advice for those just getting started in data science? Much of your time will be spent on data preparation and feature engineering. The best way to learn data science is to solve a problem you\\u2019re interested in. Sports analytics is how I got started in data science. This was an engaging way for me to stay focused not only in the algorithms but the data itself. \\u2014 Additional Medium posts by David Mezzetti: Combating COVID-19 with Data Science Building Analysis Pipelines with Kaggle When his hobbies went on hiatus, this Kaggler made fighting COVID-19 with data his mission | A\\u2026 was originally published in Kaggle Blog on Medium, where people are continuing the conversation by highlighting and responding to this story.\",\"500\":\"Title: Knowledge is Power: Understanding your Data through EDA and Visualisations Abstract: We will discuss the power of exploratory data analysis (EDA) to gain a deeper understanding of your data sets and machine learning problems. In particular, data visualisation techniques can quickly reveal key characteristics, promising features, and important caveats. Beyond data exploration, visuals are crucial for interpreting and communicating your findings on a variety of levels. Kaggle is a treasure trove for EDA tools and techniques - as well as for masterful examples of how to structure your analysis. We will discuss outstanding examples and underrated gems in the world of Kaggle Notebooks. Through one of my own Notebooks I will share my personal approach to investigating a new dataset. Bio: Data Scientist at Edison Software | 1st Kaggle Kernels Grandmaster | PhD Astrophysicist #Data #EDA #Visualizations --- Please subscribe and like the video to help me keep motivated to make awesome videos like this one. :) To buy my book, Approaching (Almost) Any Machine Learning problem, please visit: https:\\/\\/bit.ly\\/buyaaml Follow me on: Twitter: https:\\/\\/twitter.com\\/abhi1thakur LinkedIn: https:\\/\\/www.linkedin.com\\/in\\/abhi1thakur\\/ Kaggle: https:\\/\\/kaggle.com\\/abhishek Instagram: https:\\/\\/instagram.com\\/abhi4ml\",\"1538\":\"Sertac Karaman is a professor at MIT, co-founder of the autonomous vehicle company Optimus Ride, and is one of top roboticists in the world, including robots that drive and robots that fly. Support this podcast by signing up with these sponsors: \\u2013 Cash App \\u2013 use code \\u201cLexPodcast\\u201d and download: \\u2013 Cash App (App Store): https:\\/\\/apple.co\\/2sPrUHe \\u2013 Cash App (Google Play): https:\\/\\/bit.ly\\/2MlvP5w EPISODE LINKS: Sertac\\u2019s Website: http:\\/\\/sertac.scripts.mit.edu\\/web\\/ Sertac\\u2019s Twitter: https:\\/\\/twitter.com\\/sertackaraman Optimus Ride: https:\\/\\/www.optimusride.com\\/ This conversation is part of the Artificial Intelligence podcast. If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman on Twitter, LinkedIn,\",\"4299\":\"This week Dr. Keith Duggar, Alex Stenlake and Dr. Tim Scarfe discuss the theory of computation, intelligence, Bayesian model selection, the intelligence explosion and the the phenomenon of \\\"interactive articles\\\". 00:00:00 Intro 00:01:27 Kernels and context-free grammars 00:06:04 Theory of computation 00:18:41 Intelligence 00:22:03 Bayesian model selection 00:44:05 AI-IQ Measure \\/ Intelligence explosion 00:52:09 Interactive articles 01:12:32 Outro\",\"1138\":\"A number of galactic proportions... See more #MegaFavNumbers on this playlist - https:\\/\\/bit.ly\\/MegaFavNumbers - and read below to contribute your own. More links & stuff in full description below \\u2193\\u2193\\u2193 See our full collection of video on the sum of three cubes at: http:\\/\\/bit.ly\\/SumOfCubes (*) the cube (which we depicted quite loosely and artistically, of course) would in all likelihood collapse to form a black hole because of the incredible mass contained with the radius #MegaFavNumbers This video was part of the MegaFavNumbers collaboration between Math YouTubers celebrating favourite numbers bigger than one million. Make a video about YOUR favourite mega-number. Upload your videos to YouTube with the hashtag #MegaFavNumbers and with MegaFavNumbers in the title, and your video will be added to the megafavnumbers playlist. https:\\/\\/bit.ly\\/MegaFavNumbers Submissions close Wednesday, 2nd September, 2020. SUM OF THREE CUBES T-SHIRT: https:\\/\\/teespring.com\\/42-sum-cubes-numberphile Thanks to Prof Mike Merrifield for some discussion on this topic. Numberphile is supported by the Mathematical Sciences Research Institute (MSRI): http:\\/\\/bit.ly\\/MSRINumberphile We are also supported by Science Sandbox, a Simons Foundation initiative dedicated to engaging everyone with the process of science. https:\\/\\/www.simonsfoundation.org\\/outreach\\/science-sandbox\\/ And support from Math For America - https:\\/\\/www.mathforamerica.org\\/ NUMBERPHILE Website: http:\\/\\/www.numberphile.com\\/ Numberphile on Facebook: http:\\/\\/www.facebook.com\\/numberphile Numberphile tweets: https:\\/\\/twitter.com\\/numberphile Subscribe: http:\\/\\/bit.ly\\/Numberphile_Sub Videos by Brady Haran Patreon: http:\\/\\/www.patreon.com\\/numberphile Numberphile T-Shirts and Merch: https:\\/\\/teespring.com\\/stores\\/numberphile Brady's videos subreddit: http:\\/\\/www.reddit.com\\/r\\/BradyHaran\\/ Brady's latest videos across all channels: http:\\/\\/www.bradyharanblog.com\\/ Sign up for (occasional) emails: http:\\/\\/eepurl.com\\/YdjL9\",\"3190\":\"Semi-supervised learning refers to algorithms that attempt to make use of both labeled and unlabeled training data. Semi-supervised learning algorithms are unlike supervised learning algorithms that are only able to learn from labeled training data. A popular approach to semi-supervised learning is to create a graph that connects examples in the training dataset and propagate known labels through the edges of the graph to label unlabeled examples. An example of this approach to semi-supervised learning is the label propagation algorithm for classification predictive modeling. In this tutorial, you will discover how to apply the label propagation algorithm to a semi-supervised learning classification dataset. After completing this tutorial, you will know: An intuition for how the label propagation semi-supervised learning algorithm works. How to develop a semi-supervised classification dataset and establish a baseline in performance with a supervised learning algorithm. How to develop and evaluate a label propagation algorithm and use the model output to train a supervised learning algorithm. Let\\u2019s get started. Semi-Supervised Learning With Label Propagation Photo by TheBluesDude, some rights reserved. Tutorial Overview This tutorial is divided into three parts; they are: Label Propagation Algorithm Semi-Supervised Classification Dataset Label Propagation for Semi-Supervised Learning Label Propagation Algorithm Label Propagation is a semi-supervised learning algorithm. The algorithm was proposed in the 2002 technical report by Xiaojin Zhu and Zoubin Ghahramani titled \\u201cLearning From Labeled And Unlabeled Data With Label Propagation.\\u201d The intuition for the algorithm is that a graph is created that connects all examples (rows) in the dataset based on their distance, such as Euclidean distance. Nodes in the graph then have label soft labels or label distribution based on the labels or label distributions of examples connected nearby in the graph. Many semi-supervised learning algorithms rely on the geometry of the data induced by both labeled and unlabeled examples to improve on supervised methods that use only the labeled data. This geometry can be naturally represented by an empirical graph g = (V,E) where nodes V = {1,\\u2026,n} represent the training data and edges E represent similarities between them \\u2014 Page 193, Semi-Supervised Learning, 2006. Propagation refers to the iterative nature that labels are assigned to nodes in the graph and propagate along the edges of the graph to connected nodes. This procedure is sometimes called label propagation, as it \\u201cpropagates\\u201d labels from the labeled vertices (which are fixed) gradually through the edges to all the unlabeled vertices. \\u2014 Page 48, Introduction to Semi-Supervised Learning, 2009. The process is repeated for a fixed number of iterations to strengthen the labels assigned to unlabeled examples. Starting with nodes 1, 2,\\u2026,l labeled with their known label (1 or \\u22121) and nodes l + 1,\\u2026,n labeled with 0, each node starts to propagate its label to its neighbors, and the process is repeated until convergence. \\u2014 Page 194, Semi-Supervised Learning, 2006. Now that we are familiar with the Label Propagation algorithm, let\\u2019s look at how we might use it on a project. First, we must define a semi-supervised classification dataset. Semi-Supervised Classification Dataset In this section, we will define a dataset for semis-supervised learning and establish a baseline in performance on the dataset. First, we can define a synthetic classification dataset using the make_classification() function. We will define the dataset with two classes (binary classification) and two input variables and 1,000 examples. ... # define dataset X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, random_state=1) Next, we will split the dataset into train and test datasets with an equal 50-50 split (e.g. 500 rows in each). ... # split into train and test X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50, random_state=1, stratify=y) Finally, we will split the training dataset in half again into a portion that will have labels and a portion that we will pretend is unlabeled. ... # split train into labeled and unlabeled X_train_lab, X_test_unlab, y_train_lab, y_test_unlab = train_test_split(X_train, y_train, test_size=0.50, random_state=1, stratify=y_train) Tying this together, the complete example of preparing the semi-supervised learning dataset is listed below. # prepare semi-supervised learning dataset from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split # define dataset X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, random_state=1) # split into train and test X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50, random_state=1, stratify=y) # split train into labeled and unlabeled X_train_lab, X_test_unlab, y_train_lab, y_test_unlab = train_test_split(X_train, y_train, test_size=0.50, random_state=1, stratify=y_train) # summarize training set size print('Labeled Train Set:', X_train_lab.shape, y_train_lab.shape) print('Unlabeled Train Set:', X_test_unlab.shape, y_test_unlab.shape) # summarize test set size print('Test Set:', X_test.shape, y_test.shape) Running the example prepares the dataset and then summarizes the shape of each of the three portions. The results confirm that we have a test dataset of 500 rows, a labeled training dataset of 250 rows, and 250 rows of unlabeled data. Labeled Train Set: (250, 2) (250,) Unlabeled Train Set: (250, 2) (250,) Test Set: (500, 2) (500,) A supervised learning algorithm will only have 250 rows from which to train a model. A semi-supervised learning algorithm will have the 250 labeled rows as well as the 250 unlabeled rows that could be used in numerous ways to improve the labeled training dataset. Next, we can establish a baseline in performance on the semi-supervised learning dataset using a supervised learning algorithm fit only on the labeled training data. This is important because we would expect a semi-supervised learning algorithm to outperform a supervised learning algorithm fit on the labeled data alone. If this is not the case, then the semi-supervised learning algorithm does not have skill. In this case, we will use a logistic regression algorithm fit on the labeled portion of the training dataset. ... # define model model = LogisticRegression() # fit model on labeled dataset model.fit(X_train_lab, y_train_lab) The model can then be used to make predictions on the entire hold out test dataset and evaluated using classification accuracy. ... # make predictions on hold out test set yhat = model.predict(X_test) # calculate score for test set score = accuracy_score(y_test, yhat) # summarize score print('Accuracy: %.3f' % (score*100)) Tying this together, the complete example of evaluating a supervised learning algorithm on the semi-supervised learning dataset is listed below. # baseline performance on the semi-supervised learning dataset from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from sklearn.linear_model import LogisticRegression # define dataset X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, random_state=1) # split into train and test X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50, random_state=1, stratify=y) # split train into labeled and unlabeled X_train_lab, X_test_unlab, y_train_lab, y_test_unlab = train_test_split(X_train, y_train, test_size=0.50, random_state=1, stratify=y_train) # define model model = LogisticRegression() # fit model on labeled dataset model.fit(X_train_lab, y_train_lab) # make predictions on hold out test set yhat = model.predict(X_test) # calculate score for test set score = accuracy_score(y_test, yhat) # summarize score print('Accuracy: %.3f' % (score*100)) Running the algorithm fits the model on the labeled training dataset and evaluates it on the holdout dataset and prints the classification accuracy. Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome. In this case, we can see that the algorithm achieved a classification accuracy of about 84.8 percent. We would expect an effective semi-supervised learning algorithm to achieve better accuracy than this. Accuracy: 84.800 Next, let\\u2019s explore how to apply the label propagation algorithm to the dataset. Label Propagation for Semi-Supervised Learning The Label Propagation algorithm is available in the scikit-learn Python machine learning library via the LabelPropagation class. The model can be fit just like any other classification model by calling the fit() function and used to make predictions for new data via the predict() function. ... # define model model = LabelPropagation() # fit model on training dataset model.fit(..., ...) # make predictions on hold out test set yhat = model.predict(...) Importantly, the training dataset provided to the fit() function must include labeled examples that are integer encoded (as per normal) and unlabeled examples marked with a label of -1. The model will then determine a label for the unlabeled examples as part of fitting the model. After the model is fit, the estimated labels for the labeled and unlabeled data in the training dataset is available via the \\u201ctransduction_\\u201d attribute on the LabelPropagation class. ... # get labels for entire training dataset data tran_labels = model.transduction_ Now that we are familiar with how to use the Label Propagation algorithm in scikit-learn, let\\u2019s look at how we might apply it to our semi-supervised learning dataset. First, we must prepare the training dataset. We can concatenate the input data of the training dataset into a single array. ... # create the training dataset input X_train_mixed = concatenate((X_train_lab, X_test_unlab)) We can then create a list of -1 valued (unlabeled) for each row in the unlabeled portion of the training dataset. ... # create \\\"no label\\\" for unlabeled data nolabel = [-1 for _ in range(len(y_test_unlab))] This list can then be concatenated with the labels from the labeled portion of the training dataset to correspond with the input array for the training dataset. ... # recombine training dataset labels y_train_mixed = concatenate((y_train_lab, nolabel)) We can now train the LabelPropagation model on the entire training dataset. ... # define model model = LabelPropagation() # fit model on training dataset model.fit(X_train_mixed, y_train_mixed) Next, we can use the model to make predictions on the holdout dataset and evaluate the model using classification accuracy. ... # make predictions on hold out test set yhat = model.predict(X_test) # calculate score for test set score = accuracy_score(y_test, yhat) # summarize score print('Accuracy: %.3f' % (score*100)) Tying this together, the complete example of evaluating label propagation on the semi-supervised learning dataset is listed below. # evaluate label propagation on the semi-supervised learning dataset from numpy import concatenate from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from sklearn.semi_supervised import LabelPropagation # define dataset X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, random_state=1) # split into train and test X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50, random_state=1, stratify=y) # split train into labeled and unlabeled X_train_lab, X_test_unlab, y_train_lab, y_test_unlab = train_test_split(X_train, y_train, test_size=0.50, random_state=1, stratify=y_train) # create the training dataset input X_train_mixed = concatenate((X_train_lab, X_test_unlab)) # create \\\"no label\\\" for unlabeled data nolabel = [-1 for _ in range(len(y_test_unlab))] # recombine training dataset labels y_train_mixed = concatenate((y_train_lab, nolabel)) # define model model = LabelPropagation() # fit model on training dataset model.fit(X_train_mixed, y_train_mixed) # make predictions on hold out test set yhat = model.predict(X_test) # calculate score for test set score = accuracy_score(y_test, yhat) # summarize score print('Accuracy: %.3f' % (score*100)) Running the algorithm fits the model on the entire training dataset and evaluates it on the holdout dataset and prints the classification accuracy. Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome. In this case, we can see that the label propagation model achieves a classification accuracy of about 85.6 percent, which is slightly higher than a logistic regression fit only on the labeled training dataset that achieved an accuracy of about 84.8 percent. Accuracy: 85.600 So far, so good. Another approach we can use with the semi-supervised model is to take the estimated labels for the training dataset and fit a supervised learning model. Recall that we can retrieve the labels for the entire training dataset from the label propagation model as follows: ... # get labels for entire training dataset data tran_labels = model.transduction_ We can then use these labels along with all of the input data to train and evaluate a supervised learning algorithm, such as a logistic regression model. The hope is that the supervised learning model fit on the entire training dataset would achieve even better performance than the semi-supervised learning model alone. ... # define supervised learning model model2 = LogisticRegression() # fit supervised learning model on entire training dataset model2.fit(X_train_mixed, tran_labels) # make predictions on hold out test set yhat = model2.predict(X_test) # calculate score for test set score = accuracy_score(y_test, yhat) # summarize score print('Accuracy: %.3f' % (score*100)) Tying this together, the complete example of using the estimated training set labels to train and evaluate a supervised learning model is listed below. # evaluate logistic regression fit on label propagation for semi-supervised learning from numpy import concatenate from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from sklearn.semi_supervised import LabelPropagation from sklearn.linear_model import LogisticRegression # define dataset X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, random_state=1) # split into train and test X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50, random_state=1, stratify=y) # split train into labeled and unlabeled X_train_lab, X_test_unlab, y_train_lab, y_test_unlab = train_test_split(X_train, y_train, test_size=0.50, random_state=1, stratify=y_train) # create the training dataset input X_train_mixed = concatenate((X_train_lab, X_test_unlab)) # create \\\"no label\\\" for unlabeled data nolabel = [-1 for _ in range(len(y_test_unlab))] # recombine training dataset labels y_train_mixed = concatenate((y_train_lab, nolabel)) # define model model = LabelPropagation() # fit model on training dataset model.fit(X_train_mixed, y_train_mixed) # get labels for entire training dataset data tran_labels = model.transduction_ # define supervised learning model model2 = LogisticRegression() # fit supervised learning model on entire training dataset model2.fit(X_train_mixed, tran_labels) # make predictions on hold out test set yhat = model2.predict(X_test) # calculate score for test set score = accuracy_score(y_test, yhat) # summarize score print('Accuracy: %.3f' % (score*100)) Running the algorithm fits the semi-supervised model on the entire training dataset, then fits a supervised learning model on the entire training dataset with inferred labels and evaluates it on the holdout dataset, printing the classification accuracy. Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome. In this case, we can see that this hierarchical approach of the semi-supervised model followed by supervised model achieves a classification accuracy of about 86.2 percent on the holdout dataset, even better than the semi-supervised learning used alone that achieved an accuracy of about 85.6 percent. Accuracy: 86.200 Can you achieve better results by tuning the hyperparameters of the LabelPropagation model? Let me know what you discover in the comments below. Further Reading This section provides more resources on the topic if you are looking to go deeper. Books Introduction to Semi-Supervised Learning, 2009. Chapter 11: Label Propagation and Quadratic Criterion, Semi-Supervised Learning, 2006. Papers Learning From Labeled And Unlabeled Data With Label Propagation, 2002. APIs sklearn.semi_supervised.LabelPropagation API. Section 1.14. Semi-Supervised, Scikit-Learn User Guide. sklearn.model_selection.train_test_split API. sklearn.linear_model.LogisticRegression API. sklearn.datasets.make_classification API. Articles Semi-supervised learning, Wikipedia. Summary In this tutorial, you discovered how to apply the label propagation algorithm to a semi-supervised learning classification dataset. Specifically, you learned: An intuition for how the label propagation semi-supervised learning algorithm works. How to develop a semi-supervised classification dataset and establish a baseline in performance with a supervised learning algorithm. How to develop and evaluate a label propagation algorithm and use the model output to train a supervised learning algorithm. Do you have any questions? Ask your questions in the comments below and I will do my best to answer. The post Semi-Supervised Learning With Label Propagation appeared first on Machine Learning Mastery.\",\"748\":\"Going through a phase of obsessively trying and evaluating all the flavors of Philz. Today\\u2019s \\u201cSilken Splendor\\u201d is allegedly claimed to be \\u201cDark Cocoa, Citrus, Butterscotch\\u201d. I wonder how they determine that\",\"3188\":\"Gradient descent is an optimization algorithm that follows the negative gradient of an objective function in order to locate the minimum of the function. A limitation of gradient descent is that a single step size (learning rate) is used for all input variables. Extensions to gradient descent like AdaGrad and RMSProp update the algorithm to use a separate step size for each input variable but may result in a step size that rapidly decreases to very small values. The Adaptive Movement Estimation algorithm, or Adam for short, is an extension to gradient descent and a natural successor to techniques like AdaGrad and RMSProp that automatically adapts a learning rate for each input variable for the objective function and further smooths the search process by using an exponentially decreasing moving average of the gradient to make updates to variables. In this tutorial, you will discover how to develop gradient descent with Adam optimization algorithm from scratch. After completing this tutorial, you will know: Gradient descent is an optimization algorithm that uses the gradient of the objective function to navigate the search space. Gradient descent can be updated to use an automatically adaptive step size for each input variable using a decaying average of partial derivatives, called Adam. How to implement the Adam optimization algorithm from scratch and apply it to an objective function and evaluate the results. Let\\u2019s get started. Gradient Descent Optimization With Adam From Scratch Photo by Don Graham, some rights reserved. Tutorial Overview This tutorial is divided into three parts; they are: Gradient Descent Adam Optimization Algorithm Gradient Descent With Adam Two-Dimensional Test Problem Gradient Descent Optimization With Adam Visualization of Adam Gradient Descent Gradient descent is an optimization algorithm. It is technically referred to as a first-order optimization algorithm as it explicitly makes use of the first-order derivative of the target objective function. First-order methods rely on gradient information to help direct the search for a minimum \\u2026 \\u2014 Page 69, Algorithms for Optimization, 2019. The first-order derivative, or simply the \\u201cderivative,\\u201d is the rate of change or slope of the target function at a specific point, e.g. for a specific input. If the target function takes multiple input variables, it is referred to as a multivariate function and the input variables can be thought of as a vector. In turn, the derivative of a multivariate target function may also be taken as a vector and is referred to generally as the gradient. Gradient: First-order derivative for a multivariate objective function. The derivative or the gradient points in the direction of the steepest ascent of the target function for a specific input. Gradient descent refers to a minimization optimization algorithm that follows the negative of the gradient downhill of the target function to locate the minimum of the function. The gradient descent algorithm requires a target function that is being optimized and the derivative function for the objective function. The target function f() returns a score for a given set of inputs, and the derivative function f'() gives the derivative of the target function for a given set of inputs. The gradient descent algorithm requires a starting point (x) in the problem, such as a randomly selected point in the input space. The derivative is then calculated and a step is taken in the input space that is expected to result in a downhill movement in the target function, assuming we are minimizing the target function. A downhill movement is made by first calculating how far to move in the input space, calculated as the step size (called alpha or the learning rate) multiplied by the gradient. This is then subtracted from the current point, ensuring we move against the gradient, or down the target function. x(t) = x(t-1) \\u2013 step_size * f'(x(t-1)) The steeper the objective function at a given point, the larger the magnitude of the gradient and, in turn, the larger the step taken in the search space. The size of the step taken is scaled using a step size hyperparameter. Step Size (alpha): Hyperparameter that controls how far to move in the search space against the gradient each iteration of the algorithm. If the step size is too small, the movement in the search space will be small and the search will take a long time. If the step size is too large, the search may bounce around the search space and skip over the optima. Now that we are familiar with the gradient descent optimization algorithm, let\\u2019s take a look at the Adam algorithm. Adam Optimization Algorithm Adaptive Movement Estimation algorithm, or Adam for short, is an extension to the gradient descent optimization algorithm. The algorithm was described in the 2014 paper by Diederik Kingma and Jimmy Lei Ba titled \\u201cAdam: A Method for Stochastic Optimization.\\u201d Adam is designed to accelerate the optimization process, e.g. decrease the number of function evaluations required to reach the optima, or to improve the capability of the optimization algorithm, e.g. result in a better final result. This is achieved by calculating a step size for each input parameter that is being optimized. Importantly, each step size is automatically adapted throughput the search process based on the gradients (partial derivatives) encountered for each variable. We propose Adam, a method for efficient stochastic optimization that only requires first-order gradients with little memory requirement. The method computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients; the name Adam is derived from adaptive moment estimation \\u2014 Adam: A Method for Stochastic Optimization This involves maintaining a first and second moment of the gradient, e.g. an exponentially decaying mean gradient (first moment) and variance (second moment) for each input variable. The moving averages themselves are estimates of the 1st moment (the mean) and the 2nd raw moment (the uncentered variance) of the gradient. \\u2014 Adam: A Method for Stochastic Optimization Let\\u2019s step through each element of the algorithm. First, we must maintain a moment vector and exponentially weighted infinity norm for each parameter being optimized as part of the search, referred to as m and v (really the Greek letter nu) respectively. They are initialized to 0.0 at the start of the search. m = 0 v = 0 The algorithm is executed iteratively over time t starting at t=1, and each iteration involves calculating a new set of parameter values x, e.g. going from x(t-1) to x(t). It is perhaps easy to understand the algorithm if we focus on updating one parameter, which generalizes to updating all parameters via vector operations. First, the gradient (partial derivatives) are calculated for the current time step. g(t) = f'(x(t-1)) Next, the first moment is updated using the gradient and a hyperparameter beta1. m(t) = beta1 * m(t-1) + (1 \\u2013 beta1) * g(t) Then the second moment is updated using the squared gradient and a hyperparameter beta2. v(t) = beta2 * v(t-1) + (1 \\u2013 beta2) * g(t)^2 The first and second moments are biased because they are initialized with zero values. \\u2026 these moving averages are initialized as (vectors of) 0\\u2019s, leading to moment estimates that are biased towards zero, especially during the initial timesteps, and especially when the decay rates are small (i.e. the betas are close to 1). The good news is that this initialization bias can be easily counteracted, resulting in bias-corrected estimates \\u2026 \\u2014 Adam: A Method for Stochastic Optimization Next the first and second moments are bias-corrected, starring with the first moment: mhat(t) = m(t) \\/ (1 \\u2013 beta1(t)) And then the second moment: vhat(t) = v(t) \\/ (1 \\u2013 beta2(t)) Note, beta1(t) and beta2(t) refer to the beta1 and beta2 hyperparameters that are decayed on a schedule over the iterations of the algorithm. A static decay schedule can be used, although the paper recommend the following: beta1(t) = beta1^t beta2(t) = beta2^t Finally, we can calculate the value for the parameter for this iteration. x(t) = x(t-1) \\u2013 alpha * mhat(t) \\/ (sqrt(vhat(t)) + eps) Where alpha is the step size hyperparameter, eps is a small value (epsilon) such as 1e-8 that ensures we do not encounter a divide by zero error, and sqrt() is the square root function. Note, a more efficient reordering of the update rule listed in the paper can be used: alpha(t) = alpha * sqrt(1 \\u2013 beta2(t)) \\/ (1 \\u2013 beta1(t)) x(t) = x(t-1) \\u2013 alpha(t) * m(t) \\/ (sqrt(v(t)) + eps) To review, there are three hyperparameters for the algorithm, they are: alpha: Initial step size (learning rate), a typical value is 0.001. beta1: Decay factor for first momentum, a typical value is 0.9. beta2: Decay factor for infinity norm, a typical value is 0.999. And that\\u2019s it. For full derivation of the Adam algorithm in the context of the Adam algorithm, I recommend reading the paper. Adam: A Method for Stochastic Optimization Next, let\\u2019s look at how we might implement the algorithm from scratch in Python. Gradient Descent With Adam In this section, we will explore how to implement the gradient descent optimization algorithm with Adam. Two-Dimensional Test Problem First, let\\u2019s define an optimization function. We will use a simple two-dimensional function that squares the input of each dimension and define the range of valid inputs from -1.0 to 1.0. The objective() function below implements this function # objective function def objective(x, y): return x**2.0 + y**2.0 We can create a three-dimensional plot of the dataset to get a feeling for the curvature of the response surface. The complete example of plotting the objective function is listed below. # 3d plot of the test function from numpy import arange from numpy import meshgrid from matplotlib import pyplot # objective function def objective(x, y): return x**2.0 + y**2.0 # define range for input r_min, r_max = -1.0, 1.0 # sample input range uniformly at 0.1 increments xaxis = arange(r_min, r_max, 0.1) yaxis = arange(r_min, r_max, 0.1) # create a mesh from the axis x, y = meshgrid(xaxis, yaxis) # compute targets results = objective(x, y) # create a surface plot with the jet color scheme figure = pyplot.figure() axis = figure.gca(projection='3d') axis.plot_surface(x, y, results, cmap='jet') # show the plot pyplot.show() Running the example creates a three-dimensional surface plot of the objective function. We can see the familiar bowl shape with the global minima at f(0, 0) = 0. Three-Dimensional Plot of the Test Objective Function We can also create a two-dimensional plot of the function. This will be helpful later when we want to plot the progress of the search. The example below creates a contour plot of the objective function. # contour plot of the test function from numpy import asarray from numpy import arange from numpy import meshgrid from matplotlib import pyplot # objective function def objective(x, y): return x**2.0 + y**2.0 # define range for input bounds = asarray([[-1.0, 1.0], [-1.0, 1.0]]) # sample input range uniformly at 0.1 increments xaxis = arange(bounds[0,0], bounds[0,1], 0.1) yaxis = arange(bounds[1,0], bounds[1,1], 0.1) # create a mesh from the axis x, y = meshgrid(xaxis, yaxis) # compute targets results = objective(x, y) # create a filled contour plot with 50 levels and jet color scheme pyplot.contourf(x, y, results, levels=50, cmap='jet') # show the plot pyplot.show() Running the example creates a two-dimensional contour plot of the objective function. We can see the bowl shape compressed to contours shown with a color gradient. We will use this plot to plot the specific points explored during the progress of the search. Two-Dimensional Contour Plot of the Test Objective Function Now that we have a test objective function, let\\u2019s look at how we might implement the Adam optimization algorithm. Gradient Descent Optimization With Adam We can apply the gradient descent with Adam to the test problem. First, we need a function that calculates the derivative for this function. f(x) = x^2 f'(x) = x * 2 The derivative of x^2 is x * 2 in each dimension. The derivative() function implements this below. # derivative of objective function def derivative(x, y): return asarray([x * 2.0, y * 2.0]) Next, we can implement gradient descent optimization. First, we can select a random point in the bounds of the problem as a starting point for the search. This assumes we have an array that defines the bounds of the search with one row for each dimension and the first column defines the minimum and the second column defines the maximum of the dimension. ... # generate an initial point x = bounds[:, 0] + rand(len(bounds)) * (bounds[:, 1] - bounds[:, 0]) score = objective(x[0], x[1]) Next, we need to initialize the first and second moments to zero. ... # initialize first and second moments m = [0.0 for _ in range(bounds.shape[0])] v = [0.0 for _ in range(bounds.shape[0])] We then run a fixed number of iterations of the algorithm defined by the \\u201cn_iter\\u201d hyperparameter. ... # run iterations of gradient descent for t in range(n_iter): ... The first step is to calculate the gradient for the current solution using the derivative() function. ... # calculate gradient gradient = derivative(solution[0], solution[1]) The first step is to calculate the derivative for the current set of parameters. ... # calculate gradient g(t) g = derivative(x[0], x[1]) Next, we need to perform the Adam update calculations. We will perform these calculations one variable at a time using an imperative programming style for readability. In practice, I recommend using NumPy vector operations for efficiency. ... # build a solution one variable at a time for i in range(x.shape[0]): ... First, we need to calculate the moment. ... # m(t) = beta1 * m(t-1) + (1 - beta1) * g(t) m[i] = beta1 * m[i] + (1.0 - beta1) * g[i] Then the second moment. ... # v(t) = beta2 * v(t-1) + (1 - beta2) * g(t)^2 v[i] = beta2 * v[i] + (1.0 - beta2) * g[i]**2 Then the bias correction for the first and second moments. ... # mhat(t) = m(t) \\/ (1 - beta1(t)) mhat = m[i] \\/ (1.0 - beta1**(t+1)) # vhat(t) = v(t) \\/ (1 - beta2(t)) vhat = v[i] \\/ (1.0 - beta2**(t+1)) Then finally the updated variable value. ... # x(t) = x(t-1) - alpha * mhat(t) \\/ (sqrt(vhat(t)) + eps) x[i] = x[i] - alpha * mhat \\/ (sqrt(vhat) + eps) This is then repeated for each parameter that is being optimized. At the end of the iteration we can evaluate the new parameter values and report the performance of the search. ... # evaluate candidate point score = objective(x[0], x[1]) # report progress print('\\/preem\\/empre class=\\\"urvanov-syntax-highlighter-plain-tag\\\"\",\"4295\":\"In this episode of Machine Learning Street Talk, Tim Scarfe, Yannic Kilcher and Connor Shorten interviewed Harri Valpola, CEO and Founder of Curious AI. We continued our discussion of System 1 and System 2 thinking in Deep Learning, as well as miscellaneous topics around Model-based Reinforcement Learning. Dr. Valpola describes some of the challenges of modelling industrial control processes such as water sewage filters and paper mills with the use of model-based RL. Dr. Valpola and his collaborators recently published \\u201cRegularizing Trajectory Optimization with Denoising Autoencoders\\u201d that addresses some of the concerns of planning algorithms that exploit inaccuracies in their world models! 00:00:00 Intro to Harri and Curious AI System1\\/System 2 00:04:50 Background on model-based RL challenges from Tim 00:06:26 Other interesting research papers on model-based RL from Connor 00:08:36 Intro to Curious AI recent NeurIPS paper on model-based RL and denoising autoencoders from Yannic 00:21:00 Main show kick off, system 1\\/2 00:31:50 Where does the simulator come from? 00:33:59 Evolutionary priors 00:37:17 Consciousness 00:40:37 How does one build a company like Curious AI? 00:46:42 Deep Q Networks 00:49:04 Planning and Model based RL 00:53:04 Learning good representations 00:55:55 Typical problem Curious AI might solve in industry 01:00:56 Exploration 01:08:00 Their paper - regularizing trajectory optimization with denoising 01:13:47 What is Epistemic uncertainty 01:16:44 How would Curious develop these models 01:18:00 Explainability and simulations 01:22:33 How system 2 works in humans 01:26:11 Planning 01:27:04 Advice for starting an AI company 01:31:31 Real world implementation of planning models 01:33:49 Publishing research and openness We really hope you enjoy this episode, please subscribe! Regularizing Trajectory Optimization with Denoising Autoencoders: https:\\/\\/papers.nips.cc\\/paper\\/8552-regularizing-trajectory-optimization-with-denoising-autoencoders.pdf Pulp, Paper & Packaging: A Future Transformed through Deep Learning: https:\\/\\/thecuriousaicompany.com\\/pulp-paper-packaging-a-future-transformed-through-deep-learning\\/ Curious AI: https:\\/\\/thecuriousaicompany.com\\/ Harri Valpola Publications: https:\\/\\/scholar.google.com\\/citations?user=1uT7-84AAAAJ&hl=en&oi=ao Some interesting papers around Model-Based RL: GameGAN: https:\\/\\/cdn.arstechnica.net\\/wp-content\\/uploads\\/2020\\/05\\/Nvidia_GameGAN_Research.pdf Plan2Explore: https:\\/\\/ramanans1.github.io\\/plan2explore\\/ World Models: https:\\/\\/worldmodels.github.io\\/ MuZero: https:\\/\\/arxiv.org\\/pdf\\/1911.08265.pdf PlaNet: A Deep Planning Network for RL: https:\\/\\/ai.googleblog.com\\/2019\\/02\\/introducing-planet-deep-planning.html Dreamer: Scalable RL using World Models: https:\\/\\/ai.googleblog.com\\/2020\\/03\\/introducing-dreamer-scalable.html Model Based RL for Atari: https:\\/\\/arxiv.org\\/pdf\\/1903.00374.pdf\",\"3876\":\"Sumit acknowledges 5 sets of people in this 5-minute award acceptance talk for his \\\"most influential\\\" POPL 2011 paper, which describes the technology behind the popular Flash Fill feature in Excel. He shares inside stories of the role that these actors played in this invention and its impact. Most Influential Test of Time paper: https:\\/\\/www.microsoft.com\\/en-us\\/research\\/publication\\/automating-string-processing-spreadsheets-using-input-output-examples\\/ The PROSE research and engineering team: https:\\/\\/www.microsoft.com\\/en-us\\/research\\/group\\/prose\\/\",\"1450\":\"Matt Botvinick is the Director of Neuroscience Research at DeepMind. He is a brilliant cross-disciplinary mind navigating effortlessly between cognitive psychology, computational neuroscience, and artificial intelligence. Support this podcast by supporting these sponsors: \\u2013 The Jordan Harbinger Show: https:\\/\\/www.jordanharbinger.com\\/lex \\u2013 Magic Spoon: https:\\/\\/magicspoon.com\\/lex and use code LEX at checkout If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman on Twitter, LinkedIn, Facebook, Medium, or YouTube where you can watch the video versions of these conversations. If you enjoy the podcast, please rate it 5 stars on Apple Podcasts, follow on Spotify,\",\"483\":\"Though many contact tracing apps involve location tracking, they don\\u2019t have to. This is a video adaptation of a post by Nicky Case: https:\\/\\/ncase.me\\/contact-tracing\\/ New post by Nicky on COVID-19: https:\\/\\/ncase.me\\/covid-19\\/ Consider supporting his work: https:\\/\\/www.patreon.com\\/ncase\\/ Supporters of this video: https:\\/\\/3b1b.co\\/ctracing-thanks DP-3T Whitepaper: https:\\/\\/github.com\\/DP-3T\\/documents\\/blob\\/master\\/DP3T%20White%20Paper.pdf Full repo: https:\\/\\/github.com\\/DP-3T Where did the number 60% come from? https:\\/\\/science.sciencemag.org\\/content\\/early\\/2020\\/04\\/09\\/science.abb6936 The specific figure is Figure 3, the last panel showing instant contact tracing (with an app). It shows what combination of % (symptomatic) cases isolated & % (pre\\/a-symptomatic) contacts quarantined would contain the virus or not. Mental health resources (copied form a John Green video, since he knows what he's doing): National Suicide Prevention Lifeline: 1-800-273-TALK (8255) I know it can be scary to make a phone call, but people are nice, I promise. Substance Abuse and Mental Health Administration helpline: 1-800-662-HELP (4357) The Trevor Project: 866-488-7386 International helplines: https:\\/\\/togetherweare-strong.tumblr.com\\/helpline Mental Health Resource list and links: http:\\/\\/activeminds.org\\/ The Anxiety and Depression Association of America: http:\\/\\/www.adaa.org\\/ Mental Health screening tools: https:\\/\\/screening.mhanational.org\\/screening-tools Mind: http:\\/\\/mind.org.uk\\/information-support\\/ ------------------ Music by Vincent Rubinetti. Download the music on Bandcamp: https:\\/\\/vincerubinetti.bandcamp.com\\/album\\/the-music-of-3blue1brown Stream the music on Spotify: https:\\/\\/open.spotify.com\\/album\\/1dVyjwS8FBqXhRunaG5W5u If you want to contribute translated subtitles or to help review those that have already been made by others and need approval, you can click the gear icon in the video and go to subtitles\\/cc, then \\\"add subtitles\\/cc\\\". I really appreciate those who do this, as it helps make the lessons accessible to more people. ------------------ 3blue1brown is a channel about animating math, in all senses of the word animate. And you know the drill with YouTube, if you want to stay posted on new videos, subscribe: http:\\/\\/3b1b.co\\/subscribe Various social media stuffs: Website: https:\\/\\/www.3blue1brown.com Twitter: https:\\/\\/twitter.com\\/3blue1brown Reddit: https:\\/\\/www.reddit.com\\/r\\/3blue1brown Instagram: https:\\/\\/www.instagram.com\\/3blue1brown_animations\\/ Patreon: https:\\/\\/patreon.com\\/3blue1brown Facebook: https:\\/\\/www.facebook.com\\/3blue1brown\",\"1580\":\"Sean Carroll is a theoretical physicist at Caltech, specializing in quantum mechanics, gravity, and cosmology. He is the author of several popular books: one on the arrow of time called From Eternity to Here, one on the Higgs boson called The Particle at the End of the Universe, and one on science and philosophy called The Big Picture: On the Origins of Life, Meaning, and the Universe Itself. He has an upcoming book on Quantum Mechanics that you can preorder now called Something Deeply Hidden. Finally, and perhaps most famously, he is the host of a podcast called Mindscape that\",\"4267\":\"Tweet Share Share Semi-supervised learning refers to algorithms that attempt to make use of both labeled and unlabeled training data. Semi-supervised learning algorithms are unlike supervised learning algorithms that are only able to learn from labeled training data. A popular approach to semi-supervised learning is to create a graph that connects examples in the training dataset and propagate known labels through the edges of the graph to label unlabeled examples. An example of this approach to semi-supervised learning is the label propagation algorithm for classification predictive modeling. In this tutorial, you will discover how to apply the label propagation algorithm to a semi-supervised learning classification dataset. After completing this tutorial, you will know: An intuition for how the label propagation semi-supervised learning algorithm works. How to develop a semi-supervised classification dataset and establish a baseline in performance with a supervised learning algorithm. How to develop and evaluate a label propagation algorithm and use the model output to train a supervised learning algorithm. Let\\u2019s get started. Semi-Supervised Learning With Label Propagation Photo by TheBluesDude, some rights reserved. Tutorial Overview This tutorial is divided into three parts; they are: Label Propagation Algorithm Semi-Supervised Classification Dataset Label Propagation for Semi-Supervised Learning Label Propagation Algorithm Label Propagation is a semi-supervised learning algorithm. The algorithm was proposed in the 2002 technical report by Xiaojin Zhu and Zoubin Ghahramani titled \\u201cLearning From Labeled And Unlabeled Data With Label Propagation.\\u201d The intuition for the algorithm is that a graph is created that connects all examples (rows) in the dataset based on their distance, such as Euclidean distance. Nodes in the graph then have label soft labels or label distribution based on the labels or label distributions of examples connected nearby in the graph. Many semi-supervised learning algorithms rely on the geometry of the data induced by both labeled and unlabeled examples to improve on supervised methods that use only the labeled data. This geometry can be naturally represented by an empirical graph g = (V,E) where nodes V = {1,\\u2026,n} represent the training data and edges E represent similarities between them \\u2014 Page 193, Semi-Supervised Learning, 2006. Propagation refers to the iterative nature that labels are assigned to nodes in the graph and propagate along the edges of the graph to connected nodes. This procedure is sometimes called label propagation, as it \\u201cpropagates\\u201d labels from the labeled vertices (which are fixed) gradually through the edges to all the unlabeled vertices. \\u2014 Page 48, Introduction to Semi-Supervised Learning, 2009. The process is repeated for a fixed number of iterations to strengthen the labels assigned to unlabeled examples. Starting with nodes 1, 2,\\u2026,l labeled with their known label (1 or \\u22121) and nodes l + 1,\\u2026,n labeled with 0, each node starts to propagate its label to its neighbors, and the process is repeated until convergence. \\u2014 Page 194, Semi-Supervised Learning, 2006. Now that we are familiar with the Label Propagation algorithm, let\\u2019s look at how we might use it on a project. First, we must define a semi-supervised classification dataset. Semi-Supervised Classification Dataset In this section, we will define a dataset for semis-supervised learning and establish a baseline in performance on the dataset. First, we can define a synthetic classification dataset using the make_classification() function. We will define the dataset with two classes (binary classification) and two input variables and 1,000 examples. ... # define dataset X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, random_state=1) Next, we will split the dataset into train and test datasets with an equal 50-50 split (e.g. 500 rows in each). ... # split into train and test X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50, random_state=1, stratify=y) Finally, we will split the training dataset in half again into a portion that will have labels and a portion that we will pretend is unlabeled. ... # split train into labeled and unlabeled X_train_lab, X_test_unlab, y_train_lab, y_test_unlab = train_test_split(X_train, y_train, test_size=0.50, random_state=1, stratify=y_train) Tying this together, the complete example of preparing the semi-supervised learning dataset is listed below. # prepare semi-supervised learning dataset from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split # define dataset X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, random_state=1) # split into train and test X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50, random_state=1, stratify=y) # split train into labeled and unlabeled X_train_lab, X_test_unlab, y_train_lab, y_test_unlab = train_test_split(X_train, y_train, test_size=0.50, random_state=1, stratify=y_train) # summarize training set size print('Labeled Train Set:', X_train_lab.shape, y_train_lab.shape) print('Unlabeled Train Set:', X_test_unlab.shape, y_test_unlab.shape) # summarize test set size print('Test Set:', X_test.shape, y_test.shape) Running the example prepares the dataset and then summarizes the shape of each of the three portions. The results confirm that we have a test dataset of 500 rows, a labeled training dataset of 250 rows, and 250 rows of unlabeled data. Labeled Train Set: (250, 2) (250,) Unlabeled Train Set: (250, 2) (250,) Test Set: (500, 2) (500,) A supervised learning algorithm will only have 250 rows from which to train a model. A semi-supervised learning algorithm will have the 250 labeled rows as well as the 250 unlabeled rows that could be used in numerous ways to improve the labeled training dataset. Next, we can establish a baseline in performance on the semi-supervised learning dataset using a supervised learning algorithm fit only on the labeled training data. This is important because we would expect a semi-supervised learning algorithm to outperform a supervised learning algorithm fit on the labeled data alone. If this is not the case, then the semi-supervised learning algorithm does not have skill. In this case, we will use a logistic regression algorithm fit on the labeled portion of the training dataset. ... # define model model = LogisticRegression() # fit model on labeled dataset model.fit(X_train_lab, y_train_lab) The model can then be used to make predictions on the entire hold out test dataset and evaluated using classification accuracy. ... # make predictions on hold out test set yhat = model.predict(X_test) # calculate score for test set score = accuracy_score(y_test, yhat) # summarize score print('Accuracy: %.3f' % (score*100)) Tying this together, the complete example of evaluating a supervised learning algorithm on the semi-supervised learning dataset is listed below. # baseline performance on the semi-supervised learning dataset from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from sklearn.linear_model import LogisticRegression # define dataset X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, random_state=1) # split into train and test X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50, random_state=1, stratify=y) # split train into labeled and unlabeled X_train_lab, X_test_unlab, y_train_lab, y_test_unlab = train_test_split(X_train, y_train, test_size=0.50, random_state=1, stratify=y_train) # define model model = LogisticRegression() # fit model on labeled dataset model.fit(X_train_lab, y_train_lab) # make predictions on hold out test set yhat = model.predict(X_test) # calculate score for test set score = accuracy_score(y_test, yhat) # summarize score print('Accuracy: %.3f' % (score*100)) Running the algorithm fits the model on the labeled training dataset and evaluates it on the holdout dataset and prints the classification accuracy. Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome. In this case, we can see that the algorithm achieved a classification accuracy of about 84.8 percent. We would expect an effective semi-supervised learning algorithm to achieve better accuracy than this. Accuracy: 84.800 Next, let\\u2019s explore how to apply the label propagation algorithm to the dataset. Label Propagation for Semi-Supervised Learning The Label Propagation algorithm is available in the scikit-learn Python machine learning library via the LabelPropagation class. The model can be fit just like any other classification model by calling the fit() function and used to make predictions for new data via the predict() function. ... # define model model = LabelPropagation() # fit model on training dataset model.fit(..., ...) # make predictions on hold out test set yhat = model.predict(...) Importantly, the training dataset provided to the fit() function must include labeled examples that are integer encoded (as per normal) and unlabeled examples marked with a label of -1. The model will then determine a label for the unlabeled examples as part of fitting the model. After the model is fit, the estimated labels for the labeled and unlabeled data in the training dataset is available via the \\u201ctransduction_\\u201d attribute on the LabelPropagation class. ... # get labels for entire training dataset data tran_labels = model.transduction_ Now that we are familiar with how to use the Label Propagation algorithm in scikit-learn, let\\u2019s look at how we might apply it to our semi-supervised learning dataset. First, we must prepare the training dataset. We can concatenate the input data of the training dataset into a single array. ... # create the training dataset input X_train_mixed = concatenate((X_train_lab, X_test_unlab)) We can then create a list of -1 valued (unlabeled) for each row in the unlabeled portion of the training dataset. ... # create \\\"no label\\\" for unlabeled data nolabel = [-1 for _ in range(len(y_test_unlab))] This list can then be concatenated with the labels from the labeled portion of the training dataset to correspond with the input array for the training dataset. ... # recombine training dataset labels y_train_mixed = concatenate((y_train_lab, nolabel)) We can now train the LabelPropagation model on the entire training dataset. ... # define model model = LabelPropagation() # fit model on training dataset model.fit(X_train_mixed, y_train_mixed) Next, we can use the model to make predictions on the holdout dataset and evaluate the model using classification accuracy. ... # make predictions on hold out test set yhat = model.predict(X_test) # calculate score for test set score = accuracy_score(y_test, yhat) # summarize score print('Accuracy: %.3f' % (score*100)) Tying this together, the complete example of evaluating label propagation on the semi-supervised learning dataset is listed below. # evaluate label propagation on the semi-supervised learning dataset from numpy import concatenate from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from sklearn.semi_supervised import LabelPropagation # define dataset X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, random_state=1) # split into train and test X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50, random_state=1, stratify=y) # split train into labeled and unlabeled X_train_lab, X_test_unlab, y_train_lab, y_test_unlab = train_test_split(X_train, y_train, test_size=0.50, random_state=1, stratify=y_train) # create the training dataset input X_train_mixed = concatenate((X_train_lab, X_test_unlab)) # create \\\"no label\\\" for unlabeled data nolabel = [-1 for _ in range(len(y_test_unlab))] # recombine training dataset labels y_train_mixed = concatenate((y_train_lab, nolabel)) # define model model = LabelPropagation() # fit model on training dataset model.fit(X_train_mixed, y_train_mixed) # make predictions on hold out test set yhat = model.predict(X_test) # calculate score for test set score = accuracy_score(y_test, yhat) # summarize score print('Accuracy: %.3f' % (score*100)) Running the algorithm fits the model on the entire training dataset and evaluates it on the holdout dataset and prints the classification accuracy. Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome. In this case, we can see that the label propagation model achieves a classification accuracy of about 85.6 percent, which is slightly higher than a logistic regression fit only on the labeled training dataset that achieved an accuracy of about 84.8 percent. Accuracy: 85.600 So far, so good. Another approach we can use with the semi-supervised model is to take the estimated labels for the training dataset and fit a supervised learning model. Recall that we can retrieve the labels for the entire training dataset from the label propagation model as follows: ... # get labels for entire training dataset data tran_labels = model.transduction_ We can then use these labels along with all of the input data to train and evaluate a supervised learning algorithm, such as a logistic regression model. The hope is that the supervised learning model fit on the entire training dataset would achieve even better performance than the semi-supervised learning model alone. ... # define supervised learning model model2 = LogisticRegression() # fit supervised learning model on entire training dataset model2.fit(X_train_mixed, tran_labels) # make predictions on hold out test set yhat = model2.predict(X_test) # calculate score for test set score = accuracy_score(y_test, yhat) # summarize score print('Accuracy: %.3f' % (score*100)) Tying this together, the complete example of using the estimated training set labels to train and evaluate a supervised learning model is listed below. # evaluate logistic regression fit on label propagation for semi-supervised learning from numpy import concatenate from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from sklearn.semi_supervised import LabelPropagation from sklearn.linear_model import LogisticRegression # define dataset X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, random_state=1) # split into train and test X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50, random_state=1, stratify=y) # split train into labeled and unlabeled X_train_lab, X_test_unlab, y_train_lab, y_test_unlab = train_test_split(X_train, y_train, test_size=0.50, random_state=1, stratify=y_train) # create the training dataset input X_train_mixed = concatenate((X_train_lab, X_test_unlab)) # create \\\"no label\\\" for unlabeled data nolabel = [-1 for _ in range(len(y_test_unlab))] # recombine training dataset labels y_train_mixed = concatenate((y_train_lab, nolabel)) # define model model = LabelPropagation() # fit model on training dataset model.fit(X_train_mixed, y_train_mixed) # get labels for entire training dataset data tran_labels = model.transduction_ # define supervised learning model model2 = LogisticRegression() # fit supervised learning model on entire training dataset model2.fit(X_train_mixed, tran_labels) # make predictions on hold out test set yhat = model2.predict(X_test) # calculate score for test set score = accuracy_score(y_test, yhat) # summarize score print('Accuracy: %.3f' % (score*100)) Running the algorithm fits the semi-supervised model on the entire training dataset, then fits a supervised learning model on the entire training dataset with inferred labels and evaluates it on the holdout dataset, printing the classification accuracy. Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome. In this case, we can see that this hierarchical approach of the semi-supervised model followed by supervised model achieves a classification accuracy of about 86.2 percent on the holdout dataset, even better than the semi-supervised learning used alone that achieved an accuracy of about 85.6 percent. Accuracy: 86.200 Can you achieve better results by tuning the hyperparameters of the LabelPropagation model? Let me know what you discover in the comments below. Further Reading This section provides more resources on the topic if you are looking to go deeper. Books Introduction to Semi-Supervised Learning, 2009. Chapter 11: Label Propagation and Quadratic Criterion, Semi-Supervised Learning, 2006. Papers Learning From Labeled And Unlabeled Data With Label Propagation, 2002. APIs sklearn.semi_supervised.LabelPropagation API. Section 1.14. Semi-Supervised, Scikit-Learn User Guide. sklearn.model_selection.train_test_split API. sklearn.linear_model.LogisticRegression API. sklearn.datasets.make_classification API. Articles Semi-supervised learning, Wikipedia. Summary In this tutorial, you discovered how to apply the label propagation algorithm to a semi-supervised learning classification dataset. Specifically, you learned: An intuition for how the label propagation semi-supervised learning algorithm works. How to develop a semi-supervised classification dataset and establish a baseline in performance with a supervised learning algorithm. How to develop and evaluate a label propagation algorithm and use the model output to train a supervised learning algorithm. Do you have any questions? Ask your questions in the comments below and I will do my best to answer. Tweet Share Share The post Semi-Supervised Learning With Label Propagation appeared first on Machine Learning Mastery.\",\"1607\":\"Wikipedia is arguably the greatest website ever made. Happy 20th.\",\"1446\":\"Dan Kokotov is VP of Engineering at Rev.ai, an automatic speech recognition company. Please support this podcast by checking out our sponsors: \\u2013 Athletic Greens: https:\\/\\/athleticgreens.com\\/lex and use code LEX to get 1 month of fish oil \\u2013 Blinkist: https:\\/\\/blinkist.com\\/lex and use code LEX to get 25% off premium \\u2013 Business Wars: https:\\/\\/wondery.com\\/business-wars\\/ \\u2013 Cash App: https:\\/\\/cash.app\\/ and use code LexPodcast to get $10 EPISODE LINKS: Rev: https:\\/\\/www.rev.com Rev.ai: https:\\/\\/www.rev.ai PODCAST INFO: Podcast website: https:\\/\\/lexfridman.com\\/podcast Apple Podcasts: https:\\/\\/apple.co\\/2lwqZIr Spotify: https:\\/\\/spoti.fi\\/2nEwCF8 RSS: https:\\/\\/lexfridman.com\\/feed\\/podcast\\/ YouTube Full Episodes: https:\\/\\/youtube.com\\/lexfridman YouTube Clips: https:\\/\\/youtube.com\\/lexclips SUPPORT & CONNECT: \\u2013 Check out the sponsors above, it\\u2019s the best way to support this podcast \\u2013 Support on Patreon: https:\\/\\/www.patreon.com\\/lexfridman \\u2013 Twitter: https:\\/\\/twitter.com\\/lexfridman \\u2013 Instagram: https:\\/\\/www.instagram.com\\/lexfridman \\u2013 LinkedIn: https:\\/\\/www.linkedin.com\\/in\\/lexfridman \\u2013 Facebook: https:\\/\\/www.facebook.com\\/LexFridmanPage \\u2013 Medium: https:\\/\\/medium.com\\/@lexfridman OUTLINE: Here\\u2019s the timestamps for the episode. On some podcast players you should be able to click the timestamp to jump to that time. (00:00) \\u2013 Introduction (09:17) \\u2013 Dune (12:34) \\u2013 Rev (18:33) \\u2013 Translation (25:22) \\u2013 Gig economy (34:02) \\u2013 Automatic speech recognition (44:53) \\u2013 Create products that people love (53:02) \\u2013 The future of podcasts at Spotify (1:14:41) \\u2013 Book recommendations (1:16:02) \\u2013 Stories of our dystopian future (1:19:45) \\u2013 Movies about Stalin and Hitler (1:24:59) \\u2013 Interviewing Putin (1:30:56) \\u2013 Meaning of life\",\"1703\":\"Read the full story\",\"491\":\"In this video, I will show you how to build a model for (almost) any text classification problem, i.e., binary classification, multi-class classification or multi-label classification. You can expand this to any language and any type of deep learning model (LSTM, GRU, Transformers, etc). In this video, I will be using BERT model from Transformers and Tez. You can find Tez here: https:\\/\\/github.com\\/abhishekkrthakur\\/tez #NLP #DeepLearning #PyTorch Please subscribe and like the video to help me keep motivated to make awesome videos like this one. :) To buy my book, Approaching (Almost) Any Machine Learning problem, please visit: https:\\/\\/bit.ly\\/buyaaml Follow me on: Twitter: https:\\/\\/twitter.com\\/abhi1thakur LinkedIn: https:\\/\\/www.linkedin.com\\/in\\/abhi1thakur\\/ Kaggle: https:\\/\\/kaggle.com\\/abhishek Instagram: https:\\/\\/instagram.com\\/abhi4ml\",\"706\":\"Day 3 | November 19, 2020 Theme: Unblocking the Pipeline from Education to Employment Beth Rosenberg, Tech Kids Unlimited @Ability Project\\/NYU The Accessible Computer Science Education Fall Workshop was hosted by Microsoft, University of Washington CREATE, and University of Colorado\\u2019s Coleman Institute. It took place November 17-19, 2020 and consisted of three half-days of talks, discussions, and planning for new research dedicated to making Computer Science education learning experiences more accessible for people with disabilities. More information on this workshop can be found at https:\\/\\/www.microsoft.com\\/en-us\\/research\\/event\\/accessible-cs-education-fall-workshop\\/\",\"4287\":\"This week Dr. Tim Scarfe and Dr. Keith Duggar discuss Explainability, Reasoning, Priors and GPT-3. We check out Christoph Molnar's book on intepretability, talk about priors vs experience in NNs, whether NNs are reasoning and also cover articles by Gary Marcus and Walid Saba critiquing deep learning. We finish with a brief discussion of Chollet's ARC challenge and intelligence paper. 00:00:00 Intro 00:01:17 Explainability and Christoph Molnars book on Intepretability 00:26:45 Explainability - Feature visualisation 00:33:28 Architecture \\/ CPPNs 00:36:10 Invariance and data parsimony, priors and experience, manifolds 00:42:04 What NNs learn \\/ logical view of modern AI (Walid Saba article) 00:47:10 Core knowledge 00:55:33 Priors vs experience 00:59:44 Mathematical reasoning 01:01:56 Gary Marcus on GPT-3 01:09:14 Can NNs reason at all? 01:18:05 Chollet intelligence paper\\/ARC challenge\",\"501\":\"In this first video of the #NLP series, I talk about what is #stemming and #lemmatization. I hope you like the video ;) Please subscribe and like the video to help me keep motivated to make awesome videos like this one. :) To buy my book, Approaching (Almost) Any Machine Learning problem, please visit: https:\\/\\/bit.ly\\/buyaaml Follow me on: Twitter: https:\\/\\/twitter.com\\/abhi1thakur LinkedIn: https:\\/\\/www.linkedin.com\\/in\\/abhi1thakur\\/ Kaggle: https:\\/\\/kaggle.com\\/abhishek Instagram: https:\\/\\/instagram.com\\/abhi4ml\",\"1501\":\"Alex Filippenko is an astrophysicist and professor of astronomy at Berkeley. Please support this podcast by checking out our sponsors: \\u2013 Neuro: https:\\/\\/www.getneuro.com and use code LEX to get 15% off \\u2013 BetterHelp: https:\\/\\/betterhelp.com\\/lex to get 10% off \\u2013 MasterClass: https:\\/\\/masterclass.com\\/lex to get 15% off annual sub \\u2013 Cash App: https:\\/\\/cash.app\\/ and use code LexPodcast to get $10 EPISODE LINKS: Alex\\u2019s Website: https:\\/\\/astro.berkeley.edu\\/people\\/alex-filippenko\\/ PODCAST INFO: Podcast website: https:\\/\\/lexfridman.com\\/podcast Apple Podcasts: https:\\/\\/apple.co\\/2lwqZIr Spotify: https:\\/\\/spoti.fi\\/2nEwCF8 RSS: https:\\/\\/lexfridman.com\\/feed\\/podcast\\/ YouTube Full Episodes: https:\\/\\/youtube.com\\/lexfridman YouTube Clips: https:\\/\\/youtube.com\\/lexclips SUPPORT & CONNECT: \\u2013 Check out the sponsors above, it\\u2019s the best way to support this podcast \\u2013\",\"240\":\"Maybe it's because I am travel starved, but I am really getting into and enjoying a growing genre of 4K walking videos around the world, e.g. openculture.com\\/2020\\/03\\/expl\\u2026 has a few examples. Interesting to leave running on TV in the background, unscripted samples of human condition\",\"492\":\"In this video, I show you all the features of the new #NVIDIA #Broadcast application which uses #AI for some of the most powerful features that you can use for streaming, broadcasting, and for online meetings! Please subscribe and like the video to help me keep motivated to make awesome videos like this one. :) To buy my book, Approaching (Almost) Any Machine Learning problem, please visit: https:\\/\\/bit.ly\\/buyaaml Follow me on: Twitter: https:\\/\\/twitter.com\\/abhi1thakur LinkedIn: https:\\/\\/www.linkedin.com\\/in\\/abhi1thakur\\/ Kaggle: https:\\/\\/kaggle.com\\/abhishek Instagram: https:\\/\\/instagram.com\\/abhi4ml\",\"703\":\"A panel of four researchers across academia and industry discuss their different career paths in research. Microsoft's third PhD Summit was a two-day virtual workshop. It was an opportunity for top PhD students to enhance their skills, build a network, and discuss research within a community of peers and notable Microsoft researchers. Speakers: Gonzalo Ramos, Principal Researcher, Microsoft [Discussion Lead] Valerie Taylor, CEO and President, CMD-IT & Director, Mathematics and Computer Science Division, Argonne National Laboratory Armando Solar-Lezama, Professor in EECS and Associate Director and COO of CSAIL Kristin Lauter, Principal Researcher and Partner Research Manager, Microsoft More information on this workshop can be found at https:\\/\\/www.microsoft.com\\/en-us\\/research\\/event\\/phd-summit-2020\\/\",\"4468\":\"I hope everyone stays safe out there today.\",\"1054\":\"STEMerch Store: https:\\/\\/stemerch.com\\/ Second channel: https:\\/\\/www.youtube.com\\/zachstarhimself Don't be a jerk: https:\\/\\/stemerch.com\\/collections\\/dont-be-a-jerk i double dot: https:\\/\\/stemerch.com\\/collections\\/cursed-math-memes-i-double-dot Fractal Designs: https:\\/\\/stemerch.com\\/collections\\/fractals Engineering Clock: https:\\/\\/stemerch.com\\/collections\\/clocks-watches-1 \\u25baFollow me Instagram: https:\\/\\/www.instagram.com\\/zachstar\\/ Twitter: https:\\/\\/twitter.com\\/ImZachStar Engineering Students be like... (Part 1): https:\\/\\/youtu.be\\/D5lkJvMGpVE Support the Channel: https:\\/\\/www.patreon.com\\/zachstar PayPal(one time donation): https:\\/\\/www.paypal.me\\/ZachStarYT Join this channel to get access to perks: https:\\/\\/www.youtube.com\\/channel\\/UCpCSAcbqs-sjEVfk_hMfY9w\\/join Animations: Brainup Studios ( http:\\/\\/brainup.in\\/ ) Check out my Spanish channel here: https:\\/\\/www.youtube.com\\/channel\\/UCnkNu2xQBLASpj6cKC8vtpA \\u25baMy Setup: Space Pictures: https:\\/\\/amzn.to\\/2CC4Kqj Magnetic Floating Globe: https:\\/\\/amzn.to\\/2VgPdn0 Camera: https:\\/\\/amzn.to\\/2RivYu5 Mic: https:\\/\\/amzn.to\\/35bKiri Tripod: https:\\/\\/amzn.to\\/2RgMTNL Equilibrium Tube: https:\\/\\/amzn.to\\/2SowDrh \\u25baCheck out the Amazon Store: https:\\/\\/www.amazon.com\\/shop\\/zachstar\",\"1553\":\"Keoki Jackson is the CTO of Lockheed Martin, a company that through its long history has created some of the most incredible engineering marvels that human beings have ever built, including planes that fly fast and undetected, defense systems that intersect threats that could take the lives of millions in the case of nuclear weapons, and spacecraft systems that venture out into space, the moon, Mars, and beyond with and without humans on-board. This conversation is part of the Artificial Intelligence podcast. If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman on\",\"4281\":\"This week Dr. Tim Scarfe, Dr. Keith Duggar and Connor Leahy chat with Prof. Karl Friston. Professor Friston is a British neuroscientist at University College London and an authority on brain imaging. In 2016 he was ranked the most influential neuroscientist on Semantic Scholar. His main contribution to theoretical neurobiology is the variational Free energy principle, also known as active inference in the Bayesian brain. The FEP is a formal statement that the existential imperative for any system which survives in the changing world can be cast as an inference problem. Bayesian Brain Hypothesis states that the brain is confronted with ambiguous sensory evidence, which it interprets by making inferences about the hidden states which caused the sensory data. So is the brain an inference engine? The key concept separating Friston's idea from traditional stochastic reinforcement learning methods and even Bayesian reinforcement learning is moving away from goal-directed optimisation. Remember to subscribe! Enjoy the show! 00:00:00 Show teaser intro 00:16:24 Main formalism for FEP 00:28:29 Path Integral 00:30:52 How did we feel talking to friston? 00:34:06 Skit - on cultures (checked, but maybe make shorter) 00:36:02 Friston joins 00:36:33 Main show introduction 00:40:51 Is prediction all it takes for intelligence? 00:48:21 balancing accuracy with flexibility 00:57:36 belief-free vs belief-based; beliefs are crucial 01:04:53 Fuzzy Markov Blankets and Wandering Sets 01:12:37 The Free Energy Principle conforms to itself 01:14:50 useful false beliefs 01:19:14 complexity minimization is the heart of free energy [01:19:14 ]Keith: 01:23:25 An Alpha to tip the scales? Absoute not! Absolutely yes! 01:28:47 FEP applied to brain anatomy 01:36:28 Are there multiple non-FEP forms in the brain? 01:43:11 a positive conneciton to backpropagation 01:47:12 The FEP does not explain the origin of FEP systems 01:49:32 Post-show banter https:\\/\\/www.fil.ion.ucl.ac.uk\\/~karl\\/ #machinelearning\",\"2343\":\"[link] [comments]\",\"1643\":\"Read the full story\",\"1468\":\"Ayanna Howard is a roboticist and professor at Georgia Tech, director of Human-Automation Systems lab, with research interests in human-robot interaction, assistive robots in the home, therapy gaming apps, and remote robotic exploration of extreme environments. This conversation is part of the Artificial Intelligence podcast. If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman on Twitter, LinkedIn, Facebook, Medium, or YouTube where you can watch the video versions of these conversations. If you enjoy the podcast, please rate it 5 stars on Apple Podcasts, follow on Spotify, or support it on Patreon.\",\"1142\":\"Tom Crawford talks foxes and rabbits. More links & stuff in full description below \\u2193\\u2193\\u2193 More of Tom on Numberphile: http:\\/\\/bit.ly\\/TomPlaylist Tom's website: https:\\/\\/tomrocksmaths.com Numberphile is supported by the Mathematical Sciences Research Institute (MSRI): http:\\/\\/bit.ly\\/MSRINumberphile We are also supported by Science Sandbox, a Simons Foundation initiative dedicated to engaging everyone with the process of science. https:\\/\\/www.simonsfoundation.org\\/outreach\\/science-sandbox\\/ And support from Math For America - https:\\/\\/www.mathforamerica.org\\/ NUMBERPHILE Website: http:\\/\\/www.numberphile.com\\/ Numberphile on Facebook: http:\\/\\/www.facebook.com\\/numberphile Numberphile tweets: https:\\/\\/twitter.com\\/numberphile Subscribe: http:\\/\\/bit.ly\\/Numberphile_Sub Videos by Brady Haran Animated by Pete McPartlan Patreon: http:\\/\\/www.patreon.com\\/numberphile Numberphile T-Shirts and Merch: https:\\/\\/teespring.com\\/stores\\/numberphile Brady's videos subreddit: http:\\/\\/www.reddit.com\\/r\\/BradyHaran\\/ Brady's latest videos across all channels: http:\\/\\/www.bradyharanblog.com\\/ Sign up for (occasional) emails: http:\\/\\/eepurl.com\\/YdjL9 With thanks to Patreon supporters: Ben Delo Arjun Chakroborty Jeff Straathof Andy B Yana Chernobilsky Ken Baron Gnare Tom Marshall Jes\\u00fas Saucedo John Zelinka Ubiquity Ventures Mateusz Swiatkowski Garrett Smith Ron Hochsprung Jeremy Buchanan Nat Tyce Matthew Schuster Mitch Harding RAD Donato Andrei M Burke Ben White Adam Savage Steve Crutchfield James Bissonette Jubal John Jordan W Oja Anna M Mirik Gogri That Asymptote Alex Khein Kermit Norlund Bodhisattva Debnath Bernd Sing Charles Southerland Alfred Wallace Valentin Arnas Kristian Joensen Tracy Parry Ian George Walker\",\"1479\":\"Ian Hutchinson is a nuclear engineer and plasma physicist at MIT. He has made a number of important contributions in plasma physics including the magnetic confinement of plasmas seeking to enable fusion reactions, which is the energy source of the stars, to be used for practical energy production. Current nuclear reactors are based on fission as we discuss. Ian has also written on the philosophy of science and the relationship between science and religion. Support this podcast by supporting our sponsors: \\u2013 Sun Basket, use code LEX: https:\\/\\/sunbasket.com\\/lex \\u2013 PowerDot, use code LEX: https:\\/\\/powerdot.com\\/lex If you would like to get\",\"1047\":\"Get 25% off a year subscription to CuriosityStream, ends Jan 3rd 2021: (use code \\\"zachstar\\\" at sign up): https:\\/\\/curiositystream.thld.co\\/zachstarnov18 STEMerch Store: https:\\/\\/stemerch.com\\/ Support the Channel: https:\\/\\/www.patreon.com\\/zachstar PayPal(one time donation): https:\\/\\/www.paypal.me\\/ZachStarYT Join this channel to get access to perks: https:\\/\\/www.youtube.com\\/channel\\/UCpCSAcbqs-sjEVfk_hMfY9w\\/join \\u25baPrevious videos about self teaching Video 1: https:\\/\\/youtu.be\\/Ev3jF4vjeGw Video 2: https:\\/\\/youtu.be\\/pZSD3DFyN7c \\u25baGet the book here (associate link): https:\\/\\/amzn.to\\/3eXvIKn \\u25baFollow me Instagram: https:\\/\\/www.instagram.com\\/zachstar\\/ Twitter: https:\\/\\/twitter.com\\/ImZachStar 2D Graphing Software: https:\\/\\/www.desmos.com\\/calculator Check out my Spanish channel here: https:\\/\\/www.youtube.com\\/channel\\/UCnkNu2xQBLASpj6cKC8vtpA 00:00 Intro 2:41 What is real analysis? 5:30 How long did the book take me? 6:18 How to approach practice problems 8:08 Did I like the course? 8:42 Quick example 10:53 Advice for self teaching 15:38 Textbook I used 17:50 Ending\\/Sponsorship \\u25baMy Setup: Space Pictures: https:\\/\\/amzn.to\\/2CC4Kqj Magnetic Floating Globe: https:\\/\\/amzn.to\\/2VgPdn0 Camera: https:\\/\\/amzn.to\\/2RivYu5 Mic: https:\\/\\/amzn.to\\/35bKiri Tripod: https:\\/\\/amzn.to\\/2RgMTNL Equilibrium Tube: https:\\/\\/amzn.to\\/2SowDrh \\u25baCheck out my Amazon Store: https:\\/\\/www.amazon.com\\/shop\\/zachstar\",\"493\":\"In this video, we will learn about the basics of #Tokenization in #NLP. Please note that this is a basic video about tokenization. If you are looking for advanced tokenization techniques, check out the advanced video. Please subscribe and like the video to help me keep motivated to make awesome videos like this one. :) To buy my book, Approaching (Almost) Any Machine Learning problem, please visit: https:\\/\\/bit.ly\\/buyaaml Follow me on: Twitter: https:\\/\\/twitter.com\\/abhi1thakur LinkedIn: https:\\/\\/www.linkedin.com\\/in\\/abhi1thakur\\/ Kaggle: https:\\/\\/kaggle.com\\/abhishek Instagram: https:\\/\\/instagram.com\\/abhi4ml\",\"4305\":\"#machinelearning This week Dr. Tim Scarfe, Dr. Keith Duggar and Yannic Kilcher speak with veteran NLU expert Dr. Walid Saba. Walid is an old-school AI expert. He is a polymath, a neuroscientist, psychologist, linguist, philosopher, statistician, and logician. He thinks the missing information problem and lack of a typed ontology is the key issue with NLU, not sample efficiency or generalisation. He is a big critic of the deep learning movement and BERTology. We also cover GPT-3 in some detail in today's session, covering Luciano Floridi's recent article \\\"GPT\\u20113: Its Nature, Scope, Limits, and Consequences\\\" and a commentary on the incredible power of GPT-3 to perform tasks with just a few examples including the Yann LeCun commentary on Facebook and Hackernews. Time stamps on the YouTube version 0:00:00 Walid intro 00:05:03 Knowledge acquisition bottleneck 00:06:11 Language is ambiguous 00:07:41 Language is not learned 00:08:32 Language is a formal language 00:08:55 Learning from data doesn\\u2019t work 00:14:01 Intelligence 00:15:07 Lack of domain knowledge these days 00:16:37 Yannic Kilcher thuglife comment 00:17:57 Deep learning assault 00:20:07 The way we evaluate language models is flawed 00:20:47 Humans do type checking 00:23:02 Ontologic 00:25:48 Comments On GPT3 00:30:54 Yann lecun and reddit 00:33:57 Minds and machines - Luciano 00:35:55 Main show introduction 00:39:02 Walid introduces himself 00:40:20 science advances one funeral at a time 00:44:58 Deep learning obsession syndrome and inception 00:46:14 BERTology \\/ empirical methods are not NLU 00:49:55 Pattern recognition vs domain reasoning, is the knowledge in the data 00:56:04 Natural language understanding is about decoding and not compression, it's not learnable. 01:01:46 Intelligence is about not needing infinite amounts of time 01:04:23 We need an explicit ontological structure to understand anything 01:06:40 Ontological concepts 01:09:38 Word embeddings 01:12:20 There is power in structure 01:15:16 Language models are not trained on pronoun disambiguation and resolving scopes 01:17:33 The information is not in the data 01:19:03 Can we generate these rules on the fly? Rules or data? 01:20:39 The missing data problem is key 01:21:19 Problem with empirical methods and lecunn reference 01:22:45 Comparison with meatspace (brains) 01:28:16 The knowledge graph game, is knowledge constructed or discovered 01:29:41 How small can this ontology of the world be? 01:33:08 Walids taxonomy of understanding 01:38:49 The trend seems to be, less rules is better not the othe way around? 01:40:30 Testing the latest NLP models with entailment 01:42:25 Problems with the way we evaluate NLP 01:44:10 Winograd Schema challenge 01:45:56 All you need to know now is how to build neural networks, lack of rigour in ML research 01:50:47 Is everything learnable 01:53:02 How should we elevate language systems? 01:54:04 10 big problems in language (missing information) 01:55:59 Multiple inheritance is wrong 01:58:19 Language is ambiguous 02:01:14 How big would our world ontology need to be? 02:05:49 How to learn more about NLU 02:09:10 AlphaGo Walid's blog: https:\\/\\/medium.com\\/@ontologik LinkedIn: https:\\/\\/www.linkedin.com\\/in\\/walidsaba\\/\",\"5680\":\"LangHuAn, a simple NLP human tagging app that can be driven from a pandas dataframe. Supports for classification and NER tasks. Build supervised data for machine learning https:\\/\\/github.com\\/raynardj\\/langhuan \\u200b https:\\/\\/preview.redd.it\\/8tbeany8n0d61.png?width=2758&format=png&auto=webp&s=193b3675e0592a2ac54eaa3f7776f6e004552070 Allows: Change the tagged entries Simpler user segregations Edit options during the run n users verifying same entry for k times. (n k both okay) You can order the entries by a score column (predictions from a model, rule base score etc ). You can deploy strategy having taggers jump between the lowest and highest score [link] [comments]\",\"4306\":\"*Note this is an episode from Tim's Machine Learning Dojo YouTube channel. Join Eric Craeymeersch on a wonderful discussion all about ML engineering, computer vision, siamese networks, contrastive loss, one shot learning and metric learning. 00:00:00 Introduction 00:11:47 ML Engineering Discussion 00:35:59 Intro to the main topic 00:42:13 Siamese Networks 00:48:36 Mining strategies 00:51:15 Contrastive Loss 00:57:44 Trip loss paper 01:09:35 Quad loss paper 01:25:49 Eric's Quadloss Medium Article 02:17:32 Metric learning reality check 02:21:06 Engineering discussion II 02:26:22 Outro In our second paper review call, Tess Ferrandez covered off the FaceNet paper from Google which was a one-shot siamese network with the so called triplet loss. It was an interesting change of direction for NN architecture i.e. using a contrastive loss instead of having a fixed number of output classes. Contrastive architectures have been taking over the ML landscape recently i.e. SimCLR, MOCO, BERT. Eric wrote an article about this at the time: https:\\/\\/medium.com\\/@crimy\\/one-shot-learning-siamese-networks-and-triplet-loss-with-keras-2885ed022352 He then discovered there was a new approach to one shot learning in vision using a quadruplet loss and metric learning. Eric wrote a new article and several experiments on this @ https:\\/\\/medium.com\\/@crimy\\/beyond-triplet-loss-one-shot-learning-experiments-with-quadruplet-loss-16671ed51290?source=friends_link&sk=bf41673664ad8a52e322380f2a456e8b Paper details: Beyond triplet loss: a deep quadruplet network for person re-identification https:\\/\\/arxiv.org\\/abs\\/1704.01719 (Chen at al '17) \\\"Person re-identification (ReID) is an important task in wide area video surveillance which focuses on identifying people across different cameras. Recently, deep learning networks with a triplet loss become a common framework for person ReID. However, the triplet loss pays main attentions on obtaining correct orders on the training set. It still suffers from a weaker generalization capability from the training set to the testing set, thus resulting in inferior performance. In this paper, we design a quadruplet loss, which can lead to the model output with a larger inter-class variation and a smaller intra-class variation compared to the triplet loss. As a result, our model has a better generalization ability and can achieve a higher performance on the testing set. In particular, a quadruplet deep network using a margin-based online hard negative mining is proposed based on the quadruplet loss for the person ReID. In extensive experiments, the proposed network outperforms most of the state-of-the-art algorithms on representative datasets which clearly demonstrates the effectiveness of our proposed method.\\\" Original facenet paper; https:\\/\\/arxiv.org\\/abs\\/1503.03832 #deeplearning #machinelearning\",\"1626\":\"Read the full story\",\"199\":\"Kaggler, deoxy takes first-place and sets the stage for his next competition. Please join us in congratulating Linsho Kaku (aka deoxy) on his solo first-place win in our Bengali.AI Handwritten Grapheme Classification challenge! Read the winning solution here: 1st Place Solution with Code Random Ink by Sankarshan Mukhopadhyay @FlickrLet\\u2019s meet Linsho!Linsho, what would you like to share about yourself? Linsho: I am a student in the Rio Yokota Laboratory at the Tokyo Institute of Technology. The main theme of the lab is high performance computing with advanced architectures including GPUs. We also deal with deep learning as one of its applications. I\\u2019m also an intern at Future Inc. working on an OCR task. Did you have any prior experience or domain knowledge that helped you succeed in this competition? The experience of working on OCR tasks as an intern was a big advantage for me. The ease in which I was able to pre-process data and create models was thanks to my intern experience. I\\u2019ve never specialized in Few-Shot Learning, which has been a major factor in the scores of the top teams this time around. However, I think my knowledge of the paper, which was shared in the lab, made a big difference. Let\\u2019s get a bit more technicalDid any past research or previous competitions inform your approach? In thinking about the approach, I went through the Kaggle discussions that were presented here and here. In addition, I often consulted Science Direct and other resources to achieve Few-Shot Learning. Did you do any preprocessing? No pre-processing, such as cropping or noise reduction, was done to the images. These processes did not improve recognition accuracy, but rather tended to reduce the amount of information needed. The disadvantages of cropping a smaller area than the required character area and erasing extra characters far outweigh the advantages of giving clean input. What was your most important insight into the data? The most essential task needed was not the classification of the three types of components per se, but the creation of a model that could recognize classes that were not given. The classification into three types of components was only a hint to solve this essential task. This is not to say the division into manually determined components is appropriate. Abstracting the structures that can appear in a character is more likely to improve the accuracy of classification of unknown classes. This time, I used a method to generate font image characters from handwritten characters. This generation model is based on a style and style transformation model called CycleGAN. A series of models up to the font image classification model connected to this generative model can be considered as a handwritten character classification model. In such a view, the Font image generated by the generative model can be considered as a feature of the middle layer of this series of handwriting classification models. It\\u2019s highly likely that each pixel of the font image, which is an intermediate feature, has generated the structure of the font image by observing a relatively narrow portion of the handwriting. This can be thought of as generating features of a more abstract character structure. I think being able to build this system was the biggest factor in my approach. Which tools did you use? I used Pytorch as a deep learning framework and Jupyter Notebook as an IDE. What about your hardware setup? I use some servers, which has 4 Tesla V100. What was the run time of your winning solution? CycleGAN takes the following time: Training time: 4 Tesla V100 2.5 days Prediction time : 40 min x 2(ensemble 2 model) Apart from this, it took some time to work on the usual class classification models and so on. Words of wisdomOK, are you walking away with anything new as a result of this competition? I gained new skills that will allow for a consistent approach to future challenges. Any advice for those just getting started in data science? Grandmaster\\u2019s common sense (or what seems most obvious) is not always the best to win. Just for funIf you could run a Kaggle competition, what problem would you want to pose to other Kagglers? I\\u2019d like to propose a more practical OCR, that is, a task that is evaluated end-to-end from handwriting detection to recognition. (For example, something that aims at transcription and digitization of handwritten notes). I feel a general-purpose method of detection has not yet been established in spite of sufficient recognition accuracy. However, in the field of Object Detection, there are quite a variety of methods being considered, and I think there\\u2019s a lot of hope for active discussion and development. Linsho Kaku is a Master Student at Tokyo Institute of Technology, supervised by Rio Yokota. His research interests include deep learning, image processing and optical character recognition. Top Marks for Student Kaggler in Bengali.AI | A Winner\\u2019s Interview with Linsho Kaku was originally published in Kaggle Blog on Medium, where people are continuing the conversation by highlighting and responding to this story.\",\"4879\":\"I'm running 48 miles in 48 hours with @davidgoggins on March 5-7, starting 8pm PST. Join us for this challenge! See his page for live videos & details. Podcast with him after. He's throwing in other stuff & refuses to tell me what. I think he thinks he can break me. Good luck. \\ud83d\\ude0e\",\"1444\":\"Richard Dawkins is an evolutionary biologist, and author of The Selfish Gene, The Blind Watchmaker, The God Delusion, The Magic of Reality, The Greatest Show on Earth, and his latest Outgrowing God. He is the originator and popularizer of a lot of fascinating ideas in evolutionary biology and science in general, including funny enough the introduction of the word meme in his 1976 book The Selfish Gene, which in the context of a gene-centered view of evolution is an exceptionally powerful idea. He is outspoken, bold, and often fearless in his defense of science and reason, and in this way,\",\"1628\":\"Data Science has changed the way organizations collect, analyze, and process different types of information. Read the full story\",\"654\":\"A couple hours of me traveling to this beautiful alternate reality. It was fun. Please check out our sponsors: - Theragun: https:\\/\\/theragun.com\\/lex to get 30 day trial - Grammarly: https:\\/\\/grammarly.com\\/lex to get 20% off premium OUTLINE: 0:00 - Introduction 1:28 - Character creation 10:41 - Gameplay starts 31:08 - Tutorial on shooting and hacking 38:07 - Rescue Sandra Dorsett 42:42 - Going home 49:46 - Neurosurgery and cybernetic implants 57:27 - Meeting with Dexter 1:03:06 - Picking up the robot dog 1:07:21 - Closing thoughts CONNECT: - Subscribe to this YouTube channel - Twitter: https:\\/\\/twitter.com\\/lexfridman - LinkedIn: https:\\/\\/www.linkedin.com\\/in\\/lexfridman - Facebook: https:\\/\\/www.facebook.com\\/LexFridmanPage - Instagram: https:\\/\\/www.instagram.com\\/lexfridman - Medium: https:\\/\\/medium.com\\/@lexfridman - Support on Patreon: https:\\/\\/www.patreon.com\\/lexfridman\",\"1608\":\"I feel honored and excited to be back to @LexFridman's podcast; he's really deep, and has a unique knack for getting into the most important and exciting questions about tech, life & reality. invidious.snopyta.org\\/RL4j4KPwNGM nitter.net\\/lexfridman\\/status\\/1351053401438773248#m\",\"202\":\"I participated in the R package recommendation engine competition on Kaggle for two reasons. First, I use R a lot. I cannot learn statistics without R. This competition is my chance to give back to the community a R package recommendation engine. Second, during my day job as an engineer behind a machine learning service in the cloud, product recommendation is one of the most popular applications our early adopters want to use the web service for. This competition is my opportunity to stand in users\\u2019 shoes and identify their pain points associated with building a recommendation system. I treat R package recommendation as a binary classification task: a classifier $latex f(u, p)$ takes as input a user $latex u$ and a package $latex p$, and predicts whether the user will install the package or not. My final submission (team name: Record Me Men) combines four classifiers, labeled as large dots in Figure 1. The four classifiers share the same form of objective function that minimizes loss $latex L$ plus regularizers $latex R$: $latex J(\\\\theta) = \\\\sum_i L(y_i, f(u_i, p_i; \\\\theta)) + \\\\lambda R(\\\\theta)$ , where $latex f(u, v)$ is a classification model, $latex \\\\theta$ is the parameters of the classification model, $latex y_i$, $latex u_i$, $latex p_i$ are the class label (package installed or not), user and package of the i-th training example, respectively. $latex \\\\lambda$ controls the penalty from the regularizing function, and is chosen using cross validation. Model 1: Baseline. Model 1 is example_model_2.R that the competition organizer provides as a baseline. In this model, a user is encoded as dummy variables $latex \\\\mathbf{u}$, and a package is represented by seven features $latex \\\\mathbf{p} = \\\\{p_1, \\\\dots, p_7\\\\}$ (e.g, the logarithmic count of dependency. See the R script for the complete list). The classification model is a linear combination of user dummy variables and package features with weight parameters, $latex f(u, p) = \\\\mathbf{\\\\theta_u}^T \\\\mathbf{u} + \\\\mathbf{\\\\theta_p}^T \\\\mathbf{p}$. The parameters are $latex \\\\theta_u$ and $latex \\\\theta_p$ (and intercept). The loss function $latex L$ is negative logistic log likelihood. There is no regularizer in the model. The first model establishes a strong baseline, achieving AUC of ~0.94, labeled as M1b2 in Figure 1. A variant of this model that omits the user feature (see example_model_1.R, also provided by the contest organizers) achieves noticeable lower AUC of 0.81 (not shown in Figure 1). This suggests that some users are more likely to install R packages than others. In the next model, I will explore a classification model that incorporates not only user variations but also package variations. Model 2: Latent factor Models. In contrast to Model 1 with features derived from metadata, I focus on the user-package rating matrix. Model 2 consists of two components: baseline estimates and latent factors. Baseline estimates are linear combinations of three parts: global (one parameter $latex \\\\mu$), user (one parameter per user $latex \\\\mu_u$), and package (one parameter per package $latex \\\\mu_p$). For latent factors, I assumes that there are K latent factors for each user, $latex \\\\mathbf{\\\\beta}_u$ and each package $latex \\\\mathbf{\\\\beta}_p$, and the inner product of these two factors captures the interaction between a user and a package. The classifier model in Model 2 is $latex f(u, p) = \\\\mu + \\\\mu_u + \\\\mu_p + \\\\mathbf{\\\\beta}_u^T \\\\mathbf{\\\\beta}_p$. This kind of latent factor model, also known as \\u201csingular value decomposition\\u201d, has been reported with great success in previous collaborative filtering studies and at the Netlifx Prize. I choose an exponential loss function for Model 2, $latex L(y, f(u, p)) = \\\\exp(- y f(u, p))$, where $latex y_i \\\\in \\\\{1, -1\\\\}$. I choose exponential loss over squared loss because 1) exponential loss matches 0\\u20131 loss better than squared loss 2) exponential loss is differentiable. I apply L2 regularizers, $latex R(\\\\cdot) = ||\\\\mu_u||\\u00b2 + ||\\\\mu_p||\\u00b2 + ||\\\\beta_u||\\u00b2 + ||\\\\beta_p||\\u00b2$. I minimize the objective function using stochastic gradient descent. The number of latent factors, K, is chosen by cross validation. The latent factor model works very well on the R package recommendation data, achieving AUC of ~0.97 (See the M2 family in Figure 1). I plot the performance of five latent factor models with different Ks, ranging from 10 to 50, labeled as M2k10, M2k20, \\u2026, M2K50 in Figure 1. As K increases, the latent factor model becomes more expressive and fit the data better, resulting in higher AUC. Model 3: Package LDA topic. In Model 3, I explore new features not used in Model 1 and 2. The new feature is a package\\u2019s topic based LDA (Latent Dirichlet Allocation). The LDA topic of a package is inferred from the word counts of its man pages. I use the topics.csv, kindly prepared by the contest organizers, to map a R package to one of the 25 LDA topics (See topic_models.R for details on running LDA, provided by the contest organizer). The classification model of Model 3 is similar to Model 2. Model 3 replaces user factors in Model 2 with T LDA factors weights $latex \\\\mathbf{t}_u$ (T=25 here), and replaces package factors in Model 3 with T dummy variables $latex \\\\mathbf{p}$. The classification model is $latex f(u, v) = \\\\mu + \\\\mu_u + \\\\mu_p + \\\\mathbf{t}_u^T \\\\mathbf{p}$. The loss function is the same as Model 2, and the regularizer is L2, $latex R(\\\\cdot) = ||\\\\mu_u||\\u00b2 + ||\\\\mu_p||\\u00b2 + ||\\\\mathbf{t}_u||\\u00b2 $. Model 3 achieves better AUC than Model 1, resulting in AUC of ~0.97 (labeled as M3u in Figure 1). Prior to M3u, I explore a simpler model that users share the same weights $latex \\\\mathbf{t}$. The parameter space becomes smaller, but Model 3 with shared parameters (labeled as M3b in Figure 1) performs slightly worse than Model 3 with user-specific parameters. This makes sense because it is unlikely every R user shares the same interest in the same set of LDA topics. Model 4: Package task view. R packages are organized into task views (e.g., high-performance computing, survival analysis, and time series. See the page for the complete list of views). It is possible that a user interested in a particular task would install not just one but many, even all, packages in the same task view (e.g., using install.views() R function). We could improve recommendation if we know how much a user is interested in a particular task view. I use the views.csv, provided by the contest organizers, to map a package to its task view. The classification model of Model 4 is similar to Model 3, except that LDA topics in Model 3 are replaced with task views (T=29, 28 task views plus 1 unknown view). The performance of Model 4 is labeled as M4u in Figure 1. Model 4 performs well and better than Model 1. Similarly, I experiment with a variant of Model 4 that all users share the same task view parameters $latex \\\\mathbf{t}$, labeled as M4b i n Figure 1, and it performs worse than Model 4 with per-user parameters. The finding is consistent with Model 3, and R users, at least on the R recommendation data set, seem to have different preferences in task views. Ensemble learning. I combine four classifiers using logistic regression. To collect out-of-sample training data for the ensemble learner, I divide the training data into three sets: 80%, 10%, and 10%. I train individual classifier on the 80%. I then apply the trained classifiers on the 20%, and combine the scores from individual classifiers as training data for the ensemble learner. Both individual classifiers and the combiner are evaluated on the last 10% data set (as those shown in Figure 1 and Figure 2). After evaluating one fold, I shift data and conduct another folds until all data are used, resulting in a total of 10 ensemble learners. The output of these 10 ensemble learners are averaged to produce final predictions. The ensemble learning works very well, as shown in Figure 2. Combining M1 and M2 achieves AUC of 0.9721, which is higher than either M1 0.94 or M2 0.9702 alone. When I combine more models and include M3 and M4, the performance gets even better. The submission of combining four models achieves best performance on the training set among all models. On the test set the final model achieves AUC of 0.983289 (after post-processing, see below), the highest among my submissions. The success of ensemble training is possibly because the individual models are strong performers, and models are diverse with different classification models and features such that they complement each other. Post-processing. I apply two post-processing steps before submitting each entry. First, the label and (user, package) association on the training set is memorized. For any (user, package) pairs in the test set that are already seen in the training set, their labels on the training set are recalled and used instead. Second, I assume when a user install a package P, the user also installs the packages that P depends on. I record all packages a user installs on the training set as well as their dependent packages. Given a pair (U, P) on the test set, if the package P is found in the set of the packages on which the user U\\u2019 installed packages depend, I ignore the prediction and output 1.0 instead. Although the dependency assumption does not completely hold true (there are known contradictory examples on the training set), the two filters combined increases absolute AUC ~0.004, which matters in a competition with razor-thin margins between leading submissions. I develop the classification training programs for Model 2, 3, and 4 in Python. I use R to explore data, run logistic regression (glm() in the stats library), calculate AUC (performance() in the ROCR library), and plot results (ggplot() in the ggplot2 library). All programs are developed and run on a commodity PC with dual-core 2GHz CPU and 4G RAM. I make the source codes available at github. Although my submissions are based on only the data the contest organizers provide and simple linear models, by no means you should stop here. There are many directions worth exploring, for examples, crawling CRAN to analyze the rich graphs of depends, imports, suggests, and enhances between R packages. In an open-end contest like this, the sky is the limit. Max Lin is a software engineer with Google Research in New York City office, and the tech lead of the Google Prediction API. Prior to Google, he published research work in video content analysis, sentiment analysis, machine learning, and cross-lingual information retrieval. He has a PhD in Computer Science from Carnegie Mellon University. He would like to thank the organizers of the R package recommendation engine competition for their hard work and efforts in putting this competition together. He participates the competition as individuals, and writes all codes in his spare time. Nothing expressed here may or should be attached to his employer. Originally published at blog.kaggle.com on February 22, 2011. Max Lin on finishing second in the R Challenge was originally published in Kaggle Blog on Medium, where people are continuing the conversation by highlighting and responding to this story.\",\"1059\":\"Get free access to over 2500 documentaries on CuriosityStream: https:\\/\\/curiositystream.thld.co\\/zachstarsep24 (use code \\\"zachstar\\\" at sign up) STEMerch Store: https:\\/\\/stemerch.com\\/ Support the Channel: https:\\/\\/www.patreon.com\\/zachstar PayPal(one time donation): https:\\/\\/www.paypal.me\\/ZachStarYT Join this channel to get access to perks: https:\\/\\/www.youtube.com\\/channel\\/UCpCSAcbqs-sjEVfk_hMfY9w\\/join 2D plot software: https:\\/\\/www.desmos.com\\/calculator 3D plot software: https:\\/\\/runiter.com\\/ \\u25baFollow me Instagram: https:\\/\\/www.instagram.com\\/zachstar\\/ Twitter: https:\\/\\/twitter.com\\/ImZachStar Animations: Brainup Studios ( http:\\/\\/brainup.in\\/ ) Check out my Spanish channel here: https:\\/\\/www.youtube.com\\/channel\\/UCnkNu2xQBLASpj6cKC8vtpA \\u25baMy Setup: Space Pictures: https:\\/\\/amzn.to\\/2CC4Kqj Magnetic Floating Globe: https:\\/\\/amzn.to\\/2VgPdn0 Camera: https:\\/\\/amzn.to\\/2RivYu5 Mic: https:\\/\\/amzn.to\\/35bKiri Tripod: https:\\/\\/amzn.to\\/2RgMTNL Equilibrium Tube: https:\\/\\/amzn.to\\/2SowDrh \\u25baCheck out my Amazon Store: https:\\/\\/www.amazon.com\\/shop\\/zachstar\",\"494\":\"In this unusual video, I will introduce you to #DataMuni which is a FREE platform for writing and reading good quality articles about #MachineLearning & #DataScience (and everything else data science like artificial intelligence, deep learning, python, etc.) Visit the platform here: www.DataMuni.com Please subscribe and like the video to help me keep motivated to make awesome videos like this one. :) To buy my book, Approaching (Almost) Any Machine Learning problem, please visit: https:\\/\\/bit.ly\\/buyaaml Follow me on: Twitter: https:\\/\\/twitter.com\\/abhi1thakur LinkedIn: https:\\/\\/www.linkedin.com\\/in\\/abhi1thakur\\/ Kaggle: https:\\/\\/kaggle.com\\/abhishek Instagram: https:\\/\\/instagram.com\\/abhi4ml\",\"2340\":\"Transfer learning aims to improve target learners' performance on target domains by transferring the knowledge contained in different but related source domains. In NLP \\u200b Training the language model in BERT is done by predicting 15% of the tokens in the input, that were randomly picked. These tokens are pre-processed as follows \\u2014 80% are replaced with a \\u201c[MASK]\\u201d token, 10% with a random word, and 10% use the original word. The intuition that led the authors to pick this approach is as follows (Thanks to Jacob Devlin from Google for the insight): If we used [MASK] 100% of the time, the model wouldn\\u2019t necessarily produce good token representations for non-masked words. The non-masked tokens were still used for context, but the model was optimized for predicting masked words. If we used [MASK] 90% of the time and random words 10% of the time, this would teach the model that the observed word is never correct. If we used [MASK] 90% of the time and kept the same word 10% of the time, the model could trivially copy the non-contextual embedding. Source Apart from masking methods, I wondered what other methods could be used to train a large model and use it on downstream tasks. [link] [comments]\",\"4880\":\"I was on @joerogan's podcast yesterday, and had, on air, one of the most special moments of my life: Joe took off his favorite watch and gifted it to me. To have one of my heroes do that means more than I can put into words. I'm not worthy but I'll work hard to live up to it.\",\"260\":\"OpenAI: this code is too dangerous and too complex for you mere mortals. @lucidrains: hold my beer nitter.net\\/lucidrains\\/status\\/1346871881366847488#m\",\"1602\":\"Tegmark is an exceptionally smart & good human\",\"230\":\"\\u201cWould aliens also have X?\\u201d for almost any X tickles the brain a lot. The X that primed it for me just now (again) is stainless steel, but almost any generalization of it works.\",\"1353\":\"#ai #tech #news This soccer camera is operated by an AI to track the ball. However, the AI has an interesting failure mode and repeatedly mixes up the ball with the bald head of a referee. This raises some interesting questions about the role of ethics in AI research. Footage from SPFL Championship : ICTFC 1 v 1 AYR : 24\\/10\\/2020 Links: YouTube: https:\\/\\/www.youtube.com\\/c\\/yannickilcher Twitter: https:\\/\\/twitter.com\\/ykilcher Discord: https:\\/\\/discord.gg\\/4H8xxDF BitChute: https:\\/\\/www.bitchute.com\\/channel\\/yannic-kilcher Minds: https:\\/\\/www.minds.com\\/ykilcher Parler: https:\\/\\/parler.com\\/profile\\/YannicKilcher LinkedIn: https:\\/\\/www.linkedin.com\\/in\\/yannic-kilcher-488534136\\/ If you want to support me, the best thing to do is to share out the content :) If you want to support me financially (completely optional and voluntary, but a lot of people have asked for this): SubscribeStar: https:\\/\\/www.subscribestar.com\\/yannickilcher Patreon: https:\\/\\/www.patreon.com\\/yannickilcher Bitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq Ethereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2 Litecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m Monero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\",\"1457\":\"Tuomas Sandholm is a professor at CMU and co-creator of Libratus, which is the first AI system to beat top human players at the game of Heads-Up No-Limit Texas Hold\\u2019em. He has published over 450 papers on game theory and machine learning, including a best paper in 2017 at NIPS \\/ NeurIPS. His research and companies have had wide-reaching impact in the real world, especially because he and his group not only propose new ideas, but also build systems to prove these ideas work in the real world. Video version is available on YouTube. If you would like to get more\",\"5674\":\"Haven't seen news about it here yet; how did it go for everyone? Did the rebuttal help? Any hopes for getting offline conferences back in 2022? [link] [comments]\",\"1652\":\"This Top 10 ranking is produced by Dr. Roman V. Yampolskiy (University of Louisville) and is based solely on his biased opinion. (To reduce bias University of Louisville is Not Ranked) To a certain degree the ranking is also based on perceived reputation, Google scholar listings under AI Safety, quality and quantity of papers, Google search rankings, impact of publications and number of scholars working in the area full time. Many other universities do work on AI Safety but are not ranked this year. By definition the list excludes all industry labs. Read the full story\",\"1498\":\"Eric Weinstein is a mathematical physicist, podcaster, and intellectual. Please support this podcast by checking out our sponsors: \\u2013 Grammarly: https:\\/\\/grammarly.com\\/lex to get 20% off premium \\u2013 Sun Basket: https:\\/\\/sunbasket.com\\/lex and use code LEX to get $35 off \\u2013 SEMrush: https:\\/\\/www.semrush.com\\/partner\\/lex\\/ to get a free month of Guru \\u2013 ExpressVPN: https:\\/\\/expressvpn.com\\/lexpod and use code LexPod to get 3 months free EPISODE LINKS: Eric\\u2019s Twitter: https:\\/\\/twitter.com\\/EricRWeinstein Eric\\u2019s YouTube: https:\\/\\/www.youtube.com\\/ericweinsteinphd The Portal podcast: https:\\/\\/podcasts.apple.com\\/us\\/podcast\\/the-portal\\/id1469999563 PODCAST INFO: Podcast website: https:\\/\\/lexfridman.com\\/podcast Apple Podcasts: https:\\/\\/apple.co\\/2lwqZIr Spotify: https:\\/\\/spoti.fi\\/2nEwCF8 RSS: https:\\/\\/lexfridman.com\\/feed\\/podcast\\/ YouTube Full Episodes: https:\\/\\/youtube.com\\/lexfridman YouTube Clips: https:\\/\\/youtube.com\\/lexclips SUPPORT & CONNECT: \\u2013 Check out the sponsors above,\",\"4275\":\"According to Yann Le Cun, the next big thing in machine learning is unsupervised learning. Self-supervision has changed the entire game in the last few years in deep learning, first transforming the language world with word2vec and BERT -- but now it's turning computer vision upside down. This week Yannic, Connor and I spoke with one of the authors, Aravind Srinivas who recently co-led the hot-off-the-press CURL: Contrastive Unsupervised Representations for Reinforcement Learning alongside Michael (Misha) Laskin. CURL has had an incredible reception in the ML community in the last month or so. Remember the Deep Mind paper which solved the Atari games using the raw pixels? Aravind's approach uses contrastive unsupervised learning to featurise the pixels before applying RL. CURL is the first image-based algorithm to nearly match the sample-efficiency and performance of methods that use state-based features! This is a huge step forwards in being able to apply RL in the real world. We explore RL and self-supervision for computer vision in detail and find out about how Aravind got into machine learning. Original YouTube Video: https:\\/\\/youtu.be\\/1MprzvYNpY8 Paper: CURL: Contrastive Unsupervised Representations for Reinforcement Learning Aravind Srinivas, Michael Laskin, Pieter Abbeel https:\\/\\/arxiv.org\\/pdf\\/2004.04136.pdf Yannic's analysis video: https:\\/\\/www.youtube.com\\/watch?v=hg2Q_O5b9w4 #machinelearning #reinforcementlearning #curl #timscarfe #yannickilcher #connorshorten Music credit; https:\\/\\/soundcloud.com\\/errxrmusic\\/in-my-mind\",\"4285\":\"This week we had a super insightful conversation with Jordan Edwards, Principal Program Manager for the AzureML team! Jordan is on the coalface of turning machine learning software engineering into a reality for some of Microsoft's largest customers. ML DevOps is all about increasing the velocity of- and orchastrating the non-interactive phase of- software deployments for ML. We cover ML DevOps and Microsoft Azure ML. We discuss model governance, testing, intepretability, tooling. We cover the age-old discussion of the dichotomy between science and engineering and how you can bridge the gap with ML DevOps. We cover Jordan's maturity model for ML DevOps. We also cover off some of the exciting ML announcments from the recent Microsoft Build conference i.e. FairLearn, IntepretML, SEAL, WhiteNoise, OpenAI code generation, OpenAI GPT-3. 00:00:04 Introduction to ML DevOps and Microsoft Build ML Announcements 00:10:29 Main show kick-off 00:11:06 Jordan's story 00:14:36 Typical ML DevOps workflow 00:17:38 Tim's articulation of ML DevOps 00:19:31 Intepretability \\/ Fairness 00:24:31 Testing \\/ Robustness 00:28:10 Using GANs to generate testing data 00:30:26 Gratuitous DL? 00:33:46 Challenges of making an ML DevOps framework \\/ IaaS 00:38:48 Cultural battles in ML DevOps 00:43:04 Maturity Model for Ml DevOps 00:49:19 ML: High interest credit card of technical debt paper 00:50:19 ML Engineering at Microsoft 01:01:20 ML Flow 01:03:05 Company-wide governance 01:08:15 What's coming next 01:12:10 Jordan's hillarious piece of advice for his younger self Super happy with how this turned out, this is not one to miss folks! #deeplearning #machinelearning #devops #mldevops\",\"1539\":\"James Gosling is the founder and lead designer of the Java programming language. Please check out our sponsors to get a discount and to support this podcast: \\u2013 Public Goods: https:\\/\\/publicgoods.com\\/lex and use code LEX \\u2013 BetterHelp: https:\\/\\/betterhelp.com\\/lex \\u2013 ExpressVPN: https:\\/\\/www.expressvpn.com\\/lexpod If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/podcast or connect with @lexfridman on Twitter, LinkedIn, Facebook, Medium, or YouTube where you can watch the video versions of these conversations. If you enjoy the podcast, please rate it 5 stars on Apple Podcasts, follow on Spotify, or support it on Patreon. Here\\u2019s the outline of\",\"1641\":\"This post provides a short technical overview of Nevermined\\u2019s capabilities Read the full story\",\"1603\":\"Here's my conversation with Avi Loeb, an astrophysicist at Harvard who argues that the Oumuamua interstellar object that passed Earth in 2017 may be alien technology. We talk about aliens, black holes, and space exploration. This was truly fascinating. invidious.snopyta.org\\/watch?v=plcc6E-E\\u2026\",\"1866\":\"We\\u2019re ironically searching for counterexamples to the Riemann Hypothesis. Setting up Pytest Adding a Database Search Strategies Unbounded Integers In this article we\\u2019ll deploy the application on a server, so that it can search for RH counterexamples even when I close my laptop. Servers and containers When deploying applications to servers, reproducibility is crucial. You don\\u2019t want your application to depend on the details of the computer it\\u2019s running on. This is a higher-level version of the same principle behind Python virtual environments, but it applies to collections of programs, possibly written in different languages and running on different computers. In our case, we have a postgres database, the pgmp extension, the populate_database program, and plans for a web server. The principle of an application not depending on the system it\\u2019s running on is called hermeticity (the noun form of hermetic, meaning air-tight). Hermeticity is good for the following reasons. When a server crashes, you don\\u2019t have to remember what you did to set it up. Instead you run a build\\/install that works on any machine. Newcomers also don\\u2019t have to guess what unknown aspects of a running server are sensitive. Another benefit is that you can test it on your local machine identically to how it will run in production. It also allows you to easily migrate from one cloud provider to another, which allows you to defer expensive commitments until you have more information about your application\\u2019s needs. Finally, if you have multiple applications running on the same server, you don\\u2019t want to have their needs conflict with each other, which can happen easily if two applications have dependencies that transitively depend on different versions of the same software. This is called \\u201cdependency hell.\\u201d In all of these, you protect yourself from becoming dependent on arbitrary choices you made before you knew better. One industry-strength approach to hermeticity is to use containers. A container is a virtual machine devoted to running a single program, with explicitly-defined exposure to the outside world. We will set up three containers: one to run the database, one for the search application, and (later) one for a web server. We\\u2019ll start by deploying them all on the same machine, but could also deploy them to different machines. Docker is a popular containerization system. Before going on, I should stress that Docker, while I like it, is not sacred by any means. In a decade Docker may disappear, but the principle of hermeticity and the need for reproducible deployments will persist. Docker allows you to describe your container by first starting from an existing (trusted) container, such as one that has an operating system and postgres already installed, and extend it for your application. This includes installing dependencies, fetching the application code from git, copying files into the container from the host system, exposing the container\\u2019s network ports, and launching the application. You save the commands that accomplish that in a Dockerfile with some special syntax. To deploy it, you copy the Dockerfile to the server (say, via git) and run docker commands to launch the container. You only have to get the Dockerfile right once, you can test it locally, and then it will work on any server just the same. The only caveat I\\u2019ve seen here is that if you migrate to a server with a different processor architecture, the install script (in our case, pip install numba) may fail to find a pre-compiled binary for the target architecture, and it may fall back to compiling from source, which can add additional requirements or force you to change which OS your container is derived from. This reduces our \\u201cset up a new server\\u201d script to just a few operations: (1) install docker (2) fetch the repository (3) launch the docker containers from their respective Dockerfiles. In my experience, writing a Dockerfile is no small task, but figuring out how to install stuff is awful in all cases, and doing it for Docker gives you an artifact tracing the steps, and a reasonable expectation of not having to do it again. Thankfully, you dear readers can skip my head-banging and see the Dockerfiles after I figured it out. The Postgres Dockerfile This commit adds a Dockerfile for the database, and makes some small changes to the project to allow it to run. It has only 15 lines, but it took me a few hours to figure out. The process was similar to installing confusing software on your own machine: try to install, see some error like \\\"missing postgres.h\\u201c, go hunt around on the internet to figure out what you have to install to get past the error, and repeat. Let\\u2019s go through each line of the Dockerfile. FROM postgres:12 The first line defines the container image that this container starts from, which is officially maintained by the Postgres team. Looking at their Dockerfile, it starts from debian:buster-slim, which is a Debian Linux instance that is \\u201cslimmed\\u201d down to be suitable for docker containers, meaning it has few packages pre-installed. Most importantly, \\u201cDebian\\u201d tells us what package manager to use (apt-get) in our Dockerfile. It\\u2019s also worth noting at this point that, when docker builds the container, each command in a docker file results in a new image. An image is a serialized copy of all the data in a docker container, so that it can be started or extended easily. And if you change a line halfway through your Dockerfile, docker only has to rebuild images from that step onward. You can publish images on the web, and other docker users can use them as a base. This is like forking a project on Github, and is exactly what happens when Docker executes FROM postgres:12. ENV POSTGRES_USER docker ENV POSTGRES_PASSWORD docker ENV POSTGRES_DB divisor These lines declare configuration for the database that the base postgres image will create when the container is started. The variable names are described in the \\u201cEnvironment Variables\\u201d section of the Postgres image\\u2019s documentation. The ENV command tells docker to instantiate environment variables (like the PATH variable in a terminal shell), that running programs can access. I\\u2019m insecurely showing the password and username here because the server the docker containers will run on won\\u2019t yet expose anything to the outside world. Later in this post you will see how to pass an environment variable from the docker command line when the container is run, and you would use something close to that to set configuration secrets securely. RUN apt-get update \\\\ && apt-get install -y pgxnclient build-essential libgmp3-dev postgresql-server-dev-12 libmpc-dev The RUN command allows you to run any shell command you\\u2019d like, in this case a command to update apt and install the dependencies needed to build the pgmp extension. This includes gcc and make via build-essential, and the gmp-specific libraries. RUN apt-get install -y python3.7 python3-setuptools python3-pip python-pip python3.7-dev \\\\ && pip3 install wheel \\\\ && pip install six Next we do something a bit strange. We install python3.7 and pip (because we will need to pip3 install our project\\u2019s requirements.txt), but also python2\\u2019s pip. Here\\u2019s what\\u2019s going on. The pgmp postgres extension needs to be built from source, and it has a dependency on python2.7 and the python2-six library. So the first RUN line here installs all the python-related tools we need. RUN pgxn install pgmp Then we install the pgmp extension. COPY . \\/divisor WORKDIR \\\"\\/divisor\\\" These next two lines copy the current directory on the host machine to the container\\u2019s file system, and sets the working directory for all future commands to that directory. Note that whenever the contents of our project change, docker needs to rebuild the image from this step because any subsequent steps like pip install -r requirements.txt might have a different outcome. RUN python3 -m pip install --upgrade pip RUN pip3 install -r requirements.txt Next we upgrade pip (which is oddly required for the numba dependency, though I can\\u2019t re-find the Github issue where I discovered this) and install the python dependencies for the project. The only reason this is required is because we included the database schema setup in the python script riemann\\/postgres_batabase.py. So this makes the container a bit more complicated than absolutely necessary. It can be improved later if need be. ENV PGUSER=docker ENV PGPASSWORD=docker ENV PGDATABASE=divisor These next lines are environment variables used by the psycopg2 python library to infer how to connect to postgres if no database spec is passed in. It would be nice if this was shared with the postgres environment variables, but duplicating it is no problem. COPY setup_schema.sh \\/docker-entrypoint-initdb.d\\/ The last line copies a script to a special directory specified by the base postgres Dockerfile. The base dockerfile specifies that any scripts in this directory will be run when the container is started up. In our case, we just call the (idempotent) command to create the database. In a normal container we might specify a command to run when the container is started (our search container, defined next, will do this), but the postgres base image handles this for us by starting the postgres database and exposing the right ports. Finally we can build and run the container docker build -t divisordb -f divisordb.Dockerfile . # ... lots of output ... docker run -d -p 5432:5432 --name divisordb divisordb:latest After the docker build command\\u2014which will take a while\\u2014you will be able to see the built images by running docker images, and the final image will have a special tag divisordb. The run command additionally tells docker to run the container as a daemon (a.k.a. in the background) with -d and to -p to publish port 5432 on the host machine and map it to 5432 on the container. This allows external programs and programs on other computers to talk to the container by hitting 0.0.0.0:5432. It also allows other containers to talk to this container, but as we\\u2019ll see shortly that requires a bit more work, because inside a container 0.0.0.0 means the container, not the host machine. Finally, one can run the following code on the host machine to check that the database is accepting connections. pg_isready --host 0.0.0.0 --username docker --port 5432 --dbname divisor If you want to get into the database to run queries, you can run psql with the same flags as pg_isready, or manually enter the container with docker exec -it divisordb bash and run psql from there. psql --host 0.0.0.0 --username docker --port 5432 --dbname divisor Password for user docker: docker divisor=# \\\\d List of relations Schema | Name | Type | Owner --------+--------------------+-------+-------- public | riemanndivisorsums | table | docker public | searchmetadata | table | docker (2 rows) Look at that. You wanted to disprove the Riemann Hypothesis, and here you are running docker containers. The Search Container Next we\\u2019ll add a container for the main search application. Before we do this, it will help to make the main entry point to the program a little bit simpler. This commit modifies populate_database.py\\u2018s main routine to use argparse and some sensible defaults. Now we can run the application with just python -m riemann.populate_database. Then the Dockerfile for the search part is defined in this commit. I\\u2019ve copied it below. It\\u2019s much simpler than the database, but somehow took just as long for me to build as the database Dockerfile, because I originally chose a base image called \\u201calpine\\u201d that is (unknown to me at the time) really bad for Python if your dependencies have compiled C code, like numba does. FROM python:3.7-slim-buster RUN apt-get update \\\\ && apt-get install -y build-essential libgmp3-dev libmpc-dev COPY . \\/divisor WORKDIR \\\"\\/divisor\\\" RUN pip3 install -r requirements.txt ENV PGUSER=docker ENV PGPASSWORD=docker ENV PGDATABASE=divisor ENTRYPOINT [\\\"python3\\\", \\\"-m\\\", \\\"riemann.populate_database\\\"] The base image is again Debian, with Python3.7 pre-installed. Then we can build it and (almost) run it docker build -t divisorsearch -f divisorsearch.Dockerfile . docker run -d --name divisorsearch --env PGHOST=\\\"$PGHOST\\\" divisorsearch:latest What\\u2019s missing here is the PGHOST environment variable, which psycopg2 uses to find the database. The problem is, inside the container \\u201clocalhost\\u201d and 0.0.0.0 are interpreted by the operating system to mean the container itself, not the host machine. To get around this problem, docker maintains IP addresses for each docker container, and uses those to route network requests between containers. The docker inspect command exposes information about this. Here\\u2019s a sample of the output $ docker inspect divisordb [ { \\\"Id\\\": \\\"f731a78bde50be3de1d77ae1cff6d23c7fe21d4dbe6a82b31332c3ef3f6bbbb4\\\", \\\"Path\\\": \\\"docker-entrypoint.sh\\\", \\\"Args\\\": [ \\\"postgres\\\" ], \\\"State\\\": { \\\"Status\\\": \\\"running\\\", \\\"Running\\\": true, \\\"Paused\\\": false, ... }, ... \\\"NetworkSettings\\\": { ... \\\"Ports\\\": { \\\"5432\\/tcp\\\": [ { \\\"HostIp\\\": \\\"0.0.0.0\\\", \\\"HostPort\\\": \\\"5432\\\" } ] }, ... \\\"IPAddress\\\": \\\"172.17.0.2\\\", ... } } ] The part that matters for us is the ip address, and the following extracts it to the environment variable PGHOST. export PGHOST=$(docker inspect -f \\\"{{ .NetworkSettings.IPAddress }}\\\" divisordb) Once the two containers are running\\u2014see docker ps for the running containers, docker ps -a to see any containers that were killed due to an error, and docker logs to see the container\\u2019s logged output\\u2014you can check the database to see it\\u2019s being populated. divisor=# select * from SearchMetadata order by start_time desc limit 10; start_time | end_time | search_state_type | starting_search_state | ending_search_state ----------------------------+----------------------------+-------------------------------+-----------------------+--------------------- 2020-12-27 03:10:01.256996 | 2020-12-27 03:10:03.594773 | SuperabundantEnumerationIndex | 29,1541 | 31,1372 2020-12-27 03:09:59.160157 | 2020-12-27 03:10:01.253247 | SuperabundantEnumerationIndex | 26,705 | 29,1541 2020-12-27 03:09:52.035991 | 2020-12-27 03:09:59.156464 | SuperabundantEnumerationIndex | 1,0 | 26,705 Ship it! I have an AWS account, so let\\u2019s use Amazon for this. Rather than try the newfangled beanstalks or lightsails or whatever AWS-specific frameworks they\\u2019re trying to sell, for now I\\u2019ll provision a single Ubuntu EC2 server and run everything on it. I picked a t2.micro for testing (which is free). There\\u2019s a bit of setup to configure and launch the server\\u2014such as picking the server image, downloading an ssh key, and finding the IP address. I\\u2019ll skip those details since they are not (yet) relevant to the engineering process. Once I have my server, I can ssh in, install docker, git clone the project, and run the deploy script. # install docker, see get.docker.com curl -fsSL https:\\/\\/get.docker.com -o get-docker.sh sudo sh get-docker.sh sudo usermod -aG docker ubuntu # log out and log back in git clone https:\\/\\/github.com\\/j2kun\\/riemann-divisor-sum && cd riemann-divisor-sum bash deploy.sh And it works! Sadly, within an hour the divisorsearch container crashes because the instance runs out of RAM and CPU. Upgrading to a t2.medium (4 GiB RAM), it goes for about 2 hours before exhausting RAM. We could profile it and find the memory hotspots, but instead let\\u2019s apply a theorem due to billionaire mathematician Jim Simons: throw money at the problem. Upgrading to a r5.large (16 GiB RAM), and it runs comfortably all day. Four days later, logging back into the VM and and I notice things are sluggish, even though the docker instance isn\\u2019t exhausting the total available RAM or CPU. docker stats also shows low CPU usage on divisorsearch. The database shows that it has only got up to 75 divisors, which is just as far as it got when I ran it (not in Docker) on my laptop for a few hours in the last article. Something is amiss, and we\\u2019ll explore what happened next time. Notes A few notes on improvements that didn\\u2019t make it into this article. In our deployment, we rebuild the docker containers each time, even when nothing changes. What one could do instead is store the built images in what\\u2019s called a container registry, and pull them instead of re-building them on every deploy. This would only save us a few minutes of waiting, but is generally good practice. We could also use docker compose and a corresponding configuration file to coordinate launching a collection of containers that have dependencies on each other. For our case, the divisorsearch container depended on the divisordb container, and our startup script added a sleep 5 to ensure the latter was running before starting the former. docker compose would automatically handle that, as well as the configuration for naming, resource limits, etc. With only two containers it\\u2019s not that much more convenient, given that docker compose is an extra layer of indirection to learn that hides the lower-level commands. In this article we deployed a single database container and a single \\u201csearch\\u201d container. Most of the time the database container is sitting idle while the search container does its magic. If we wanted to scale up, an obvious way would be to have multiple workers. But it would require some decent feature work. A sketch: reorganize the SearchMetadata table so that it contains a state attribute, like \\u201cnot started\\u201d, \\u201cstarted\\u201d, or \\u201cfinished,\\u201d then add functionality so that a worker (atomically) asks for the oldest \\u201cnot started\\u201d block and updates the row\\u2019s state to \\u201cstarted.\\u201d When a worker finishes a block, it updates the database and marks the block as finished. If no \\u201cnot started\\u201d blocks are found, the worker proceeds to create some number of new \\u201cnot started\\u201d blocks. There are details to be ironed out around race conditions between multiple workers, but Postgres is designed to make such things straightforward. Finally, we could reduce the database size by keeping track of a summary of a search block instead of storing all the data in the block. For example, we could record the n and witness_value corresponding to the largest witness_value in a block, instead of saving every n and every witness_value. In order for this to be usable\\u2014i.e., for us to be able to say \\u201cwe checked all possible\",\"1486\":\"Oriol Vinyals is a senior research scientist at Google DeepMind. Before that he was at Google Brain and Berkeley. His research has been cited over 39,000 times. He is one of the most brilliant and impactful minds in the field of deep learning. He is behind some of the biggest papers and ideas in AI, including sequence to sequence learning, audio generation, image captioning, neural machine translation, and reinforcement learning. He is a co-lead (with David Silver) of the AlphaStar project, creating an agent that defeated a top professional at the game of StarCraft. If you would like to get\",\"772\":\"Max Tegmark is a physicist and AI researcher at MIT. Please support this podcast by checking out our sponsors: - The Jordan Harbinger Show: https:\\/\\/www.jordanharbinger.com\\/lex\\/ - Four Sigmatic: https:\\/\\/foursigmatic.com\\/lex and use code LexPod to get up to 60% off - BetterHelp: https:\\/\\/betterhelp.com\\/lex to get 10% off - ExpressVPN: https:\\/\\/expressvpn.com\\/lexpod and use code LexPod to get 3 months free EPISODE LINKS: News Project Explainer Video: https:\\/\\/www.youtube.com\\/watch?v=PRLF17Pb6vo News Project Website: https:\\/\\/www.improvethenews.org\\/ Max's Twitter: https:\\/\\/twitter.com\\/tegmark Max's Website: https:\\/\\/space.mit.edu\\/home\\/tegmark\\/ Future of Life Institute: https:\\/\\/futureoflife.org\\/ Lex Fridman Podcast #1: https:\\/\\/www.youtube.com\\/watch?v=Gi8LUnhP5yU PODCAST INFO: Podcast website: https:\\/\\/lexfridman.com\\/podcast Apple Podcasts: https:\\/\\/apple.co\\/2lwqZIr Spotify: https:\\/\\/spoti.fi\\/2nEwCF8 RSS: https:\\/\\/lexfridman.com\\/feed\\/podcast\\/ Full episodes playlist: https:\\/\\/www.youtube.com\\/playlist?list=PLrAXtmErZgOdP_8GztsuKi9nrraNbKKp4 Clips playlist: https:\\/\\/www.youtube.com\\/playlist?list=PLrAXtmErZgOeciFP3CBCIEElOJeitOr41 OUTLINE: 0:00 - Introduction 2:49 - AI and physics 16:07 - Can AI discover new laws of physics? 24:57 - AI safety 42:33 - Extinction of human species 53:31 - How to fix fake news and misinformation 1:15:05 - Autonomous weapons 1:30:28 - The man who prevented nuclear war 1:40:36 - Elon Musk and AI 1:54:14 - AI alignment 2:00:16 - Consciousness 2:09:20 - Richard Feynman 2:13:30 - Machine learning and computational physics 2:24:28 - AI and creativity 2:35:42 - Aliens 2:51:25 - Mortality CONNECT: - Subscribe to this YouTube channel - Twitter: https:\\/\\/twitter.com\\/lexfridman - LinkedIn: https:\\/\\/www.linkedin.com\\/in\\/lexfridman - Facebook: https:\\/\\/www.facebook.com\\/LexFridmanPage - Instagram: https:\\/\\/www.instagram.com\\/lexfridman - Medium: https:\\/\\/medium.com\\/@lexfridman - Support on Patreon: https:\\/\\/www.patreon.com\\/lexfridman\",\"1683\":\"In this article, I look into some of the shortages of event-driven programming and suggest behavior trees as an effective alternative, suitable for back\\/front-end application development. Read the full story\",\"1604\":\"Every time.\",\"1878\":\"The coefficient of variation (CV) is a relative measure of variability that indicates the size of a standard deviation in relation to its mean. It is a standardized, unitless measure that allows you to compare variability between disparate groups and characteristics. It is also known as the relative standard deviation (RSD). In this post, you [\\u2026] The post Coefficient of Variation in Statistics appeared first on Statistics By Jim.\",\"4676\":\"I am currently choosing a task for my bachelor thesis. Do you have any tips? Really anything (preferably rather complex and an unexplored question) within machine learning is good! :) [link] [comments]\",\"1051\":\"Get free access to over 2500 documentaries on CuriosityStream: https:\\/\\/curiositystream.thld.co\\/zachstarnov3 (use code \\\"zachstar\\\" at sign up) STEMerch Store: https:\\/\\/stemerch.com\\/ Support the Channel: https:\\/\\/www.patreon.com\\/zachstar PayPal(one time donation): https:\\/\\/www.paypal.me\\/ZachStarYT Graphing Software: https:\\/\\/www.desmos.com\\/calculator See this video for complex mapping files: https:\\/\\/www.youtube.com\\/watch?v=JGDwdwFS9fM Join this channel to get access to perks: https:\\/\\/www.youtube.com\\/channel\\/UCpCSAcbqs-sjEVfk_hMfY9w\\/join \\u25baFollow me Instagram: https:\\/\\/www.instagram.com\\/zachstar\\/ Twitter: https:\\/\\/twitter.com\\/ImZachStar Check out my Spanish channel here: https:\\/\\/www.youtube.com\\/channel\\/UCnkNu2xQBLASpj6cKC8vtpA \\u25baMy Setup: Space Pictures: https:\\/\\/amzn.to\\/2CC4Kqj Magnetic Floating Globe: https:\\/\\/amzn.to\\/2VgPdn0 Camera: https:\\/\\/amzn.to\\/2RivYu5 Mic: https:\\/\\/amzn.to\\/35bKiri Tripod: https:\\/\\/amzn.to\\/2RgMTNL Equilibrium Tube: https:\\/\\/amzn.to\\/2SowDrh \\u25baCheck out my Amazon Store: https:\\/\\/www.amazon.com\\/shop\\/zachstar\",\"4278\":\"We welcome Zak Jost from the WelcomeAIOverlords channel. Zak is an ML research scientist at Amazon. He has a great blog at http:\\/\\/blog.zakjost.com and also a Discord channel at https:\\/\\/discord.gg\\/xh2chKX WelcomeAIOverlords: https:\\/\\/www.youtube.com\\/channel\\/UCxw9_WYmLqlj5PyXu2AWU_g 00:00:00 INTRO START 00:01:07 MAIN SHOW START 00:01:59 ZAK'S STORY 00:05:06 YOUTUBE DISCUSSION 00:24:12 UNDERSTANDING PAPERS 00:29:53 CONTRASTIVE LEARNING INTRO 00:33:00 BRING YOUR OWN LATENT PAPER 01:03:13 GRAPHS IN ML AND KNOWLEDGE GRAPHS 01:21:36 GRAPH USE CASES - FRAUD 01:30:15 KNOWLEDGE GRAPHS 01:34:22 GRAPHS IN ML 01:38:53 AUTOMATED ML 01:57:32 OUTRO\",\"2324\":\"Abstract \\u2014 Transformers have become widely used for language modeling and sequence learning tasks, and are one of the most important machine learning workloads today. Training one is a very compute-intensive task, often taking days or weeks, and significant attention has been given to optimizing transformers. Despite this, existing implementations do not efficiently utilize GPUs. We find that data movement is the key bottleneck when training. Due to Amdahl\\u2019s Law and massive improvements in compute performance, training has now become memory-bound. Further, existing frameworks use suboptimal data layouts. Using these insights, we present a recipe for globally optimizing data movement in transformers. We reduce data movement by up to 22.91% and overall achieve a 1.30\\u00d7 performance improvement over state-of-the-art frameworks when training BERT. Our approach is applicable more broadly to optimizing deep neural networks, and offers insight into how to tackle emerging performance bottlenecks. Quote \\u2014 \\\"For the GPT-3 transformer model with a training cost of $12M, our optimizations could save $3.6M and more than 120 MWh energy.\\\" Link \\u2014 https:\\/\\/arxiv.org\\/abs\\/2007.00072 Some great work in analyzing the dataflow in the BERT encoder architecture using SDFGs and the DaCe environment, and optimizing it via CUDA kernels. https:\\/\\/preview.redd.it\\/aqetnmob28c61.png?width=537&format=png&auto=webp&s=0dd6d0e9fbcccc0e86d993111ca9701203f97403 Also, Multi-head attention performance can be extended to other research domains. [link] [comments]\",\"4308\":\"Three YouTubers; Tim Scarfe - Machine Learning Dojo (https:\\/\\/www.youtube.com\\/channel\\/UCXvHuBMbgJw67i5vrMBBobA), Connor Shorten - Henry AI Labs (https:\\/\\/www.youtube.com\\/channel\\/UCHB9VepY6kYvZjj0Bgxnpbw) and Yannic Kilcher (https:\\/\\/www.youtube.com\\/channel\\/UCZHmQk67mSJgfCCTn7xBfew). We made a new YouTube channel called Machine Learning Street Talk. Every week we will talk about the latest and greatest in AI. Subscribe now! Special guests this week; Dr. Mathew Salvaris (https:\\/\\/www.linkedin.com\\/in\\/drmathewsalvaris\\/), Eric Craeymeersch (https:\\/\\/www.linkedin.com\\/in\\/ericcraeymeersch\\/), Dr. Keith Duggar (https:\\/\\/www.linkedin.com\\/in\\/dr-keith-duggar\\/), Dmitri Soshnikov (https:\\/\\/www.linkedin.com\\/in\\/shwars\\/) We discuss the new concept of an open-ended, or \\\"AI-Generating\\\" algorithm. Open-endedness is a class of algorithms which generate problems and solutions to increasingly complex and diverse tasks. These algorithms create their own curriculum of learning. Complex tasks become tractable because they are now the final stepping stone in a lineage of progressions. In many respects, it's better to trust the machine to develop the learning curriculum, because the best curriculum might be counter-intuitive. These algorithms can generate a radiating tree of evolving challenges and solutions just like natural evolution. Evolution has produced an eternity of diversity and complexity and even produced human intelligence as a side-effect! Could AI-generating algorithms be the next big thing in machine learning? Wang, Rui, et al. \\\"Enhanced POET: Open-Ended Reinforcement Learning through Unbounded Invention of Learning Challenges and their Solutions.\\\" arXiv preprint arXiv:2003.08536 (2020). https:\\/\\/arxiv.org\\/abs\\/2003.08536 Wang, Rui, et al. \\\"Paired open-ended trailblazer (poet): Endlessly generating increasingly complex and diverse learning environments and their solutions.\\\" arXiv preprint arXiv:1901.01753 (2019). https:\\/\\/arxiv.org\\/abs\\/1901.01753 Watch Yannic\\u2019s video on POET: https:\\/\\/www.youtube.com\\/watch?v=8wkgDnNxiVs and on the extended POET: https:\\/\\/youtu.be\\/gbG1X8Xq-T8 Watch Connor\\u2019s video https:\\/\\/www.youtube.com\\/watch?v=jxIkPxkN10U UberAI labs video: https:\\/\\/www.youtube.com\\/watch?v=RX0sKDRq400 #reinforcementlearning #machinelearning #uber #deeplearning #rl #timscarfe #connorshorten #yannickilcher\",\"3762\":\"This might be a stupid question, but if there is an answer to this it would really help a lot. Can we sort the weights or parameters of each layer in a deep neural net based on their importance, as in how much they contribute towards the performance of the model on a task given an evaluation metric. [link] [comments]\",\"1355\":\"#memes #science #ai Antonio and I critique the creme de la creme of Deep Learning memes. Music: Sunshower - LATASH\\u00c1 Papov - Yung Logos Sunny Days - Anno Domini Beats Trinity - Jeremy Blake More memes: facebook.com\\/convolutionalmemes Links: YouTube: https:\\/\\/www.youtube.com\\/c\\/yannickilcher Twitter: https:\\/\\/twitter.com\\/ykilcher Discord: https:\\/\\/discord.gg\\/4H8xxDF BitChute: https:\\/\\/www.bitchute.com\\/channel\\/yannic-kilcher Minds: https:\\/\\/www.minds.com\\/ykilcher Parler: https:\\/\\/parler.com\\/profile\\/YannicKilcher LinkedIn: https:\\/\\/www.linkedin.com\\/in\\/yannic-kilcher-488534136\\/ If you want to support me, the best thing to do is to share out the content :) If you want to support me financially (completely optional and voluntary, but a lot of people have asked for this): SubscribeStar: https:\\/\\/www.subscribestar.com\\/yannickilcher Patreon: https:\\/\\/www.patreon.com\\/yannickilcher Bitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq Ethereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2 Litecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m Monero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\",\"1606\":\"Social media & the press are currently incentivized to drastically exaggerate narratives of division. This in turn creates more division & the downward spiral continues. I hope to build tech that changes these incentives. I believe there is much more love than hate in the world.\",\"1634\":\"Read the full story\",\"1616\":\"The world\\u2019s preeminent cyber startup foundry DataTribe selects Scanta as one of three companies worldwide to compete for the chance at $2M in seed capital. Read the full story\",\"1617\":\"Read the full story\",\"251\":\"Temporarily breaking out of my Twitter-minimization for a short thread on issues around free speech and the mass deplatformings of the last week. Obviously the riots were terrible, people still supporting DT are crazy, so moving on to some things that have not yet been said...\",\"1135\":\"Author and economist Tim Harford talking statistics - more from this interview at https:\\/\\/youtu.be\\/NOTN2FsdUHQ More video links & book stuff in full description below \\u2193\\u2193\\u2193 Buy Tim's new book \\\"How To Make The World Add Up\\\" (SIGNED COPIES at no extra cost) from Maths Gear: http:\\/\\/bit.ly\\/World_Add_Up Tim Harford books on Amazon: https:\\/\\/amzn.to\\/3lbt2vt And Tim's own website: https:\\/\\/timharford.com Also catch Tim in this recent Standupmaths video: https:\\/\\/youtu.be\\/esC4HB-AjgI Numberphile is supported by the Mathematical Sciences Research Institute (MSRI): http:\\/\\/bit.ly\\/MSRINumberphile We are also supported by Science Sandbox, a Simons Foundation initiative dedicated to engaging everyone with the process of science. https:\\/\\/www.simonsfoundation.org\\/outreach\\/science-sandbox\\/ And support from Math For America - https:\\/\\/www.mathforamerica.org\\/ NUMBERPHILE Website: http:\\/\\/www.numberphile.com\\/ Numberphile on Facebook: http:\\/\\/www.facebook.com\\/numberphile Numberphile tweets: https:\\/\\/twitter.com\\/numberphile Subscribe: http:\\/\\/bit.ly\\/Numberphile_Sub Videos by Brady Haran Animation by Pete McPartlan Patreon: http:\\/\\/www.patreon.com\\/numberphile Numberphile T-Shirts and Merch: https:\\/\\/teespring.com\\/stores\\/numberphile Brady's videos subreddit: http:\\/\\/www.reddit.com\\/r\\/BradyHaran\\/ Brady's latest videos across all channels: http:\\/\\/www.bradyharanblog.com\\/ Sign up for (occasional) emails: http:\\/\\/eepurl.com\\/YdjL9\",\"1657\":\"Facial recognition, is one of the largest areas of research within computer vision. This article will introduce 5 face recognition papers for data scientists. Read the full story\",\"1350\":\"#ai #research #attention Transformers, having already captured NLP, have recently started to take over the field of Computer Vision. So far, the size of images as input has been challenging, as the Transformers' Attention Mechanism's memory requirements grows quadratic in its input size. LambdaNetworks offer a way around this requirement and capture long-range interactions without the need to build expensive attention maps. They reach a new state-of-the-art in ImageNet and compare favorably to both Transformers and CNNs in terms of efficiency. OUTLINE: 0:00 - Introduction & Overview 6:25 - Attention Mechanism Memory Requirements 9:30 - Lambda Layers vs Attention Layers 17:10 - How Lambda Layers Work 31:50 - Attention Re-Appears in Lambda Layers 40:20 - Positional Encodings 51:30 - Extensions and Experimental Comparisons 58:00 - Code Paper: https:\\/\\/openreview.net\\/forum?id=xTJEN-ggl1b Lucidrains' Code: https:\\/\\/github.com\\/lucidrains\\/lambda-networks Abstract: We present a general framework for capturing long-range interactions between an input and structured contextual information (e.g. a pixel surrounded by other pixels). Our method, called the lambda layer, captures such interactions by transforming available contexts into linear functions, termed lambdas, and applying these linear functions to each input separately. Lambda layers are versatile and may be implemented to model content and position-based interactions in global, local or masked contexts. As they bypass the need for expensive attention maps, lambda layers can routinely be applied to inputs of length in the thousands, en-abling their applications to long sequences or high-resolution images. The resulting neural network architectures, LambdaNetworks, are computationally efficient and simple to implement using direct calls to operations available in modern neural network libraries. Experiments on ImageNet classification and COCO object detection and instance segmentation demonstrate that LambdaNetworks significantly outperform their convolutional and attentional counterparts while being more computationally efficient. Finally, we introduce LambdaResNets, a family of LambdaNetworks, that considerably improve the speed-accuracy tradeoff of image classification models. LambdaResNets reach state-of-the-art accuracies on ImageNet while being \\u223c4.5x faster than the popular EfficientNets on modern machine learning accelerators. Authors: Anonymous Links: YouTube: https:\\/\\/www.youtube.com\\/c\\/yannickilcher Twitter: https:\\/\\/twitter.com\\/ykilcher Discord: https:\\/\\/discord.gg\\/4H8xxDF BitChute: https:\\/\\/www.bitchute.com\\/channel\\/yannic-kilcher Minds: https:\\/\\/www.minds.com\\/ykilcher Parler: https:\\/\\/parler.com\\/profile\\/YannicKilcher LinkedIn: https:\\/\\/www.linkedin.com\\/in\\/yannic-kilcher-488534136\\/ If you want to support me, the best thing to do is to share out the content :) If you want to support me financially (completely optional and voluntary, but a lot of people have asked for this): SubscribeStar: https:\\/\\/www.subscribestar.com\\/yannickilcher Patreon: https:\\/\\/www.patreon.com\\/yannickilcher Bitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq Ethereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2 Litecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m Monero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\",\"1700\":\"This is the first episode of a podcast series on Machine Learning and Data privacy. Read the full story\",\"1512\":\"Chris Lattner is a world-class software & hardware engineer, leading projects at Apple, Tesla, Google, and SiFive. Please support this podcast by checking out our sponsors: \\u2013 Blinkist: https:\\/\\/blinkist.com\\/lex and use code LEX to get a free week of premium \\u2013 Neuro: https:\\/\\/www.getneuro.com and use code LEX to get 15% off \\u2013 MasterClass: https:\\/\\/masterclass.com\\/lex to get 15% off annual sub \\u2013 Cash App: https:\\/\\/cash.app\\/ and use code LexPodcast to get $10 EPISODE LINKS: Chris\\u2019s Twitter: https:\\/\\/twitter.com\\/clattner_llvm Chris\\u2019s Website: http:\\/\\/nondot.org\\/sabre\\/ PODCAST INFO: Podcast website: https:\\/\\/lexfridman.com\\/podcast Apple Podcasts: https:\\/\\/apple.co\\/2lwqZIr Spotify: https:\\/\\/spoti.fi\\/2nEwCF8 RSS: https:\\/\\/lexfridman.com\\/feed\\/podcast\\/ YouTube Full Episodes: https:\\/\\/youtube.com\\/lexfridman YouTube Clips: https:\\/\\/youtube.com\\/lexclips SUPPORT &\",\"233\":\"Actually now that I think of it this barely even scratches the surface of the weirdness of inverted computers in a Tenet universe. Legitimately breaks my brain and I love it. Quite a bit depends on the (understandably skimmed over) physics of interaction between fwd\\/bwd objects nitter.net\\/karpathy\\/status\\/1346134712817864708#m\",\"1045\":\"Sign up with brilliant and get 20% off your annual subscription: https:\\/\\/brilliant.org\\/ZachStar\\/ STEMerch Store: https:\\/\\/stemerch.com\\/ Support the Channel: https:\\/\\/www.patreon.com\\/zachstar PayPal(one time donation): https:\\/\\/www.paypal.me\\/ZachStarYT \\u25baFollow me Instagram: https:\\/\\/www.instagram.com\\/zachstar\\/ Twitter: https:\\/\\/twitter.com\\/ImZachStar Animations: Brainup Studios ( http:\\/\\/brainup.in\\/ ) \\u25baMy Setup: Space Pictures: https:\\/\\/amzn.to\\/2CC4Kqj Magnetic Floating Globe: https:\\/\\/amzn.to\\/2VgPdn0 Camera: https:\\/\\/amzn.to\\/2RivYu5 Mic: https:\\/\\/amzn.to\\/35bKiri Tripod: https:\\/\\/amzn.to\\/2RgMTNL Equilibrium Tube: https:\\/\\/amzn.to\\/2SowDrh \\u25baCheck out the my Amazon Store: https:\\/\\/www.amazon.com\\/shop\\/zachstar SPOILER DOWN BELOW The answer is no, you cannot always tell the time on this clock, but most of the time you can. There are 132 moments\\/configurations in a 12 hour period where you wouldn't know the time because at those moments either hand could be the hour hand and the configuration would be valid. The first of those 132 moments comes up just after 12:05 and then roughly every 5 minutes you get another configuration where you couldn't tell the time.\",\"3757\":\"I have seen a lot of great work when it comes to visualizing attention maps \\/ activation maps \\/ heat maps in CNN based architectures (for eg. https:\\/\\/github.com\\/raghakot\\/keras-vis ) However most of my research is around image synthesis, so I was wondering if anyone had any experience with visualizing networks when it comes to a non classification task. [link] [comments]\",\"5678\":\"Does anyone working on Big Tech companies such as Google, FB, Amazon, etc. knows which are the algorithms behind ads while surfing are SO ACCURATE? I'm curious about that, just that. [link] [comments]\",\"236\":\"\\\"Do You Love Me?\\\" new Boston Dynamics video \\ud83e\\udd16 invidious.snopyta.org\\/fn3KWM1kuAw\",\"975\":\"If you are yearning for a little OpenAI DALL\\u00b7E action, make sure to check out @ykilcher's video on it. Very exhaustive, and I think it's the best one out there. So good! invidious.snopyta.org\\/watch?v=j4xgkjWl\\u2026\",\"5669\":\"https:\\/\\/t.me\\/McPromptFace_bot My silly project that I figured I'd share in case anyone finds it fun! It's based on GPT-2 and uses the largest available public model. You give it a short (\",\"1645\":\"Read the full story\",\"4082\":\"Hello, hope y'all are doing well! I'm wondering if there's any difference in terms of predictions quality when using .predict & .predict_on_batch? Aside from its inner workings for processing inputs as a specified batch size or taking one input as one batch, is there anything else that will materially impact the predictions when using both prediction methods? In theory it should produce the same predictions but for future model prediction workflow consistency, I'm curious if there's anything I should watch out for. Thanks in advance for any help. [link] [comments]\",\"4293\":\"In this episode of Machine Learning Street Talk, Tim Scarfe, Yannic Kilcher and Connor Shorten chat about Large-scale Transfer Learning in Natural Language Processing. The Text-to-Text Transfer Transformer (T5) model from Google AI does an exhaustive survey of what\\u2019s important for Transfer Learning in NLP and what\\u2019s not. In this conversation, we go through the key takeaways of the paper, text-to-text input\\/output format, architecture choice, dataset size and composition, fine-tuning strategy, and how to best use more computation. Beginning with these topics, we diverge into exciting ideas such as embodied cognition, meta-learning, and the measure of intelligence. We are still beginning our podcast journey and really appreciate any feedback from our listeners. Is the chat too technical? Do you prefer group discussions, interviewing experts, or chats between the three of us? Thanks for watching and if you haven\\u2019t already, Please Subscribe! Paper Links discussed in the chat: Text-to-Text Transfer Transformer: https:\\/\\/arxiv.org\\/abs\\/1910.10683 Experience Grounds Language (relevant to divergent discussion about embodied cognition): https:\\/\\/arxiv.org\\/pdf\\/2004.10151.pdf On the Measure of Intelligence: https:\\/\\/arxiv.org\\/abs\\/1911.01547 Train Large, Then Compress: https:\\/\\/arxiv.org\\/pdf\\/2002.11794.pdf Scaling Laws for Neural Language Models: https:\\/\\/arxiv.org\\/pdf\\/2001.08361.pdf The Illustrated Transformer: http:\\/\\/jalammar.github.io\\/illustrated... ELECTRA: https:\\/\\/arxiv.org\\/pdf\\/2003.10555.pdf Transformer-XL: https:\\/\\/arxiv.org\\/pdf\\/1901.02860.pdf Reformer: The Efficient Transformer: https:\\/\\/openreview.net\\/pdf?id=rkgNKkHtvB The Evolved Transformer: https:\\/\\/arxiv.org\\/pdf\\/1901.11117.pdf DistilBERT: https:\\/\\/arxiv.org\\/pdf\\/1910.01108.pdf How to generate text (HIGHLY RECOMMEND): https:\\/\\/huggingface.co\\/blog\\/how-to-ge... Tokenizers: https:\\/\\/blog.floydhub.com\\/tokenization-nlp\\/\",\"484\":\"All about ln(x). Full playlist: https:\\/\\/www.youtube.com\\/playlist?list=PLZHQObOWTQDP5CVelJJ1bNDouqrAhVPev Home page: https:\\/\\/www.3blue1brown.com Brought to you by you: https:\\/\\/3b1b.co\\/ldm-thanks Beautiful pictorial summary by @ThuyNganVu: https:\\/\\/twitter.com\\/ThuyNganVu\\/status\\/1259288683489849344 Errors: At minute 16, the sum should be written with a \\\"...\\\" to indicate going to infinity. At minute 38, the exponent should have 1\\/(2s^2) instead of 1\\/s^2 for s to represent standard deviation. At minute 54, an equal sign was mistakenly used in taking the derivative of x^3 \\/ 3!. At the end, it should be pointed out that the alternating series with x^n terms only converges for values of x between -1 and 1, so the values one can't be considered proven with values of x outside that range. Everything with the argument here is fine, as it only deals with the convergent input, but that fact should still be mentioned. Related videos. Calculus series: https:\\/\\/www.youtube.com\\/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr The sum giving pi^2 \\/ 6: https:\\/\\/youtu.be\\/d-o3eB9sfls The sum giving pi \\/ 4: https:\\/\\/youtu.be\\/NaL_Cb42WyY https:\\/\\/youtu.be\\/00w8gu2aL-w (Mathologer) ------------------- Video timeline (thanks to user \\\"noonesperfect\\\") 0:00:14 - Question 1 0:02:29 - Answer 1 0:06:27 - Prime nos. in Infinite Geometric Series (Basel problem) and their relationship with Natural logarithm 0:12:01 - More examples of prime numbers in infinite series and their relationship with ln 0:17:25 - Question 2 0:19:20 - Answer 2 and explanation using ln 0:22:25 - Question 3 and families of curves 0:26:37 - Answer 3 and explanation 0:28:50 - Imaginary exponential 0:30:57 - Derivatives of exponential terms 0:37:21 - Why derivative of e^t is the same as that e^t itself? 0:41:21 - Question 4 0:44:12 - Answer 4 and explanation using Python 0:46:02 - Taylor Series for e^x 0:48:29 - Derivatives of polynomial terms\\/Derivatives of e^x 0:50:56 - Derivative of natural logarithm using graph 0:56:07 - Question 5 0:57:37 - Answer 5 and explanation 1:02:15 - Euler\\u2013Mascheroni constant 1:08:37 - Question 6 1:12:41 - Connecting dots to the familiarity of different expression in math ------------------ The live question setup with stats on-screen is powered by Itempool. https:\\/\\/itempool.com\\/ Curious about other animations? https:\\/\\/www.3blue1brown.com\\/faq#manim Music by Vincent Rubinetti. Download the music on Bandcamp: https:\\/\\/vincerubinetti.bandcamp.com\\/album\\/the-music-of-3blue1brown Stream the music on Spotify: https:\\/\\/open.spotify.com\\/album\\/1dVyjwS8FBqXhRunaG5W5u If you want to contribute translated subtitles or to help review those that have already been made by others and need approval, you can click the gear icon in the video and go to subtitles\\/cc, then \\\"add subtitles\\/cc\\\". I really appreciate those who do this, as it helps make the lessons accessible to more people. ------------------ 3blue1brown is a channel about animating math, in all senses of the word animate. And you know the drill with YouTube, if you want to stay posted on new videos, subscribe: http:\\/\\/3b1b.co\\/subscribe Various social media stuffs: Website: https:\\/\\/www.3blue1brown.com Twitter: https:\\/\\/twitter.com\\/3blue1brown Reddit: https:\\/\\/www.reddit.com\\/r\\/3blue1brown Instagram: https:\\/\\/www.instagram.com\\/3blue1brown_animations\\/ Patreon: https:\\/\\/patreon.com\\/3blue1brown Facebook: https:\\/\\/www.facebook.com\\/3blue1brown\",\"1529\":\"Colin Angle is the CEO and co-founder of iRobot, a robotics company that for 29 years has been creating robots that operate successfully in the real world, not as a demo or on a scale of dozens, but on a scale of thousands and millions. As of this year, iRobot has sold more than 25 million robots to consumers, including the Roomba vacuum cleaning robot, the Braava floor mopping robot, and soon the Terra lawn mowing robot. 25 million robots successfully operating autonomously in people\\u2019s homes to me is an incredible accomplishment of science, engineering, logistics, and all kinds of\",\"1456\":\"Melanie Mitchell is a professor of computer science at Portland State University and an external professor at Santa Fe Institute. She has worked on and written about artificial intelligence from fascinating perspectives including adaptive complex systems, genetic algorithms, and the Copycat cognitive architecture which places the process of analogy making at the core of human cognition. From her doctoral work with her advisors Douglas Hofstadter and John Holland to today, she has contributed a lot of important ideas to the field of AI, including her recent book, simply called Artificial Intelligence: A Guide for Thinking Humans. This conversation is part\",\"1495\":\"Michael Kearns is a professor at University of Pennsylvania and a co-author of the new book Ethical Algorithm that is the focus of much of our conversation, including algorithmic fairness, bias, privacy, and ethics in general. But, that is just one of many fields that Michael is a world-class researcher in, some of which we touch on quickly including learning theory or theoretical foundations of machine learning, game theory, algorithmic trading, quantitative finance, computational social science, and more. This conversation is part of the Artificial Intelligence podcast. If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai\",\"1556\":\"Lee Smolin is a theoretical physicist, co-inventor of loop quantum gravity, and a contributor of many interesting ideas to cosmology, quantum field theory, the foundations of quantum mechanics, theoretical biology, and the philosophy of science. He is the author of several books including one that critiques the state of physics and string theory called The Trouble with Physics, and his latest book, Einstein\\u2019s Unfinished Revolution: The Search for What Lies Beyond the Quantum. EPISODE LINKS: Books mentioned: \\u2013 Einstein\\u2019s Unfinished Revolution by Lee Smolin: https:\\/\\/amzn.to\\/2TsF5c3 \\u2013 The Trouble With Physics by Lee Smolin: https:\\/\\/amzn.to\\/2v1FMzy \\u2013 Against Method by Paul Feyerabend:\",\"5691\":\"https:\\/\\/youtu.be\\/_KVV9jXSzvo Hi! I filmed a video tutorial on unit testing of deep learning code. Specifically, I focused on mocking. I picked this topic because I think that there are not that many resources on it. I hope you will like it and feel free to leave a comment! Cheers [link] [comments]\",\"206\":\"Because I have recently started employment with Kaggle, I am not eligible to win any prizes. Which means the prize-winner for this comp is Quan Sun (team \\u2018student1\\u2019)! Congratulations! My approach to this competition was to first analyze the data in Excel pivottables. I looked for groups which had high or low application success rates. In this way, I found a large number of strong predictors \\u2014 including by date (new years day is a strong predictor, as are applications processed on a Sunday), and for many fields a null value was highly predictive. I then used C# to normalize the data into Grants and Persons objects, and constructed a dataset for modeling including these features: CatCode, NumPerPerson, PersonId, NumOnDate, AnyHasPhd, Country, Dept, DayOfWeek, HasPhd, IsNY, Month, NoClass, NoSpons, RFCD, Role, SEO, Sponsor, ValueBand, HasID, AnyHasID, AnyHasSucc, HasSucc, People.Count, AStarPapers, APapers, BPapers, CPapers, Papers, MaxAStarPapers, MaxCPapers, MaxPapers, NumSucc, NumUnsucc, MinNumSucc, MinNumUnsucc, PctRFCD, PctSEO, MaxYearBirth, MinYearUni, YearBirth, YearUni . Most of these are fairly obvious as to what they mean. Field names starting with \\u2018Any\\u2019 are true if any person attached to the grant has that feature (e.g. \\u2018AnyHasPhd\\u2019). For most fields I had one predictor that just looks at person 1 (e.g. \\u2018APapers\\u2019 is number of A papers from person 1), and one for the maximum of all people in the application (e.g. \\u2018MaxAPapers\\u2019). Once I had created these features, I used a generalization of the random forest algorithm to build a model. I\\u2019ll try to write some detail about how this algorithm works when I have more time, but really, the difference between it and a regular random forest is not that great. I pre-processed the data before running it through the model by grouping up small groups in categorical variables, and replacing continuous columns with null values with 2 columns (one containing a binary predictor that is true only where the continuous column is null, the other containing the original column, with nulls replaced by the median). Other than the Excel pivottables at the start, all the pre-processing and modelling was done in C#, using libraries I developed during this competition. I hope to document and release these libraries at some point \\u2014 perhaps after tuning them in future comps. Originally published at blog.kaggle.com on February 21, 2011. Jeremy Howard on winning the Predict Grant Applications Competition was originally published in Kaggle Blog on Medium, where people are continuing the conversation by highlighting and responding to this story.\",\"487\":\"An introduction to group theory, and the monster group. Viewer supported: https:\\/\\/3b1b.co\\/monster-thanks Minor error corrections below\\u2193\\u2193\\u2193 This video is part of the #MegaFavNumbers project: https:\\/\\/www.youtube.com\\/playlist?list=PLar4u0v66vIodqt3KSZPsYyuULD5meoAo To join the gang, upload your own video on your own favorite number over 1,000,000 with the hashtag #MegaFavNumbers, and the word MegaFavNumbers in the title by September 2nd, 2020, and it'll be added to the playlist above. Errors: *Typo on the \\\"hard problem\\\" at 14:11, it should be a\\/(b+c) + b\\/(a+c) + c\\/(a+b) = 4 *Typo-turned-speako: The classification of quasithin groups is 1221 pages long, not 12,000. The full collection of papers proving the CFSG theorem do make up tens of thousands of pages, but no one paper was quite _that_ crazy. Thanks to Richard Borcherds for helpful comments while putting this video together. He has a wonderful hidden gem of a channel: https:\\/\\/youtu.be\\/a9k_QmZbwX8 You may also enjoy this brief article giving an overview of this monster: http:\\/\\/www.ams.org\\/notices\\/200209\\/what-is.pdf If you want to learn more about group theory, check out the expository papers here: https:\\/\\/kconrad.math.uconn.edu\\/blurbs\\/ Videos with John Conway talking about the Monster: https:\\/\\/youtu.be\\/jsSeoGpiWsw https:\\/\\/youtu.be\\/lbN8EMcOH5o More on Noether's Theorem: https:\\/\\/youtu.be\\/CxlHLqJ9I0A https:\\/\\/youtu.be\\/04ERSb06dOg The symmetry ambigram was designed by Punya Mishra: https:\\/\\/punyamishra.com\\/2013\\/05\\/31\\/symmetry-new-ambigram\\/ The Monster image comes from the Noun Project, via Nicky Knicky ------------------ These animations are largely made using manim, a scrappy open-source python library: https:\\/\\/github.com\\/3b1b\\/manim If you want to check it out, I feel compelled to warn you that it's not the most well-documented tool, and it has many other quirks you might expect in a library someone wrote with only their own use in mind. Music by Vincent Rubinetti. Download the music on Bandcamp: https:\\/\\/vincerubinetti.bandcamp.com\\/album\\/the-music-of-3blue1brown Stream the music on Spotify: https:\\/\\/open.spotify.com\\/album\\/1dVyjwS8FBqXhRunaG5W5u If you want to contribute translated subtitles or to help review those that have already been made by others and need approval, you can click the gear icon in the video and go to subtitles\\/cc, then \\\"add subtitles\\/cc\\\". I really appreciate those who do this, as it helps make the lessons accessible to more people. ------------------ 3blue1brown is a channel about animating math, in all senses of the word animate. And you know the drill with YouTube, if you want to stay posted on new videos, subscribe: http:\\/\\/3b1b.co\\/subscribe Various social media stuffs: Website: https:\\/\\/www.3blue1brown.com Twitter: https:\\/\\/twitter.com\\/3blue1brown Reddit: https:\\/\\/www.reddit.com\\/r\\/3blue1brown Instagram: https:\\/\\/www.instagram.com\\/3blue1brown_animations\\/ Patreon: https:\\/\\/patreon.com\\/3blue1brown Facebook: https:\\/\\/www.facebook.com\\/3blue1brown\",\"1636\":\"Read the full story\",\"1658\":\"Key methods to understanding and utilizing pandas Read the full story\",\"252\":\"@fchollet I see you like the new TensorFlow (which honestly is quite beautiful), therefore I'd be curious to know what you think about Mesh Tensorflow. Personally, I find it much easier to use than PyTorch or even Keras, as most of the backend definitions are just there.\",\"1870\":\"We\\u2019re glibly searching for counterexamples to the Riemann Hypothesis, to trick you into learning about software engineering principles. In the first two articles we configured a testing framework and showed how to hide implementation choices behind an interface. Next, we\\u2019ll improve the algorithm\\u2019s core routine. As before, I\\u2019ll link to specific git commits in the final code repository to show how the project evolves. Superabundant numbers A superabundant number is one which has \\u201cmaximal relative divisor sums\\u201d in the following sense: for all\",\"1532\":\"My thoughts on 8 possible long-term futures of Neuralink after attending the August 2020 progress update. This is a solo episode #2 of the podcast. Hopefully it\\u2019s interesting to some folks. The aim is for these episodes to be focused on a particular topic, at times challenging, at times personal, at times exciting to me on a technical and philosophical level like the episode today. If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/podcast or connect with @lexfridman on Twitter, LinkedIn, Facebook, Medium, or YouTube where you can watch the video versions of these conversations. If\",\"4897\":\"A bit late to the party, but \\ud83d\\udc83NEW VIDEO\\ud83d\\udd7a on Switch Transformers by @GoogleAI. Hard Routing, selective dropout, mixed precision & more to achieve a \\ud83d\\udd25ONE TRILLION parameters\\ud83d\\udd25 language model. Watch to learn how it's done\\ud83e\\uddd9\\ud83d\\udcaa invidious.snopyta.org\\/iAR8LkkMMIM @LiamFedus @barret_zoph\",\"1666\":\"Read the full story\",\"490\":\"What does it mean to compute e^{pi i}? Full playlist: https:\\/\\/www.youtube.com\\/playlist?list=PLZHQObOWTQDP5CVelJJ1bNDouqrAhVPev Home page: https:\\/\\/www.3blue1brown.com Brought to you by you: https:\\/\\/3b1b.co\\/ldm-thanks Beautiful pictorial summary by @ThuyNganVu: https:\\/\\/twitter.com\\/ThuyNganVu\\/status\\/1258220129327800320 https:\\/\\/twitter.com\\/ThuyNganVu\\/status\\/1258220541686628353 ------------------ Video Timeline (Thanks to user \\\"Just TIEriffic\\\") 0:00:00 Welcome 0:00:20 Ending Animation Preview 0:01:15 Reminders from previous lecture 0:03:30 Q1: Prompt (Relationship with e^i\\u03b8=\\u2026) 0:05:40 Q1: Results 0:07:15 WTF, Whats The Function 0:10:00 Exploring exp(x) 0:11:45 Exploring exp(x) in Python 0:14:45 Important exp(x) property 0:15:55 Q2: Prompt (Given f(a+b) = f(a)f(b)\\u2026) 0:17:30 Ask: Which is more interesting, special cases or the general case 0:20:00 Q2: Results 0:23:50 Will a zero break Q2? 0:25:40 The e^x convention 0:27:10 Q3: Prompt (i^2 = -1, i^n = -1) 0:27:45 Ask: Zero does not break Q2 0:30:20 Q3: Results 0:31:05 Comparison to Rotation 0:33:00 Visualizing this relationship 0:36:50 The special case of \\u03c0 0:39:20 Periodic nature of this relationship 0:39:40 Q4: Prompt (e^3i) 0:41:35 Q4: Results 0:43:55 Explaining the celebrity equation 0:45:55 Homework \\/ Things to think about 0:49:15 Ask: Zero does break Q2. 0:50:30 Closing Remarks ------------------ The live question setup with stats on-screen is powered by Itempool. https:\\/\\/itempool.com\\/ Curious about other animations? https:\\/\\/www.3blue1brown.com\\/faq#manim Music by Vincent Rubinetti. Download the music on Bandcamp: https:\\/\\/vincerubinetti.bandcamp.com\\/album\\/the-music-of-3blue1brown Stream the music on Spotify: https:\\/\\/open.spotify.com\\/album\\/1dVyjwS8FBqXhRunaG5W5u If you want to contribute translated subtitles or to help review those that have already been made by others and need approval, you can click the gear icon in the video and go to subtitles\\/cc, then \\\"add subtitles\\/cc\\\". I really appreciate those who do this, as it helps make the lessons accessible to more people. ------------------ 3blue1brown is a channel about animating math, in all senses of the word animate. And you know the drill with YouTube, if you want to stay posted on new videos, subscribe: http:\\/\\/3b1b.co\\/subscribe Various social media stuffs: Website: https:\\/\\/www.3blue1brown.com Twitter: https:\\/\\/twitter.com\\/3blue1brown Reddit: https:\\/\\/www.reddit.com\\/r\\/3blue1brown Instagram: https:\\/\\/www.instagram.com\\/3blue1brown_animations\\/ Patreon: https:\\/\\/patreon.com\\/3blue1brown Facebook: https:\\/\\/www.facebook.com\\/3blue1brown\",\"4310\":\"In this episode of Machine Learning Street Talk, we chat with Jonathan Frankle, author of The Lottery Ticket Hypothesis. Frankle has continued researching Sparse Neural Networks, Pruning, and Lottery Tickets leading to some really exciting follow-on papers! This chat discusses some of these papers such as Linear Mode Connectivity, Comparing and Rewinding and Fine-tuning in Neural Network Pruning, and more (full list of papers linked below). We also chat about how Jonathan got into Deep Learning research, his Information Diet, and work on developing Technology Policy for Artificial Intelligence! This was a really fun chat, I hope you enjoy listening to it and learn something from it! Thanks for watching and please subscribe! Huge thanks to everyone on r\\/MachineLearning who asked questions! Paper Links discussed in the chat: The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks: https:\\/\\/arxiv.org\\/abs\\/1803.03635 Linear Mode Connectivity and the Lottery Ticket Hypothesis: https:\\/\\/arxiv.org\\/abs\\/1912.05671 Dissecting Pruned Neural Networks: https:\\/\\/arxiv.org\\/abs\\/1907.00262 Training BatchNorm and Only BatchNorm: On the Expressive Power of Random Features in CNNs: https:\\/\\/arxiv.org\\/abs\\/2003.00152 What is the State of Neural Network Pruning? https:\\/\\/arxiv.org\\/abs\\/2003.03033 The Early Phase of Neural Network Training: https:\\/\\/arxiv.org\\/abs\\/2002.10365 Comparing Rewinding and Fine-tuning in Neural Network Pruning: https:\\/\\/arxiv.org\\/abs\\/2003.02389 (Also Mentioned) Block-Sparse GPU Kernels: https:\\/\\/openai.com\\/blog\\/block-sparse-gpu-kernels\\/ Balanced Sparsity for Efficient DNN Inference on GPU: https:\\/\\/arxiv.org\\/pdf\\/1811.00206.pdf Playing the Lottery with Rewards and Multiple Languages: Lottery Tickets in RL and NLP: https:\\/\\/arxiv.org\\/pdf\\/1906.02768.pdf r\\/MachineLearning question list: https:\\/\\/www.reddit.com\\/r\\/MachineLearning\\/comments\\/g9jqe0\\/d_lottery_ticket_hypothesis_ask_the_author_a\\/ (edited) #machinelearning #deeplearning\",\"2323\":\"Here is a script working in both TamperMonkey (Chrome) and GreaseMonkey (Firefox): FileSave\\/arxiv_pdf_preview.js at master \\u00b7 OneMoreSecond\\/FileSave (github.com) Motivations: Why do I prefer the abstract page to PDF page? Sometimes the title of PDF page is only something like 2006.04664.pdf, which is not informational enough when saved to browser favorites. Sometimes the PDF link is an old version one like 2006.04664_v1.pdf. The abstract page helps you view the latest submission. [link] [comments]\",\"1483\":\"Kevin Scott is the CTO of Microsoft. Before that, he was the Senior Vice President of Engineering and Operations at LinkedIn. And before that, he oversaw mobile ads engineering at Google. This conversation is part of the Artificial Intelligence podcast. If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman on Twitter, LinkedIn, Facebook, Medium, or YouTube where you can watch the video versions of these conversations. If you enjoy the podcast, please rate it 5 stars on iTunes or support it on Patreon.\",\"4264\":\"Tweet Share Share Function optimization involves finding the input that results in the optimal value from an objective function. Optimization algorithms navigate the search space of input variables in order to locate the optima, and both the shape of the objective function and behavior of the algorithm in the search space are opaque on real-world problems. As such, it is common to study optimization algorithms using simple low-dimensional functions that can be easily visualized directly. Additionally, the samples in the input space of these simple functions made by an optimization algorithm can be visualized with their appropriate context. Visualization of lower-dimensional functions and algorithm behavior on those functions can help to develop the intuitions that can carry over to more complex higher-dimensional function optimization problems later. In this tutorial, you will discover how to create visualizations for function optimization in Python. After completing this tutorial, you will know: Visualization is an important tool when studying function optimization algorithms. How to visualize one-dimensional functions and samples using line plots. How to visualize two-dimensional functions and samples using contour and surface plots. Let\\u2019s get started. Visualization for Function Optimization in Python Photo by Nathalie, some rights reserved. Tutorial Overview This tutorial is divided into three parts; they are: Visualization for Function Optimization Visualize 1D Function Optimization Test Function Sample Test Function Line Plot of Test Function Scatter Plot of Test Function Line Plot with Marked Optima Line Plot with Samples Visualize 2D Function Optimization Test Function Sample Test Function Contour Plot of Test Function Filled Contour Plot of Test Function Filled Contour Plot of Test Function with Samples Surface Plot of Test Function Visualization for Function Optimization Function optimization is a field of mathematics concerned with finding the inputs to a function that result in the optimal output for the function, typically a minimum or maximum value. Optimization may be straightforward for simple differential functions where the solution can be calculated analytically. However, most functions we\\u2019re interested in solving in applied machine learning may or may not be well behaved and may be complex, nonlinear, multivariate, and non-differentiable. As such, it is important to have an understanding of a wide range of different algorithms that can be used to address function optimization problems. An important aspect of studying function optimization is understanding the objective function that is being optimized and understanding the behavior of an optimization algorithm over time. Visualization plays an important role when getting started with function optimization. We can select simple and well-understood test functions to study optimization algorithms. These simple functions can be plotted to understand the relationship between the input to the objective function and the output of the objective function and highlighting hills, valleys, and optima. In addition, the samples selected from the search space by an optimization algorithm can also be plotted on top of plots of the objective function. These plots of algorithm behavior can provide insight and intuition into how specific optimization algorithms work and navigate a search space that can generalize to new problems in the future. Typically, one-dimensional or two-dimensional functions are chosen to study optimization algorithms as they are easy to visualize using standard plots, like line plots and surface plots. We will explore both in this tutorial. First, let\\u2019s explore how we might visualize a one-dimensional function optimization. Visualize 1D Function Optimization A one-dimensional function takes a single input variable and outputs the evaluation of that input variable. Input variables are typically continuous, represented by a real-valued floating-point value. Often, the input domain is unconstrained, although for test problems we impose a domain of interest. Test Function In this case we will explore function visualization with a simple x^2 objective function: f(x) = x^2 This has an optimal value with an input of x=0.0, which equals 0.0. The example below implements this objective function and evaluates a single input. # example of a 1d objective function # objective function def objective(x): return x**2.0 # evaluate inputs to the objective function x = 4.0 result = objective(x) print('f(%.3f) = %.3f' % (x, result)) Running the example evaluates the value 4.0 with the objective function, which equals 16.0. f(4.000) = 16.000 Sample the Test Function The first thing we might want to do with a new function is define an input range of interest and sample the domain of interest using a uniform grid. This sample will provide the basis for generating a plot later. In this case, we will define a domain of interest around the optima of x=0.0 from x=-5.0 to x=5.0 and sample a grid of values in this range with 0.1 increments, such as -5.0, -4.9, -4.8, etc. ... # define range for input r_min, r_max = -5.0, 5.0 # sample input range uniformly at 0.1 increments inputs = arange(r_min, r_max, 0.1) # summarize some of the input domain print(inputs[:5]) We can then evaluate each of the x values in our sample. ... # compute targets results = objective(inputs) # summarize some of the results print(results[:5]) Finally, we can check some of the input and their corresponding outputs. ... # create a mapping of some inputs to some results for i in range(5): print('f(%.3f) = %.3f' % (inputs[i], results[i])) Tying this together, the complete example of sampling the input space and evaluating all points in the sample is listed below. # sample 1d objective function from numpy import arange # objective function def objective(x): return x**2.0 # define range for input r_min, r_max = -5.0, 5.0 # sample input range uniformly at 0.1 increments inputs = arange(r_min, r_max, 0.1) # summarize some of the input domain print(inputs[:5]) # compute targets results = objective(inputs) # summarize some of the results print(results[:5]) # create a mapping of some inputs to some results for i in range(5): print('f(%.3f) = %.3f' % (inputs[i], results[i])) Running the example first generates a uniform sample of input points as we expected. The input points are then evaluated using the objective function and finally, we can see a simple mapping of inputs to outputs of the objective function. [-5. -4.9 -4.8 -4.7 -4.6] [25. 24.01 23.04 22.09 21.16] f(-5.000) = 25.000 f(-4.900) = 24.010 f(-4.800) = 23.040 f(-4.700) = 22.090 f(-4.600) = 21.160 Now that we have some confidence in generating a sample of inputs and evaluating them with the objective function, we can look at generating plots of the function. Line Plot of Test Function We could sample the input space randomly, but the benefit of a uniform line or grid of points is that it can be used to generate a smooth plot. It is smooth because the points in the input space are ordered from smallest to largest. This ordering is important as we expect (hope) that the output of the objective function has a similar smooth relationship between values, e.g. small changes in input result in locally consistent (smooth) changes in the output of the function. In this case, we can use the samples to generate a line plot of the objective function with the input points (x) on the x-axis of the plot and the objective function output (results) on the y-axis of the plot. ... # create a line plot of input vs result pyplot.plot(inputs, results) # show the plot pyplot.show() Tying this together, the complete example is listed below. # line plot of input vs result for a 1d objective function from numpy import arange from matplotlib import pyplot # objective function def objective(x): return x**2.0 # define range for input r_min, r_max = -5.0, 5.0 # sample input range uniformly at 0.1 increments inputs = arange(r_min, r_max, 0.1) # compute targets results = objective(inputs) # create a line plot of input vs result pyplot.plot(inputs, results) # show the plot pyplot.show() Running the example creates a line plot of the objective function. We can see that the function has a large U-shape, called a parabola. This is a common shape when studying curves, e.g. the study of calculus. Line Plot of a One-Dimensional Function Scatter Plot of Test Function The line is a construct. It is not really the function, just a smooth summary of the function. Always keep this in mind. Recall that we, in fact, generated a sample of points in the input space and corresponding evaluation of those points. As such, it would be more accurate to create a scatter plot of points; for example: # scatter plot of input vs result for a 1d objective function from numpy import arange from matplotlib import pyplot # objective function def objective(x): return x**2.0 # define range for input r_min, r_max = -5.0, 5.0 # sample input range uniformly at 0.1 increments inputs = arange(r_min, r_max, 0.1) # compute targets results = objective(inputs) # create a scatter plot of input vs result pyplot.scatter(inputs, results) # show the plot pyplot.show() Running the example creates a scatter plot of the objective function. We can see the familiar shape of the function, but we don\\u2019t gain anything from plotting the points directly. The line and the smooth interpolation between the points it provides are more useful as we can draw other points on top of the line, such as the location of the optima or the points sampled by an optimization algorithm. Scatter Plot of a One-Dimensional Function Line Plot with Marked Optima Next, let\\u2019s draw the line plot again and this time draw a point where the known optima of the function is located. This can be helpful when studying an optimization algorithm as we might want to see how close an optimization algorithm can get to the optima. First, we must define the input for the optima, then evaluate that point to give the x-axis and y-axis values for plotting. ... # define the known function optima optima_x = 0.0 optima_y = objective(optima_x) We can then plot this point with any shape or color we like, in this case, a red square. ... # draw the function optima as a red square pyplot.plot([optima_x], [optima_y], 's', color='r') Tying this together, the complete example of creating a line plot of the function with the optima highlighted by a point is listed below. # line plot of input vs result for a 1d objective function and show optima from numpy import arange from matplotlib import pyplot # objective function def objective(x): return x**2.0 # define range for input r_min, r_max = -5.0, 5.0 # sample input range uniformly at 0.1 increments inputs = arange(r_min, r_max, 0.1) # compute targets results = objective(inputs) # create a line plot of input vs result pyplot.plot(inputs, results) # define the known function optima optima_x = 0.0 optima_y = objective(optima_x) # draw the function optima as a red square pyplot.plot([optima_x], [optima_y], 's', color='r') # show the plot pyplot.show() Running the example creates the familiar line plot of the function, and this time, the optima of the function, e.g. the input that results in the minimum output of the function, is marked with a red square. Line Plot of a One-Dimensional Function With Optima Marked by a Red Square This is a very simple function and the red square for the optima is easy to see. Sometimes the function might be more complex, with lots of hills and valleys, and we might want to make the optima more visible. In this case, we can draw a vertical line across the whole plot. ... # draw a vertical line at the optimal input pyplot.axvline(x=optima_x, ls='--', color='red') Tying this together, the complete example is listed below. # line plot of input vs result for a 1d objective function and show optima as line from numpy import arange from matplotlib import pyplot # objective function def objective(x): return x**2.0 # define range for input r_min, r_max = -5.0, 5.0 # sample input range uniformly at 0.1 increments inputs = arange(r_min, r_max, 0.1) # compute targets results = objective(inputs) # create a line plot of input vs result pyplot.plot(inputs, results) # define the known function optima optima_x = 0.0 # draw a vertical line at the optimal input pyplot.axvline(x=optima_x, ls='--', color='red') # show the plot pyplot.show() Running the example creates the same plot and this time draws a red line clearly marking the point in the input space that marks the optima. Line Plot of a One-Dimensional Function With Optima Marked by a Red Line Line Plot with Samples Finally, we might want to draw the samples of the input space selected by an optimization algorithm. We will simulate these samples with random points drawn from the input domain. ... # simulate a sample made by an optimization algorithm seed(1) sample = r_min + rand(10) * (r_max - r_min) # evaluate the sample sample_eval = objective(sample) We can then plot this sample, in this case using small black circles. ... # plot the sample as black circles pyplot.plot(sample, sample_eval, 'o', color='black') The complete example of creating a line plot of a function with the optima marked by a red line and an algorithm sample drawn with small black dots is listed below. # line plot of domain for a 1d function with optima and algorithm sample from numpy import arange from numpy.random import seed from numpy.random import rand from matplotlib import pyplot # objective function def objective(x): return x**2.0 # define range for input r_min, r_max = -5.0, 5.0 # sample input range uniformly at 0.1 increments inputs = arange(r_min, r_max, 0.1) # compute targets results = objective(inputs) # simulate a sample made by an optimization algorithm seed(1) sample = r_min + rand(10) * (r_max - r_min) # evaluate the sample sample_eval = objective(sample) # create a line plot of input vs result pyplot.plot(inputs, results) # define the known function optima optima_x = 0.0 # draw a vertical line at the optimal input pyplot.axvline(x=optima_x, ls='--', color='red') # plot the sample as black circles pyplot.plot(sample, sample_eval, 'o', color='black') # show the plot pyplot.show() Running the example creates the line plot of the domain and marks the optima with a red line as before. This time, the sample from the domain selected by an algorithm (really a random sample of points) is drawn with black dots. We can imagine that a real optimization algorithm will show points narrowing in on the domain as it searches down-hill from a starting point. Line Plot of a One-Dimensional Function With Optima Marked by a Red Line and Samples Shown with Black Dots Next, let\\u2019s look at how we might perform similar visualizations for the optimization of a two-dimensional function. Visualize 2D Function Optimization A two-dimensional function is a function that takes two input variables, e.g. x and y. Test Function We can use the same x^2 function and scale it up to be a two-dimensional function; for example: f(x, y) = x^2 + y^2 This has an optimal value with an input of [x=0.0, y=0.0], which equals 0.0. The example below implements this objective function and evaluates a single input. # example of a 2d objective function # objective function def objective(x, y): return x**2.0 + y**2.0 # evaluate inputs to the objective function x = 4.0 y = 4.0 result = objective(x, y) print('f(%.3f, %.3f) = %.3f' % (x, y, result)) Running the example evaluates the point [x=4, y=4], which equals 32. f(4.000, 4.000) = 32.000 Next, we need a way to sample the domain so that we can, in turn, sample the objective function. Sample Test Function A common way for sampling a two-dimensional function is to first generate a uniform sample along each variable, x and y, then use these two uniform samples to create a grid of samples, called a mesh grid. This is not a two-dimensional array across the input space; instead, it is two two-dimensional arrays that, when used together, define a grid across the two input variables. This is achieved by duplicating the entire x sample array for each y sample point and similarly duplicating the entire y sample array for each x sample point. This can be achieved using the meshgrid() NumPy function; for example: ... # define range for input r_min, r_max = -5.0, 5.0 # sample input range uniformly at 0.1 increments xaxis = arange(r_min, r_max, 0.1) yaxis = arange(r_min, r_max, 0.1) # create a mesh from the axis x, y = meshgrid(xaxis, yaxis) # summarize some of the input domain print(x[:5, :5]) We can then evaluate each pair of points using our objective function. ... # compute targets results = objective(x, y) # summarize some of the results print(results[:5, :5]) Finally, we can review the mapping of some of the inputs to their corresponding output values. ... # create a mapping of some inputs to some results for i in range(5): print('f(%.3f, %.3f) = %.3f' % (x[i,0], y[i,0], results[i,0])) The example below demonstrates how we can create a uniform sample grid across the two-dimensional input space and objective function. # sample 2d objective function from numpy import arange from numpy import meshgrid # objective function def objective(x, y): return x**2.0 + y**2.0 # define range for input r_min, r_max = -5.0, 5.0 # sample input range uniformly at 0.1 increments xaxis = arange(r_min, r_max, 0.1) yaxis = arange(r_min, r_max, 0.1) # create a mesh from the axis x, y = meshgrid(xaxis, yaxis) # summarize some of the input domain print(x[:5, :5]) # compute targets results = objective(x, y) # summarize some of the results print(results[:5, :5]) # create a mapping of some inputs to some results for i in range(5): print('f(%.3f, %.3f) = %.3f' % (x[i,0], y[i,0], results[i,0])) Running the example first summarizes some points in the mesh grid, then the objective function evaluation for some points. Finally, we enumerate coordinates in the two-dimensional input space and their corresponding function evaluation. [[-5. -4.9 -4.8 -4.7 -4.6] [-5. -4.9 -4.8 -4.7 -4.6] [-5. -4.9 -4.8 -4.7 -4.6] [-5. -4.9 -4.8 -4.7 -4.6] [-5. -4.9 -4.8 -4.7 -4.6]] [[50. 49.01 48.04 47.09 46.16] [49.01 48.02 47.05 46.1 45.17] [48.04 47.05 46.08 45.13 44.2 ] [47.09 46.1 45.13 44.18 43.25] [46.16 45.17 44.2 43.25 42.32]] f(-5.000, -5.000) = 50.000 f(-5.000, -4.900) = 49.010 f(-5.000, -4.800) = 48.040 f(-5.000, -4.700) = 47.090 f(-5.000, -4.600) = 46.160 Now that we are familiar with how to sample the input space and evaluate points, let\\u2019s look at how we might plot the function. Contour Plot of Test Function A popular plot for two-dimensional functions is a contour plot. This plot creates a flat representation of the objective function outputs for each x and y coordinate where the color and contour lines indicate the relative value or height of the output of the objective function. This is just like a contour map of a landscape where mountains can be distinguished from valleys. This can be achieved using the contour() Matplotlib function that takes the mesh grid and the evaluation of the mesh grid as input directly. We can then specify the number of levels to draw on the contour and the color scheme to use. In this case, we will use 50 levels and a popular \\u201cjet\\u201d color scheme where low-levels use a cold color scheme (blue) and high-levels use a hot color scheme (red). ... # create a contour plot with 50 levels and jet color scheme pyplot.contour(x, y, results, 50, alpha=1.0, cmap='jet') # show the plot pyplot.show() Tying this together, the complete example of creating a contour plot of the two-dimensional objective function is listed below. # create a contour plot with 50 levels and jet color scheme pyplot.contour(x, y, results, 50, alpha=1.0, cmap='jet') # show the plot pyplot.show() Tying this together, the complete example of creating a contour plot of the two-dimensional objective function is listed below. # contour plot for 2d objective function from numpy import arange from numpy import meshgrid from matplotlib import pyplot # objective function def objective(x, y): return x**2.0 + y**2.0 # define range for input r_min, r_max = -5.0, 5.0 # sample input range uniformly at 0.1 increments xaxis = arange(r_min, r_max, 0.1) yaxis = arange(r_min, r_max, 0.1) # create a mesh from the axis x, y = meshgrid(xaxis, yaxis) # compute targets results = objective(x, y) # create a contour plot with 50 levels and jet color scheme pyplot.contour(x, y, results, 50, alpha=1.0, cmap='jet') # show the plot pyplot.show() Running the example creates the contour plot. We can see that the more curved parts of the surface around the edges have more contours to show the detail, and the less curved parts of the surface in the middle have fewer contours. We can see that the lowest part of the domain is the middle, as expected. Contour Plot of a Two-Dimensional Objective Function Filled Contour Plot of Test Function It is also helpful to color the plot between the contours to show a more complete surface. Again, the colors are just a simple linear interpolation, not the true function evaluation. This must be kept in mind on more complex functions where fine detail will not be shown. We can fill the contour plot using the contourf() version of the function that takes the same arguments. ... # create a filled contour plot with 50 levels and jet color scheme pyplot.contourf(x, y, results, levels=50, cmap='jet') We can also show the optima on the plot, in this case as a white star that will stand out against the blue background color of the lowest part of the plot. ... # define the known function optima optima_x = [0.0, 0.0] # draw the function optima as a white star pyplot.plot([optima_x[0]], [optima_x[1]], '*', color='white') Tying this together, the complete example of a filled contour plot with the optima marked is listed below. # filled contour plot for 2d objective function and show the optima from numpy import arange from numpy import meshgrid from matplotlib import pyplot # objective function def objective(x, y): return x**2.0 + y**2.0 # define range for input r_min, r_max = -5.0, 5.0 # sample input range uniformly at 0.1 increments xaxis = arange(r_min, r_max, 0.1) yaxis = arange(r_min, r_max, 0.1) # create a mesh from the axis x, y = meshgrid(xaxis, yaxis) # compute targets results = objective(x, y) # create a filled contour plot with 50 levels and jet color scheme pyplot.contourf(x, y, results, levels=50, cmap='jet') # define the known function optima optima_x = [0.0, 0.0] # draw the function optima as a white star pyplot.plot([optima_x[0]], [optima_x[1]], '*', color='white') # show the plot pyplot.show() Running the example creates the filled contour plot that gives a better idea of the shape of the objective function. The optima at [x=0, y=0] is then marked clearly with a white star. Filled Contour Plot of a Two-Dimensional Objective Function With Optima Marked by a White Star Filled Contour Plot of Test Function with Samples We may want to show the progress of an optimization algorithm to get an idea of its behavior in the context of the shape of the objective function. In this case, we can simulate the points chosen by an optimization algorithm with random coordinates in the input space. ... # simulate a sample made by an optimization algorithm seed(1) sample_x = r_min + rand(10) * (r_max - r_min) sample_y = r_min + rand(10) * (r_max - r_min) These points can then be plotted directly as black circles and their context color can give an idea of their relative quality. ... # plot the sample as black circles pyplot.plot(sample_x, sample_y, 'o', color='black') Tying this together, the complete example of a filled contour plot with optimal and input sample plotted is listed below. # filled contour plot for 2d objective function and show the optima and sample from numpy import arange from numpy import meshgrid from numpy.random import seed from numpy.random import rand from matplotlib import pyplot # objective function def objective(x, y): return x**2.0 + y**2.0 # define range for input r_min, r_max = -5.0, 5.0 # sample input range uniformly at 0.1 increments xaxis = arange(r_min, r_max, 0.1) yaxis = arange(r_min, r_max, 0.1) # create a mesh from the axis x, y = meshgrid(xaxis, yaxis) # compute targets results = objective(x, y) # simulate a sample made by an optimization algorithm seed(1) sample_x = r_min + rand(10) * (r_max - r_min) sample_y = r_min + rand(10) * (r_max - r_min) # create a filled contour plot with 50 levels and jet color scheme pyplot.contourf(x, y, results, levels=50, cmap='jet') # define the known function optima optima_x = [0.0, 0.0] # draw the function optima as a white star pyplot.plot([optima_x[0]], [optima_x[1]], '*', color='white') # plot the sample as black circles pyplot.plot(sample_x, sample_y, 'o', color='black') # show the plot pyplot.show() Running the example, we can see the filled contour plot as before with the optima marked. We can now see the sample drawn as black dots and their surrounding color and relative distance to the optima gives an idea of how close the algorithm (random points in this case) got to solving the problem. Filled Contour Plot of a Two-Dimensional Objective Function With Optima and Input Sample Marked Surface Plot of Test Function Finally, we may want to create a three-dimensional plot of the objective function to get a fuller idea of the curvature of the function. This can be achieved using the plot_surface() Matplotlib function, that, like the contour plot, takes the mesh grid and function evaluation directly. ... # create a surface plot with the jet color scheme figure = pyplot.figure() axis = figure.gca(projection='3d') axis.plot_surface(x, y, results, cmap='jet') The complete example of creating a surface plot is listed below. # surface plot for 2d objective function from numpy import arange from numpy import meshgrid from matplotlib import pyplot from mpl_toolkits.mplot3d import Axes3D # objective function def objective(x, y): return x**2.0 + y**2.0 # define range for input r_min, r_max = -5.0, 5.0 # sample input range uniformly at 0.1 increments xaxis = arange(r_min, r_max, 0.1) yaxis = arange(r_min, r_max, 0.1) # create a mesh from the axis x, y = meshgrid(xaxis, yaxis) # compute targets results = objective(x, y) # create a surface plot with the jet color scheme figure = pyplot.figure() axis = figure.gca(projection='3d') axis.plot_surface(x, y, results, cmap='jet') # show the plot pyplot.show() Running the example creates a three-dimensional surface plot of the objective function. Surface Plot of a Two-Dimensional Objective Function Additionally, the plot is interactive, meaning that you can use the mouse to drag the perspective on the surface around and view it from different angles. Surface Plot From a Different Angle of a Two-Dimensional Objective Function Further Reading This section provides more resources on the topic if you are looking to go deeper. APIs Optimization and root finding (scipy.optimize) Optimization (scipy.optimize) numpy.meshgrid API. matplotlib.pyplot.contour API. matplotlib.pyplot.contourf API. mpl_toolkits.mplot3d.Axes3D.plot_surface API. Articles Mathematical optimization, Wikipedia. Parabola, Wikipedia. Summary In this tutorial, you discovered how to create visualizations for function optimization in Python. Specifically, you learned: Visualization is an important tool when studying function optimization algorithms. How to visualize one-dimensional functions and samples using line plots. How to visualize two-dimensional functions and samples using contour and surface plots. Do you have any questions? Ask your questions in the comments below and I will do my best to answer. Tweet Share Share The post Visualization for Function Optimization in Python appeared first on Machine Learning Mastery.\",\"248\":\"If you wanted to give @OpenAI's newest Text-to-Image Transformer (DALL\\u00b7E) a try, now you can in @Pytorch, thanks to @lucidrains! \\ud83d\\udc0d\\ud83d\\udd25 Breathtaking possibilities in generating high-quality images from arbitrary descriptions! \\u25b6\\ufe0f github.com\\/lucidrains\\/DALLE-\\u2026 h\\/t @andfanilo @ykilcher\",\"649\":\"Simulated existential crisis in the form of a video game. Please check out our sponsors: - Tryolabs: https:\\/\\/tryolabs.com\\/lex - Vincero: https:\\/\\/vincerowatches.com\\/lex to get up to 25% off + free shipping OUTLINE: 0:00 - Introduction 0:48 - Round 1 - The Matrix 8:12 - Round 2 - Reincarnation 9:58 - Round 3 - Winning 12:39 - Round 4 - Adventure line 16:30 - Round 5 - Confusion 19:58 - Round 6 - Mind control 35:09 - Round 7 - A dream within a dream 42:28 - Round 8 - Ego death 47:49 - Death becomes meaningless CONNECT: - Subscribe to this YouTube channel - Twitter: https:\\/\\/twitter.com\\/lexfridman - LinkedIn: https:\\/\\/www.linkedin.com\\/in\\/lexfridman - Facebook: https:\\/\\/www.facebook.com\\/LexFridmanPage - Instagram: https:\\/\\/www.instagram.com\\/lexfridman - Medium: https:\\/\\/medium.com\\/@lexfridman - Support on Patreon: https:\\/\\/www.patreon.com\\/lexfridman\",\"1490\":\"Manolis Kellis is a professor at MIT and head of the MIT Computational Biology Group. He is interested in understanding the human genome from a computational, evolutionary, biological, and other cross-disciplinary perspectives. Support this podcast by supporting our sponsors: \\u2013 Blinkist: https:\\/\\/blinkist.com\\/lex \\u2013 Eight Sleep: https:\\/\\/eightsleep.com\\/lex \\u2013 MasterClass: https:\\/\\/masterclass.com\\/lex If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman on Twitter, LinkedIn, Facebook, Medium, or YouTube where you can watch the video versions of these conversations. If you enjoy the podcast, please rate it 5 stars on Apple Podcasts, follow on Spotify,\",\"502\":\"Title: Haven-AI: Build Large-Scale Machine Learning Projects and Manage Thousands of Experiments Abstract: In this demo I will briefly describe why Haven-AI (https:\\/\\/github.com\\/haven-ai\\/haven-ai) is useful for quickly building large-scale reproducible Machine Learning benchmarks, and how it allows us to easily define, launch and visualize thousands of experiments. Next, starting from an empty project, I will show how to compare between 4 optimizers on two datasets and obtain visualizations that are ready to be presented in a research paper or project report. By the end of the demo you will be able to use Haven-AI to build a deep learning benchmark, launch large-scale experiments, visualize them, debug failed experiments, and generate publishable results for a final machine learning report. Bio: I am a postdoc at McGill under Derek Nowrouzezahrai and ElementAI under David Vazquez. My position is supported by the Postdoc MITACS Accelerate Scholarship. I have graduated with a PhD from University of British Columbia under the supervision of Mark Schmidt in May 2020. I focus on (1) weakly-supervised computer vision methods, (2) new machine learning optimization methods, and (3) tools that help developers define, manage, and visualize large-scale experiments for machine learning projects. ------------------------------------------------------------ Please subscribe and like the video to help me keep motivated to make awesome videos like this one. :) To buy my book, Approaching (Almost) Any Machine Learning problem, please visit: https:\\/\\/bit.ly\\/buyaaml Follow me on: Twitter: https:\\/\\/twitter.com\\/abhi1thakur LinkedIn: https:\\/\\/www.linkedin.com\\/in\\/abhi1thakur\\/ Kaggle: https:\\/\\/kaggle.com\\/abhishek Instagram: https:\\/\\/instagram.com\\/abhi4ml\",\"200\":\"Halla Yang finished 2nd ahead of 1,191 other data scientists. His experience working with time series data helped him use unsupervised\\u2026 Continue reading on Kaggle Blog \\u00bb\",\"261\":\"Conform! nitter.net\\/viaCristiano\\/status\\/1347705178699558912#m\",\"1558\":\"Pamela McCorduck is an author who has written on the history and philosophical significance of artificial intelligence, the future of engineering, and the role of women and technology. Her books include Machines Who Think in 1979, The Fifth Generation in 1983 with Ed Feigenbaum who is considered to be the father of expert systems, the Edge of Chaos, The Futures of Women, and more. Through her literary work, she has spent a lot of time with the seminal figures of artificial intelligence, includes the founding fathers of AI from the 1956 Dartmouth summer workshop where the field was launched. This\",\"1510\":\"Leonard Susskind is a professor of theoretical physics at Stanford University, and founding director of the Stanford Institute for Theoretical Physics. He is widely regarded as one of the fathers of string theory and in general as one of the greatest physicists of our time both as a researcher and an educator. This conversation is part of the Artificial Intelligence podcast. If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman on Twitter, LinkedIn, Facebook, Medium, or YouTube where you can watch the video versions of these conversations. If you enjoy the podcast,\",\"1232\":\"This Webinar and Q&A was brought to you by XP Inc. For more information about XP Inc, see: https:\\/\\/www.xpinc.com\\/ \\u2b50 NOTE: When I code, I use Kite, a free AI-powered coding assistant that will help you code faster and smarter. The Kite plugin integrates with all the top editors and IDEs to give you smart completions and documentation while you\\u2019re typing. I love it! https:\\/\\/www.kite.com\\/get-kite\\/?utm_medium=referral&utm_source=youtube&utm_campaign=statquest&utm_content=description-only For a complete index of all the StatQuest videos, check out: https:\\/\\/statquest.org\\/video-index\\/ If you'd like to support StatQuest, please consider... Patreon: https:\\/\\/www.patreon.com\\/statquest ...or... YouTube Membership: https:\\/\\/www.youtube.com\\/channel\\/UCtYLUTtgS3k1Fg4y5tAhLbw\\/join ...a cool StatQuest t-shirt or sweatshirt (USA\\/Europe): https:\\/\\/teespring.com\\/stores\\/statquest (everywhere): https:\\/\\/www.redbubble.com\\/people\\/starmer\\/works\\/40421224-statquest-double-bam?asc=u&p=t-shirt ...buying one or two of my songs (or go large and get a whole album!) https:\\/\\/joshuastarmer.bandcamp.com\\/ ...or just donating to StatQuest! https:\\/\\/www.paypal.me\\/statquest Lastly, if you want to keep up with me as I research and create new StatQuests, follow me on twitter: https:\\/\\/twitter.com\\/joshuastarmer 0:00 Introduction 1:16 The History of StatQuest 4:55 Rule #1: Focus on the Main Ideas 9:59 Rule #2: Have empathy with your audience 11:31 Rule #3: Use pictures 14:17 Rules #4 and #5: Repetition is helpful and Do the math 21:01 Rule #6 Don't repeat existing explanations 23:08 Rule #7 Limit presentation to 3 Main Ideas 24:50 Don't use a laser pointer 27:27 Dare to look stupid 29:16 Summary 32:58 Q&A: How to communicate 34:32 Q&A: The future of Big Data 39:58 Q&A: What about AutoML? 43:54 How to prepare for the future of Data Science 48:03 Academics vs Industry 52:11 Data Science vs PowerPoint 57:00 Are Neural Nets overrated? 1:00:17 What is the story behind the silly songs and BAM? 1:02:35 What is my favorite ML algorithm? 1:03:32 When and why did I decide to become a full time YouTuber 1:07:18 What do I think of the 5% cutoff for p-values? 1:12:50 A silly song!!! #StatQuest\",\"1659\":\"From self-driving cars and facial recognition to AI surveillance and GANs, computer vision tech has been the poster child of the AI industry in recent years. With such a collaborative global data science community, the advancements have come both from research teams, big tech, and computer vision startups alike. Read the full story\",\"642\":\"Dmitry Korkin is a professor of bioinformatics and computational biology at WPI. Please support this podcast by checking out our sponsors: - Brave: https:\\/\\/brave.com\\/lex - NetSuite: http:\\/\\/netsuite.com\\/lex to get free product tour - Magic Spoon: https:\\/\\/magicspoon.com\\/lex and use code LEX to get $5 off - Eight Sleep: https:\\/\\/www.eightsleep.com\\/lex and use code LEX to get special savings EPISODE LINKS: Dmitry's Website: http:\\/\\/korkinlab.org\\/ Dmitry's Twitter: https:\\/\\/twitter.com\\/dmkorkin PODCAST INFO: Podcast website: https:\\/\\/lexfridman.com\\/podcast Apple Podcasts: https:\\/\\/apple.co\\/2lwqZIr Spotify: https:\\/\\/spoti.fi\\/2nEwCF8 RSS: https:\\/\\/lexfridman.com\\/feed\\/podcast\\/ Full episodes playlist: https:\\/\\/www.youtube.com\\/playlist?list=PLrAXtmErZgOdP_8GztsuKi9nrraNbKKp4 Clips playlist: https:\\/\\/www.youtube.com\\/playlist?list=PLrAXtmErZgOeciFP3CBCIEElOJeitOr41 OUTLINE: 0:00 - Introduction 1:57 - Proteins and the building blocks of life 9:00 - Spike protein 15:48 - Coronavirus biological structure explained 20:45 - Virus mutations 27:16 - Evolution of proteins 37:02 - Self-replicating computer programs 44:38 - Origin of life 52:11 - Extraterrestrial life in our solar system 54:08 - Joshua Lederberg 1:00:07 - Dendral 1:03:01 - Why did expert systems fail? 1:05:12 - AlphaFold 2 1:26:50 - Will AI revolutionize art and music? 1:33:49 - Multi-protein folding 1:38:16 - Will AlphaFold 2 result in a Nobel Prize? 1:40:47 - Will AI be used to engineer deadly viruses? 1:55:54 - Book recommendations 2:05:37 - Family 2:08:15 - A poem in Russian CONNECT: - Subscribe to this YouTube channel - Twitter: https:\\/\\/twitter.com\\/lexfridman - LinkedIn: https:\\/\\/www.linkedin.com\\/in\\/lexfridman - Facebook: https:\\/\\/www.facebook.com\\/LexFridmanPage - Instagram: https:\\/\\/www.instagram.com\\/lexfridman - Medium: https:\\/\\/medium.com\\/@lexfridman - Support on Patreon: https:\\/\\/www.patreon.com\\/lexfridman\",\"1057\":\"Get free access to over 2500 documentaries on CuriosityStream: http:\\/\\/go.thoughtleaders.io\\/1621220200713 (use promo code \\\"zachstar\\\" at sign up) STEMerch Store: https:\\/\\/stemerch.com\\/ Support the Channel: https:\\/\\/www.patreon.com\\/zachstar PayPal(one time donation): https:\\/\\/www.paypal.me\\/ZachStarYT Download the \\\"Curved Spaces\\\" Application: http:\\/\\/www.geometrygames.org\\/CurvedSpaces\\/index.html.en Previous Video: https:\\/\\/youtu.be\\/lmcT2mP2bfE Follow up Video (spherical universe): https:\\/\\/youtu.be\\/iiGe2x8t6mA Possible Shapes of the Universe Article: https:\\/\\/www.americanscientist.org\\/sites\\/americanscientist.org\\/files\\/200522415348_306.pdf \\u25baFollow me Instagram: https:\\/\\/www.instagram.com\\/zachstar\\/ Twitter: https:\\/\\/twitter.com\\/ImZachStar Animations: Brainup Studios ( http:\\/\\/brainup.in\\/ ) \\u25baMy Setup: Space Pictures: https:\\/\\/amzn.to\\/2CC4Kqj Magnetic Floating Globe: https:\\/\\/amzn.to\\/2VgPdn0 Camera: https:\\/\\/amzn.to\\/2RivYu5 Mic: https:\\/\\/amzn.to\\/35bKiri Tripod: https:\\/\\/amzn.to\\/2RgMTNL Equilibrium Tube: https:\\/\\/amzn.to\\/2SowDrh \\u25baCheck out the my Amazon Store: https:\\/\\/www.amazon.com\\/shop\\/zachstar\",\"2336\":\"Hello everyone, I recently worked on creating a fork of a visual coding tool for python that implements machine learning algorithms with scikit-learn. Its called Ryven, and I managed to use it to create a solution to the Titanic problem on Kaggle. I wrote a fairly detailed notebook about the working process here. The goal is to make it easier to use data science and machine learning for people with limited coding experience. Do you think machine learning could be simplified and made more accessible in this way? We would really appreciate feedback on both the tool and the concept (especially in this early stage of the project). If you would like to test it, Ryven is available here: https:\\/\\/github.com\\/frecklebars\\/Ryven And the Kaggle notebook solving the Titanic problem here: https:\\/\\/www.kaggle.com\\/frecklebars\\/ml-flow-based-visual-coding-using-ryven If you have any questions, comments or suggestions about this project, please reach out either on this discussion thread, or directly message them to me on my Tweeter , @\\/frecklebars Thank you \\u200b edit: correcting a misspelling [link] [comments]\",\"1605\":\"Accurate.\",\"1674\":\"Let\\u2019s talk about self-supervised machine learning - a way to teach a model a lot without manual markup, as well as an opportunity to avoid deep learning when setting a model up to solve a problem. This material requires an intermediate level of preparation; there are many references to original publications. Read the full story\",\"4268\":\"Tweet Share Share Multinomial logistic regression is an extension of logistic regression that adds native support for multi-class classification problems. Logistic regression, by default, is limited to two-class classification problems. Some extensions like one-vs-rest can allow logistic regression to be used for multi-class classification problems, although they require that the classification problem first be transformed into multiple binary classification problems. Instead, the multinomial logistic regression algorithm is an extension to the logistic regression model that involves changing the loss function to cross-entropy loss and predict probability distribution to a multinomial probability distribution to natively support multi-class classification problems. In this tutorial, you will discover how to develop multinomial logistic regression models in Python. After completing this tutorial, you will know: Multinomial logistic regression is an extension of logistic regression for multi-class classification. How to develop and evaluate multinomial logistic regression and develop a final model for making predictions on new data. How to tune the penalty hyperparameter for the multinomial logistic regression model. Let\\u2019s get started. Multinomial Logistic Regression With Python Photo by Nicolas R\\u00e9nac, some rights reserved. Tutorial Overview This tutorial is divided into three parts; they are: Multinomial Logistic Regression Evaluate Multinomial Logistic Regression Model Tune Penalty for Multinomial Logistic Regression Multinomial Logistic Regression Logistic regression is a classification algorithm. It is intended for datasets that have numerical input variables and a categorical target variable that has two values or classes. Problems of this type are referred to as binary classification problems. Logistic regression is designed for two-class problems, modeling the target using a binomial probability distribution function. The class labels are mapped to 1 for the positive class or outcome and 0 for the negative class or outcome. The fit model predicts the probability that an example belongs to class 1. By default, logistic regression cannot be used for classification tasks that have more than two class labels, so-called multi-class classification. Instead, it requires modification to support multi-class classification problems. One popular approach for adapting logistic regression to multi-class classification problems is to split the multi-class classification problem into multiple binary classification problems and fit a standard logistic regression model on each subproblem. Techniques of this type include one-vs-rest and one-vs-one wrapper models. An alternate approach involves changing the logistic regression model to support the prediction of multiple class labels directly. Specifically, to predict the probability that an input example belongs to each known class label. The probability distribution that defines multi-class probabilities is called a multinomial probability distribution. A logistic regression model that is adapted to learn and predict a multinomial probability distribution is referred to as Multinomial Logistic Regression. Similarly, we might refer to default or standard logistic regression as Binomial Logistic Regression. Binomial Logistic Regression: Standard logistic regression that predicts a binomial probability (i.e. for two classes) for each input example. Multinomial Logistic Regression: Modified version of logistic regression that predicts a multinomial probability (i.e. more than two classes) for each input example. If you are new to binomial and multinomial probability distributions, you may want to read the tutorial: Discrete Probability Distributions for Machine Learning Changing logistic regression from binomial to multinomial probability requires a change to the loss function used to train the model (e.g. log loss to cross-entropy loss), and a change to the output from a single probability value to one probability for each class label. Now that we are familiar with multinomial logistic regression, let\\u2019s look at how we might develop and evaluate multinomial logistic regression models in Python. Evaluate Multinomial Logistic Regression Model In this section, we will develop and evaluate a multinomial logistic regression model using the scikit-learn Python machine learning library. First, we will define a synthetic multi-class classification dataset to use as the basis of the investigation. This is a generic dataset that you can easily replace with your own loaded dataset later. The make_classification() function can be used to generate a dataset with a given number of rows, columns, and classes. In this case, we will generate a dataset with 1,000 rows, 10 input variables or columns, and 3 classes. The example below generates the dataset and summarizes the shape of the arrays and the distribution of examples across the three classes. # test classification dataset from collections import Counter from sklearn.datasets import make_classification # define dataset X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, n_classes=3, random_state=1) # summarize the dataset print(X.shape, y.shape) print(Counter(y)) Running the example confirms that the dataset has 1,000 rows and 10 columns, as we expected, and that the rows are distributed approximately evenly across the three classes, with about 334 examples in each class. (1000, 10) (1000,) Counter({1: 334, 2: 334, 0: 332}) Logistic regression is supported in the scikit-learn library via the LogisticRegression class. The LogisticRegression class can be configured for multinomial logistic regression by setting the \\u201cmulti_class\\u201d argument to \\u201cmultinomial\\u201d and the \\u201csolver\\u201d argument to a solver that supports multinomial logistic regression, such as \\u201clbfgs\\u201c. ... # define the multinomial logistic regression model model = LogisticRegression(multi_class='multinomial', solver='lbfgs') The multinomial logistic regression model will be fit using cross-entropy loss and will predict the integer value for each integer encoded class label. Now that we are familiar with the multinomial logistic regression API, we can look at how we might evaluate a multinomial logistic regression model on our synthetic multi-class classification dataset. It is a good practice to evaluate classification models using repeated stratified k-fold cross-validation. The stratification ensures that each cross-validation fold has approximately the same distribution of examples in each class as the whole training dataset. We will use three repeats with 10 folds, which is a good default, and evaluate model performance using classification accuracy given that the classes are balanced. The complete example of evaluating multinomial logistic regression for multi-class classification is listed below. # evaluate multinomial logistic regression model from numpy import mean from numpy import std from sklearn.datasets import make_classification from sklearn.model_selection import cross_val_score from sklearn.model_selection import RepeatedStratifiedKFold from sklearn.linear_model import LogisticRegression # define dataset X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, n_classes=3, random_state=1) # define the multinomial logistic regression model model = LogisticRegression(multi_class='multinomial', solver='lbfgs') # define the model evaluation procedure cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) # evaluate the model and collect the scores n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1) # report the model performance print('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores))) Running the example reports the mean classification accuracy across all folds and repeats of the evaluation procedure. Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome. In this case, we can see that the multinomial logistic regression model with default penalty achieved a mean classification accuracy of about 68.1 percent on our synthetic classification dataset. Mean Accuracy: 0.681 (0.042) We may decide to use the multinomial logistic regression model as our final model and make predictions on new data. This can be achieved by first fitting the model on all available data, then calling the predict() function to make a prediction for new data. The example below demonstrates how to make a prediction for new data using the multinomial logistic regression model. # make a prediction with a multinomial logistic regression model from sklearn.datasets import make_classification from sklearn.linear_model import LogisticRegression # define dataset X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, n_classes=3, random_state=1) # define the multinomial logistic regression model model = LogisticRegression(multi_class='multinomial', solver='lbfgs') # fit the model on the whole dataset model.fit(X, y) # define a single row of input data row = [1.89149379, -0.39847585, 1.63856893, 0.01647165, 1.51892395, -3.52651223, 1.80998823, 0.58810926, -0.02542177, -0.52835426] # predict the class label yhat = model.predict([row]) # summarize the predicted class print('Predicted Class: %d' % yhat[0]) Running the example first fits the model on all available data, then defines a row of data, which is provided to the model in order to make a prediction. In this case, we can see that the model predicted the class \\u201c1\\u201d for the single row of data. Predicted Class: 1 A benefit of multinomial logistic regression is that it can predict calibrated probabilities across all known class labels in the dataset. This can be achieved by calling the predict_proba() function on the model. The example below demonstrates how to predict a multinomial probability distribution for a new example using the multinomial logistic regression model. # predict probabilities with a multinomial logistic regression model from sklearn.datasets import make_classification from sklearn.linear_model import LogisticRegression # define dataset X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, n_classes=3, random_state=1) # define the multinomial logistic regression model model = LogisticRegression(multi_class='multinomial', solver='lbfgs') # fit the model on the whole dataset model.fit(X, y) # define a single row of input data row = [1.89149379, -0.39847585, 1.63856893, 0.01647165, 1.51892395, -3.52651223, 1.80998823, 0.58810926, -0.02542177, -0.52835426] # predict a multinomial probability distribution yhat = model.predict_proba([row]) # summarize the predicted probabilities print('Predicted Probabilities: %s' % yhat[0]) Running the example first fits the model on all available data, then defines a row of data, which is provided to the model in order to predict class probabilities. Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome. In this case, we can see that class 1 (e.g. the array index is mapped to the class integer value) has the largest predicted probability with about 0.50. Predicted Probabilities: [0.16470456 0.50297138 0.33232406] Now that we are familiar with evaluating and using multinomial logistic regression models, let\\u2019s explore how we might tune the model hyperparameters. Tune Penalty for Multinomial Logistic Regression An important hyperparameter to tune for multinomial logistic regression is the penalty term. This term imposes pressure on the model to seek smaller model weights. This is achieved by adding a weighted sum of the model coefficients to the loss function, encouraging the model to reduce the size of the weights along with the error while fitting the model. A popular type of penalty is the L2 penalty that adds the (weighted) sum of the squared coefficients to the loss function. A weighting of the coefficients can be used that reduces the strength of the penalty from full penalty to a very slight penalty. By default, the LogisticRegression class uses the L2 penalty with a weighting of coefficients set to 1.0. The type of penalty can be set via the \\u201cpenalty\\u201d argument with values of \\u201cl1\\u201c, \\u201cl2\\u201c, \\u201celasticnet\\u201d (e.g. both), although not all solvers support all penalty types. The weighting of the coefficients in the penalty can be set via the \\u201cC\\u201d argument. ... # define the multinomial logistic regression model with a default penalty LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', C=1.0) The weighting for the penalty is actually the inverse weighting, perhaps penalty = 1 \\u2013 C. From the documentation: C : float, default=1.0 Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization. This means that values close to 1.0 indicate very little penalty and values close to zero indicate a strong penalty. A C value of 1.0 may indicate no penalty at all. C close to 1.0: Light penalty. C close to 0.0: Strong penalty. The penalty can be disabled by setting the \\u201cpenalty\\u201d argument to the string \\u201cnone\\u201c. ... # define the multinomial logistic regression model without a penalty LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='none') Now that we are familiar with the penalty, let\\u2019s look at how we might explore the effect of different penalty values on the performance of the multinomial logistic regression model. It is common to test penalty values on a log scale in order to quickly discover the scale of penalty that works well for a model. Once found, further tuning at that scale may be beneficial. We will explore the L2 penalty with weighting values in the range from 0.0001 to 1.0 on a log scale, in addition to no penalty or 0.0. The complete example of evaluating L2 penalty values for multinomial logistic regression is listed below. # tune regularization for multinomial logistic regression from numpy import mean from numpy import std from sklearn.datasets import make_classification from sklearn.model_selection import cross_val_score from sklearn.model_selection import RepeatedStratifiedKFold from sklearn.linear_model import LogisticRegression from matplotlib import pyplot # get the dataset def get_dataset(): X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=1, n_classes=3) return X, y # get a list of models to evaluate def get_models(): models = dict() for p in [0.0, 0.0001, 0.001, 0.01, 0.1, 1.0]: # create name for model key = '%.4f' % p # turn off penalty in some cases if p == 0.0: # no penalty in this case models[key] = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='none') else: models[key] = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', C=p) return models # evaluate a give model using cross-validation def evaluate_model(model, X, y): # define the evaluation procedure cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) # evaluate the model scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1) return scores # define dataset X, y = get_dataset() # get the models to evaluate models = get_models() # evaluate the models and store results results, names = list(), list() for name, model in models.items(): # evaluate the model and collect the scores scores = evaluate_model(model, X, y) # store the results results.append(scores) names.append(name) # summarize progress along the way print('\\/prestrong\\/stronga href=\\\"https:\\/\\/machinelearningmastery.com\\/different-results-each-time-in-machine-learning\\/\\\"\\/apre class=\\\"urvanov-syntax-highlighter-plain-tag\\\"\",\"1528\":\"My attempt to find the words to honor my grandmother, an amazing woman who is responsible for much of who I am, who taught me how to be a man, taught me about strength, about wisdom, about compassion, about love. If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/podcast or connect with @lexfridman on Twitter, LinkedIn, Facebook, Medium, or YouTube where you can watch the video versions of these conversations. If you enjoy the podcast, please rate it 5 stars on Apple Podcasts, follow on Spotify, or support it on Patreon. Here\\u2019s the outline of the\",\"1471\":\"Dan Gable is one of the greatest Olympic athletes and wrestling coaches of all time. Please support this podcast by checking out our sponsors: \\u2013 Tryolabs: https:\\/\\/tryolabs.com\\/lex \\u2013 ExpressVPN: https:\\/\\/expressvpn.com\\/lexpod and use code LexPod to get 3 months free \\u2013 Grammarly: https:\\/\\/grammarly.com\\/lex to get 20% off premium \\u2013 SimpliSafe: https:\\/\\/simplisafe.com\\/lex and use code LEX to get a free security camera EPISODE LINKS: Dan\\u2019s Twitter: https:\\/\\/twitter.com\\/dannygable Dan\\u2019s Website: https:\\/\\/dangable.com\\/ Dan\\u2019s Books: https:\\/\\/amzn.to\\/2VK5nbn PODCAST INFO: Podcast website: https:\\/\\/lexfridman.com\\/podcast Apple Podcasts: https:\\/\\/apple.co\\/2lwqZIr Spotify: https:\\/\\/spoti.fi\\/2nEwCF8 RSS: https:\\/\\/lexfridman.com\\/feed\\/podcast\\/ YouTube Full Episodes: https:\\/\\/youtube.com\\/lexfridman YouTube Clips: https:\\/\\/youtube.com\\/lexclips SUPPORT & CONNECT: \\u2013 Check out the sponsors above, it\\u2019s the best way to support this podcast \\u2013 Support on Patreon: https:\\/\\/www.patreon.com\\/lexfridman \\u2013 Twitter: https:\\/\\/twitter.com\\/lexfridman \\u2013 Instagram: https:\\/\\/www.instagram.com\\/lexfridman \\u2013 LinkedIn: https:\\/\\/www.linkedin.com\\/in\\/lexfridman \\u2013 Facebook: https:\\/\\/www.facebook.com\\/LexFridmanPage \\u2013 Medium: https:\\/\\/medium.com\\/@lexfridman OUTLINE: Here\\u2019s the timestamps for the episode. On some podcast players you should be able to click the timestamp to jump to that time. (00:00) \\u2013 Introduction (09:31) \\u2013 Russian wrestling (11:09) \\u2013 Coaching the science, art, and toughness of wrestling (18:05) \\u2013 The pain of defeat and the tattoo of a hawk clawing out the heart (21:04) \\u2013 Roger Bannister and the 4 minute mile (24:09) \\u2013 The dream of becoming an Olympic champion (26:38) \\u2013 The day in 1972 of the Olympic final (30:10) \\u2013 Sauna story (31:39) \\u2013 Match against the Russian (37:13) \\u2013 The role of fear in wrestling (42:14) \\u2013 The line between physical wrestling and anger (46:53) \\u2013 Tragic loss of Dan\\u2019s sister (54:21) \\u2013 The role of family in wrestling (59:43) \\u2013 Wrestling being voted out of the Olympics (1:04:26) \\u2013 To beat the best you must study the best (1:09:39) \\u2013 The role of luck (Old Man and the Sea)\",\"4303\":\"Dr. Tim Scarfe, Yannic Kilcher and Sayak Paul chat with Sara Hooker from the Google Brain team! We discuss her recent hardware lottery paper, pruning \\/ sparsity, bias mitigation and intepretability. The hardware lottery -- what causes inertia or friction in the marketplace of ideas? Is there a meritocracy of ideas or do the previous decisions we have made enslave us? Sara Hooker calls this a lottery because she feels that machine learning progress is entirely beholdant to the hardware and software landscape. Ideas succeed if they are compatible with the hardware and software at the time and also the existing inventions. The machine learning community is exceptional because the pace of innovation is fast and we operate largely in the open, this is largely because we don't build anything physical which is expensive, slow and the cost of being scooped is high. We get stuck in basins of attraction based on our technology decisions and it's expensive to jump outside of these basins. So is this story unique to hardware and AI algorithms or is it really just the story of all innovation? Every great innovation must wait for the right stepping stone to be in place before it can really happen. We are excited to bring you Sara Hooker to give her take. YouTube version (including TOC): https:\\/\\/youtu.be\\/sQFxbQ7ade0 Show notes; https:\\/\\/drive.google.com\\/file\\/d\\/1S_rHnhaoVX4Nzx_8e3ESQq4uSswASNo7\\/view?usp=sharing Sara Hooker page; https:\\/\\/www.sarahooker.me\",\"1600\":\"I love America.\",\"495\":\"This Christmas #NVIDIA gifted me an #RTX #3090 GPU and I decided to put it to good use. But before that, I needed to unbox and install it. So, I decided to make a video around it. This is my first unboxing video. I hope you like it :) Please subscribe and like the video to help me keep motivated to make awesome videos like this one. :) To buy my book, Approaching (Almost) Any Machine Learning problem, please visit: https:\\/\\/bit.ly\\/buyaaml Follow me on: Twitter: https:\\/\\/twitter.com\\/abhi1thakur LinkedIn: https:\\/\\/www.linkedin.com\\/in\\/abhi1thakur\\/ Kaggle: https:\\/\\/kaggle.com\\/abhishek Instagram: https:\\/\\/instagram.com\\/abhi4ml\",\"4877\":\"\\u201cDiversity is the collective strength of any successful organization Unconscious Bias in Job Descriptions Unconscious bias affects us all, in one way or the other. It is defined as the prejudice or unsupported judgments in favor of or against one thing, person, or group as compared to another, in a way that is usually considered unfair. Unconscious bias is being discussed in colleges, universities, and across big and small workplaces today. One of the most prominent examples of unconscious bias is observed in the hiring process adopted by companies. Often, the job bulletins that are put forth contain elements that favor a particular gender or group. Biased job descriptions not only limits the candidate pool but also diversity in the workplace. Therefore, the companies need to check out for any biases in the job descriptions and eliminate them to create a healthy and fair culture in an organization. So how can one detect biased job descriptions in the first place? Well, Artificial Intelligence technologies can come to the rescue. How can Artificial Intelligence provide an answer? Today machine learning and Artificial Intelligence have made it possible to analyze data from various sources with a lot more accuracy and precision. Whether it is structured or unstructured data, AI-backed technologies can provide superior results compared to manual processing. Hence, It\\u2019ll be a great idea if these AI applications are infused into a platform that enables business users to directly interact with data without a lot of hassle and complexities. Well, H2O Wave has been created to do precisely this, and in this article, we\\u2019ll clearly see how to implement this idea. But before we go further, let\\u2019s quickly understand what H2O Wave is and what are its advantages. H2O Wave H2O Wave is an open-source Python development framework that makes it fast and easy for data scientists, machine learning engineers, and software developers to develop real-time interactive AI apps with sophisticated visualizations. H2O Wave accelerates development with a wide variety of user-interface components and charts, including dashboard templates, dialogs, themes, widgets, and many more. There are thousands of potential use cases, and some of them are: Using Wave\\u2019s \\u2018Hiring Bias\\u2019 App to detect unconscious bias in the Job Description dataset. Let\\u2019s now see how AI can help us detect unconscious bias in a dataset containing job descriptions. In this article, we shall be working with a preprocessed version of the Los Angeles Job Description dataset from Kaggle, which includes the following attributes: Our job is to detect whether the text in the description_text the column contains unconscious bias or not. Methodology Hiring Bias Analysis App This dataset is first loaded into the app. The \\u2018Hiring App\\u2019 consists of several machine learning algorithms and models that analyze different parts of the job description text and perform analysis based on the word choices, text structure, tone, sentiment, etc. The app then generates a detailed report containing multiple insights and findings in form of a dashboard. Here are some of the detailed findings grouped under the following headings, i.e., Word Choice, Text Structure, or Tone \\/ Sentiment. 1. Analysis of Word Choice Unconscious Bias in job descriptions towards a specific gender can limit the candidate pool and diversity. The figure below shows some of how the choice of words can lead to bias. Let\\u2019s see some of the insights presented by the app : Use of Gendered Keywords in Job Descriptions Using gender-specific words in the job description can isolate a specific gender from applying to certain jobs. It has been observed that words that are more \\u201caggressive,\\u201d \\u201cassertive,\\u201d or \\u201cindependent\\u201d typically put off women from applying to specific roles. The plots below show the male and female-specific keywords identified in the dataset. Higher use of Superlatives The following example shows certain job descriptions where highly superlative keywords, specifically \\u201cMaster\\u201d and \\u201cExpert,\\u201d are used. These keywords have a very strong masculine tone and hence show a preference towards a particular gender. Use of Gendered pronouns It is a wrong practice to specify only two genders in the job descriptions in the form of \\u201che\\/she.\\u201d This usage is another form of unconscious gender bias that is commonly present in the descriptions today. 2. Analysis of Text Structures Several studies (LinkedIn, Forbes, and Glassdoor) have suggested that both the quality and the quantity of an applicant\\u2019s pool can be significantly influenced by how a job description is written. \\u201cA well-written, complete, and insightful job description can result in attracting some of the top and diverse talents for the role.\\u201d On the other hand, a description that lacks vital features (for example \\u2014 an optimal word limit, choice of the words, language used, overall tone) may result in attracting fewer candidates. The text structure of a job description can be analyzed in several ways, namely : Analysis of Difficulty level of keywords in job descriptions Since readability is of paramount importance, it is a good idea to focus on the words that are difficult to read for people and to eliminate them. The graph below shows the high usage of difficult words in the descriptions of various jobs. Analysis of the readability of Job descriptions Similarly, companies should refrain from making job descriptions too complex or challenging to read. For instance, on analysis, it was found that the following job descriptions for the following posts were the least readable. 3. Analysis of Tone \\/ Sentiment Sentiment analysis is a sub-field of Natural Language Processing (NLP)that tries to identify and extract opinions from a given text. Sentiment analysis of the job descriptions can help the companies to gauge their tone and overall sentiment. The sentiments or the tone conveyed in the job descriptions should not be too negative or demanding, resulting in fewer people applying for the job. Overall Sentiment of Job Descriptions Use of sentences and words that convey moderate or high negative sentiments should be avoided. The following plot shows the distribution of the negative sentiments in the job descriptions. The tone of Job Descriptions Companies must ensure that the tone of the job descriptions shouldn\\u2019t be too negative. For instance, keywords containing negative sentiments that have been automatically highlighted below should be avoided. Use of Strict Keywords in Job Descriptions Again, excessive use of demanding keywords in nature is also not desirable in a job description. Phrases such as \\u201cwho fail,\\u201d \\u201cwill not be considered,\\u201d \\u201cmust-have,\\u201d etc., should be avoided or replaced with positive and encouraging words like \\u201cgood to have,\\u201d \\u201cadd-on,\\u201d etc. The analysis clearly demonstrated that the idea of job descriptions is to encourage a wider group of people to apply. Thus, the choice of language and words in job descriptions plays a crucial role in promoting applicant diversity. Hence, factors like content, tone, language, and format can directly or indirectly influence the hiring process of a company. Create your own apps with H2O Wave The \\u2018Hiring Bias\\u2019 app has been created using H2O Wave and we have made it easy for others also to create similar interactive and visual AI applications. The best part is that all this can be done using only Python without having to learn HTML, CSS, or Javascript. Wave comes with robust documentation with numerous examples to get you started immediately. We are waiting to see what you create with Wave. The post Using AI to unearth the unconscious bias in job descriptions appeared first on Open Source Leader in AI and ML.\",\"705\":\"Episode 007 | December 21, 2020 One of Microsoft Research India\\u2019s goals is to help strengthen the research ecosystem and encourage young students to look at research as a career. But it is not always easy for students to understand what research is all about and how to figure out if research is the right career for them. The Research Fellow program at Microsoft Research India enables bright young students to work on real-world research problems with top notch researchers across the research lifecycle, including ideation, implementation, evaluation, and deployment. Many of the students who have been part of the program have gone on to become researchers, engineers and entrepreneurs. Show notes and transcript: https:\\/\\/www.microsoft.com\\/en-us\\/research\\/lab\\/microsoft-research-india\\/articles\\/podcast-helping-young-students-build-a-career-in-research-through-the-msr-india-research-fellow-program-with-shruti-rijhwani-and-dr-vivek-seshadri\\/ Research Fellows Program at Microsoft Research India: https:\\/\\/www.microsoft.com\\/en-us\\/research\\/academic-program\\/research-fellows-program-at-microsoft-research-india\\/ See more Microsoft Research India podcast episodes and learn about the research: https:\\/\\/www.microsoft.com\\/en-us\\/research\\/lab\\/microsoft-research-india\\/\",\"4899\":\"r\\/woooosh nitter.net\\/ylecun\\/status\\/1352247446320132096#m\",\"3897\":\"STEMerch Store: https:\\/\\/stemerch.com\\/ Second channel (just skits): https:\\/\\/www.youtube.com\\/zachstarhimself Support the Channel: https:\\/\\/www.patreon.com\\/zachstar PayPal(one time donation): https:\\/\\/www.paypal.me\\/ZachStarYT Join this channel to get access to perks: https:\\/\\/www.youtube.com\\/channel\\/UCpCSAcbqs-sjEVfk_hMfY9w\\/join \\u25baFollow me Instagram: https:\\/\\/www.instagram.com\\/zachstar\\/ Twitter: https:\\/\\/twitter.com\\/ImZachStar 2D Graphing Software: https:\\/\\/www.desmos.com\\/calculator Animations: Brainup Studios ( http:\\/\\/brainup.in\\/ ) Check out my Spanish channel here: https:\\/\\/www.youtube.com\\/channel\\/UCnkNu2xQBLASpj6cKC8vtpA \\u25baMy Setup: Space Pictures: https:\\/\\/amzn.to\\/2CC4Kqj Magnetic Floating Globe: https:\\/\\/amzn.to\\/2VgPdn0 Camera: https:\\/\\/amzn.to\\/2RivYu5 Mic: https:\\/\\/amzn.to\\/35bKiri Tripod: https:\\/\\/amzn.to\\/2RgMTNL Equilibrium Tube: https:\\/\\/amzn.to\\/2SowDrh \\u25baCheck out my Amazon Store: https:\\/\\/www.amazon.com\\/shop\\/zachstar\",\"5670\":\"So I've been worked on various machine learning experiments - in both academic and industry roles - and was consistently bugged by how hard it was to train models on GPU boxes. You have to set up a cloud compute account, ssh into some box, move files around with git, and figure out how log and track the results of a training run... I have yet to find good options on the market (have looked at anyscale, determined, databricks, etc.) so I wrote some software to do what I wanted and decided to turn it into a venture. I want to open up this thread for discussion about issues with machine learning versioning, tracking, and training in general - and maybe see if this could be a valid solution - https:\\/\\/latch.ai\\/. Would love to get some substantive conversations going below! [link] [comments]\",\"1439\":\"Yann LeCun is one of the fathers of deep learning, the recent revolution in AI that has captivated the world with the possibility of what machines can learn from data. He is a professor at New York University, a Vice President & Chief AI Scientist at Facebook, co-recipient of the Turing Award for his work on deep learning. He is probably best known as the founder of convolutional neural networks, in particular their early application to optical character recognition. This conversation is part of the Artificial Intelligence podcast. If you would like to get more information about this podcast go to\",\"2345\":\"https:\\/\\/youtu.be\\/YDAcc20GyU0 Dr. Nathan Kundtz presentation to the NGA on learnings from the Rendered.ai Common Applications Framework for Synthetic Data. How physics based synthetic data and simulation libraries represent only part of the solution. [link] [comments]\",\"1541\":\"A conversation with Max Tegmark as part of MIT course on Artificial General Intelligence. Video version is available on YouTube. He is a Physics Professor at MIT, co-founder of the Future of Life Institute, and author of \\u201cLife 3.0: Being Human in the Age of Artificial Intelligence.\\u201d If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman on Twitter, LinkedIn, Facebook, or YouTube where you can watch the video versions of these conversations.\",\"482\":\"An information puzzle with an interesting twist Solution on Stand-up Maths: https:\\/\\/youtu.be\\/as7Gkm7Y7h4 Paid for by viewers, like you: https:\\/\\/3b1b.co\\/chess-thanks Home page: https:\\/\\/www.3blue1brown.com ------------------ 0:00 Introduction 3:58 Visualizing the two-square case 5:46 Visualizing the three-square case 12:19 Proof that it's impossible 16:22 Explicit painting of the hypercube ------------------ Thanks to everyone who endured me probing them with this puzzle and provided helpful discussion, especially Cam Christensen, Matt Parker, and Mike Sklar. Mike, by the way, deserves credit for coming up with the particularly clean way to see that it's impossible when n is not a power of 2. These animations are largely made using manim, a scrappy open-source python library: https:\\/\\/github.com\\/3b1b\\/manim If you want to check it out, I feel compelled to warn you that it's not the most well-documented tool, and it has many other quirks you might expect in a library someone wrote with only their own use in mind. Music by Vincent Rubinetti. Download the music on Bandcamp: https:\\/\\/vincerubinetti.bandcamp.com\\/album\\/the-music-of-3blue1brown Stream the music on Spotify: https:\\/\\/open.spotify.com\\/album\\/1dVyjwS8FBqXhRunaG5W5u If you want to contribute translated subtitles or to help review those that have already been made by others and need approval, you can click the gear icon in the video and go to subtitles\\/cc, then \\\"add subtitles\\/cc\\\". I really appreciate those who do this, as it helps make the lessons accessible to more people. ------------------ 3blue1brown is a channel about animating math, in all senses of the word animate. And you know the drill with YouTube, if you want to stay posted on new videos, subscribe: http:\\/\\/3b1b.co\\/subscribe Various social media stuffs: Website: https:\\/\\/www.3blue1brown.com Twitter: https:\\/\\/twitter.com\\/3blue1brown Reddit: https:\\/\\/www.reddit.com\\/r\\/3blue1brown Instagram: https:\\/\\/www.instagram.com\\/3blue1brown_animations\\/ Patreon: https:\\/\\/patreon.com\\/3blue1brown Facebook: https:\\/\\/www.facebook.com\\/3blue1brown\",\"5671\":\"I'm looking for some general thoughts on how you would go about building predictions where the output is a type of range or set of ranges, with inputs that are best guesses from a number of models, with somewhat known accuracies. Each individual model tends not to be very good, but almost always at least *one* model tends to get it right, or mostly right. Ranges tend to be continuous but it is possible in some cases they may be discontinuous (e.g. usually the truth data is a single range but it may be as many as 2-3, but probably not more). For example, let's say I want to predict what time some action is valid. I have a range of [0, 3600) (e.g. minutes in a day). Model 1 has a known accuracy of 60% and they say the valid range is [0, 1200). Model 2 has a known accuracy of 70% and they say the valid range is [0, 1100) and also [1300, 1500). Maybe the truth data is that the valid range is [0, 1200) and [1300, 1500), which would be a combination of both models. Naively, I could approach this by discretizing the ranges into individual predictions (e.g. I could predict 0, 15, 30...) . I would probably featurize each model's binary prediction for validity at that moment as well as their scores, including few other attributes. However, I feel like this might have some issues that it could result in discontinuity (e.g. model predicts a series of Y\\/N\\/Y\\/N\\/Y\\/N over some range that should be continuous). I could always attempt to smooth discontinuity in post-processing but that seems coarse at best. I feel like abstractly this is some sort of 1D edge\\/object detection, but all of the literature I see seems to apply this to images. I am also up against the issue that I have very limited truth data (tens of thousands of samples, maybe), but I do have a lot of model data. Maybe the best approach would be some heuristic-driven statistical model instead? I'd love some thoughts on how you might approach this, or if you're aware of any literature dealing with this. [link] [comments]\",\"4878\":\"One of my favorite podcasts ever with my Russian brother @lexfridman! We really had a great flow. Available now in video and audio on @spotify instagram.com\\/p\\/CKZYl4iF338\\/\\u2026\",\"1554\":\"Ilya Sutskever is the co-founder of OpenAI, is one of the most cited computer scientist in history with over 165,000 citations, and to me, is one of the most brilliant and insightful minds ever in the field of deep learning. There are very few people in this world who I would rather talk to and brainstorm with about deep learning, intelligence, and life than Ilya, on and off the mic. Support this podcast by signing up with these sponsors: \\u2013 Cash App \\u2013 use code \\u201cLexPodcast\\u201d and download: \\u2013 Cash App (App Store): https:\\/\\/apple.co\\/2sPrUHe \\u2013 Cash App (Google Play): https:\\/\\/bit.ly\\/2MlvP5w EPISODE LINKS:\",\"4304\":\"This week on Machine Learning Street Talk, Dr. Tim Scarfe, Dr. Keith Duggar, Alex Stenlake and Yannic Kilcher have a conversation with the founder and principal researcher at the Montreal AI Ethics institute -- Abhishek Gupta. We cover several topics from the Social Dilemma film and AI Ethics in general. 00:00:00 Introduction 00:03:57 Overcome our weaknesses 00:14:30 threat landscape blind spots 00:18:35 differential reality vs universal shaping 00:24:21 shared reality incentives and tools 00:32:01 transparency and knowledge to avoid pathology 00:40:09 federated informed autonomy 00:49:48 diversity is a metric, inclusion is a strategy 00:59:58 locally aligned pockets can stabilize global diversity 01:10:58 making inclusion easier with tools 01:23:35 enabling community feedback 01:26:16 open source the algorithms 01:33:02 the N+1 cost of inclusion 01:38:08 broader impact statement https:\\/\\/atg-abhishek.github.io https:\\/\\/www.linkedin.com\\/in\\/abhishekguptamcgill\\/\",\"497\":\"In this video, I will show you how easy and simple it is to build your own #Trainer for training #PyTorch models. This will help you make all kinds of customizations you want. This video is also an intro to #Tez, which is a small, simple, to-the-point, no-nonsense trainer\\/wrapper that I have built. You can find it here: https:\\/\\/github.com\\/abhishekkrthakur\\/tez. I will be using Tez in all future videos to keep the videos short and precise while still doing the same things. If you like Tez, do star the repository. Please subscribe and like the video to help me keep motivated to make awesome videos like this one. :) To buy my book, Approaching (Almost) Any Machine Learning problem, please visit: https:\\/\\/bit.ly\\/buyaaml Follow me on: Twitter: https:\\/\\/twitter.com\\/abhi1thakur LinkedIn: https:\\/\\/www.linkedin.com\\/in\\/abhi1thakur\\/ Kaggle: https:\\/\\/kaggle.com\\/abhishek Instagram: https:\\/\\/instagram.com\\/abhi4ml\",\"485\":\"Tips on problem-solving, with examples from geometry, trig, and probability. Past episodes with integrated quizzes: https:\\/\\/itempool.com\\/c\\/3b1b Full playlist: https:\\/\\/www.youtube.com\\/playlist?list=PLZHQObOWTQDP5CVelJJ1bNDouqrAhVPev Home page: https:\\/\\/www.3blue1brown.com Brought to you by you: https:\\/\\/3b1b.co\\/ldm-thanks Huge huge thanks to Ben Eater: https:\\/\\/www.youtube.com\\/user\\/eaterbc And Cam Christensen, creator of ItemPool: https:\\/\\/itempool.com\\/ Notes by Ng\\u00e2n V\\u0169: https:\\/\\/twitter.com\\/ThuyNganVu\\/status\\/1265480770832855040 Mistakes: 50:35, there should be a dx in the integral 54:40, if you notice the mistake here and are inclined to complain, keep watching ------------------ Video timeline (thanks to user \\\"noonesperfect\\\") 0:34 9-Problem Solving Principles\\/Tip 1:15 Question 1 (Probability) 2:08 Who is Ben Eater? 4:25 Inscribed angle theorem, \\u03b8L=2*\\u03b8s 5:58 Tip-1 7:48 Tip-2 8:16 Question 2 9:34 Answer 2 10:29 Tip-3 15:17 Tip-4 22:48 Question 3 25:56 Answer 3 (Marked incorrectly, Answer: Option D) 26:31 Answer 1 27:28 Explanation for Q1, Floor function 30:38 Tip-5 32:53 Tip-6 33:36 Question 4 34:43 Answer 4 36:37 Question 5 38:10 Answer 5 41:48 Probability graph in Desmos 44:08 Revisiting an alternating series sum for ln(2) 47:29 Tip-7 51:08 Tip-8 55:23 Audience questions through tweets 57:28 Tip-9 58:29 Python programming for various probability question 1:04:31 Arts created using Desmos graph tool with mathematical expressions 1:05:54 Thank you, appreciation to the team and all. ------------------ The live question setup with stats on-screen is powered by Itempool. https:\\/\\/itempool.com\\/ Curious about other animations? https:\\/\\/www.3blue1brown.com\\/faq#manim Music by Vincent Rubinetti. Download the music on Bandcamp: https:\\/\\/vincerubinetti.bandcamp.com\\/album\\/the-music-of-3blue1brown Stream the music on Spotify: https:\\/\\/open.spotify.com\\/album\\/1dVyjwS8FBqXhRunaG5W5u If you want to contribute translated subtitles or to help review those that have already been made by others and need approval, you can click the gear icon in the video and go to subtitles\\/cc, then \\\"add subtitles\\/cc\\\". I really appreciate those who do this, as it helps make the lessons accessible to more people. ------------------ 3blue1brown is a channel about animating math, in all senses of the word animate. And you know the drill with YouTube, if you want to stay posted on new videos, subscribe: http:\\/\\/3b1b.co\\/subscribe Various social media stuffs: Website: https:\\/\\/www.3blue1brown.com Twitter: https:\\/\\/twitter.com\\/3blue1brown Reddit: https:\\/\\/www.reddit.com\\/r\\/3blue1brown Instagram: https:\\/\\/www.instagram.com\\/3blue1brown_animations\\/ Patreon: https:\\/\\/patreon.com\\/3blue1brown Facebook: https:\\/\\/www.facebook.com\\/3blue1brown\",\"235\":\"I somehow missed tenet, a new Nolan movie from back in August. Watched it last night bracing for disappointment because of mediocre reviews but when the disorientation settled I realized this may be one of my favorite movies ever. Not certain yet, have to watch a few more times.\",\"2342\":\"Highlights: Authors show that GPT-3 contains a strong Muslim-violence bias. The authors test it in different ways, including a setup where they generate Humans-of-New-York-style captions, and ones with Muslims generate violent captions. Paper: https:\\/\\/arxiv.org\\/pdf\\/2101.05783.pdf [link] [comments]\",\"1706\":\"Read the full story\",\"4081\":\"I wanted to share a small example of training SimSiam (from paper Exploring Simple Siamese Representation Learning,https:\\/\\/arxiv.org\\/pdf\\/2011.10566.pdf) on cifar10 with a kNN callback mechanism. The kNN callback creates features of the training set at the end of every epoch and then uses weighted kNN to make predictions on the test set. The mechanism for kNN is pretty simple thanks to PyTorch Lightning. The SimSiam implementation is from lightly. Training to 91% takes 800 epochs, which is around 8h on my V100 GPU. The code can be found here: https:\\/\\/github.com\\/IgorSusmelj\\/simsiam-cifar10 [link] [comments]\",\"1551\":\"Scott Aaronson is a professor at UT Austin, director of its Quantum Information Center, and previously a professor at MIT. His research interests center around the capabilities and limits of quantum computers and computational complexity theory more generally. This conversation is part of the Artificial Intelligence podcast. If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman on Twitter, LinkedIn, Facebook, Medium, or YouTube where you can watch the video versions of these conversations. If you enjoy the podcast, please rate it 5 stars on Apple Podcasts, follow on Spotify, or support it\",\"1673\":\"Read the full story\",\"2346\":\"https:\\/\\/youtu.be\\/1ZbLA7ofasY Hi! I filmed a video on Forward Hooks in PyTorch. I would be more than happy to get any feedback and constructive criticism! I am planning to do these \\\"applied ML\\/DL videos\\\" on a regular basis. I am a big proponent of free online education and have learned a bunch from it in my life. I feel like giving back with the little I know:). [link] [comments]\",\"239\":\"CC @AMPRobotics rollingstone.com\\/culture\\/cul\\u2026\",\"1595\":\"More people die from suicide than all other forms of violence combined. People are struggling. I see a lot of outrage online at the moment. What I don't see much of is compassion. As Gandhi said: \\\"An eye for an eye will make the whole world blind.\\\"\",\"707\":\"Day 1 | November 17, 2020 Theme: Envisioning the Future of Tech for Inclusion Chancey Fleet, New York Public Library The Accessible Computer Science Education Fall Workshop was hosted by Microsoft, University of Washington CREATE, and University of Colorado\\u2019s Coleman Institute. It took place November 17-19, 2020 and consisted of three half-days of talks, discussions, and planning for new research dedicated to making Computer Science education learning experiences more accessible for people with disabilities. More information on this workshop can be found at https:\\/\\/www.microsoft.com\\/en-us\\/research\\/event\\/accessible-cs-education-fall-workshop\\/\",\"1552\":\"Dan Carlin is a historian, political thinker, and podcaster. Please support this podcast by checking out our sponsors: \\u2013 Athletic Greens: https:\\/\\/athleticgreens.com\\/lex and use code LEX to get free vitamin D \\u2013 SimpliSafe: https:\\/\\/simplisafe.com\\/lex and use code LEX to get a free security camera \\u2013 Magic Spoon: https:\\/\\/magicspoon.com\\/lex and use code LEX to get free shipping \\u2013 Cash App: https:\\/\\/cash.app\\/ and use code LexPodcast to get $10 EPISODE LINKS: Dan\\u2019s Twitter: https:\\/\\/twitter.com\\/hardcorehistory Dan\\u2019s Website: https:\\/\\/www.dancarlin.com\\/ Hardcore History podcast: https:\\/\\/apple.co\\/2HX7hAA Common Sense podcast: https:\\/\\/apple.co\\/3mM6WPZ PODCAST INFO: Podcast website: https:\\/\\/lexfridman.com\\/podcast Apple Podcasts: https:\\/\\/apple.co\\/2lwqZIr Spotify: https:\\/\\/spoti.fi\\/2nEwCF8 RSS: https:\\/\\/lexfridman.com\\/feed\\/podcast\\/ YouTube Full Episodes: https:\\/\\/youtube.com\\/lexfridman YouTube\",\"1585\":\"Leslie Kaelbling is a roboticist and professor at MIT. She is recognized for her work in reinforcement learning, planning, robot navigation, and several other topics in AI. She won the IJCAI Computers and Thought Award and was the editor-in-chief of the prestigious Journal of Machine Learning Research. Video version is available on YouTube. If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman on Twitter, LinkedIn, Facebook, Medium, or YouTube where you can watch the video versions of these conversations.\",\"4283\":\"In this special edition, Dr. Tim Scarfe, Yannic Kilcher and Keith Duggar speak with Gary Marcus and Connor Leahy about GPT-3. We have all had a significant amount of time to experiment with GPT-3 and show you demos of it in use and the considerations. Note that this podcast version is significantly truncated, watch the youtube version for the TOC and experiments with GPT-3 https:\\/\\/www.youtube.com\\/watch?v=iccd86vOz3w\",\"1589\":\"David Ferrucci led the team that built Watson, the IBM question-answering system that beat the top humans in the world at the game of Jeopardy. He is also the Founder, CEO, and Chief Scientist of Elemental Cognition, a company working engineer AI systems that understand the world the way people do. This conversation is part of the Artificial Intelligence podcast. If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman on Twitter, LinkedIn, Facebook, Medium, or YouTube where you can watch the video versions of these conversations. If you enjoy the podcast, please\",\"708\":\"Three students present their research programming project to the Microsoft Research Real World Reinforcement Learning team online. 0:00 Intro to Day 2 1:12 Pushing the Limits of VW with Flatbuffers Speaker: Sharad Chitlangia 20:30 Contextual Bandits Data Visualization with Jupyter Notebooks Speaker: Milind Agarwal 38:37 VW and ONNX Speaker: Harish Kamath Learn more about Microsoft Research's RL Open Source Fest: https:\\/\\/www.microsoft.com\\/en-us\\/research\\/academic-program\\/rl-open-source-fest\\/\",\"1701\":\"If you\\u2019ve been itching to get your feet wet in the field, these steps will provide you with lots of valuable ideas and suggestions to kickstart your career. Read the full story\",\"4307\":\"Professor Kenneth Stanley is currently a research science manager at OpenAI in San Fransisco. We've Been dreaming about getting Kenneth on the show since the very begininning of Machine Learning Street Talk. Some of you might recall that our first ever show was on the enhanced POET paper, of course Kenneth had his hands all over it. He's been cited over 16000 times, his most popular paper with over 3K citations was the NEAT algorithm. His interests are neuroevolution, open-endedness, NNs, artificial life, and AI. He invented the concept of novelty search with no clearly defined objective. His key idea is that there is a tyranny of objectives prevailing in every aspect of our lives, society and indeed our algorithms. Crucially, these objectives produce convergent behaviour and thinking and distract us from discovering stepping stones which will lead to greatness. He thinks that this monotonic objective obsession, this idea that we need to continue to improve benchmarks every year is dangerous. He wrote about this in detail in his recent book \\\"greatness can not be planned\\\" which will be the main topic of discussion in the show. We also cover his ideas on open endedness in machine learning. 00:00:00 Intro to Kenneth 00:01:16 Show structure disclaimer 00:04:16 Passionate discussion 00:06:26 WHy greatness cant be planned and the tyranny of objectives 00:14:40 Chinese Finger Trap 00:16:28 Perverse Incentives and feedback loops 00:18:17 Deception 00:23:29 Maze example 00:24:44 How can we define curiosity or interestingness 00:26:59 Open endedness 00:33:01 ICML 2019 and Yannic, POET, first MSLST 00:36:17 evolutionary algorithms++ 00:43:18 POET, the first MLST 00:45:39 A lesson to GOFAI people 00:48:46 Machine Learning -- the great stagnation 00:54:34 Actual scientific successes are usually luck, and against the odds -- Biontech 00:56:21 Picbreeder and NEAT 01:10:47 How Tim applies these ideas to his life and why he runs MLST 01:14:58 Keith Skit about UCF 01:15:13 Main show kick off 01:18:02 Why does Kenneth value serindipitous exploration so much 01:24:10 Scientific support for Keneths ideas in normal life 01:27:12 We should drop objectives to achieve them. An oxymoron? 01:33:13 Isnt this just resource allocation between exploration and exploitation? 01:39:06 Are objectives merely a matter of degree? 01:42:38 How do we allocate funds for treasure hunting in society 01:47:34 A keen nose for what is interesting, and voting can be dangerous 01:53:00 Committees are the antithesis of innovation 01:56:21 Does Kenneth apply these ideas to his real life? 01:59:48 Divergence vs interestingness vs novelty vs complexity 02:08:13 Picbreeder 02:12:39 Isnt everything novel in some sense? 02:16:35 Imagine if there was no selection pressure? 02:18:31 Is innovation == environment exploitation? 02:20:37 Is it possible to take shortcuts if you already knew what the innovations were? 02:21:11 Go Explore -- does the algorithm encode the stepping stones? 02:24:41 What does it mean for things to be interestingly different? 02:26:11 behavioral characterization \\/ diversity measure to your broad interests 02:30:54 Shaping objectives 02:32:49 Why do all ambitious objectives have deception? Picbreeder analogy 02:35:59 Exploration vs Exploitation, Science vs Engineering 02:43:18 Schools of thought in ML and could search lead to AGI 02:45:49 Official ending\",\"4300\":\"This week Dr. Tim Scarfe, Dr. Keith Duggar, Yannic Kilcher and Connor Leahy cover a broad range of topics, ranging from academia, GPT-3 and whether prompt engineering could be the next in-demand skill, markets and economics including trading and whether you can predict the stock market, AI alignment, utilitarian philosophy, randomness and intelligence and even whether the universe is infinite! 00:00:00 Show Introduction 00:12:49 Academia and doing a Ph.D 00:15:49 From academia to wall street 00:17:08 Quants -- smoke and mirrors? Tail Risk 00:19:46 Previous results dont indicate future success in markets 00:23:23 Making money from social media signals? 00:24:41 Predicting the stock market 00:27:20 Things which are and are not predictable 00:31:40 Tim postscript comment on predicting markets 00:32:37 Connor take on markets 00:35:16 As market become more efficient.. 00:36:38 Snake oil in ML 00:39:20 GPT-3, we have changed our minds 00:52:34 Prompt engineering a new form of software development? 01:06:07 GPT-3 and prompt engineering 01:12:33 Emergent intelligence with increasingly weird abstractions 01:27:29 Wireheading and the economy 01:28:54 Free markets, dragon story and price vs value 01:33:59 Utilitarian philosophy and what does good look like? 01:41:39 Randomness and intelligence 01:44:55 Different schools of thought in ML 01:46:09 Is the universe infinite? Thanks a lot for Connor Leahy for being a guest on today's show. https:\\/\\/twitter.com\\/NPCollapse -- you can join his EleutherAI community discord here: https:\\/\\/discord.com\\/invite\\/vtRgjbM\",\"1544\":\"Scott Aaronson is a quantum computer scientist. Please support this podcast by checking out our sponsors: \\u2013 SimpliSafe: https:\\/\\/simplisafe.com\\/lex and use code LEX to get a free security camera \\u2013 Eight Sleep: https:\\/\\/www.eightsleep.com\\/lex and use code LEX to get $200 off \\u2013 ExpressVPN: https:\\/\\/expressvpn.com\\/lexpod and use code LexPod to get 3 months free \\u2013 BetterHelp: https:\\/\\/betterhelp.com\\/lex and use code LEX to get 10% off EPISODE LINKS: Scott\\u2019s Blog: https:\\/\\/www.scottaaronson.com\\/blog\\/ Our previous episode: https:\\/\\/www.youtube.com\\/watch?v=uX5t8EivCaM PODCAST INFO: Podcast website: https:\\/\\/lexfridman.com\\/podcast Apple Podcasts: https:\\/\\/apple.co\\/2lwqZIr Spotify: https:\\/\\/spoti.fi\\/2nEwCF8 RSS: https:\\/\\/lexfridman.com\\/feed\\/podcast\\/ YouTube Full Episodes: https:\\/\\/youtube.com\\/lexfridman YouTube Clips: https:\\/\\/youtube.com\\/lexclips SUPPORT & CONNECT: \\u2013 Check out the sponsors\",\"5676\":\"https:\\/\\/youtu.be\\/iAR8LkkMMIM Scale is the next frontier for AI. Google Brain uses sparsity and hard routing to massively increase a model's parameters, while keeping the FLOPs per forward pass constant. The Switch Transformer compares favorably to its dense counterparts in terms of speed and sample efficiency and breaks the next magic number: One Trillion Parameters. \\u200b OUTLINE: 0:00 - Intro & Overview 4:30 - Performance Gains from Scale 8:30 - Switch Transformer Architecture 17:00 - Model-, Data- and Expert-Parallelism 25:30 - Experimental Results 29:00 - Stabilizing Training 32:20 - Distillation into Dense Models 33:30 - Final Comments \\u200b Paper: https:\\/\\/arxiv.org\\/abs\\/2101.03961 [link] [comments]\",\"1690\":\"Read the full story\",\"1875\":\"Excel can perform various statistical analyses, including regression analysis. It is a great option because nearly everyone can access Excel. This post is an excellent introduction to performing and interpreting regression analysis, even if Excel isn\\u2019t your primary statistical software package. In this post, I provide step-by-step instructions for using Excel to perform multiple regression [\\u2026] The post How to Perform Regression Analysis using Excel appeared first on Statistics By Jim.\",\"3751\":\"Right now, I am currently serving recommendations through a deterministic model, but eventually want to move toward using an ML model. For an example, consider this to be something like Netflix and assume we want to use a content based clustering model where we cluster our users into personas and cluster our products into clustered based on metadata and that within each product cluster is a ranking based on internal data. This process seems pretty straightforward when the set of possible recommendations is of fixed size N, but what if N grows regularly, e.g., movies are added to the set of possible recommendations? Is there an industry standard approach to this type of problem? 0 Comments [link] [comments]\",\"3136\":\"At H2O.ai, we have been busy. Not only do we have our most significant new software launch coming up (details here), but we also are thrilled to announce the latest release of our flagship enterprise platform H2O Driverless AI 1.9.1. With that said, let\\u2019s jump into what is new: Faster Python scoring pipelines with embedded MOJOs for real-time inference, interpretability and explainability for production models Improved user experience for Shapley feature importances in original and transformed feature space Greatly improved time-series modeling for short-horizon forecasting problems New out-of-the-box recipe for Monotonic Gradient Boosted Machines (GBM) and various new controls for enforcing custom monotonic models Enhanced MOJO Visualization pipeline, displaying first tree or LightGBM and XGBoost models Expanded AutoDoc configuration options for Shapley Values, Monotonicity Constraints, and Imbalanced Models Enhanced string\\/numeric feature detection and conversion Improved automatic feature engineering for date\\/time differences Preview of multi-GPU, multi-node Dask\\/RAPIDS XGBoost and LightGBM with Optuna hyper-parameter tuning Additional Machine Learning Interpretability (MLI) Features: MLI Bring Your Own Recipe (BYOR): Customizability with any machine learning interpretability code Exposed sampling parameter for all explainers Added MOJO support for k-LIME (with download option) Added ability to download raw k-LIME data from MLI UI Added ability to change threshold for Disparate Impact Analysis in DIA expert settings Added ability to run PDP on out of range data Added max runtime parameter to Kernel Shapley in MLI expert settings Added ability to run PD\\/ICE for multinomial models in DAI Added ability to run MLI TS in typical MLI view (IID) Added ability to see rules in Decision Tree surrogate model Enhanced MLI AutoDoc configuration options such as k-Lime and Surrogate Decision Tree explanations So, there is a lot here. While we won\\u2019t dive into each individual bullet, there are absolutely some features that should be called out. MLI Bring Your Own Recipe: Custom recipes for data preprocessing, models, feature engineering and metrics is one of the major features of H2O Driverless AI as it provides user-mode customizations (with flexible Python code snippets) of the entire machine learning pipeline and opens up the platform to the entire Python ecosystem. With version 1.9.1, custom recipes are now available for MLI as well. We recognize that we spend a lot of our time trying to discover the latest and greatest explainable AI and Machine Learning Interpretability methods, but that users might want to try out others. The Bring Your Own Recipe framework for MLI enables users to bring in any explainability or interpretability method into Driverless AI. Check out the open-source recipes for custom explainers here. MOJO Metrics: Obviously, the primary goal of most Machine Learning exercises is to ultimately get into production and start automatically making better decisions and driving more value, but often the very immediate secondary goal is to understand how those models are performing in production over time. We continue to expand the number of metrics embedded in our production model objects (MOJOs) so users can not only see the predictions of the model but see the progressive changes in Shapley Values, k-Lime, real-time AutoDoc documentation, and other explainability metrics to determine if the model is changing focus over time, or potentially getting more or less bias over time. Better Constraints & Visualizations: Many of H2O.ai\\u2019s longest-standing customers, partners, and investors are in financial services and other highly regulated industries. For that reason, we have always been customer-centric in how we can build a better product for them. For industries requiring very simple, interpretable models, we continue to add a host of constraining options on model universe, feature engineering, and more. In H2O Driverless AI 1.9.1, built-in Monotonic recipes enable interpretable feature engineering under strict monotonicity constraints. Additionally, we have enhanced the MOJO visualization pipeline for Light GBMs and XGBoost so users can better visualize the first tree in the model to see the most essential features and important characteristics of the model structure, such as tree depth. How to Get Started? If you are new to H2O Driverless AI, we recommend our risk-free, web-based test drive in H2O Aquarium Cloud. Each lab session lasts for two hours, and you can keep trying our software for free. No license key is required. We also have self-paced tutorials to guide you through the journey. Note: We are in the process of updating the materials to H2O Driverless AI 1.9. The new tutorials should be available in the coming weeks. For existing users with license keys, please download the latest version from our website. You can also find the links to different cloud marketplaces on the same page. I hope you enjoy reading this quick overview. Please give it a spin and share your experience with us. Learning Resources Self-paced tutorials and instructor-led courses from our Learning Center. H2O Documentation H2O Blogs Be part of our community; find a meetup group near you. H2O.ai Events Overview Related webinars: July 9: What the Future of AI Looks Like with Arno Candel, CTO July 16: More Use Cases and More Value with Automated Computer Vision Modeling July 23: State of The Art NLP Models in H2O Driverless AI 1.9 July 30: Further Exploration into Model Explainability with H2O Driverless AI 1.9 July 30: Making to Production with Machine Learning The post H2O Driverless AI 1.9.1: Continuing to Push the Boundaries for Responsible AI appeared first on Open Source Leader in AI and ML.\",\"4290\":\"Join Dr Tim Scarfe, Sayak Paul, Yannic Kilcher, and Alex Stenlake have a conversation with Mr. Chai Time Data Science; Sanyam Bhutani! 00:00:00 Introduction 00:03:42 Show kick off 00:06:34 How did Sanyam get started into ML 00:07:46 Being a content creator 00:09:01 Can you be self taught without a formal education in ML? 00:22:54 Kaggle 00:33:41 H20 product \\/ job 00:40:58 Intepretability \\/ bias \\/ engineering skills 00:43:22 Get that first job in DS 00:46:29 AWS ML Ops architecture \\/ ml engineering 01:14:19 Patterns 01:18:09 Testability 01:20:54 Adversarial examples Sanyam's blog -- https:\\/\\/sanyambhutani.com\\/tag\\/chaitimedatascience\\/ Chai Time Data Science -- https:\\/\\/www.youtube.com\\/c\\/ChaiTimeDataScience\",\"1564\":\"Lisa Feldman Barrett is a neuroscientist, psychologist, and author. Please support this podcast by checking out our sponsors: \\u2013 Athletic Greens: https:\\/\\/athleticgreens.com\\/lex and use code LEX to get free vitamin D3\\/K2 \\u2013 Magic Spoon: https:\\/\\/magicspoon.com\\/lex and use code LEX to get free shipping \\u2013 Cash App: https:\\/\\/cash.app\\/ and use code LexPodcast to get $10 EPISODE LINKS: Seven and a Half Lessons About the Brain (book): https:\\/\\/amzn.to\\/2Sp5ar9 How Emotions Are Made (book): https:\\/\\/amzn.to\\/2GwAFg6 Lisa\\u2019s Twitter: https:\\/\\/twitter.com\\/LFeldmanBarrett Lisa\\u2019s Website: https:\\/\\/lisafeldmanbarrett.com\\/ PODCAST INFO: Podcast website: https:\\/\\/lexfridman.com\\/podcast Apple Podcasts: https:\\/\\/apple.co\\/2lwqZIr Spotify: https:\\/\\/spoti.fi\\/2nEwCF8 RSS: https:\\/\\/lexfridman.com\\/feed\\/podcast\\/ YouTube Full Episodes: https:\\/\\/youtube.com\\/lexfridman YouTube Clips: https:\\/\\/youtube.com\\/lexclips SUPPORT & CONNECT:\",\"3763\":\"I'm wondering what tool do researchers use to log key ideas of the papers they read. Sorry for the vague question, I guess I am thinking of some kind of digital whiteboard with boxes representing papers and bullet points with main contributions and then some kind of personal comments and relations among papers. Is there anything with a similar purpose you would recommend? [link] [comments]\",\"3760\":\"If Roger Penrose's theory of consciousness is correct (there's a good chance it's true, quantum events in microtubuls show it.) Normal computer systems will not be enough to make General Artificial Intelligence. So I'd like to ask, is it possible to create machane learning, deep learning, and general AI systems in quantum computers? (Or I should say super quantum computers.) When you think about it, the answer is logically yes, but I also wonder if there's a mathematical and technical basis for it. [link] [comments]\",\"700\":\"Day 3 | November 19, 2020 Theme: Unblocking the Pipeline from Education to Employment Cecily Morrison, Microsoft The Accessible Computer Science Education Fall Workshop was hosted by Microsoft, University of Washington CREATE, and University of Colorado\\u2019s Coleman Institute. It took place November 17-19, 2020 and consisted of three half-days of talks, discussions, and planning for new research dedicated to making Computer Science education learning experiences more accessible for people with disabilities. More information on this workshop can be found at https:\\/\\/www.microsoft.com\\/en-us\\/research\\/event\\/accessible-cs-education-fall-workshop\\/\",\"5679\":\"For example, if I had a set of reference shirts and each shirt had a different style. If I wanted to do a reverse image similarity search on those shirts, what is the current SOTA that would achieve the best accuracy? [link] [comments]\",\"1502\":\"Jeremy Howard is the founder of fast.ai, a research institute dedicated to make deep learning more accessible. He is also a Distinguished Research Scientist at the University of San Francisco, a former president of Kaggle as well a top-ranking competitor there, and in general, he\\u2019s a successful entrepreneur, educator, research, and an inspiring personality in the AI community. This conversation is part of the Artificial Intelligence podcast. If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman on Twitter, LinkedIn, Facebook, Medium, or YouTube where you can watch the video versions of these\",\"5067\":\"Recommender systems may be the most common type of predictive model that the average person may encounter. They provide the basis for recommendations on services such as Amazon, Spotify, and Youtube. Recommender systems are a huge daunting topic if you\\u2019re just getting started. There is a myriad of data preparation techniques, algorithms, and model evaluation methods. Not all of the techniques will be relevant, and in fact, the state-of-the-art can be ignored for now as you will likely get very good results by focusing on the fundamentals, e.g. treat it as a straightforward classification or regression problem. It is important to know the basics and have it all laid out for you in a systematic way. For this, I recommend skimming or reading the standard books and papers on the topic and looking at some of the popular libraries. In this tutorial, you will discover resources you can use to get started with recommender systems. After completing this tutorial, you will know: The top review papers on recommender systems you can use to quickly understand the state of the field. The top books on recommender systems from which you can learn the algorithms and techniques required when developing and evaluating recommender systems. The top Python libraries and APIs that you can use to prototype and develop your own recommender systems. Let\\u2019s get started. How to Get Started With Recommender Systems Photo by Paul Toogood, some right reserved. Tutorial Overview This tutorial is divided into three parts; they are: Papers on Recommender Systems Books on Recommender Systems Recommender Systems Libraries Papers on Recommender Systems Research papers on recommender systems can help you very quickly get up to speed on the state of the field. Specifically, review papers that use precise language to define what a recommender system is, the algorithms that can be used, standard datasets and metrics for comparing algorithms, and hints at the state of the art techniques. By skimming or reading a handful of review papers on recommender systems, you can quickly develop a foundation from which to dive deeper and start developing your own systems. The field does not change that quickly, and techniques from 10 or 20 years ago will give you solid results. Review papers on recommender systems I recommended to establish a foundational understanding include: Amazon.com Recommendations: Item-to-item Collaborative Filtering, 2003. Matrix Factorization Techniques for Recommender Systems, 2009. Recommender Systems, 2012. Recommender Systems Survey, 2013. Advances in Collaborative Filtering, 2015. Matrix Factorization Techniques for Recommender Systems Once you have questions about specific techniques, you can then find papers that focus on those techniques and dive deeper. You can search for papers on specific techniques here: Google Scholar Do you know of additional good review papers on recommender systems? Let me know in the comments below. Books on Recommender Systems Books on recommender systems provide the space to lay out the field and take you on a tour of the techniques and give you the detail you need to understand them, with more breadth and detail than a much shorter review paper. Again, given that the field is quite mature, older books, such as those published a decade ago, should not be immediately neglected. Some top textbooks published by key researchers in the field include the following: Recommender Systems: An Introduction, 2010. Recommender Systems: The Textbook, 2016. I own a hard copy of \\u201cRecommender Systems: An Introduction\\u201d and cannot recommend it highly enough. This book offers an overview of approaches to developing state-of-the-art recommender systems. The authors present current algorithmic approaches for generating personalized buying proposals, such as collaborative and content-based filtering, as well as more interactive and knowledge- based approaches. They also discuss how to measure the effectiveness of recommender systems and illustrate the methods with practical case studies. \\u2014 Recommender Systems: An Introduction, 2010. The table of contents for this book is as follows: Chapter 1: Introduction Chapter 2: Collaborative recommendation Chapter 3: Content-based recommendation Chapter 4: Knowledge-based recommendation Chapter 5: Hybrid recommendation approaches Chapter 6: Explanations in recommender systems Chapter 7: Evaluating recommender systems Chapter 8: Case study: Personalized game recommendations on the mobile Internet Chapter 9: Attacks on collaborative recommender systems Chapter 10: Online consumer decision making Chapter 11: Recommender systems and the next-generation web Chapter 12: Recommendations in ubiquitous environments Chapter 13: Summary and outlook Recommender Systems: An Introduction It can be good to get a handbook on the topic with chapters written by different academics summarizing or championing their preferred techniques and methods. I recommend this handbook: Recommender Systems Handbook, 2015. If you are looking for a more hands-on book, I recommend: Practical Recommender Systems, 2019. Have you read one of these books? Or do you know another great book on the topic? Let me know in the comments below. Recommender Systems Libraries You probably don\\u2019t need to dive into the start of the art, at least not immediately. As such, standard machine learning libraries are a great place to start. For example, you can develop an effective recommender system using matrix factorization methods (SVD) or even a straight forward k-nearest neighbors model by items or by users. As such, I recommend starting with some experiments with scikit-learn: Scikit-Learn Python Machine Learning Library. You can practice on standard recommender system datasets if your own data is not yet accessible or available, or you just want to get the hang of things first. Popular standard datasets for recommender systems include: MovieLens Yahoo datasets (music, urls, movies, etc.) If you are ready for state-of-the-art techniques, a great place to start is \\u201cpapers with code\\u201d that lists both academic papers and links to the source code for the methods described in the paper: Papers With Code: Recommendation Systems There are a number of proprietary and open-source libraries and services for recommender systems. I recommend sticking with open-source Python libraries in the beginning, such as: Surprise: A Python scikit for building and analyzing recommender systems Case Recommender: A Flexible and Extensible Python Framework for Recommender Systems Have you used any of these libraries to develop a recommender system? Let me know in the comments below. Summary In this tutorial, you discovered resources you can use to get started with recommender systems. Specifically, you learned: The top review papers on recommender systems you can use to quickly understand the state of the field. The top books on recommender systems from which you can learn the algorithms and techniques required when developing and evaluating recommender systems. The top Python libraries and APIs that you can use to prototype and develop your own recommender systems. Do you have any questions? Ask your questions in the comments below and I will do my best to answer. The post How to Get Started With Recommender Systems appeared first on Machine Learning Mastery.\",\"1612\":\"Read the full story\",\"1534\":\"Alexander Fridman is a professor at Drexel University and the director of the Nyheim Plasma Institute. He is one of the top plasma physicists and plasma chemists in the world. And most importantly to me, he is my dad. Support this podcast by supporting these sponsors: The Jordan Harbinger Show: https:\\/\\/www.jordanharbinger.com\\/subscribe Magic Spoon: https:\\/\\/magicspoon.com\\/lex and use code LEX at checkout! This conversation is part of the Artificial Intelligence podcast. If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman on Twitter, LinkedIn, Facebook, Medium, or YouTube where you can watch the video versions\",\"1627\":\"Read the full story\",\"2338\":\"Hi! NLPRule does fast grammatical error correction for English and German by checking thousands of rules. It is written in Rust and has bindings for Python. Repository: https:\\/\\/github.com\\/bminixhofer\\/nlprule Synopsis from nlprule import Tokenizer, Rules, SplitOn tokenizer = Tokenizer.load(\\\"en\\\") rules = Rules.load(\\\"en\\\", tokenizer, SplitOn([\\\".\\\", \\\"?\\\", \\\"!\\\"])) rules.correct(\\\"He wants that you send him an email.\\\") # returns: 'He wants you to send him an email.' rules.correct(\\\"I can due his homework.\\\") # returns: 'I can do his homework.' rules.correct(\\\"It is enough for all intensive purposes.\\\") # returns: 'It is enough for all intents and purposes.' suggestions = rules.suggest(\\\"She was not been here since Monday.\\\") for s in suggestions: print(s.start, s.end, s.replacements, s.source, s.message) # prints: # 4 16 ['was not', 'has not been'] WAS_BEEN.1 Did you mean was not or has not been? Background I've been interested in grammatical error correction for a while and came across LanguageTool which is based on thousands of rules for error correction in an XML file. You can think of the rule syntax as a restricted form of Regex where the atoms are words annotated with lemmas, part-of-speech tags, and chunks. I'm not a big fan of Java, wanted to improve my Rust and was interested in how these rules are parsed so I made a proof of concept reverse-engineering the LanguageTool logic in Rust. I had this lying around for quite some time and decided to finish it up and make it into a usable library now during the holidays. Relation to more sophisticated GEC approaches There's lots of research in using Neural Networks for Grammatical Error Correction and there are some exciting recent approaches which capture many more errors than a rule-based approach could. Still, for me there are two reasons to use rules: Speed. On my machine with an 8th Gen Intel CPU it takes less than 1ms to correct a sentence. Dealing with extreme data sparsity of some errors. The above example \\\"It is enough for all intensive purposes.\\\" contains a well known error. Yet, I would be surprised if a current ML model corrects this error unless specifically accounted for since it will almost never have appeared in its training data. This is even more true for similarly rare errors in other languages where there is less data available than for English. So I believe rules are especially useful in conjunction with a more powerful ML model. I think of NLPRule as a kind of \\\"sanity-check\\\" for text. Rule-based postprocessing for NLG Two areas where NLPRule might be interesting are preprocessing for NLP and postprocessing for NLG. I've tried the latter with texts generated from GPT2. Applying NLPRule yields a significant amount of suggestions: Generated 192300 tokens. misspelling: 35 suggestions (0.18 per 1000 tokens) style: 53 suggestions (0.28 per 1000 tokens) typographical: 112 suggestions (0.58 per 1000 tokens) grammar: 29 suggestions (0.15 per 1000 tokens) none: 3 suggestions (0.02 per 1000 tokens) inconsistency: 2 suggestions (0.01 per 1000 tokens) Not all of these are errors, some are just suggestions for improvement. More information here. Although strictly speaking this project does not focus on Machine Learning I thought people here might be interested. I'm happy to discuss anything in the comments! Edit: Sorry, the attribute to get replacements in the example should have been .replacements not .text. Fixed now! [link] [comments]\",\"1349\":\"#ai #openai #technology Paper Title: Learning Transferable Visual Models From Natural Language Supervision CLIP trains on 400 million images scraped from the web, along with text descriptions to learn a model that can connect the two modalities. The core idea is a contrastive objective combined with a large batch size. The resulting model can be turned into arbitrary zero-shot classifiers for new image & text tasks. OUTLINE: 0:00 - Introduction 3:15 - Overview 4:40 - Connecting Images & Text 9:00 - Building Zero-Shot Classifiers 14:40 - CLIP Contrastive Training Objective 22:25 - Encoder Choices 25:00 - Zero-Shot CLIP vs Linear ResNet-50 31:50 - Zero-Shot vs Few-Shot 35:35 - Scaling Properties 36:35 - Comparison on different tasks 37:40 - Robustness to Data Shift 44:20 - Broader Impact Section 47:00 - Conclusion & Comments Paper: https:\\/\\/cdn.openai.com\\/papers\\/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf Blog: https:\\/\\/openai.com\\/blog\\/clip\\/ Code: https:\\/\\/github.com\\/openai\\/CLIP Abstract: State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. Authors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever Links: TabNine Code Completion (Referral): http:\\/\\/bit.ly\\/tabnine-yannick YouTube: https:\\/\\/www.youtube.com\\/c\\/yannickilcher Twitter: https:\\/\\/twitter.com\\/ykilcher Discord: https:\\/\\/discord.gg\\/4H8xxDF BitChute: https:\\/\\/www.bitchute.com\\/channel\\/yannic-kilcher Minds: https:\\/\\/www.minds.com\\/ykilcher Parler: https:\\/\\/parler.com\\/profile\\/YannicKilcher LinkedIn: https:\\/\\/www.linkedin.com\\/in\\/yannic-kilcher-488534136\\/ BiliBili: https:\\/\\/space.bilibili.com\\/1824646584 If you want to support me, the best thing to do is to share out the content :) If you want to support me financially (completely optional and voluntary, but a lot of people have asked for this): SubscribeStar: https:\\/\\/www.subscribestar.com\\/yannickilcher Patreon: https:\\/\\/www.patreon.com\\/yannickilcher Bitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq Ethereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2 Litecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m Monero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\",\"5687\":\"This paper from Microsoft presents a large, tunable neural conversational response generation model, DialoGPT (dialogue generative pre-trained transformer) model trained on 147M conversation-like exchanges extracted from Reddit comments. Researchers show that the conversational systems that leverage DialoGPT generate more relevant, contentful, and context-consistent responses than strong baseline systems. Paper Walkthrough: https:\\/\\/youtu.be\\/Zo679MYoJns \\u23e9 Paper: https:\\/\\/www.aclweb.org\\/anthology\\/2020.acl-demos.30.pdf [link] [comments]\",\"1531\":\"Gavin Miller is the Head of Adobe Research. Adobe have empowered artists, designers, and creative minds from all professions working in the digital medium for over 30 years with software such as Photoshop, Illustrator, Premiere, After Effects, InDesign, Audition that work with images, video, and audio. Adobe Research is working to define the future evolution of these products in a way that makes the life of creatives easier, automates the tedious tasks, and gives more & more time to operate in the idea space instead of pixel space. This is where the cutting-edge deep learning methods of the past decade\",\"1610\":\"A beginner level tutorial to get started with data visualization by creating an interesting and intuitive JavaScript bubble map Read the full story\",\"1644\":\"In the healthcare landscape, providers and lawmakers alike are faced with the challenge of making the best possible decisions for patients and the industry as a whole. From choosing the best treatments to using resources in a responsible manner, medical leaders are making decisions on a daily basis that can significantly impact health outcomes and costs. Read the full story\",\"1536\":\"Michael I. Jordan is a professor at Berkeley, and one of the most influential people in the history of machine learning, statistics, and artificial intelligence. He has been cited over 170,000 times and has mentored many of the world-class researchers defining the field of AI today, including Andrew Ng, Zoubin Ghahramani, Ben Taskar, and Yoshua Bengio. EPISODE LINKS: (Blog post) Artificial Intelligence\\u2014The Revolution Hasn\\u2019t Happened Yet This conversation is part of the Artificial Intelligence podcast. If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman on Twitter, LinkedIn, Facebook, Medium, or YouTube where\",\"3750\":\"https:\\/\\/www.youtube.com\\/watch?v=lhYGXYeMq_E Special edition! Professor Kenneth Stanley is currently a research science manager at OpenAI in San Fransisco. We've Been dreaming about getting Kenneth on the show since the very beginning of Machine Learning Street Talk. Some of you might recall that our first ever show was on the enhanced POET paper, of course Kenneth had his hands all over it. His interests are neuroevolution, open-endedness, NNs, artificial life, and AI. He invented the concept of novelty search. His key idea is that there is a tyranny of objectives prevailing in every aspect of our lives, society and indeed our algorithms. Crucially, these objectives produce convergent behaviour and thinking and distract us from discovering stepping stones which will lead to greatness. He thinks that this monotonic objective obsession, this idea that we need to continue to improve benchmarks every year is dangerous. He wrote about this in detail in his recent book \\\"greatness can not be planned\\\" which will be the main topic of discussion in the show. We also cover his ideas on open endedness in machine learning. There is also an audio version: https:\\/\\/anchor.fm\\/machinelearningstreettalk\\/episodes\\/038---Professor-Kenneth-Stanley---Why-Greatness-Cannot-Be-Planned-ep7116 [link] [comments]\",\"1485\":\"Fran\\u00e7ois Chollet is an AI researcher at Google and creator of Keras. Support this podcast by supporting our sponsors (and get discount): \\u2013 Babbel: https:\\/\\/babbel.com and use code LEX \\u2013 MasterClass: https:\\/\\/masterclass.com\\/lex \\u2013 Cash App: download app & use code \\u201cLexPodcast\\u201d Episode links: Francois\\u2019s Twitter: https:\\/\\/twitter.com\\/fchollet Francois\\u2019s Website: https:\\/\\/fchollet.com\\/ On the Measure of Intelligence (paper): https:\\/\\/arxiv.org\\/abs\\/1911.01547 If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/podcast or connect with @lexfridman on Twitter, LinkedIn, Facebook, Medium, or YouTube where you can watch the video versions of these conversations. If you enjoy the podcast, please rate it 5 stars\",\"2326\":\"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead! Thread will stay alive until next one so keep posting after the date in the title. Thanks to everyone for answering questions in the previous thread! [link] [comments]\",\"1463\":\"Ray Dalio is the founder, Co-Chairman and Co-Chief Investment Officer of Bridgewater Associates, one of the world\\u2019s largest and most successful investment firms that is famous for the principles of radical truth and transparency that underlie its culture. Ray is one of the wealthiest people in the world, with ideas that extend far beyond the specifics of how he made that wealth. His ideas, applicable to everyone, are brilliantly summarized in his book Principles. This conversation is part of the Artificial Intelligence podcast. If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman\",\"3189\":\"Semi-supervised learning refers to algorithms that attempt to make use of both labeled and unlabeled training data. Semi-supervised learning algorithms are unlike supervised learning algorithms that are only able to learn from labeled training data. A popular approach to semi-supervised learning is to create a graph that connects examples in the training dataset and propagates known labels through the edges of the graph to label unlabeled examples. An example of this approach to semi-supervised learning is the label spreading algorithm for classification predictive modeling. In this tutorial, you will discover how to apply the label spreading algorithm to a semi-supervised learning classification dataset. After completing this tutorial, you will know: An intuition for how the label spreading semi-supervised learning algorithm works. How to develop a semi-supervised classification dataset and establish a baseline in performance with a supervised learning algorithm. How to develop and evaluate a label spreading algorithm and use the model output to train a supervised learning algorithm. Let\\u2019s get started. Semi-Supervised Learning With Label Spreading Photo by Jernej Furman, some rights reserved. Tutorial Overview This tutorial is divided into three parts; they are: Label Spreading Algorithm Semi-Supervised Classification Dataset Label Spreading for Semi-Supervised Learning Label Spreading Algorithm Label Spreading is a semi-supervised learning algorithm. The algorithm was introduced by Dengyong Zhou, et al. in their 2003 paper titled \\u201cLearning With Local And Global Consistency.\\u201d The intuition for the broader approach of semi-supervised learning is that nearby points in the input space should have the same label, and points in the same structure or manifold in the input space should have the same label. The key to semi-supervised learning problems is the prior assumption of consistency, which means: (1) nearby points are likely to have the same label; and (2) points on the same structure typically referred to as a cluster or a manifold) are likely to have the same label. \\u2014 Learning With Local And Global Consistency, 2003. The label spreading is inspired by a technique from experimental psychology called spreading activation networks. This algorithm can be understood intuitively in terms of spreading activation networks from experimental psychology. \\u2014 Learning With Local And Global Consistency, 2003. Points in the dataset are connected in a graph based on their relative distances in the input space. The weight matrix of the graph is normalized symmetrically, much like spectral clustering. Information is passed through the graph, which is adapted to capture the structure in the input space. The approach is very similar to the label propagation algorithm for semi-supervised learning. Another similar label propagation algorithm was given by Zhou et al.: at each step a node i receives a contribution from its neighbors j (weighted by the normalized weight of the edge (i,j)), and an additional small contribution given by its initial value \\u2014 Page 196, Semi-Supervised Learning, 2006. After convergence, labels are applied based on nodes that passed on the most information. Finally, the label of each unlabeled point is set to be the class of which it has received most information during the iteration process. \\u2014 Learning With Local And Global Consistency, 2003. Now that we are familiar with the label spreading algorithm, let\\u2019s look at how we might use it on a project. First, we must define a semi-supervised classification dataset. Semi-Supervised Classification Dataset In this section, we will define a dataset for semis-supervised learning and establish a baseline in performance on the dataset. First, we can define a synthetic classification dataset using the make_classification() function. We will define the dataset with two classes (binary classification) and two input variables and 1,000 examples. ... # define dataset X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, random_state=1) Next, we will split the dataset into train and test datasets with an equal 50-50 split (e.g. 500 rows in each). ... # split into train and test X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50, random_state=1, stratify=y) Finally, we will split the training dataset in half again into a portion that will have labels and a portion that we will pretend is unlabeled. ... # split train into labeled and unlabeled X_train_lab, X_test_unlab, y_train_lab, y_test_unlab = train_test_split(X_train, y_train, test_size=0.50, random_state=1, stratify=y_train) Tying this together, the complete example of preparing the semi-supervised learning dataset is listed below. # prepare semi-supervised learning dataset from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split # define dataset X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, random_state=1) # split into train and test X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50, random_state=1, stratify=y) # split train into labeled and unlabeled X_train_lab, X_test_unlab, y_train_lab, y_test_unlab = train_test_split(X_train, y_train, test_size=0.50, random_state=1, stratify=y_train) # summarize training set size print('Labeled Train Set:', X_train_lab.shape, y_train_lab.shape) print('Unlabeled Train Set:', X_test_unlab.shape, y_test_unlab.shape) # summarize test set size print('Test Set:', X_test.shape, y_test.shape) Running the example prepares the dataset and then summarizes the shape of each of the three portions. The results confirm that we have a test dataset of 500 rows, a labeled training dataset of 250 rows, and 250 rows of unlabeled data. Labeled Train Set: (250, 2) (250,) Unlabeled Train Set: (250, 2) (250,) Test Set: (500, 2) (500,) A supervised learning algorithm will only have 250 rows from which to train a model. A semi-supervised learning algorithm will have the 250 labeled rows as well as the 250 unlabeled rows that could be used in numerous ways to improve the labeled training dataset. Next, we can establish a baseline in performance on the semi-supervised learning dataset using a supervised learning algorithm fit only on the labeled training data. This is important because we would expect a semi-supervised learning algorithm to outperform a supervised learning algorithm fit on the labeled data alone. If this is not the case, then the semi-supervised learning algorithm does not have skill. In this case, we will use a logistic regression algorithm fit on the labeled portion of the training dataset. ... # define model model = LogisticRegression() # fit model on labeled dataset model.fit(X_train_lab, y_train_lab) The model can then be used to make predictions on the entire holdout test dataset and evaluated using classification accuracy. ... # make predictions on hold out test set yhat = model.predict(X_test) # calculate score for test set score = accuracy_score(y_test, yhat) # summarize score print('Accuracy: %.3f' % (score*100)) Tying this together, the complete example of evaluating a supervised learning algorithm on the semi-supervised learning dataset is listed below. # baseline performance on the semi-supervised learning dataset from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from sklearn.linear_model import LogisticRegression # define dataset X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, random_state=1) # split into train and test X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50, random_state=1, stratify=y) # split train into labeled and unlabeled X_train_lab, X_test_unlab, y_train_lab, y_test_unlab = train_test_split(X_train, y_train, test_size=0.50, random_state=1, stratify=y_train) # define model model = LogisticRegression() # fit model on labeled dataset model.fit(X_train_lab, y_train_lab) # make predictions on hold out test set yhat = model.predict(X_test) # calculate score for test set score = accuracy_score(y_test, yhat) # summarize score print('Accuracy: %.3f' % (score*100)) Running the algorithm fits the model on the labeled training dataset and evaluates it on the holdout dataset and prints the classification accuracy. Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome. In this case, we can see that the algorithm achieved a classification accuracy of about 84.8 percent. We would expect an effective semi-supervised learning algorithm to achieve a better accuracy than this. Accuracy: 84.800 Next, let\\u2019s explore how to apply the label spreading algorithm to the dataset. Label Spreading for Semi-Supervised Learning The label spreading algorithm is available in the scikit-learn Python machine learning library via the LabelSpreading class. The model can be fit just like any other classification model by calling the fit() function and used to make predictions for new data via the predict() function. ... # define model model = LabelSpreading() # fit model on training dataset model.fit(..., ...) # make predictions on hold out test set yhat = model.predict(...) Importantly, the training dataset provided to the fit() function must include labeled examples that are ordinal encoded (as per normal) and unlabeled examples marked with a label of -1. The model will then determine a label for the unlabeled examples as part of fitting the model. After the model is fit, the estimated labels for the labeled and unlabeled data in the training dataset is available via the \\u201ctransduction_\\u201d attribute on the LabelSpreading class. ... # get labels for entire training dataset data tran_labels = model.transduction_ Now that we are familiar with how to use the label spreading algorithm in scikit-learn, let\\u2019s look at how we might apply it to our semi-supervised learning dataset. First, we must prepare the training dataset. We can concatenate the input data of the training dataset into a single array. ... # create the training dataset input X_train_mixed = concatenate((X_train_lab, X_test_unlab)) We can then create a list of -1 valued (unlabeled) for each row in the unlabeled portion of the training dataset. ... # create \\\"no label\\\" for unlabeled data nolabel = [-1 for _ in range(len(y_test_unlab))] This list can then be concatenated with the labels from the labeled portion of the training dataset to correspond with the input array for the training dataset. ... # recombine training dataset labels y_train_mixed = concatenate((y_train_lab, nolabel)) We can now train the LabelSpreading model on the entire training dataset. ... # define model model = LabelSpreading() # fit model on training dataset model.fit(X_train_mixed, y_train_mixed) Next, we can use the model to make predictions on the holdout dataset and evaluate the model using classification accuracy. ... # make predictions on hold out test set yhat = model.predict(X_test) # calculate score for test set score = accuracy_score(y_test, yhat) # summarize score print('Accuracy: %.3f' % (score*100)) Tying this together, the complete example of evaluating label spreading on the semi-supervised learning dataset is listed below. # evaluate label spreading on the semi-supervised learning dataset from numpy import concatenate from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from sklearn.semi_supervised import LabelSpreading # define dataset X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, random_state=1) # split into train and test X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50, random_state=1, stratify=y) # split train into labeled and unlabeled X_train_lab, X_test_unlab, y_train_lab, y_test_unlab = train_test_split(X_train, y_train, test_size=0.50, random_state=1, stratify=y_train) # create the training dataset input X_train_mixed = concatenate((X_train_lab, X_test_unlab)) # create \\\"no label\\\" for unlabeled data nolabel = [-1 for _ in range(len(y_test_unlab))] # recombine training dataset labels y_train_mixed = concatenate((y_train_lab, nolabel)) # define model model = LabelSpreading() # fit model on training dataset model.fit(X_train_mixed, y_train_mixed) # make predictions on hold out test set yhat = model.predict(X_test) # calculate score for test set score = accuracy_score(y_test, yhat) # summarize score print('Accuracy: %.3f' % (score*100)) Running the algorithm fits the model on the entire training dataset and evaluates it on the holdout dataset and prints the classification accuracy. Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome. In this case, we can see that the label spreading model achieves a classification accuracy of about 85.4 percent, which is slightly higher than a logistic regression fit only on the labeled training dataset that achieved an accuracy of about 84.8 percent. Accuracy: 85.400 So far so good. Another approach we can use with the semi-supervised model is to take the estimated labels for the training dataset and fit a supervised learning model. Recall that we can retrieve the labels for the entire training dataset from the label spreading model as follows: ... # get labels for entire training dataset data tran_labels = model.transduction_ We can then use these labels, along with all of the input data, to train and evaluate a supervised learning algorithm, such as a logistic regression model. The hope is that the supervised learning model fit on the entire training dataset would achieve even better performance than the semi-supervised learning model alone. ... # define supervised learning model model2 = LogisticRegression() # fit supervised learning model on entire training dataset model2.fit(X_train_mixed, tran_labels) # make predictions on hold out test set yhat = model2.predict(X_test) # calculate score for test set score = accuracy_score(y_test, yhat) # summarize score print('Accuracy: %.3f' % (score*100)) Tying this together, the complete example of using the estimated training set labels to train and evaluate a supervised learning model is listed below. # evaluate logistic regression fit on label spreading for semi-supervised learning from numpy import concatenate from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from sklearn.semi_supervised import LabelSpreading from sklearn.linear_model import LogisticRegression # define dataset X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, random_state=1) # split into train and test X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50, random_state=1, stratify=y) # split train into labeled and unlabeled X_train_lab, X_test_unlab, y_train_lab, y_test_unlab = train_test_split(X_train, y_train, test_size=0.50, random_state=1, stratify=y_train) # create the training dataset input X_train_mixed = concatenate((X_train_lab, X_test_unlab)) # create \\\"no label\\\" for unlabeled data nolabel = [-1 for _ in range(len(y_test_unlab))] # recombine training dataset labels y_train_mixed = concatenate((y_train_lab, nolabel)) # define model model = LabelSpreading() # fit model on training dataset model.fit(X_train_mixed, y_train_mixed) # get labels for entire training dataset data tran_labels = model.transduction_ # define supervised learning model model2 = LogisticRegression() # fit supervised learning model on entire training dataset model2.fit(X_train_mixed, tran_labels) # make predictions on hold out test set yhat = model2.predict(X_test) # calculate score for test set score = accuracy_score(y_test, yhat) # summarize score print('Accuracy: %.3f' % (score*100)) Running the algorithm fits the semi-supervised model on the entire training dataset, then fits a supervised learning model on the entire training dataset with inferred labels and evaluates it on the holdout dataset, printing the classification accuracy. Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome. In this case, we can see that this hierarchical approach of semi-supervised model followed by supervised model achieves a classification accuracy of about 85.8 percent on the holdout dataset, slightly better than the semi-supervised learning algorithm used alone that achieved an accuracy of about 85.6 percent. Accuracy: 85.800 Can you achieve better results by tuning the hyperparameters of the LabelSpreading model? Let me know what you discover in the comments below. Further Reading This section provides more resources on the topic if you are looking to go deeper. Books Introduction to Semi-Supervised Learning, 2009. Chapter 11: Label Propagation and Quadratic Criterion, Semi-Supervised Learning, 2006. Papers Learning With Local And Global Consistency, 2003. APIs sklearn.semi_supervised.LabelSpreading API. Section 1.14. Semi-Supervised, Scikit-Learn User Guide. sklearn.model_selection.train_test_split API. sklearn.linear_model.LogisticRegression API. sklearn.datasets.make_classification API. Articles Semi-supervised learning, Wikipedia. Summary In this tutorial, you discovered how to apply the label spreading algorithm to a semi-supervised learning classification dataset. Specifically, you learned: An intuition for how the label spreading semi-supervised learning algorithm works. How to develop a semi-supervised classification dataset and establish a baseline in performance with a supervised learning algorithm. How to develop and evaluate a label spreading algorithm and use the model output to train a supervised learning algorithm. Do you have any questions? Ask your questions in the comments below and I will do my best to answer. The post Semi-Supervised Learning With Label Spreading appeared first on Machine Learning Mastery.\",\"1874\":\"When comparing groups in your data, you can have either independent or dependent samples. The type of samples in your design impacts sample size requirements, statistical power, the proper analysis, and even your study\\u2019s costs. Understanding the implications of each type of sample can help you design a better study. For example, we often think [\\u2026] The post Independent and Dependent Samples in Statistics appeared first on Statistics By Jim.\",\"3193\":\"Optimization is a field of mathematics concerned with finding a good or best solution among many candidates. It is an important foundational topic required in machine learning as most machine learning algorithms are fit on historical data using an optimization algorithm. Additionally, broader problems, such as model selection and hyperparameter tuning, can also be framed as an optimization problem. Although having some background in optimization is critical for machine learning practitioners, it can be a daunting topic given that it is often described using highly mathematical language. In this post, you will discover top books on optimization that will be helpful to machine learning practitioners. Let\\u2019s get started. Books on Optimization for Machine Learning Photo by Patrick Alexander, some rights reserved. Overview The field of optimization is enormous as it touches many other fields of study. As such, there are hundreds of books on the topic, and most are textbooks filed with math and proofs. This is fair enough given that it is a highly mathematical subject. Nevertheless, there are books that provide a more approachable description of optimization algorithms. Not all optimization algorithms are relevant to machine learning; instead, it is useful to focus on a small subset of algorithms. Frankly, it is hard to group optimization algorithms as there are many concerns. Nevertheless, it is important to have some idea of the optimization that underlies simpler algorithms, such as linear regression and logistic regression (e.g. convex optimization, least squares, newton methods, etc.), and neural networks (first-order methods, gradient descent, etc.). These are foundational optimization algorithms covered in most optimization textbooks. Not all optimization problems in machine learning are well behaved, such as optimization used in AutoML and hyperparameter tuning. Therefore, knowledge of stochastic optimization algorithms is required (simulated annealing, genetic algorithms, particle swarm, etc.). Although these are optimization algorithms, they are also a type of learning algorithm referred to as biologically inspired computation or computational intelligence. Therefore, we will take a look at both books that cover classical optimization algorithms as well as books on alternate optimization algorithms. In fact, the first book we will look at covers both types of algorithms, and much more. Algorithms for Optimization This book was written by Mykel Kochenderfer and Tim Wheeler and was published in 2019. Algorithms for Optimization This book might be one of the very few textbooks that I\\u2019ve seen that broadly covers the field of optimization techniques relevant to modern machine learning. This book provides a broad introduction to optimization with a focus on practical algorithms for the design of engineering systems. We cover a wide variety of optimization topics, introducing the underlying mathematical problem formulations and the algorithms for solving them. Figures, examples, and exercises are provided to convey the intuition behind the various approaches. \\u2014 Page xiiix, Algorithms for Optimization, 2019. Importantly the algorithms range from univariate methods (bisection, line search, etc.) to first-order methods (gradient descent), second-order methods (Newton\\u2019s method), direct methods (pattern search), stochastic methods (simulated annealing), and population methods (genetic algorithms, particle swarm), and so much more. It includes both technical descriptions of algorithms with references and worked examples of algorithms in Julia. It\\u2019s a shame the examples are not in Python as this would make the book near perfect in my eyes. The complete table of contents for the book is listed below. Chapter 01: Introduction Chapter 02: Derivatives and Gradients Chapter 03: Bracketing Chapter 04: Local Descent Chapter 05: First-Order Methods Chapter 06: Second-Order Methods Chapter 07: Direct Methods Chapter 08: Stochastic Methods Chapter 09: Population Methods Chapter 10: Constraints Chapter 11: Linear Constrained Optimization Chapter 12: Multiobjective Optimization Chapter 13: Sampling Plans Chapter 14: Surrogate Models Chapter 15: Probabilistic Surrogate Models Chapter 16: Surrogate Optimization Chapter 17: Optimization under Uncertainty Chapter 18: Uncertainty Propagation Chapter 19: Discrete Optimization Chapter 20: Expression Optimization Chapter 21: Multidisciplinary Optimization I like this book a lot; it is full of valuable practical advice. I highly recommend it! Learn More: Algorithms for Optimization, 2019. Numerical Optimization This book was written by Jorge Nocedal and Stephen Wright and was published in 2006. Numerical Optimization This book is focused on the math and theory of the optimization algorithms presented and does cover many of the foundational techniques used by common machine learning algorithms. It may be a little too heavy for the average practitioner. The book is intended as a textbook for graduate students in mathematical subjects. We intend that this book will be used in graduate-level courses in optimization, as offered in engineering, operations research, computer science, and mathematics departments. \\u2014 Page xviii, Numerical Optimization, 2006. Even though it is highly mathematical, the descriptions of the algorithms are precise and may provide a useful alternative description to complement the other books listed. The complete table of contents for the book is listed below. Chapter 01: Introduction Chapter 02: Fundamentals of Unconstrained Optimization Chapter 03: Line Search Methods Chapter 04: Trust-Region Methods Chapter 05: Conjugate Gradient Methods Chapter 06: Quasi-Newton Methods Chapter 07: Large-Scale Unconstrained Optimization Chapter 08: Calculating Derivatives Chapter 09: Derivative-Free Optimization Chapter 10: Least-Squares Problems Chapter 11: Nonlinear Equations Chapter 12: Theory of Constrained Optimization Chapter 13: Linear Programming: The Simplex Method Chapter 14: Linear Programming: Interior-Point Methods Chapter 15: Fundamentals of Algorithms for Nonlinear Constrained Optimization Chapter 16: Quadratic Programming Chapter 17: Penalty and Augmented Lagrangian Methods Chapter 18: Sequential Quadratic Programming Chapter 19: Interior-Point Methods for Nonlinear Programming It\\u2019s a solid textbook on optimization. Learn More: Numerical Optimization, 2006. If you do prefer the theoretical approach to the subject, another widely used mathematical book on optimization is \\u201cConvex Optimization\\u201d written by Stephen Boyd and Lieven Vandenberghe and published in 2004. Computational Intelligence: An Introduction This book was written by Andries Engelbrecht and published in 2007. Computational Intelligence: An Introduction This book provides an excellent overview of the field of nature-inspired optimization algorithms, also referred to as computational intelligence. This includes fields such as evolutionary computation and swarm intelligence. This book is far less mathematical than the previous textbooks and is more focused on the metaphor of the inspired system and how to configure and use the specific algorithms with lots of pseudocode explanations. While the material is introductory in nature, it does not shy away from details, and does present the mathematical foundations to the interested reader. The intention of the book is not to provide thorough attention to all computational intelligence paradigms and algorithms, but to give an overview of the most popular and frequently used models. \\u2014 Page xxix, Computational Intelligence: An Introduction, 2007. Algorithms like genetic algorithms, genetic programming, evolutionary strategies, differential evolution, and particle swarm optimization are useful to know for machine learning model hyperparameter tuning and perhaps even model selection. They also form the core of many modern AutoML systems. The complete table of contents for the book is listed below. Part I Introduction Chapter 01: Introduction to Computational Intelligence Part II Artificial Neural Networks Chapter 02: The Artificial Neuron Chapter 03: Supervised Learning Neural Networks Chapter 04: Unsupervised Learning Neural Networks Chapter 05: Radial Basis Function Networks Chapter 06: Reinforcement Learning Chapter 07: Performance Issues (Supervised Learning) Part III Evolutionary Computation Chapter 08: Introduction to Evolutionary Computation Chapter 09: Genetic Algorithms Chapter 10: Genetic Programming Chapter 11: Evolutionary Programming Chapter 12: Evolution Strategies Chapter 13: Differential Evolution Chapter 14: Cultural Algorithms Chapter 15: Coevolution Part IV Computational Swarm Intelligence Chapter 16: Particle Swarm Optimization Chapter 17: Ant Algorithms Part V Artificial Immune Systems Chapter 18: Natural Immune System Chapter 19: Artificial Immune Models Part VI Fuzzy Systems Chapter 20: Fuzzy Sets Chapter 21: Fuzzy Logic and Reasoning I\\u2019m a fan of this book and recommend it. Learn More: Computational Intelligence: An Introduction, 2007. Summary In this post, you discovered books on optimization algorithms that are helpful to know for applied machine learning. Did I miss a good book on optimization? Let me know in the comments below. Have you read any of the books listed? Let me know what you think of it in the comments. The post 3 Books on Optimization for Machine Learning appeared first on Machine Learning Mastery.\",\"1598\":\"\\\"Don't join the book burners. Don't think you're going to conceal faults by concealing evidence that they ever existed. Don't be afraid to go in your library and read every book.\\\" - Dwight D. Eisenhower\",\"246\":\"this site really doesn't want our love to be true\",\"1441\":\"Donald Knuth is one of the greatest and most impactful computer scientists and mathematicians ever. He is the recipient in 1974 of the Turing Award, considered the Nobel Prize of computing. He is the author of the multi-volume work, the magnum opus, The Art of Computer Programming. He made several key contributions to the rigorous analysis of the computational complexity of algorithms. He popularized asymptotic notation, that we all affectionately know as the big-O notation. He also created the TeX typesetting which most computer scientists, physicists, mathematicians, and scientists and engineers use to write technical papers and make them look\",\"1434\":\"Nick Bostrom is a philosopher at University of Oxford and the director of the Future of Humanity Institute. He has worked on fascinating and important ideas in existential risks, simulation hypothesis, human enhancement ethics, and the risks of superintelligent AI systems, including in his book Superintelligence. I can see talking to Nick multiple times on this podcast, many hours each time, but we have to start somewhere. Support this podcast by signing up with these sponsors: \\u2013 Cash App \\u2013 use code \\u201cLexPodcast\\u201d and download: \\u2013 Cash App (App Store): https:\\/\\/apple.co\\/2sPrUHe \\u2013 Cash App (Google Play): https:\\/\\/bit.ly\\/2MlvP5w EPISODE LINKS: Nick\\u2019s\",\"1445\":\"Matthew W. Johnson is a professor and psychedelics researcher at Johns Hopkins. Please support this podcast by checking out our sponsors: \\u2013 Brave: https:\\/\\/brave.com\\/lex \\u2013 Neuro: https:\\/\\/www.getneuro.com and use code LEX to get 15% off \\u2013 Four Sigmatic: https:\\/\\/foursigmatic.com\\/lex and use code LexPod to get up to 60% off \\u2013 Cash App: https:\\/\\/cash.app\\/ and use code LexPodcast to get $10 EPISODE LINKS: Matt\\u2019s Twitter: https:\\/\\/twitter.com\\/Drug_Researcher Matt\\u2019s Website: https:\\/\\/www.hopkinsmedicine.org\\/profiles\\/results\\/directory\\/profile\\/0800020\\/matthew-johnson Study Website: https:\\/\\/hopkinspsychedelic.org\\/ PODCAST INFO: Podcast website: https:\\/\\/lexfridman.com\\/podcast Apple Podcasts: https:\\/\\/apple.co\\/2lwqZIr Spotify: https:\\/\\/spoti.fi\\/2nEwCF8 RSS: https:\\/\\/lexfridman.com\\/feed\\/podcast\\/ YouTube Full Episodes: https:\\/\\/youtube.com\\/lexfridman YouTube Clips: https:\\/\\/youtube.com\\/lexclips SUPPORT & CONNECT: \\u2013 Check out the sponsors above, it\\u2019s\",\"1590\":\"Andrew Huberman is a neuroscientist at Stanford. Please support this podcast by checking out our sponsors: \\u2013 Eight Sleep: https:\\/\\/www.eightsleep.com\\/lex and use code LEX to get $200 off \\u2013 SEMrush: https:\\/\\/www.semrush.com\\/partner\\/lex\\/ to get a free month of Guru \\u2013 Cash App: https:\\/\\/cash.app\\/ and use code LexPodcast to get $10 EPISODE LINKS: Andrew\\u2019s Instagram: https:\\/\\/www.instagram.com\\/hubermanlab Andrew\\u2019s Wikipedia: https:\\/\\/en.wikipedia.org\\/wiki\\/Andrew_D._Huberman Andrew\\u2019s Website: http:\\/\\/www.hubermanlab.com\\/ PODCAST INFO: Podcast website: https:\\/\\/lexfridman.com\\/podcast Apple Podcasts: https:\\/\\/apple.co\\/2lwqZIr Spotify: https:\\/\\/spoti.fi\\/2nEwCF8 RSS: https:\\/\\/lexfridman.com\\/feed\\/podcast\\/ YouTube Full Episodes: https:\\/\\/youtube.com\\/lexfridman YouTube Clips: https:\\/\\/youtube.com\\/lexclips SUPPORT & CONNECT: \\u2013 Check out the sponsors above, it\\u2019s the best way to support this podcast \\u2013 Support on Patreon:\",\"1225\":\"NOTE: You can support StatQuest by purchasing the Jupyter Notebook and Python code seen in this video here: https:\\/\\/statquest.org\\/product\\/jupyter-notebook-support-vector-machines-in-python\\/ \\u2b50 NOTE: When I code, I use Kite, a free AI-powered coding assistant that will help you code faster and smarter. The Kite plugin integrates with all the top editors and IDEs to give you smart completions and documentation while you\\u2019re typing. I love it! https:\\/\\/www.kite.com\\/get-kite\\/?utm_medium=referral&utm_source=youtube&utm_campaign=statquest&utm_content=description-only This webinar was recorded 20200609 at 11:00am (New York Time) NOTE: This StatQuest assumes that you are already familiar with: Support Vector Machines: https:\\/\\/youtu.be\\/efR1C6CvhmE The Radial Basis Function: https:\\/\\/youtu.be\\/Qc5IyLW_hns Regularization: https:\\/\\/youtu.be\\/Q81RR3yKn30 Cross Validation: https:\\/\\/youtu.be\\/fSytzGwwBVw Confusion Matrices: https:\\/\\/youtu.be\\/Kdsp6soqA7o For a complete index of all the StatQuest videos, check out: https:\\/\\/statquest.org\\/video-index\\/ If you'd like to support StatQuest, please consider... Patreon: https:\\/\\/www.patreon.com\\/statquest ...or... YouTube Membership: https:\\/\\/www.youtube.com\\/channel\\/UCtYLUTtgS3k1Fg4y5tAhLbw\\/join ...a cool StatQuest t-shirt or sweatshirt (USA\\/Europe): https:\\/\\/teespring.com\\/stores\\/statquest (everywhere): https:\\/\\/www.redbubble.com\\/people\\/starmer\\/works\\/40421224-statquest-double-bam?asc=u&p=t-shirt ...buying one or two of my songs (or go large and get a whole album!) https:\\/\\/joshuastarmer.bandcamp.com\\/ ...or just donating to StatQuest! https:\\/\\/www.paypal.me\\/statquest Lastly, if you want to keep up with me as I research and create new StatQuests, follow me on twitter: https:\\/\\/twitter.com\\/joshuastarmer 0:00 Awesome song and introduction 4:16 Import Modules 6:36 Import Data 11:27 Missing Data Part 1: Identifying 16:57 Missing Data Part 2: Dealing with it 21:04 Downsampling the data 24:35 Format Data Part 1: X and y 26:35 Format Data Part 2: One-Hot Encoding 31:25 Format Data Part 3: Centering and Scaling 32:45 Build a Preliminary SVM 34:55 Optimize Parameters with Cross Validation (GridSearchCV) 37:58 Build and Draw Final SVM #StatQuest #ML #SVM\",\"4291\":\"In today's episode, Dr. Keith Duggar, Alex Stenlake and Dr. Tim Scarfe chat about the education chapter in Kenneth Stanley's \\\"Greatness cannot be planned\\\" book, and we relate it to our Algoshambes conversation a few weeks ago. We debate whether objectives in education are a good thing and whether they cause perverse incentives and stifle creativity and innovation. Next up we dissect capsule networks from the top down! We finish off talking about fast algorithms and quantum computing. 00:00:00 Introduction 00:01:13 Greatness cannot be planned \\/ education 00:12:03 Perverse incentives 00:19:25 Treasure hunting 00:30:28 Capsule Networks 00:46:08 Capsules As Compositional Networks 00:52:45 Capsule Routing 00:57:10 Loss and Warps 01:09:55 Fast Algorithms and Quantum Computing\",\"1049\":\"Get free access to over 2500 documentaries on CuriosityStream: http:\\/\\/go.thoughtleaders.io\\/1621720200804 (use promo code \\\"zachstar\\\" at sign up) STEMerch Store: https:\\/\\/stemerch.com\\/ Support the Channel: https:\\/\\/www.patreon.com\\/zachstar PayPal(one time donation): https:\\/\\/www.paypal.me\\/ZachStarYT Cantor Set Video: https:\\/\\/youtu.be\\/eSgogjYj_uw \\u25baFollow me Instagram: https:\\/\\/www.instagram.com\\/zachstar\\/ Twitter: https:\\/\\/twitter.com\\/ImZachStar Animations: Brainup Studios ( http:\\/\\/brainup.in\\/ ) \\u25baMy Setup: Space Pictures: https:\\/\\/amzn.to\\/2CC4Kqj Magnetic Floating Globe: https:\\/\\/amzn.to\\/2VgPdn0 Camera: https:\\/\\/amzn.to\\/2RivYu5 Mic: https:\\/\\/amzn.to\\/35bKiri Tripod: https:\\/\\/amzn.to\\/2RgMTNL Equilibrium Tube: https:\\/\\/amzn.to\\/2SowDrh \\u25baCheck out the my Amazon Store: https:\\/\\/www.amazon.com\\/shop\\/zachstar\",\"1557\":\"Michael Malice is a political thinker, podcaster, and author. Please support this podcast by checking out our sponsors: \\u2013 NetSuite: http:\\/\\/netsuite.com\\/strategy to get free product tour \\u2013 Athletic Greens: https:\\/\\/athleticgreens.com\\/lex and use code LEX to get 1 month of fish oil \\u2013 Sun Basket: https:\\/\\/sunbasket.com\\/lex and use code LEX to get $35 off \\u2013 Cash App: https:\\/\\/cash.app\\/ and use code LexPodcast to get $10 EPISODE LINKS: Michael\\u2019s Twitter: https:\\/\\/twitter.com\\/michaelmalice Michael\\u2019s Community: https:\\/\\/malice.locals.com\\/ Michael\\u2019s YouTube: https:\\/\\/www.youtube.com\\/channel\\/UC5tj5QCpJKIl-KIa4Gib5Xw Michael\\u2019s Website: http:\\/\\/michaelmalice.com\\/about\\/ Your Welcome podcast: https:\\/\\/bit.ly\\/30q8oz1 The New Right (book): https:\\/\\/amzn.to\\/34gxLo3 Dear Reader (book): https:\\/\\/amzn.to\\/2HPPlHS Podcast (Round 1): https:\\/\\/www.youtube.com\\/watch?v=BIk1zUy8ehU PODCAST INFO: Podcast website: https:\\/\\/lexfridman.com\\/podcast Apple Podcasts: https:\\/\\/apple.co\\/2lwqZIr Spotify: https:\\/\\/spoti.fi\\/2nEwCF8 RSS: https:\\/\\/lexfridman.com\\/feed\\/podcast\\/ YouTube Full Episodes: https:\\/\\/youtube.com\\/lexfridman YouTube Clips: https:\\/\\/youtube.com\\/lexclips SUPPORT & CONNECT: \\u2013 Check out the sponsors above, it\\u2019s the best way to support this podcast \\u2013 Support on Patreon: https:\\/\\/www.patreon.com\\/lexfridman \\u2013 Twitter: https:\\/\\/twitter.com\\/lexfridman \\u2013 Instagram: https:\\/\\/www.instagram.com\\/lexfridman \\u2013 LinkedIn: https:\\/\\/www.linkedin.com\\/in\\/lexfridman \\u2013 Facebook: https:\\/\\/www.facebook.com\\/LexFridmanPage \\u2013 Medium: https:\\/\\/medium.com\\/@lexfridman OUTLINE: Here\\u2019s the timestamps for the episode. On some podcast players you should be able to click the timestamp to jump to that time. (00:00) \\u2013 Introduction (09:47) \\u2013 Conversation with Alex Jones and Tim Pool (18:31) \\u2013 Michael\\u2019s outfit (26:53) \\u2013 Self-publishing a book (36:41) \\u2013 The white pill (48:05) \\u2013 What did the volcano say to his true love? (49:28) \\u2013 Myth of Sisyphus (53:09) \\u2013 Journalism failed to stop Stalin and Hitler (1:00:53) \\u2013 Good Germans (1:04:49) \\u2013 Richard Wolff (1:08:20) \\u2013 Could United States have stayed out of World War II (1:11:12) \\u2013 Trump Derangement Syndrome (1:12:58) \\u2013 Nazism and Antisemitism (1:15:40) \\u2013 Knock knock (1:22:20) \\u2013 Putin (1:30:00) \\u2013 The evil of Kim Jong-il and North Korea (1:38:32) \\u2013 Dark humor (1:43:18) \\u2013 Comedy is tragedy plus timing (1:50:34) \\u2013 Interviewing difficult guests (2:00:06) \\u2013 Curtis Yarvin (Mencius Moldbug) (2:16:24) \\u2013 Violence under anarchism (2:31:58) \\u2013 Ayn Rand (2:35:07) \\u2013 Secession in United States (2:44:46) \\u2013 Politics over next 4 years (2:52:14) \\u2013 Mars (2:56:17) \\u2013 UFOs (2:59:12) \\u2013 Psychedelics (3:03:08) \\u2013 What is love?\",\"3134\":\"For almost a decade, H2O.ai has worked to build open source and commercial products that are on the leading edge of innovation in machine learning, from AutoML to Explainable AI. We are thrilled to announce the release of what we believe to be the future of AI Applications: H2O Wave. Wave is an open source, lightweight Python development framework that enables developers to build beautiful, real-time applications powered by leading AI technology. By enabling faster and more modular development of AI powered applications, companies can build, test, collaborate, and deploy their AI applications dramatically faster than before. Wave can easily be integrated with H2O AI technologies, such as Driverless AI, H2O-3, or any Python library. Storytelling with Python Data science is more than data munging and model building. Storytelling is a crucial part of real-world data science projects. Interactive web applications are an excellent way to visualize results and showcase the value of AI use cases to business leaders. Yet, data scientists may not have all the essential skills (e.g. HTML, CSS, Javascript) for modern web application development. With Wave, users can build rich, interactive web apps using tools that they are familiar with \\u2013 Python (note: R is also on our roadmap). Make AI Apps Getting it right the first time is hard, even for seasoned app developers. Trial and error is my favorite method when it comes to web app development. Wave\\u2019s live-reload feature allows data science developers to preview the app live as they change code. This dramatically reduces the time and effort for app development. Show All The Things Sometimes more is more and you just need to see everything at a glance. Wave\\u2019s low-latency server is practical for broadcasting live information and graphics. Deploy Instantly and Run Anywhere It is likely that you will have to share your apps with others at some point. Wave apps can be stored as static executables for multiple platforms including Windows, OSX, and Linux. This makes it very easy for sharing. How to Get Started Ready to try it? H2O Wave is open source, so just download it from our GitHub website and follow the instructions for Windows\\/Mac\\/Linux. You will also find the links to examples and API there. Enjoy! The post Introducing H2O Wave appeared first on Open Source Leader in AI and ML.\",\"1667\":\"Hello there! So today we would be learning about Go variables and the different data types associated with Go. Read the full story\",\"1594\":\"Happy birthday @AndrewYang. 46 is the new 42.\",\"3755\":\"I am a doctoral student working in the field of Computer Vision (CV) and Deep Learning (DL) in a Tier-2 university in Europe. I am noticing some alarmingly wrong practices within a small group of people (let's call it Group X) around me (which spans accross different labs\\/universities) related to papers submitted to top tier ML\\/CV conferences (CVPR\\/ICCV\\/ECCV\\/BMVC etc). I am seeing this pattern of literally overstating results in papers just to get through the peer review process. People from Gourp X do not actually do proper research, but just there to come up with seemingly intutive ideas by combining few existing papers. Then they do an extremely hasty implementation of the algorithm which never works in practice (or sometimes they implement just a small part of it). However, they utilize significant amount of their time (upto 3-4 months) writing the paper with great care (nice story, good english). The paper sets a storyline that makes claims much larger in magnitude than it should be. To support these claims, they overstate significant amount of the result (including some manipulated diagrams\\/figures) so that it looks appealing to the peer review process. For obvious reasons, they do not open-source their codes or data. These people exploit the fact that computer vision is an applied domain (mostly) and it is realtively easy to come up with models\\/algorithms without doing theoretical\\/mathy works much. They specifically choose topics where reviewers will be less suspicious about the overstated results. The most worrying thing about all this is that a good amount of these papers are getting accepted in top venues like CVPR\\/ICCV\\/ECCV\\/BMVC and sometime also as Orals. This really bothers me and I couldn't help sharing with everyone. I talked to some of these people from Group X about it and what I understood is that these people are not at all interested in doing research, their sole purpose of doing a PhD is to get a job at a company with a handsome salary. The banners like \\\"need first-author papers in top conferences\\\" in ML\\/CV job-listings in big companies is whats encouraging these wrong practices. Also on a managerial level, the Supervisors\\/PIs involved in these publications are indirectly supportive of these practices. My question is obvious; What do we do ? How do we tackle this problem ? Is it just me or others also see similar practices ? PS 1: I am not generalizing it to everyone and my observations are only based on these people I personally interacted with. I am aware of the fact that there is a large number of people who actually care about research and I am thankful to them for keeping this community alive with high quality research. PS 2: I cannot disclose any identity, including mine for obvious reasons. [link] [comments]\",\"5681\":\"Suppose x_i (i=1,2,...,N) be attribute values for N samples from class W_1 with mean m_1 and y_i (i=1,2,...,N) be attribute values for N samples from class W_2 with mean m_2.For feature selection using hypothesis testing, how define H_0 hypothesis? Option (A): m_1 - m_2=0 Option (B): m_1 -m_2 \\/codea href=\\\"https:\\/\\/preview.redd.it\\/mokxgy18f4d61.png?width=882&format=png&auto=webp&s=4511114cc43c526a53060ed49608d78464a8603e\\\"\\/a!-- SC_ON --a href=\\\"https:\\/\\/teddit.net\\/r\\/MachineLearning\\/comments\\/l3h038\\/d_r_very_strange_interview_question_on_feature\\/\\\"\\/aa href=\\\"https:\\/\\/teddit.net\\/r\\/MachineLearning\\/comments\\/l3h038\\/d_r_very_strange_interview_question_on_feature\\/\\\"\\/a\",\"1447\":\"Roger Penrose is physicist, mathematician, and philosopher at University of Oxford. He has made fundamental contributions in many disciplines from the mathematical physics of general relativity and cosmology to the limitations of a computational view of consciousness. Support this podcast by signing up with these sponsors: \\u2013 ExpressVPN at https:\\/\\/www.expressvpn.com\\/lexpod \\u2013 Cash App \\u2013 use code \\u201cLexPodcast\\u201d and download: \\u2013 Cash App (App Store): https:\\/\\/apple.co\\/2sPrUHe \\u2013 Cash App (Google Play): https:\\/\\/bit.ly\\/2MlvP5w EPISODE LINKS: Cycles of Time (book): https:\\/\\/amzn.to\\/39tXtpp The Emperor\\u2019s New Mind (book): https:\\/\\/amzn.to\\/2yfeVkD This conversation is part of the Artificial Intelligence podcast. If you would like to get more information\",\"1505\":\"Kate Darling is a researcher at MIT, interested in social robotics, robot ethics, and generally how technology intersects with society. She explores the emotional connection between human beings and life-like machines, which for me, is one of the most exciting topics in all of artificial intelligence. Support this podcast by signing up with these sponsors: \\u2013 ExpressVPN at https:\\/\\/www.expressvpn.com\\/lexpod \\u2013 MasterClass: https:\\/\\/masterclass.com\\/lex EPISODE LINKS: Kate\\u2019s Website: http:\\/\\/www.katedarling.org\\/ Kate\\u2019s Twitter: https:\\/\\/twitter.com\\/grok_ This conversation is part of the Artificial Intelligence podcast. If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman on Twitter, LinkedIn, Facebook,\",\"1509\":\"Yaron Brook is a objectivist philosopher, podcaster, and author. Please support this podcast by checking out our sponsors: \\u2013 Blinkist: https:\\/\\/blinkist.com\\/lex and use code LEX to get 25% off premium \\u2013 ExpressVPN: https:\\/\\/expressvpn.com\\/lexpod and use code LexPod to get 3 months free \\u2013 Cash App: https:\\/\\/cash.app\\/ and use code LexPodcast to get $10 EPISODE LINKS: Yaron\\u2019s Twitter: https:\\/\\/twitter.com\\/yaronbrook Yaron Brook Show (YouTube): https:\\/\\/www.youtube.com\\/user\\/ybrook Free Market Revolution (book): https:\\/\\/amzn.to\\/32H0oLb Equal is Unfair (book): https:\\/\\/amzn.to\\/32K3NsC PODCAST INFO: Podcast website: https:\\/\\/lexfridman.com\\/podcast Apple Podcasts: https:\\/\\/apple.co\\/2lwqZIr Spotify: https:\\/\\/spoti.fi\\/2nEwCF8 RSS: https:\\/\\/lexfridman.com\\/feed\\/podcast\\/ YouTube Full Episodes: https:\\/\\/youtube.com\\/lexfridman YouTube Clips: https:\\/\\/youtube.com\\/lexclips SUPPORT & CONNECT: \\u2013 Check out the sponsors\",\"1473\":\"A conversation with Christof Koch as part of MIT course on Artificial General Intelligence. Video version is available on YouTube. He is the President and Chief Scientific Officer of the Allen Institute for Brain Science in Seattle. From 1986 until 2013, he was a professor at CalTech. Cited more than 105,000 times. Author of several books including \\u201cConsciousness: Confessions of a Romantic Reductionist.\\u201d If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman on Twitter, LinkedIn, Facebook, or YouTube where you can watch the video versions of these conversations.\",\"4272\":\"Tweet Share Share Last Updated on January 16, 2021 Gradient descent is an optimization algorithm that follows the negative gradient of an objective function in order to locate the minimum of the function. A limitation of gradient descent is that a single step size (learning rate) is used for all input variables. Extensions to gradient descent like AdaGrad and RMSProp update the algorithm to use a separate step size for each input variable but may result in a step size that rapidly decreases to very small values. The Adaptive Movement Estimation algorithm, or Adam for short, is an extension to gradient descent and a natural successor to techniques like AdaGrad and RMSProp that automatically adapts a learning rate for each input variable for the objective function and further smooths the search process by using an exponentially decreasing moving average of the gradient to make updates to variables. In this tutorial, you will discover how to develop gradient descent with Adam optimization algorithm from scratch. After completing this tutorial, you will know: Gradient descent is an optimization algorithm that uses the gradient of the objective function to navigate the search space. Gradient descent can be updated to use an automatically adaptive step size for each input variable using a decaying average of partial derivatives, called Adam. How to implement the Adam optimization algorithm from scratch and apply it to an objective function and evaluate the results. Let\\u2019s get started. Gradient Descent Optimization With Adam From Scratch Photo by Don Graham, some rights reserved. Tutorial Overview This tutorial is divided into three parts; they are: Gradient Descent Adam Optimization Algorithm Gradient Descent With Adam Two-Dimensional Test Problem Gradient Descent Optimization With Adam Visualization of Adam Gradient Descent Gradient descent is an optimization algorithm. It is technically referred to as a first-order optimization algorithm as it explicitly makes use of the first-order derivative of the target objective function. First-order methods rely on gradient information to help direct the search for a minimum \\u2026 \\u2014 Page 69, Algorithms for Optimization, 2019. The first-order derivative, or simply the \\u201cderivative,\\u201d is the rate of change or slope of the target function at a specific point, e.g. for a specific input. If the target function takes multiple input variables, it is referred to as a multivariate function and the input variables can be thought of as a vector. In turn, the derivative of a multivariate target function may also be taken as a vector and is referred to generally as the gradient. Gradient: First-order derivative for a multivariate objective function. The derivative or the gradient points in the direction of the steepest ascent of the target function for a specific input. Gradient descent refers to a minimization optimization algorithm that follows the negative of the gradient downhill of the target function to locate the minimum of the function. The gradient descent algorithm requires a target function that is being optimized and the derivative function for the objective function. The target function f() returns a score for a given set of inputs, and the derivative function f'() gives the derivative of the target function for a given set of inputs. The gradient descent algorithm requires a starting point (x) in the problem, such as a randomly selected point in the input space. The derivative is then calculated and a step is taken in the input space that is expected to result in a downhill movement in the target function, assuming we are minimizing the target function. A downhill movement is made by first calculating how far to move in the input space, calculated as the step size (called alpha or the learning rate) multiplied by the gradient. This is then subtracted from the current point, ensuring we move against the gradient, or down the target function. x(t) = x(t-1) \\u2013 step_size * f'(x(t-1)) The steeper the objective function at a given point, the larger the magnitude of the gradient and, in turn, the larger the step taken in the search space. The size of the step taken is scaled using a step size hyperparameter. Step Size (alpha): Hyperparameter that controls how far to move in the search space against the gradient each iteration of the algorithm. If the step size is too small, the movement in the search space will be small and the search will take a long time. If the step size is too large, the search may bounce around the search space and skip over the optima. Now that we are familiar with the gradient descent optimization algorithm, let\\u2019s take a look at the Adam algorithm. Adam Optimization Algorithm Adaptive Movement Estimation algorithm, or Adam for short, is an extension to the gradient descent optimization algorithm. The algorithm was described in the 2014 paper by Diederik Kingma and Jimmy Lei Ba titled \\u201cAdam: A Method for Stochastic Optimization.\\u201d Adam is designed to accelerate the optimization process, e.g. decrease the number of function evaluations required to reach the optima, or to improve the capability of the optimization algorithm, e.g. result in a better final result. This is achieved by calculating a step size for each input parameter that is being optimized. Importantly, each step size is automatically adapted throughput the search process based on the gradients (partial derivatives) encountered for each variable. We propose Adam, a method for efficient stochastic optimization that only requires first-order gradients with little memory requirement. The method computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients; the name Adam is derived from adaptive moment estimation \\u2014 Adam: A Method for Stochastic Optimization This involves maintaining a first and second moment of the gradient, e.g. an exponentially decaying mean gradient (first moment) and variance (second moment) for each input variable. The moving averages themselves are estimates of the 1st moment (the mean) and the 2nd raw moment (the uncentered variance) of the gradient. \\u2014 Adam: A Method for Stochastic Optimization Let\\u2019s step through each element of the algorithm. First, we must maintain a moment vector and exponentially weighted infinity norm for each parameter being optimized as part of the search, referred to as m and v (really the Greek letter nu) respectively. They are initialized to 0.0 at the start of the search. m = 0 v = 0 The algorithm is executed iteratively over time t starting at t=1, and each iteration involves calculating a new set of parameter values x, e.g. going from x(t-1) to x(t). It is perhaps easy to understand the algorithm if we focus on updating one parameter, which generalizes to updating all parameters via vector operations. First, the gradient (partial derivatives) are calculated for the current time step. g(t) = f'(x(t-1)) Next, the first moment is updated using the gradient and a hyperparameter beta1. m(t) = beta1 * m(t-1) + (1 \\u2013 beta1) * g(t) Then the second moment is updated using the squared gradient and a hyperparameter beta2. v(t) = beta2 * v(t-1) + (1 \\u2013 beta2) * g(t)^2 The first and second moments are biased because they are initialized with zero values. \\u2026 these moving averages are initialized as (vectors of) 0\\u2019s, leading to moment estimates that are biased towards zero, especially during the initial timesteps, and especially when the decay rates are small (i.e. the betas are close to 1). The good news is that this initialization bias can be easily counteracted, resulting in bias-corrected estimates \\u2026 \\u2014 Adam: A Method for Stochastic Optimization Next the first and second moments are bias-corrected, starring with the first moment: mhat(t) = m(t) \\/ (1 \\u2013 beta1(t)) And then the second moment: vhat(t) = v(t) \\/ (1 \\u2013 beta2(t)) Note, beta1(t) and beta2(t) refer to the beta1 and beta2 hyperparameters that are decayed on a schedule over the iterations of the algorithm. A static decay schedule can be used, although the paper recommend the following: beta1(t) = beta1^t beta2(t) = beta2^t Finally, we can calculate the value for the parameter for this iteration. x(t) = x(t-1) \\u2013 alpha * mhat(t) \\/ (sqrt(vhat(t)) + eps) Where alpha is the step size hyperparameter, eps is a small value (epsilon) such as 1e-8 that ensures we do not encounter a divide by zero error, and sqrt() is the square root function. Note, a more efficient reordering of the update rule listed in the paper can be used: alpha(t) = alpha * sqrt(1 \\u2013 beta2(t)) \\/ (1 \\u2013 beta1(t)) x(t) = x(t-1) \\u2013 alpha(t) * m(t) \\/ (sqrt(v(t)) + eps) To review, there are three hyperparameters for the algorithm, they are: alpha: Initial step size (learning rate), a typical value is 0.001. beta1: Decay factor for first momentum, a typical value is 0.9. beta2: Decay factor for infinity norm, a typical value is 0.999. And that\\u2019s it. For full derivation of the Adam algorithm in the context of the Adam algorithm, I recommend reading the paper. Adam: A Method for Stochastic Optimization Next, let\\u2019s look at how we might implement the algorithm from scratch in Python. Gradient Descent With Adam In this section, we will explore how to implement the gradient descent optimization algorithm with Adam. Two-Dimensional Test Problem First, let\\u2019s define an optimization function. We will use a simple two-dimensional function that squares the input of each dimension and define the range of valid inputs from -1.0 to 1.0. The objective() function below implements this function # objective function def objective(x, y): return x**2.0 + y**2.0 We can create a three-dimensional plot of the dataset to get a feeling for the curvature of the response surface. The complete example of plotting the objective function is listed below. # 3d plot of the test function from numpy import arange from numpy import meshgrid from matplotlib import pyplot # objective function def objective(x, y): return x**2.0 + y**2.0 # define range for input r_min, r_max = -1.0, 1.0 # sample input range uniformly at 0.1 increments xaxis = arange(r_min, r_max, 0.1) yaxis = arange(r_min, r_max, 0.1) # create a mesh from the axis x, y = meshgrid(xaxis, yaxis) # compute targets results = objective(x, y) # create a surface plot with the jet color scheme figure = pyplot.figure() axis = figure.gca(projection='3d') axis.plot_surface(x, y, results, cmap='jet') # show the plot pyplot.show() Running the example creates a three-dimensional surface plot of the objective function. We can see the familiar bowl shape with the global minima at f(0, 0) = 0. Three-Dimensional Plot of the Test Objective Function We can also create a two-dimensional plot of the function. This will be helpful later when we want to plot the progress of the search. The example below creates a contour plot of the objective function. # contour plot of the test function from numpy import asarray from numpy import arange from numpy import meshgrid from matplotlib import pyplot # objective function def objective(x, y): return x**2.0 + y**2.0 # define range for input bounds = asarray([[-1.0, 1.0], [-1.0, 1.0]]) # sample input range uniformly at 0.1 increments xaxis = arange(bounds[0,0], bounds[0,1], 0.1) yaxis = arange(bounds[1,0], bounds[1,1], 0.1) # create a mesh from the axis x, y = meshgrid(xaxis, yaxis) # compute targets results = objective(x, y) # create a filled contour plot with 50 levels and jet color scheme pyplot.contourf(x, y, results, levels=50, cmap='jet') # show the plot pyplot.show() Running the example creates a two-dimensional contour plot of the objective function. We can see the bowl shape compressed to contours shown with a color gradient. We will use this plot to plot the specific points explored during the progress of the search. Two-Dimensional Contour Plot of the Test Objective Function Now that we have a test objective function, let\\u2019s look at how we might implement the Adam optimization algorithm. Gradient Descent Optimization With Adam We can apply the gradient descent with Adam to the test problem. First, we need a function that calculates the derivative for this function. f(x) = x^2 f'(x) = x * 2 The derivative of x^2 is x * 2 in each dimension. The derivative() function implements this below. # derivative of objective function def derivative(x, y): return asarray([x * 2.0, y * 2.0]) Next, we can implement gradient descent optimization. First, we can select a random point in the bounds of the problem as a starting point for the search. This assumes we have an array that defines the bounds of the search with one row for each dimension and the first column defines the minimum and the second column defines the maximum of the dimension. ... # generate an initial point x = bounds[:, 0] + rand(len(bounds)) * (bounds[:, 1] - bounds[:, 0]) score = objective(x[0], x[1]) Next, we need to initialize the first and second moments to zero. ... # initialize first and second moments m = [0.0 for _ in range(bounds.shape[0])] v = [0.0 for _ in range(bounds.shape[0])] We then run a fixed number of iterations of the algorithm defined by the \\u201cn_iter\\u201d hyperparameter. ... # run iterations of gradient descent for t in range(n_iter): ... The first step is to calculate the gradient for the current solution using the derivative() function. ... # calculate gradient gradient = derivative(solution[0], solution[1]) The first step is to calculate the derivative for the current set of parameters. ... # calculate gradient g(t) g = derivative(x[0], x[1]) Next, we need to perform the Adam update calculations. We will perform these calculations one variable at a time using an imperative programming style for readability. In practice, I recommend using NumPy vector operations for efficiency. ... # build a solution one variable at a time for i in range(x.shape[0]): ... First, we need to calculate the moment. ... # m(t) = beta1 * m(t-1) + (1 - beta1) * g(t) m[i] = beta1 * m[i] + (1.0 - beta1) * g[i] Then the second moment. ... # v(t) = beta2 * v(t-1) + (1 - beta2) * g(t)^2 v[i] = beta2 * v[i] + (1.0 - beta2) * g[i]**2 Then the bias correction for the first and second moments. ... # mhat(t) = m(t) \\/ (1 - beta1(t)) mhat = m[i] \\/ (1.0 - beta1**(t+1)) # vhat(t) = v(t) \\/ (1 - beta2(t)) vhat = v[i] \\/ (1.0 - beta2**(t+1)) Then finally the updated variable value. ... # x(t) = x(t-1) - alpha * mhat(t) \\/ (sqrt(vhat(t)) + eps) x[i] = x[i] - alpha * mhat \\/ (sqrt(vhat) + eps) This is then repeated for each parameter that is being optimized. At the end of the iteration we can evaluate the new parameter values and report the performance of the search. ... # evaluate candidate point score = objective(x[0], x[1]) # report progress print('\\/preem\\/empre class=\\\"urvanov-syntax-highlighter-plain-tag\\\"\",\"3192\":\"Multinomial logistic regression is an extension of logistic regression that adds native support for multi-class classification problems. Logistic regression, by default, is limited to two-class classification problems. Some extensions like one-vs-rest can allow logistic regression to be used for multi-class classification problems, although they require that the classification problem first be transformed into multiple binary classification problems. Instead, the multinomial logistic regression algorithm is an extension to the logistic regression model that involves changing the loss function to cross-entropy loss and predict probability distribution to a multinomial probability distribution to natively support multi-class classification problems. In this tutorial, you will discover how to develop multinomial logistic regression models in Python. After completing this tutorial, you will know: Multinomial logistic regression is an extension of logistic regression for multi-class classification. How to develop and evaluate multinomial logistic regression and develop a final model for making predictions on new data. How to tune the penalty hyperparameter for the multinomial logistic regression model. Let\\u2019s get started. Multinomial Logistic Regression With Python Photo by Nicolas R\\u00e9nac, some rights reserved. Tutorial Overview This tutorial is divided into three parts; they are: Multinomial Logistic Regression Evaluate Multinomial Logistic Regression Model Tune Penalty for Multinomial Logistic Regression Multinomial Logistic Regression Logistic regression is a classification algorithm. It is intended for datasets that have numerical input variables and a categorical target variable that has two values or classes. Problems of this type are referred to as binary classification problems. Logistic regression is designed for two-class problems, modeling the target using a binomial probability distribution function. The class labels are mapped to 1 for the positive class or outcome and 0 for the negative class or outcome. The fit model predicts the probability that an example belongs to class 1. By default, logistic regression cannot be used for classification tasks that have more than two class labels, so-called multi-class classification. Instead, it requires modification to support multi-class classification problems. One popular approach for adapting logistic regression to multi-class classification problems is to split the multi-class classification problem into multiple binary classification problems and fit a standard logistic regression model on each subproblem. Techniques of this type include one-vs-rest and one-vs-one wrapper models. An alternate approach involves changing the logistic regression model to support the prediction of multiple class labels directly. Specifically, to predict the probability that an input example belongs to each known class label. The probability distribution that defines multi-class probabilities is called a multinomial probability distribution. A logistic regression model that is adapted to learn and predict a multinomial probability distribution is referred to as Multinomial Logistic Regression. Similarly, we might refer to default or standard logistic regression as Binomial Logistic Regression. Binomial Logistic Regression: Standard logistic regression that predicts a binomial probability (i.e. for two classes) for each input example. Multinomial Logistic Regression: Modified version of logistic regression that predicts a multinomial probability (i.e. more than two classes) for each input example. If you are new to binomial and multinomial probability distributions, you may want to read the tutorial: Discrete Probability Distributions for Machine Learning Changing logistic regression from binomial to multinomial probability requires a change to the loss function used to train the model (e.g. log loss to cross-entropy loss), and a change to the output from a single probability value to one probability for each class label. Now that we are familiar with multinomial logistic regression, let\\u2019s look at how we might develop and evaluate multinomial logistic regression models in Python. Evaluate Multinomial Logistic Regression Model In this section, we will develop and evaluate a multinomial logistic regression model using the scikit-learn Python machine learning library. First, we will define a synthetic multi-class classification dataset to use as the basis of the investigation. This is a generic dataset that you can easily replace with your own loaded dataset later. The make_classification() function can be used to generate a dataset with a given number of rows, columns, and classes. In this case, we will generate a dataset with 1,000 rows, 10 input variables or columns, and 3 classes. The example below generates the dataset and summarizes the shape of the arrays and the distribution of examples across the three classes. # test classification dataset from collections import Counter from sklearn.datasets import make_classification # define dataset X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, n_classes=3, random_state=1) # summarize the dataset print(X.shape, y.shape) print(Counter(y)) Running the example confirms that the dataset has 1,000 rows and 10 columns, as we expected, and that the rows are distributed approximately evenly across the three classes, with about 334 examples in each class. (1000, 10) (1000,) Counter({1: 334, 2: 334, 0: 332}) Logistic regression is supported in the scikit-learn library via the LogisticRegression class. The LogisticRegression class can be configured for multinomial logistic regression by setting the \\u201cmulti_class\\u201d argument to \\u201cmultinomial\\u201d and the \\u201csolver\\u201d argument to a solver that supports multinomial logistic regression, such as \\u201clbfgs\\u201c. ... # define the multinomial logistic regression model model = LogisticRegression(multi_class='multinomial', solver='lbfgs') The multinomial logistic regression model will be fit using cross-entropy loss and will predict the integer value for each integer encoded class label. Now that we are familiar with the multinomial logistic regression API, we can look at how we might evaluate a multinomial logistic regression model on our synthetic multi-class classification dataset. It is a good practice to evaluate classification models using repeated stratified k-fold cross-validation. The stratification ensures that each cross-validation fold has approximately the same distribution of examples in each class as the whole training dataset. We will use three repeats with 10 folds, which is a good default, and evaluate model performance using classification accuracy given that the classes are balanced. The complete example of evaluating multinomial logistic regression for multi-class classification is listed below. # evaluate multinomial logistic regression model from numpy import mean from numpy import std from sklearn.datasets import make_classification from sklearn.model_selection import cross_val_score from sklearn.model_selection import RepeatedStratifiedKFold from sklearn.linear_model import LogisticRegression # define dataset X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, n_classes=3, random_state=1) # define the multinomial logistic regression model model = LogisticRegression(multi_class='multinomial', solver='lbfgs') # define the model evaluation procedure cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) # evaluate the model and collect the scores n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1) # report the model performance print('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores))) Running the example reports the mean classification accuracy across all folds and repeats of the evaluation procedure. Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome. In this case, we can see that the multinomial logistic regression model with default penalty achieved a mean classification accuracy of about 68.1 percent on our synthetic classification dataset. Mean Accuracy: 0.681 (0.042) We may decide to use the multinomial logistic regression model as our final model and make predictions on new data. This can be achieved by first fitting the model on all available data, then calling the predict() function to make a prediction for new data. The example below demonstrates how to make a prediction for new data using the multinomial logistic regression model. # make a prediction with a multinomial logistic regression model from sklearn.datasets import make_classification from sklearn.linear_model import LogisticRegression # define dataset X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, n_classes=3, random_state=1) # define the multinomial logistic regression model model = LogisticRegression(multi_class='multinomial', solver='lbfgs') # fit the model on the whole dataset model.fit(X, y) # define a single row of input data row = [1.89149379, -0.39847585, 1.63856893, 0.01647165, 1.51892395, -3.52651223, 1.80998823, 0.58810926, -0.02542177, -0.52835426] # predict the class label yhat = model.predict([row]) # summarize the predicted class print('Predicted Class: %d' % yhat[0]) Running the example first fits the model on all available data, then defines a row of data, which is provided to the model in order to make a prediction. In this case, we can see that the model predicted the class \\u201c1\\u201d for the single row of data. Predicted Class: 1 A benefit of multinomial logistic regression is that it can predict calibrated probabilities across all known class labels in the dataset. This can be achieved by calling the predict_proba() function on the model. The example below demonstrates how to predict a multinomial probability distribution for a new example using the multinomial logistic regression model. # predict probabilities with a multinomial logistic regression model from sklearn.datasets import make_classification from sklearn.linear_model import LogisticRegression # define dataset X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, n_classes=3, random_state=1) # define the multinomial logistic regression model model = LogisticRegression(multi_class='multinomial', solver='lbfgs') # fit the model on the whole dataset model.fit(X, y) # define a single row of input data row = [1.89149379, -0.39847585, 1.63856893, 0.01647165, 1.51892395, -3.52651223, 1.80998823, 0.58810926, -0.02542177, -0.52835426] # predict a multinomial probability distribution yhat = model.predict_proba([row]) # summarize the predicted probabilities print('Predicted Probabilities: %s' % yhat[0]) Running the example first fits the model on all available data, then defines a row of data, which is provided to the model in order to predict class probabilities. Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome. In this case, we can see that class 1 (e.g. the array index is mapped to the class integer value) has the largest predicted probability with about 0.50. Predicted Probabilities: [0.16470456 0.50297138 0.33232406] Now that we are familiar with evaluating and using multinomial logistic regression models, let\\u2019s explore how we might tune the model hyperparameters. Tune Penalty for Multinomial Logistic Regression An important hyperparameter to tune for multinomial logistic regression is the penalty term. This term imposes pressure on the model to seek smaller model weights. This is achieved by adding a weighted sum of the model coefficients to the loss function, encouraging the model to reduce the size of the weights along with the error while fitting the model. A popular type of penalty is the L2 penalty that adds the (weighted) sum of the squared coefficients to the loss function. A weighting of the coefficients can be used that reduces the strength of the penalty from full penalty to a very slight penalty. By default, the LogisticRegression class uses the L2 penalty with a weighting of coefficients set to 1.0. The type of penalty can be set via the \\u201cpenalty\\u201d argument with values of \\u201cl1\\u201c, \\u201cl2\\u201c, \\u201celasticnet\\u201d (e.g. both), although not all solvers support all penalty types. The weighting of the coefficients in the penalty can be set via the \\u201cC\\u201d argument. ... # define the multinomial logistic regression model with a default penalty LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', C=1.0) The weighting for the penalty is actually the inverse weighting, perhaps penalty = 1 \\u2013 C. From the documentation: C : float, default=1.0 Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization. This means that values close to 1.0 indicate very little penalty and values close to zero indicate a strong penalty. A C value of 1.0 may indicate no penalty at all. C close to 1.0: Light penalty. C close to 0.0: Strong penalty. The penalty can be disabled by setting the \\u201cpenalty\\u201d argument to the string \\u201cnone\\u201c. ... # define the multinomial logistic regression model without a penalty LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='none') Now that we are familiar with the penalty, let\\u2019s look at how we might explore the effect of different penalty values on the performance of the multinomial logistic regression model. It is common to test penalty values on a log scale in order to quickly discover the scale of penalty that works well for a model. Once found, further tuning at that scale may be beneficial. We will explore the L2 penalty with weighting values in the range from 0.0001 to 1.0 on a log scale, in addition to no penalty or 0.0. The complete example of evaluating L2 penalty values for multinomial logistic regression is listed below. # tune regularization for multinomial logistic regression from numpy import mean from numpy import std from sklearn.datasets import make_classification from sklearn.model_selection import cross_val_score from sklearn.model_selection import RepeatedStratifiedKFold from sklearn.linear_model import LogisticRegression from matplotlib import pyplot # get the dataset def get_dataset(): X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=1, n_classes=3) return X, y # get a list of models to evaluate def get_models(): models = dict() for p in [0.0, 0.0001, 0.001, 0.01, 0.1, 1.0]: # create name for model key = '%.4f' % p # turn off penalty in some cases if p == 0.0: # no penalty in this case models[key] = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='none') else: models[key] = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', C=p) return models # evaluate a give model using cross-validation def evaluate_model(model, X, y): # define the evaluation procedure cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) # evaluate the model scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1) return scores # define dataset X, y = get_dataset() # get the models to evaluate models = get_models() # evaluate the models and store results results, names = list(), list() for name, model in models.items(): # evaluate the model and collect the scores scores = evaluate_model(model, X, y) # store the results results.append(scores) names.append(name) # summarize progress along the way print('\\/prestrong\\/stronga href=\\\"https:\\/\\/machinelearningmastery.com\\/different-results-each-time-in-machine-learning\\/\\\"\\/apre class=\\\"urvanov-syntax-highlighter-plain-tag\\\"\",\"4954\":\"#ai #technology #switchtransformer Scale is the next frontier for AI. Google Brain uses sparsity and hard routing to massively increase a model's parameters, while keeping the FLOPs per forward pass constant. The Switch Transformer compares favorably to its dense counterparts in terms of speed and sample efficiency and breaks the next magic number: One Trillion Parameters. OUTLINE: 0:00 - Intro & Overview 4:30 - Performance Gains from Scale 8:30 - Switch Transformer Architecture 17:00 - Model-, Data- and Expert-Parallelism 25:30 - Experimental Results 29:00 - Stabilizing Training 32:20 - Distillation into Dense Models 33:30 - Final Comments Paper: https:\\/\\/arxiv.org\\/abs\\/2101.03961 Codebase T5: https:\\/\\/github.com\\/google-research\\/text-to-text-transfer-transformer Abstract: In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) defies this and instead selects different parameters for each incoming example. The result is a sparsely-activated model -- with outrageous numbers of parameters -- but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs and training instability -- we address these with the Switch Transformer. We simplify the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs. Our proposed training techniques help wrangle the instabilities and we show large sparse models may be trained, for the first time, with lower precision (bfloat16) formats. We design models based off T5-Base and T5-Large to obtain up to 7x increases in pre-training speed with the same computational resources. These improvements extend into multilingual settings where we measure gains over the mT5-Base version across all 101 languages. Finally, we advance the current scale of language models by pre-training up to trillion parameter models on the \\\"Colossal Clean Crawled Corpus\\\" and achieve a 4x speedup over the T5-XXL model. Authors: William Fedus, Barret Zoph, Noam Shazeer Links: TabNine Code Completion (Referral): http:\\/\\/bit.ly\\/tabnine-yannick YouTube: https:\\/\\/www.youtube.com\\/c\\/yannickilcher Twitter: https:\\/\\/twitter.com\\/ykilcher Discord: https:\\/\\/discord.gg\\/4H8xxDF BitChute: https:\\/\\/www.bitchute.com\\/channel\\/yannic-kilcher Minds: https:\\/\\/www.minds.com\\/ykilcher Parler: https:\\/\\/parler.com\\/profile\\/YannicKilcher LinkedIn: https:\\/\\/www.linkedin.com\\/in\\/yannic-kilcher-488534136\\/ BiliBili: https:\\/\\/space.bilibili.com\\/1824646584 If you want to support me, the best thing to do is to share out the content :) If you want to support me financially (completely optional and voluntary, but a lot of people have asked for this): SubscribeStar: https:\\/\\/www.subscribestar.com\\/yannickilcher Patreon: https:\\/\\/www.patreon.com\\/yannickilcher Bitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq Ethereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2 Litecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m Monero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\",\"4301\":\"This week join Dr. Tim Scarfe, Yannic Kilcher, and Keith Duggar have a conversation with Dr. Rebecca Roache in the last of our 3-part series on the social dilemma Netflix film. Rebecca is a senior lecturer in philosophy at Royal Holloway, university of London and has written extensively about the future of friendship. People claim that friendships are not what they used to be. People are always staring at their phones, even when in public Social media has turned us into narcissists who are always managing our own PR rather than being present with each other. Anxiety about the negative effects of technology are as old as the written word. Is technology bad for friendships? Can you have friends through screens? Does social media cause polarization? And is that a bad thing? Does it promote quantity over quality? Rebecca thinks that social media and echo chambers are less ominous to friendship on closer inspection. 00:00:32 Teaser clip from Rebecca and her new manuscript on friendship 00:02:52 Introduction 00:04:56 Memorisation vs reasoning \\/ is technology enhancing friendships 00:09:29 Word of warcraft \\/ gaming communities \\/ echo chambers \\/ polarisation 00:12:34 Horizontal vs Vertical social attributes 00:17:18 Exclusion of others opinions 00:20:36 The power to silence others \\/ truth verification 00:23:58 Misinformation 00:27:28 Norms \\/ memes \\/ political terms and co-opting \\/ bullying 00:31:57 Redefinition of political terms i.e. racism 00:36:13 Virtue signalling 00:38:57 How many friends can you have \\/ spread thin \\/ Dunbars 150 00:42:54 Is it morally objectionable to believe or contemplate objectionable ideas, punishment 00:50:52 Is speaking the same thing as acting 00:52:24 Punishment - deterrence vs retribution \\/ historical 00:53:59 Yannic: contemplating is a form of speaking 00:57:32 silencing\\/blocking is intellectual laziness - what ideas are we allowed to talk about 01:04:53 Corporate AI ethics frameworks 01:09:14 Autonomous Vehicles 01:10:51 the eternal Facebook world \\/ online vs offline friendships 01:14:05 How do we get the best out of our online friendships\",\"3747\":\"I recently really enjoyed GAN music videos, i.e., latent walks synced up to a music track. So I spent a bit of time creating one using a Counterfactual Generative Network (CGN) (https:\\/\\/arxiv.org\\/abs\\/2101.06046), showcasing the CGN's shape\\/texture\\/background disentanglement. Here is the video: https:\\/\\/www.youtube.com\\/watch?v=JDwaLueR35U The code for the CGN will be available soon: https:\\/\\/github.com\\/autonomousvision\\/counterfactual_generative_networks [link] [comments]\",\"1453\":\"Charles Isbell is the Dean of the College of Computing at Georgia Tech. Michael Littman is a computer scientist at Brown University. Please support this podcast by checking out our sponsors: \\u2013 Athletic Greens: https:\\/\\/athleticgreens.com\\/lex and use code LEX to get 1 month of fish oil \\u2013 Eight Sleep: https:\\/\\/www.eightsleep.com\\/lex and use code LEX to get special savings \\u2013 MasterClass: https:\\/\\/masterclass.com\\/lex to get 2 for price of 1 \\u2013 Cash App: https:\\/\\/cash.app\\/ and use code LexPodcast to get $10 EPISODE LINKS: Charles\\u2019s Twitter: https:\\/\\/twitter.com\\/isbellHFh Charles\\u2019s Website: https:\\/\\/www.cc.gatech.edu\\/~isbell\\/ Michael\\u2019s Twitter: https:\\/\\/twitter.com\\/mlittmancs Michael\\u2019s Website: https:\\/\\/www.littmania.com\\/ Michael\\u2019s YouTube: https:\\/\\/www.youtube.com\\/user\\/mlittman PODCAST INFO: Podcast website: https:\\/\\/lexfridman.com\\/podcast Apple Podcasts: https:\\/\\/apple.co\\/2lwqZIr Spotify: https:\\/\\/spoti.fi\\/2nEwCF8 RSS: https:\\/\\/lexfridman.com\\/feed\\/podcast\\/ YouTube Full Episodes: https:\\/\\/youtube.com\\/lexfridman YouTube Clips: https:\\/\\/youtube.com\\/lexclips SUPPORT & CONNECT: \\u2013 Check out the sponsors above, it\\u2019s the best way to support this podcast \\u2013 Support on Patreon: https:\\/\\/www.patreon.com\\/lexfridman \\u2013 Twitter: https:\\/\\/twitter.com\\/lexfridman \\u2013 Instagram: https:\\/\\/www.instagram.com\\/lexfridman \\u2013 LinkedIn: https:\\/\\/www.linkedin.com\\/in\\/lexfridman \\u2013 Facebook: https:\\/\\/www.facebook.com\\/LexFridmanPage \\u2013 Medium: https:\\/\\/medium.com\\/@lexfridman OUTLINE: Here\\u2019s the timestamps for the episode. On some podcast players you should be able to click the timestamp to jump to that time. (00:00) \\u2013 Introduction (07:51) \\u2013 Is machine learning just statistics? (12:14) \\u2013 NeurIPS vs ICML (14:30) \\u2013 Data is more important than algorithm (20:14) \\u2013 The role of hardship in education (28:57) \\u2013 How Charles and Michael met (33:30) \\u2013 Key to success: never be satisfied (36:47) \\u2013 Bell Labs (48:15) \\u2013 Teaching machine learning (58:25) \\u2013 Westworld and Ex Machina (1:06:24) \\u2013 Simulation (1:13:14) \\u2013 The college experience in the times of COVID (1:41:52) \\u2013 Advice for young people (1:48:44) \\u2013 How to learn to program (2:00:07) \\u2013 Friendship\",\"1567\":\"Elon Musk is the CEO of Tesla, SpaceX, Neuralink, and a co-founder of several other companies. This is the second time Elon has been on the podcast. You can watch the first time on YouTube or listen to the first time on its episode page. You can read the transcript (PDF) here. This conversation is part of the Artificial Intelligence podcast. If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman on Twitter, LinkedIn, Facebook, Medium, or YouTube where you can watch the video versions of these conversations. If you enjoy the podcast,\",\"1452\":\"Elon Musk is the CEO of Tesla, SpaceX, Neuralink, and a co-founder of several other companies. Video version is available on YouTube. If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman on Twitter, LinkedIn, Facebook, Medium, or YouTube where you can watch the video versions of these conversations.\",\"1574\":\"David Silver leads the reinforcement learning research group at DeepMind and was lead researcher on AlphaGo, AlphaZero and co-lead on AlphaStar, and MuZero and lot of important work in reinforcement learning. Support this podcast by signing up with these sponsors: \\u2013 MasterClass: https:\\/\\/masterclass.com\\/lex \\u2013 Cash App \\u2013 use code \\u201cLexPodcast\\u201d and download: \\u2013 Cash App (App Store): https:\\/\\/apple.co\\/2sPrUHe \\u2013 Cash App (Google Play): https:\\/\\/bit.ly\\/2MlvP5w EPISODE LINKS: Reinforcement learning (book): https:\\/\\/amzn.to\\/2Jwp5zG This conversation is part of the Artificial Intelligence podcast. If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman on Twitter, LinkedIn, Facebook,\",\"1437\":\"Michael Stevens is the creator of Vsauce, one of the most popular educational YouTube channel in the world, with over 15 million subscribers and over 1.7 billion views. His videos often ask and answer questions that are both profound and entertaining, spanning topics from physics to psychology. As part of his channel he created 3 seasons of Mind Field, a series that explored human behavior. This conversation is part of the Artificial Intelligence podcast. If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman on Twitter, LinkedIn, Facebook, Medium, or YouTube where you\",\"1561\":\"Stephen Schwarzman is the CEO and Co-Founder of Blackstone, one of the world\\u2019s leading investment firms with over 530 billion dollars of assets under management. He is one of the most successful business leaders in history, all from humble beginnings back in Philly. I recommend his recent book called What It Takes that tells stories and lessons from this personal journey. Support this podcast by signing up with these sponsors: \\u2013 ExpressVPN at https:\\/\\/www.expressvpn.com\\/lexpod \\u2013 MasterClass: https:\\/\\/masterclass.com\\/lex EPISODE LINKS: What It Takes (book): https:\\/\\/amzn.to\\/2WX9cZu This conversation is part of the Artificial Intelligence podcast. If you would like to get more information\",\"4309\":\"This week Dr. Tim Scarfe, Dr. Keith Duggar and Yannic Kilcher discuss multi-arm bandits and pure exploration with Dr. Wouter M. Koolen, Senior Researcher, Machine Learning group, Centrum Wiskunde & Informatica. Wouter specialises in machine learning theory, game theory, information theory, statistics and optimisation. Wouter is currently interested in pure exploration in multi-armed bandit models, game tree search, and accelerated learning in sequential decision problems. His research has been cited 1000 times, and he has been published in NeurIPS, the number 1 ML conference 14 times as well as lots of other exciting publications. Today we are going to talk about two of the most studied settings in control, decision theory, and learning in unknown environment which are the multi-armed bandit (MAB) and reinforcement learning (RL) approaches - when can an agent stop learning and start exploiting using the knowledge it obtained - which strategy leads to minimal learning time 00:00:00 What are multi-arm bandits\\/show trailer 00:12:55 Show introduction 00:15:50 Bandits 00:18:58 Taxonomy of decision framework approaches 00:25:46 Exploration vs Exploitation 00:31:43 the sharp divide between modes 00:34:12 bandit measures of success 00:36:44 connections to reinforcement learning 00:44:00 when to apply pure exploration in games 00:45:54 bandit lower bounds, a pure exploration renaissance 00:50:21 pure exploration compiler dreams 00:51:56 what would the PX-compiler DSL look like 00:57:13 the long arms of the bandit 01:00:21 causal models behind the curtain of arms 01:02:43 adversarial bandits, arms trying to beat you 01:05:12 bandits as an optimization problem 01:11:39 asymptotic optimality vs practical performance 01:15:38 pitfalls hiding under asymptotic cover 01:18:50 adding features to bandits 01:27:24 moderate confidence regimes 01:30:33 algorithms choice is highly sensitive to bounds 01:46:09 Post script: Keith interesting piece on n quantum http:\\/\\/wouterkoolen.info https:\\/\\/www.cwi.nl\\/research-groups\\/ma... #machinelearning\",\"1460\":\"Steven Pressfield is a historian and author of War of Art, a book that had a big impact on my life and the life of millions of whose passion is to create in art, science, business, sport, and everywhere else. I highly recommend it and others of his books on this topic, including Turning Pro, Do the Work, Nobody Wants to Read Your Shit, and the Warrior Ethos. Also his books Gates of Fire about the Spartans and the battle at Thermopylae, The Lion\\u2019s Gate, Tides of War, and others are some of the best historical fiction novels ever written.\",\"1522\":\"Jim Gates (S James Gates Jr.) is a theoretical physicist and professor at Brown University working on supersymmetry, supergravity, and superstring theory. He served on former President Obama\\u2019s Council of Advisors on Science and Technology. He is the co-author of a new book titled Proving Einstein Right about the scientists who set out to prove Einstein\\u2019s theory of relativity. This conversation is part of the Artificial Intelligence podcast. If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman on Twitter, LinkedIn, Facebook, Medium, or YouTube where you can watch the video versions of\",\"1587\":\"Avi Loeb is an astrophysicist at Harvard. Please support this podcast by checking out our sponsors: \\u2013 Zero Fasting: https:\\/\\/go.zerofasting.com\\/s\\/lex-promo to get 30% off annual subscription \\u2013 LMNT: https:\\/\\/drinkLMNT.com\\/lex to get free sample pack \\u2013 Sun Basket: https:\\/\\/sunbasket.com\\/lex and use code LEX to get $35 off \\u2013 Pessimists Archive: https:\\/\\/pessimists.co\\/ EPISODE LINKS: Extraterrestrial (book): https:\\/\\/amzn.to\\/39xdnkT Avi\\u2019s Website: https:\\/\\/astronomy.fas.harvard.edu\\/people\\/avi-loeb PODCAST INFO: Podcast website: https:\\/\\/lexfridman.com\\/podcast Apple Podcasts: https:\\/\\/apple.co\\/2lwqZIr Spotify: https:\\/\\/spoti.fi\\/2nEwCF8 RSS: https:\\/\\/lexfridman.com\\/feed\\/podcast\\/ YouTube Full Episodes: https:\\/\\/youtube.com\\/lexfridman YouTube Clips: https:\\/\\/youtube.com\\/lexclips SUPPORT & CONNECT: \\u2013 Check out the sponsors above, it\\u2019s the best way to support this podcast \\u2013 Support on Patreon: https:\\/\\/www.patreon.com\\/lexfridman \\u2013 Twitter: https:\\/\\/twitter.com\\/lexfridman \\u2013 Instagram: https:\\/\\/www.instagram.com\\/lexfridman \\u2013 LinkedIn: https:\\/\\/www.linkedin.com\\/in\\/lexfridman \\u2013 Facebook: https:\\/\\/www.facebook.com\\/LexFridmanPage \\u2013 Medium: https:\\/\\/medium.com\\/@lexfridman OUTLINE: Here\\u2019s the timestamps for the episode. On some podcast players you should be able to click the timestamp to jump to that time. (00:00) \\u2013 Introduction (10:08) \\u2013 Are we alone in the universe? (14:23) \\u2013 Consciousness (19:01) \\u2013 Sending digital copies of humans to space (23:38) \\u2013 Oumuamua (45:42) \\u2013 Alien space junk (49:41) \\u2013 What do aliens look like? (1:06:58) \\u2013 Drake equation (1:08:00) \\u2013 Industrial polution from aliens (1:19:52) \\u2013 UFO sightings (1:27:48) \\u2013 How long will human civilization last? (1:30:28) \\u2013 Radio signal from Proxima Centauri (1:33:49) \\u2013 Breakthrough Starshot project (1:36:49) \\u2013 Space race (1:42:00) \\u2013 Human space exploration (1:47:15) \\u2013 Social media is a threat to society (1:52:04) \\u2013 Are humans ready for discovering an alien civilization? (1:56:15) \\u2013 Mayans used astrology to wage war (1:57:31) \\u2013 Black holes (2:16:20) \\u2013 Stephen Hawking (2:19:59) \\u2013 Grigori Perelman (2:24:24) \\u2013 Theory of everything (2:31:23) \\u2013 Dark matter (2:34:06) \\u2013 Advice for young people (2:37:10) \\u2013 Memories of my father and mother (2:41:38) \\u2013 Existentialism (2:43:52) \\u2013 Mortality (2:46:27) \\u2013 Meaning of life\",\"1671\":\"Computer vision technology is the poster child of artificial intelligence. It is the sector of the industry that gets the most media attention because of the tools and benefits the technology can provide. From autonomous vehicles and drones to cancer detection and augmented reality, technologies that once only existed in science fiction are now at our doorstep. Read the full story\",\"1227\":\"NOTE: You can support StatQuest by purchasing the Jupyter Notebook and Python code seen in this video here: https:\\/\\/statquest.org\\/product\\/jupyter-notebook-xgboost-in-python\\/ \\u2b50 NOTE: When I code, I use Kite, a free AI-powered coding assistant that will help you code faster and smarter. The Kite plugin integrates with all the top editors and IDEs to give you smart completions and documentation while you\\u2019re typing. I love it! https:\\/\\/www.kite.com\\/get-kite\\/?utm_medium=referral&utm_source=youtube&utm_campaign=statquest&utm_content=description-only NOTE: This StatQuest assumes that you are already familiar with: XGBoost for Regression: https:\\/\\/youtu.be\\/OtD8wVaFm6E XGBoost for Classification: https:\\/\\/youtu.be\\/8b1JEDvenQU XGBoost: Crazy Cool Optimizations: https:\\/\\/youtu.be\\/oRrKeUCEbq8 Regularization: https:\\/\\/youtu.be\\/Q81RR3yKn30 Cross Validation: https:\\/\\/youtu.be\\/fSytzGwwBVw Confusion Matrices: https:\\/\\/youtu.be\\/Kdsp6soqA7o For a complete index of all the StatQuest videos, check out: https:\\/\\/statquest.org\\/video-index\\/ If you'd like to support StatQuest, please consider... Patreon: https:\\/\\/www.patreon.com\\/statquest ...or... YouTube Membership: https:\\/\\/www.youtube.com\\/channel\\/UCtYLUTtgS3k1Fg4y5tAhLbw\\/join ...a cool StatQuest t-shirt or sweatshirt (USA\\/Europe): https:\\/\\/teespring.com\\/stores\\/statquest (everywhere): https:\\/\\/www.redbubble.com\\/people\\/starmer\\/works\\/40421224-statquest-double-bam?asc=u&p=t-shirt ...buying one or two of my songs (or go large and get a whole album!) https:\\/\\/joshuastarmer.bandcamp.com\\/ ...or just donating to StatQuest! https:\\/\\/www.paypal.me\\/statquest Lastly, if you want to keep up with me as I research and create new StatQuests, follow me on twitter: https:\\/\\/twitter.com\\/joshuastarmer 0:00 Awesome song and introduction 2:56 Import Modules 4:34 Import Data 13:43 Missing Data Part 1: Identifying 18:37 Missing Data Part 2: Dealing with it 24:03 Format Data Part 1: X and y 25:55 Format Data Part 2: One-Hot Encoding 33:25 XGBoost - Missing Data and One-Hot Encoding 36:43 Build a Preliminary XGBoost Model 45:01 Optimize Parameters with Cross Validation (GridSearchCV) 49:44 Build and Draw Final XGBoost Model #StatQuest #ML #XGBoost\",\"648\":\"Some thoughts based on the Drake equation. Please check out our sponsors: - Brave: https:\\/\\/brave.com\\/lex - Neuro: https:\\/\\/www.getneuro.com and use code LEX to get 15% off OUTLINE: 0:00 - Introduction 2:22 - New stars 2:45 - Planets 3:23 - Habitable planets 4:52 - Life 6:54 - Intelligence 8:20 - Communication 9:47 - Civilization lifetime 12:35 - Civilization rebirth 13:02 - Estimated number of intelligent alien civilizations 14:44 - My view and takeaway CONNECT: - Subscribe to this YouTube channel - Twitter: https:\\/\\/twitter.com\\/lexfridman - LinkedIn: https:\\/\\/www.linkedin.com\\/in\\/lexfridman - Facebook: https:\\/\\/www.facebook.com\\/LexFridmanPage - Instagram: https:\\/\\/www.instagram.com\\/lexfridman - Medium: https:\\/\\/medium.com\\/@lexfridman - Support on Patreon: https:\\/\\/www.patreon.com\\/lexfridman\",\"1459\":\"Peter Singer is a professor of bioethics at Princeton, best known for his 1975 book Animal Liberation, that makes an ethical case against eating meat. He has written brilliantly from an ethical perspective on extreme poverty, euthanasia, human genetic selection, sports doping, the sale of kidneys, and happiness including in his books Ethics in the Real World and The Life You Can Save. He was a key popularizer of the effective altruism movement and is generally considered one of the most influential philosophers in the world. Support this podcast by supporting these sponsors: \\u2013 MasterClass: https:\\/\\/masterclass.com\\/lex \\u2013 Cash App \\u2013\",\"5668\":\"The human mind is a decent AI, and the brain is a nice dedicated hardware. Not without faults, but maybe we can a learn a few things from it. As a ML researcher \\/ practitioner, which books and articles on the topic would you recommend? Why? My recommendations: Jeff Hawkins, 2004. On Intelligence (A book. Why: an engineer's perspective on the high-level functioning of the brain, presented as a falsifiable scientific theory, with some open source code, used in commercial applications) Eliasmith et al, 2012. A large-scale model of the functioning brain (An article. Why: describes the world\\u2019s largest functional brain model, mostly handcrafted, with some open source code) [link] [comments]\",\"1500\":\"Ann Druyan is the writer, producer, director, and one of the most important and impactful communicators of science in our time. She co-wrote the 1980 science documentary series Cosmos hosted by Carl Sagan, whom she married in 1981, and her love for whom, with the help of NASA, was recorded as brain waves on a golden record along with other things our civilization has to offer and launched into space on the Voyager 1 and Voyager 2 spacecraft that are now, 42 years later, still active, reaching out farther into deep space than any human-made object ever has. This was\",\"1682\":\"Read the full story\",\"1669\":\"Read the full story\",\"1476\":\"Robert Langer is a professor at MIT and one of the most cited researchers in history, specializing in biotechnology fields of drug delivery systems and tissue engineering. He has bridged theory and practice by being a key member and driving force in launching many successful biotech companies out of MIT. Support this podcast by supporting these sponsors: \\u2013 MasterClass: https:\\/\\/masterclass.com\\/lex \\u2013 Cash App \\u2013 use code \\u201cLexPodcast\\u201d and download: \\u2013 Cash App (App Store): https:\\/\\/apple.co\\/2sPrUHe \\u2013 Cash App (Google Play): https:\\/\\/bit.ly\\/2MlvP5w This conversation is part of the Artificial Intelligence podcast. If you would like to get more information about this podcast\",\"4265\":\"Tweet Share Share How to Optimize a Function with One Variable? Univariate function optimization involves finding the input to a function that results in the optimal output from an objective function. This is a common procedure in machine learning when fitting a model with one parameter or tuning a model that has a single hyperparameter. An efficient algorithm is required to solve optimization problems of this type that will find the best solution with the minimum number of evaluations of the objective function, given that each evaluation of the objective function could be computationally expensive, such as fitting and evaluating a model on a dataset. This excludes expensive grid search and random search algorithms and in favor of efficient algorithms like Brent\\u2019s method. In this tutorial, you will discover how to perform univariate function optimization in Python. After completing this tutorial, you will know: Univariate function optimization involves finding an optimal input for an objective function that takes a single continuous argument. How to perform univariate function optimization for an unconstrained convex function. How to perform univariate function optimization for an unconstrained non-convex function. Let\\u2019s get started. Univariate Function Optimization in Python Photo by Robert Haandrikman, some rights reserved. Tutorial Overview This tutorial is divided into three parts; they are: Univariate Function Optimization Convex Univariate Function Optimization Non-Convex Univariate Function Optimization Univariate Function Optimization We may need to find an optimal value of a function that takes a single parameter. In machine learning, this may occur in many situations, such as: Finding the coefficient of a model to fit to a training dataset. Finding the value of a single hyperparameter that results in the best model performance. This is called univariate function optimization. We may be interested in the minimum outcome or maximum outcome of the function, although this can be simplified to minimization as a maximizing function can be made minimizing by adding a negative sign to all outcomes of the function. There may or may not be limits on the inputs to the function, so-called unconstrained or constrained optimization, and we assume that small changes in input correspond to small changes in the output of the function, e.g. that it is smooth. The function may or may not have a single optima, although we prefer that it does have a single optima and that shape of the function looks like a large basin. If this is the case, we know we can sample the function at one point and find the path down to the minima of the function. Technically, this is referred to as a convex function for minimization (concave for maximization), and functions that don\\u2019t have this basin shape are referred to as non-convex. Convex Target Function: There is a single optima and the shape of the target function leads to this optima. Nevertheless, the target function is sufficiently complex that we don\\u2019t know the derivative, meaning we cannot just use calculus to analytically compute the minimum or maximum of the function where the gradient is zero. This is referred to as a function that is non-differentiable. Although we might be able to sample the function with candidate values, we don\\u2019t know the input that will result in the best outcome. This may be because of the many reasons it is expensive to evaluate candidate solutions. Therefore, we require an algorithm that efficiently samples input values to the function. One approach to solving univariate function optimization problems is to use Brent\\u2019s method. Brent\\u2019s method is an optimization algorithm that combines a bisecting algorithm (Dekker\\u2019s method) and inverse quadratic interpolation. It can be used for constrained and unconstrained univariate function optimization. The Brent-Dekker method is an extension of the bisection method. It is a root-finding algorithm that combines elements of the secant method and inverse quadratic interpolation. It has reliable and fast convergence properties, and it is the univariate optimization algorithm of choice in many popular numerical optimization packages. \\u2014 Pages 49-51, Algorithms for Optimization, 2019. Bisecting algorithms use a bracket (lower and upper) of input values and split up the input domain, bisecting it in order to locate where in the domain the optima is located, much like a binary search. Dekker\\u2019s method is one way this is achieved efficiently for a continuous domain. Dekker\\u2019s method gets stuck on non-convex problems. Brent\\u2019s method modifies Dekker\\u2019s method to avoid getting stuck and also approximates the second derivative of the objective function (called the Secant Method) in an effort to accelerate the search. As such, Brent\\u2019s method for univariate function optimization is generally preferred over most other univariate function optimization algorithms given its efficiency. Brent\\u2019s method is available in Python via the minimize_scalar() SciPy function that takes the name of the function to be minimized. If your target function is constrained to a range, it can be specified via the \\u201cbounds\\u201d argument. It returns an OptimizeResult object that is a dictionary containing the solution. Importantly, the \\u2018x\\u2018 key summarizes the input for the optima, the \\u2018fun\\u2018 key summarizes the function output for the optima, and the \\u2018nfev\\u2018 summarizes the number of evaluations of the target function that were performed. ... # minimize the function result = minimize_scalar(objective, method='brent') Now that we know how to perform univariate function optimization in Python, let\\u2019s look at some examples. Convex Univariate Function Optimization In this section, we will explore how to solve a convex univariate function optimization problem. First, we can define a function that implements our function. In this case, we will use a simple offset version of the x^2 function e.g. a simple parabola (u-shape) function. It is a minimization objective function with an optima at -5.0. # objective function def objective(x): return (5.0 + x)**2.0 We can plot a coarse grid of this function with input values from -10 to 10 to get an idea of the shape of the target function. The complete example is listed below. # plot a convex target function from numpy import arange from matplotlib import pyplot # objective function def objective(x): return (5.0 + x)**2.0 # define range r_min, r_max = -10.0, 10.0 # prepare inputs inputs = arange(r_min, r_max, 0.1) # compute targets targets = [objective(x) for x in inputs] # plot inputs vs target pyplot.plot(inputs, targets, '--') pyplot.show() Running the example evaluates input values in our specified range using our target function and creates a plot of the function inputs to function outputs. We can see the U-shape of the function and that the objective is at -5.0. Line Plot of a Convex Objective Function Note: in a real optimization problem, we would not be able to perform so many evaluations of the objective function so easily. This simple function is used for demonstration purposes so we can learn how to use the optimization algorithm. Next, we can use the optimization algorithm to find the optima. ... # minimize the function result = minimize_scalar(objective, method='brent') Once optimized, we can summarize the result, including the input and evaluation of the optima and the number of function evaluations required to locate the optima. ... # summarize the result opt_x, opt_y = result['x'], result['fun'] print('Optimal Input x: %.6f' % opt_x) print('Optimal Output f(x): %.6f' % opt_y) print('Total Evaluations n: %d' % result['nfev']) Finally, we can plot the function again and mark the optima to confirm it was located in the place we expected for this function. ... # define the range r_min, r_max = -10.0, 10.0 # prepare inputs inputs = arange(r_min, r_max, 0.1) # compute targets targets = [objective(x) for x in inputs] # plot inputs vs target pyplot.plot(inputs, targets, '--') # plot the optima pyplot.plot([opt_x], [opt_y], 's', color='r') # show the plot pyplot.show() The complete example of optimizing an unconstrained convex univariate function is listed below. # optimize convex objective function from numpy import arange from scipy.optimize import minimize_scalar from matplotlib import pyplot # objective function def objective(x): return (5.0 + x)**2.0 # minimize the function result = minimize_scalar(objective, method='brent') # summarize the result opt_x, opt_y = result['x'], result['fun'] print('Optimal Input x: %.6f' % opt_x) print('Optimal Output f(x): %.6f' % opt_y) print('Total Evaluations n: %d' % result['nfev']) # define the range r_min, r_max = -10.0, 10.0 # prepare inputs inputs = arange(r_min, r_max, 0.1) # compute targets targets = [objective(x) for x in inputs] # plot inputs vs target pyplot.plot(inputs, targets, '--') # plot the optima pyplot.plot([opt_x], [opt_y], 's', color='r') # show the plot pyplot.show() Running the example first solves the optimization problem and reports the result. Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome. In this case, we can see that the optima was located after 10 evaluations of the objective function with an input of -5.0, achieving an objective function value of 0.0. Optimal Input x: -5.000000 Optimal Output f(x): 0.000000 Total Evaluations n: 10 A plot of the function is created again and this time, the optima is marked as a red square. Line Plot of a Convex Objective Function with Optima Marked Non-Convex Univariate Function Optimization A convex function is one that does not resemble a basin, meaning that it may have more than one hill or valley. This can make it more challenging to locate the global optima as the multiple hills and valleys can cause the search to get stuck and report a false or local optima instead. We can define a non-convex univariate function as follows. # objective function def objective(x): return (x - 2.0) * x * (x + 2.0)**2.0 We can sample this function and create a line plot of input values to objective values. The complete example is listed below. # plot a non-convex univariate function from numpy import arange from matplotlib import pyplot # objective function def objective(x): return (x - 2.0) * x * (x + 2.0)**2.0 # define range r_min, r_max = -3.0, 2.5 # prepare inputs inputs = arange(r_min, r_max, 0.1) # compute targets targets = [objective(x) for x in inputs] # plot inputs vs target pyplot.plot(inputs, targets, '--') pyplot.show() Running the example evaluates input values in our specified range using our target function and creates a plot of the function inputs to function outputs. We can see a function with one false optima around -2.0 and a global optima around 1.2. Line Plot of a Non-Convex Objective Function Note: in a real optimization problem, we would not be able to perform so many evaluations of the objective function so easily. This simple function is used for demonstration purposes so we can learn how to use the optimization algorithm. Next, we can use the optimization algorithm to find the optima. As before, we can call the minimize_scalar() function to optimize the function, then summarize the result and plot the optima on a line plot. The complete example of optimization of an unconstrained non-convex univariate function is listed below. # optimize non-convex objective function from numpy import arange from scipy.optimize import minimize_scalar from matplotlib import pyplot # objective function def objective(x): return (x - 2.0) * x * (x + 2.0)**2.0 # minimize the function result = minimize_scalar(objective, method='brent') # summarize the result opt_x, opt_y = result['x'], result['fun'] print('Optimal Input x: %.6f' % opt_x) print('Optimal Output f(x): %.6f' % opt_y) print('Total Evaluations n: %d' % result['nfev']) # define the range r_min, r_max = -3.0, 2.5 # prepare inputs inputs = arange(r_min, r_max, 0.1) # compute targets targets = [objective(x) for x in inputs] # plot inputs vs target pyplot.plot(inputs, targets, '--') # plot the optima pyplot.plot([opt_x], [opt_y], 's', color='r') # show the plot pyplot.show() Running the example first solves the optimization problem and reports the result. Want to Get Started With Ensemble Learning? Take my free 7-day email crash course now (with sample code). Click to sign-up and also get a free PDF Ebook version of the course. Download Your FREE Mini-Course In this case, we can see that the optima was located after 15 evaluations of the objective function with an input of about 1.28, achieving an objective function value of about -9.91. Optimal Input x: 1.280776 Optimal Output f(x): -9.914950 Total Evaluations n: 15 A plot of the function is created again, and this time, the optima is marked as a red square. We can see that the optimization was not deceived by the false optima and successfully located the global optima. Line Plot of a Non-Convex Objective Function with Optima Marked Further Reading This section provides more resources on the topic if you are looking to go deeper. Books Algorithms for Optimization, 2019. APIs Optimization (scipy.optimize). Optimization and root finding (scipy.optimize) scipy.optimize.minimize_scalar API. Articles Brent\\u2019s method, Wikipedia. Secant method, Wikipedia. Summary In this tutorial, you discovered how to perform univariate function optimization in Python. Specifically, you learned: Univariate function optimization involves finding an optimal input for an objective function that takes a single continuous argument. How to perform univariate function optimization for an unconstrained convex function. How to perform univariate function optimization for an unconstrained non-convex function. Do you have any questions? Ask your questions in the comments below and I will do my best to answer. Tweet Share Share The post Univariate Function Optimization in Python appeared first on Machine Learning Mastery.\",\"238\":\"Impressive and surprising openai.com\\/blog\\/dall-e\\/ I use those words sparingly\",\"1343\":\"#deepmind #biology #ai This is Biology's AlexNet moment! DeepMind solves a 50-year old problem in Protein Folding Prediction. AlphaFold 2 improves over DeepMind's 2018 AlphaFold system with a new architecture and massively outperforms all competition. In this Video, we take a look at how AlphaFold 1 works and what we can gather about AlphaFold 2 from the little information that's out there. OUTLINE: 0:00 - Intro & Overview 3:10 - Proteins & Protein Folding 14:20 - AlphaFold 1 Overview 18:20 - Optimizing a differentiable geometric model at inference 25:40 - Learning the Spatial Graph Distance Matrix 31:20 - Multiple Sequence Alignment of Evolutionarily Similar Sequences 39:40 - Distance Matrix Output Results 43:45 - Guessing AlphaFold 2 (it's Transformers) 53:30 - Conclusion & Comments AlphaFold 2 Blog: https:\\/\\/deepmind.com\\/blog\\/article\\/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology AlphaFold 1 Blog: https:\\/\\/deepmind.com\\/blog\\/article\\/AlphaFold-Using-AI-for-scientific-discovery AlphaFold 1 Paper: https:\\/\\/www.nature.com\\/articles\\/s41586-019-1923-7 MSA Reference: https:\\/\\/arxiv.org\\/abs\\/1211.1281 CASP14 Challenge: https:\\/\\/predictioncenter.org\\/casp14\\/index.cgi CASP14 Result Bar Chart: https:\\/\\/www.predictioncenter.org\\/casp14\\/zscores_final.cgi Paper Title: High Accuracy Protein Structure Prediction Using Deep Learning Abstract: Proteins are essential to life, supporting practically all its functions. They are large complex molecules, made up of chains of amino acids, and what a protein does largely depends on its unique 3D structure. Figuring out what shapes proteins fold into is known as the \\u201cprotein folding problem\\u201d, and has stood as a grand challenge in biology for the past 50 years. In a major scientific advance, the latest version of our AI system AlphaFold has been recognised as a solution to this grand challenge by the organisers of the biennial Critical Assessment of protein Structure Prediction (CASP). This breakthrough demonstrates the impact AI can have on scientific discovery and its potential to dramatically accelerate progress in some of the most fundamental fields that explain and shape our world. Authors: John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Kathryn Tunyasuvunakool, Olaf Ronneberger, Russ Bates, Augustin \\u017d\\u00eddek, Alex Bridgland, Clemens Meyer, Simon A A Kohl, Anna Potapenko, Andrew J Ballard, Andrew Cowie, Bernardino Romera-Paredes, Stanislav Nikolov, Rishub Jain, Jonas Adler, Trevor Back, Stig Petersen, David Reiman, Martin Steinegger, Michalina Pacholska, David Silver, Oriol Vinyals, Andrew W Senior, Koray Kavukcuoglu, Pushmeet Kohli, Demis Hassabis. Links: YouTube: https:\\/\\/www.youtube.com\\/c\\/yannickilcher Twitter: https:\\/\\/twitter.com\\/ykilcher Discord: https:\\/\\/discord.gg\\/4H8xxDF BitChute: https:\\/\\/www.bitchute.com\\/channel\\/yannic-kilcher Minds: https:\\/\\/www.minds.com\\/ykilcher Parler: https:\\/\\/parler.com\\/profile\\/YannicKilcher LinkedIn: https:\\/\\/www.linkedin.com\\/in\\/yannic-kilcher-488534136\\/ If you want to support me, the best thing to do is to share out the content :) If you want to support me financially (completely optional and voluntary, but a lot of people have asked for this): SubscribeStar: https:\\/\\/www.subscribestar.com\\/yannickilcher Patreon: https:\\/\\/www.patreon.com\\/yannickilcher Bitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq Ethereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2 Litecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m Monero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\",\"1235\":\"The ReLU activation function is one of the most popular activation functions for Deep Learning and Convolutional Neural Networks. However, the function itself is deceptively simple. This StatQuest walks you through an example, step-by-step, that uses the ReLU activation function so you can see exactly what it does and how it works. \\u2b50 NOTE: When I code, I use Kite, a free AI-powered coding assistant that will help you code faster and smarter. The Kite plugin integrates with all the top editors and IDEs to give you smart completions and documentation while you\\u2019re typing. I love it! https:\\/\\/www.kite.com\\/get-kite\\/?utm_medium=referral&utm_source=youtube&utm_campaign=statquest&utm_content=description-only For a complete index of all the StatQuest videos, check out: https:\\/\\/statquest.org\\/video-index\\/ If you'd like to support StatQuest, please consider... Patreon: https:\\/\\/www.patreon.com\\/statquest ...or... YouTube Membership: https:\\/\\/www.youtube.com\\/channel\\/UCtYLUTtgS3k1Fg4y5tAhLbw\\/join ...a cool StatQuest t-shirt or sweatshirt (USA\\/Europe): https:\\/\\/teespring.com\\/stores\\/statquest (everywhere): https:\\/\\/www.redbubble.com\\/people\\/starmer\\/works\\/40421224-statquest-double-bam?asc=u&p=t-shirt ...buying one or two of my songs (or go large and get a whole album!) https:\\/\\/joshuastarmer.bandcamp.com\\/ ...or just donating to StatQuest! https:\\/\\/www.paypal.me\\/statquest Lastly, if you want to keep up with me as I research and create new StatQuests, follow me on twitter: https:\\/\\/twitter.com\\/joshuastarmer 0:00 Awesome song and introduction 1:45 ReLU in the Hidden Layer 5:35 ReLU right before the Output 7:38 The derivative of ReLU #StatQuest #NeuralNetworks #ReLU\",\"5673\":\"[link] [comments]\",\"5689\":\"Penrose is making mistakes? Or are we pursuing an impossible dream? [link] [comments]\",\"1581\":\"Gary Marcus is a professor emeritus at NYU, founder of Robust.AI and Geometric Intelligence, the latter is a machine learning company acquired by Uber in 2016. He is the author of several books on natural and artificial intelligence, including his new book Rebooting AI: Building Machines We Can Trust. Gary has been a critical voice highlighting the limits of deep learning and discussing the challenges before the AI community that must be solved in order to achieve artificial general intelligence. This conversation is part of the Artificial Intelligence podcast. If you would like to get more information about this podcast go\",\"1354\":\"#ai #privacy #tech This paper demonstrates a method to extract verbatim pieces of the training data from a trained language model. Moreover, some of the extracted pieces only appear a handful of times in the dataset. This points to serious security and privacy implications for models like GPT-3. The authors discuss the risks and propose mitigation strategies. OUTLINE: 0:00 - Intro & Overview 9:15 - Personal Data Example 12:30 - Eidetic Memorization & Language Models 19:50 - Adversary's Objective & Outlier Data 24:45 - Ethical Hedging 26:55 - Two-Step Method Overview 28:20 - Perplexity Baseline 30:30 - Improvement via Perplexity Ratios 37:25 - Weights for Patterns & Weights for Memorization 43:40 - Analysis of Main Results 1:00:30 - Mitigation Strategies 1:01:40 - Conclusion & Comments Paper: https:\\/\\/arxiv.org\\/abs\\/2012.07805 Abstract: It has become common to publish large (billion parameter) language models that have been trained on private datasets. This paper demonstrates that in such settings, an adversary can perform a training data extraction attack to recover individual training examples by querying the language model. We demonstrate our attack on GPT-2, a language model trained on scrapes of the public Internet, and are able to extract hundreds of verbatim text sequences from the model's training data. These extracted examples include (public) personally identifiable information (names, phone numbers, and email addresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible even though each of the above sequences are included in just one document in the training data. We comprehensively evaluate our extraction attack to understand the factors that contribute to its success. For example, we find that larger models are more vulnerable than smaller models. We conclude by drawing lessons and discussing possible safeguards for training large language models. Authors: Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, Alina Oprea, Colin Raffel Links: YouTube: https:\\/\\/www.youtube.com\\/c\\/yannickilcher Twitter: https:\\/\\/twitter.com\\/ykilcher Discord: https:\\/\\/discord.gg\\/4H8xxDF BitChute: https:\\/\\/www.bitchute.com\\/channel\\/yannic-kilcher Minds: https:\\/\\/www.minds.com\\/ykilcher Parler: https:\\/\\/parler.com\\/profile\\/YannicKilcher LinkedIn: https:\\/\\/www.linkedin.com\\/in\\/yannic-kilcher-488534136\\/ If you want to support me, the best thing to do is to share out the content :) If you want to support me financially (completely optional and voluntary, but a lot of people have asked for this): SubscribeStar: https:\\/\\/www.subscribestar.com\\/yannickilcher Patreon: https:\\/\\/www.patreon.com\\/yannickilcher Bitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq Ethereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2 Litecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m Monero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\",\"1056\":\"Sign up for Backblaze and get unlimited storage for Mac or PC for $6\\/month (plus a 15 day free trial): https:\\/\\/thld.co\\/backblaze_zachstar \\u25baItems from this video (support the channel) Double Pendulum: https:\\/\\/stemerch.com\\/collections\\/science-toys\\/products\\/chaos-double-pendulum Floating Globe: https:\\/\\/stemerch.com\\/collections\\/science-toys\\/products\\/led-magnetic-levitation-globe Don't be a jerk shirt: https:\\/\\/stemerch.com\\/collections\\/dont-be-a-jerk Sierpinski Triangle Poster: https:\\/\\/stemerch.com\\/collections\\/sierpinski-triangle-1 \\\"What is mathematics?\\\" (affiliate link): https:\\/\\/amzn.to\\/2KYdcXg More book recommendations: https:\\/\\/stemerch.com\\/collections\\/books Support the Channel: https:\\/\\/www.patreon.com\\/zachstar PayPal(one time donation): https:\\/\\/www.paypal.me\\/ZachStarYT Join this channel to get access to perks: https:\\/\\/www.youtube.com\\/channel\\/UCpCSAcbqs-sjEVfk_hMfY9w\\/join \\u25baFollow me Instagram: https:\\/\\/www.instagram.com\\/zachstar\\/ Twitter: https:\\/\\/twitter.com\\/ImZachStar 2D Graphing Software: https:\\/\\/www.desmos.com\\/calculator Animations: Brainup Studios ( http:\\/\\/brainup.in\\/ ) Check out my Spanish channel here: https:\\/\\/www.youtube.com\\/channel\\/UCnkNu2xQBLASpj6cKC8vtpA \\u25baMy Setup: Space Pictures: https:\\/\\/amzn.to\\/2CC4Kqj Magnetic Floating Globe: https:\\/\\/amzn.to\\/2VgPdn0 Camera: https:\\/\\/amzn.to\\/2RivYu5 Mic: https:\\/\\/amzn.to\\/35bKiri Tripod: https:\\/\\/amzn.to\\/2RgMTNL Equilibrium Tube: https:\\/\\/amzn.to\\/2SowDrh \\u25baCheck out my Amazon Store: https:\\/\\/www.amazon.com\\/shop\\/zachstar\",\"5838\":\"Any good resources to recommend to learn about GPU programming for computer vision tasks? [link] [comments]\",\"4292\":\"This week Dr. Tim Scarfe, Yannic Lightspeed Kicher, Sayak Paul and Ayush Takur interview Mathilde Caron from Facebook Research (FAIR). We discuss Mathilde's paper which she wrote with her collaborators \\\"SWaV: Unsupervised Learning of Visual Features by Contrasting Cluster Assignments\\\" @ https:\\/\\/arxiv.org\\/pdf\\/2006.09882.pdf This paper is the latest unsupervised contrastive visual representations algorithm and has a new data augmentation strategy and also a new online clustering strategy. Note; Other authors; Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, Armand Joulin Sayak Paul - @RisingSayak \\/ https:\\/\\/www.linkedin.com\\/in\\/sayak-paul\\/ Ayush Thakur - @ayushthakur0 \\/ https:\\/\\/www.linkedin.com\\/in\\/ayush-thakur-731914149\\/ The article they wrote; https:\\/\\/app.wandb.ai\\/authors\\/swav-tf\\/reports\\/Unsupervised-Visual-Representation-Learning-with-SwAV--VmlldzoyMjg3Mzg 00:00:00 Yannic probability challenge (CAN YOU SOLVE IT?) 00:01:29 Intro topic (Tim) 00:08:18 Yannic take 00:09:33 Intro show and guests 00:11:29 SWaV elevator pitch 00:17:31 Clustering approach in general 00:21:17 Sayak and Ayush's article on SWaV 00:23:49 Optional transport problem \\/ Sinkhorn-Knopp algorithm 00:31:43 Is clustering a natural approach for this? 00:44:19 Image augmentations 00:46:20 Priors vs experience (data) 00:48:32 Life at FAIR 00:52:33 Progress of image augmentation 00:56:10 When things do not go to plan with research 01:01:04 Question on architecture 01:01:43 SWaV Results 01:06:26 Reproducing Matilde's code 01:14:51 Do we need the whole dataset to set clustering loss 01:16:40 Self-supervised learning and transfer learning 01:23:25 Link to attention mechanism) 01:24:41 Sayak final thought why unsupervised better 01:25:56 Outro Abstract; \\\"Unsupervised image representations have significantly reduced the gap with supervised pretraining, notably with the recent achievements of contrastive learning methods. These contrastive methods typically work online and rely on a large number of explicit pairwise feature comparisons, which is computationally challenging. In this paper, we propose an online algorithm, SwAV, that takes advantage of contrastive methods without requiring to compute pairwise comparisons. Specifically, our method simultaneously clusters the data while enforcing consistency between cluster assignments produced for different augmentations (or \\u201cviews\\u201d) of the same image, instead of comparing features directly as in contrastive learning. Simply put, we use a \\u201cswapped\\u201d prediction mechanism where we predict the cluster assignment of a view from the representation of another view. Our method can be trained with large and small batches and can scale to unlimited amounts of data. Compared to previous contrastive methods, our method is more memory efficient since it does not require a large memory bank or a special momentum network. In addition, we also propose a new data augmentation strategy, multi-crop, that uses a mix of views with different resolutions in place of two full-resolution views, without increasing the memory or compute requirements much. We validate our findings by achieving 75.3% top-1 accuracy on ImageNet with ResNet-50, as well as surpassing supervised pretraining on all the considered transfer tasks.\\\"\",\"1697\":\"Machine ethics and robot rights are quickly becoming hot topics in artificial intelligence\\/robotics communities. We will argue that the attempts to allow machines to make ethical decisions or to have rights are misguided. Instead we propose a new science of safety engineering for intelligent artificial agents. In particular we issue a challenge to the scientific community to develop intelligent systems capable of proving that they are in fact safe even under recursive self-improvement. Read the full story\",\"1524\":\"Rosalind Picard is a professor at MIT, director of the Affective Computing Research Group at the MIT Media Lab, and co-founder of two companies, Affectiva and Empatica. Over two decades ago she launched the field of affective computing with her book of the same name. This book described the importance of emotion in artificial and natural intelligence, the vital role emotion communication has to relationships between people in general and in human-robot interaction. If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman on Twitter, LinkedIn, Facebook, Medium, or YouTube where you\",\"3765\":\"Hi all, I recently stumbled over this amazing list of papers dealing with equivariance in neural networks (disclaimer: not my list - just found it). It is hosted on GitHub and everyone can add work that they think is missing (just make a PR). It already gives a pretty holistic overview, but I guess it is far from complete. As I have seen on Twitter that it raised quite some interest among very different people I thought sharing it here might be great to some people either doing research in this field or relying on these methods in their everyday work :) Happy reading to everyone interested! [link] [comments]\",\"1684\":\"Read the full story\",\"1474\":\"Rohit Prasad is the vice president and head scientist of Amazon Alexa and one of its original creators. This conversation is part of the Artificial Intelligence podcast. If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman on Twitter, LinkedIn, Facebook, Medium, or YouTube where you can watch the video versions of these conversations. If you enjoy the podcast, please rate it 5 stars on Apple Podcasts or support it on Patreon. This episode is presented by Cash App. Download it (App Store, Google Play), use code \\u201cLexPodcast\\u201d. The episode is also supported\",\"1137\":\"Featuring author Alex Bellos - check his books (including the Language Lover's Puzzle Book) on Amazon: https:\\/\\/amzn.to\\/3oU0wjT More links & stuff in full description below \\u2193\\u2193\\u2193 More Alex on Numberphile: http:\\/\\/bit.ly\\/Bellos_Playlist Alex's own website with links to all his stuff: http:\\/\\/www.alexbellos.com Numberphile is supported by the Mathematical Sciences Research Institute (MSRI): http:\\/\\/bit.ly\\/MSRINumberphile We are also supported by Science Sandbox, a Simons Foundation initiative dedicated to engaging everyone with the process of science. https:\\/\\/www.simonsfoundation.org\\/outreach\\/science-sandbox\\/ And support from Math For America - https:\\/\\/www.mathforamerica.org\\/ NUMBERPHILE Website: http:\\/\\/www.numberphile.com\\/ Numberphile on Facebook: http:\\/\\/www.facebook.com\\/numberphile Numberphile tweets: https:\\/\\/twitter.com\\/numberphile Subscribe: http:\\/\\/bit.ly\\/Numberphile_Sub Videos by Brady Haran Patreon: http:\\/\\/www.patreon.com\\/numberphile Numberphile T-Shirts and Merch: https:\\/\\/teespring.com\\/stores\\/numberphile Brady's videos subreddit: http:\\/\\/www.reddit.com\\/r\\/BradyHaran\\/ Brady's latest videos across all channels: http:\\/\\/www.bradyharanblog.com\\/ Sign up for (occasional) emails: http:\\/\\/eepurl.com\\/YdjL9 With thanks to Patreon supporters, including: Arjun Chakroborty, Ben Delo, Jeff Straathof, Ken Baron, Yana Chernobilsky, Andy B, James Bissonette, Jubal John, Jeremy Buchanan, Steve Crutchfield, Adam Savage, Ben White, Andrei M Burke, RAD Donato, Matthew Schuster, Nat Tyce, Ron Hochsprung, Mitch Harding, Ubiquity Ventures, Mateusz Swiatkowski, John Zelinka, Gnare , Tom Marshall, Jes\\u00fas Salsero, Jordan W Oja, Tracy Parry, Ian George Walker, Arnas , Bernd Sing, Valentin , Alfred Wallace, Charles Southerland, Kristian Joensen, Bodhisattva Debnath, Alex Khein, Kermit Norlund, That Asymptote, Mirik Gogri\",\"1695\":\"Read the full story\",\"4271\":\"Tweet Share Share Semi-supervised learning refers to algorithms that attempt to make use of both labeled and unlabeled training data. Semi-supervised learning algorithms are unlike supervised learning algorithms that are only able to learn from labeled training data. A popular approach to semi-supervised learning is to create a graph that connects examples in the training dataset and propagates known labels through the edges of the graph to label unlabeled examples. An example of this approach to semi-supervised learning is the label spreading algorithm for classification predictive modeling. In this tutorial, you will discover how to apply the label spreading algorithm to a semi-supervised learning classification dataset. After completing this tutorial, you will know: An intuition for how the label spreading semi-supervised learning algorithm works. How to develop a semi-supervised classification dataset and establish a baseline in performance with a supervised learning algorithm. How to develop and evaluate a label spreading algorithm and use the model output to train a supervised learning algorithm. Let\\u2019s get started. Semi-Supervised Learning With Label Spreading Photo by Jernej Furman, some rights reserved. Tutorial Overview This tutorial is divided into three parts; they are: Label Spreading Algorithm Semi-Supervised Classification Dataset Label Spreading for Semi-Supervised Learning Label Spreading Algorithm Label Spreading is a semi-supervised learning algorithm. The algorithm was introduced by Dengyong Zhou, et al. in their 2003 paper titled \\u201cLearning With Local And Global Consistency.\\u201d The intuition for the broader approach of semi-supervised learning is that nearby points in the input space should have the same label, and points in the same structure or manifold in the input space should have the same label. The key to semi-supervised learning problems is the prior assumption of consistency, which means: (1) nearby points are likely to have the same label; and (2) points on the same structure typically referred to as a cluster or a manifold) are likely to have the same label. \\u2014 Learning With Local And Global Consistency, 2003. The label spreading is inspired by a technique from experimental psychology called spreading activation networks. This algorithm can be understood intuitively in terms of spreading activation networks from experimental psychology. \\u2014 Learning With Local And Global Consistency, 2003. Points in the dataset are connected in a graph based on their relative distances in the input space. The weight matrix of the graph is normalized symmetrically, much like spectral clustering. Information is passed through the graph, which is adapted to capture the structure in the input space. The approach is very similar to the label propagation algorithm for semi-supervised learning. Another similar label propagation algorithm was given by Zhou et al.: at each step a node i receives a contribution from its neighbors j (weighted by the normalized weight of the edge (i,j)), and an additional small contribution given by its initial value \\u2014 Page 196, Semi-Supervised Learning, 2006. After convergence, labels are applied based on nodes that passed on the most information. Finally, the label of each unlabeled point is set to be the class of which it has received most information during the iteration process. \\u2014 Learning With Local And Global Consistency, 2003. Now that we are familiar with the label spreading algorithm, let\\u2019s look at how we might use it on a project. First, we must define a semi-supervised classification dataset. Semi-Supervised Classification Dataset In this section, we will define a dataset for semis-supervised learning and establish a baseline in performance on the dataset. First, we can define a synthetic classification dataset using the make_classification() function. We will define the dataset with two classes (binary classification) and two input variables and 1,000 examples. ... # define dataset X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, random_state=1) Next, we will split the dataset into train and test datasets with an equal 50-50 split (e.g. 500 rows in each). ... # split into train and test X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50, random_state=1, stratify=y) Finally, we will split the training dataset in half again into a portion that will have labels and a portion that we will pretend is unlabeled. ... # split train into labeled and unlabeled X_train_lab, X_test_unlab, y_train_lab, y_test_unlab = train_test_split(X_train, y_train, test_size=0.50, random_state=1, stratify=y_train) Tying this together, the complete example of preparing the semi-supervised learning dataset is listed below. # prepare semi-supervised learning dataset from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split # define dataset X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, random_state=1) # split into train and test X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50, random_state=1, stratify=y) # split train into labeled and unlabeled X_train_lab, X_test_unlab, y_train_lab, y_test_unlab = train_test_split(X_train, y_train, test_size=0.50, random_state=1, stratify=y_train) # summarize training set size print('Labeled Train Set:', X_train_lab.shape, y_train_lab.shape) print('Unlabeled Train Set:', X_test_unlab.shape, y_test_unlab.shape) # summarize test set size print('Test Set:', X_test.shape, y_test.shape) Running the example prepares the dataset and then summarizes the shape of each of the three portions. The results confirm that we have a test dataset of 500 rows, a labeled training dataset of 250 rows, and 250 rows of unlabeled data. Labeled Train Set: (250, 2) (250,) Unlabeled Train Set: (250, 2) (250,) Test Set: (500, 2) (500,) A supervised learning algorithm will only have 250 rows from which to train a model. A semi-supervised learning algorithm will have the 250 labeled rows as well as the 250 unlabeled rows that could be used in numerous ways to improve the labeled training dataset. Next, we can establish a baseline in performance on the semi-supervised learning dataset using a supervised learning algorithm fit only on the labeled training data. This is important because we would expect a semi-supervised learning algorithm to outperform a supervised learning algorithm fit on the labeled data alone. If this is not the case, then the semi-supervised learning algorithm does not have skill. In this case, we will use a logistic regression algorithm fit on the labeled portion of the training dataset. ... # define model model = LogisticRegression() # fit model on labeled dataset model.fit(X_train_lab, y_train_lab) The model can then be used to make predictions on the entire holdout test dataset and evaluated using classification accuracy. ... # make predictions on hold out test set yhat = model.predict(X_test) # calculate score for test set score = accuracy_score(y_test, yhat) # summarize score print('Accuracy: %.3f' % (score*100)) Tying this together, the complete example of evaluating a supervised learning algorithm on the semi-supervised learning dataset is listed below. # baseline performance on the semi-supervised learning dataset from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from sklearn.linear_model import LogisticRegression # define dataset X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, random_state=1) # split into train and test X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50, random_state=1, stratify=y) # split train into labeled and unlabeled X_train_lab, X_test_unlab, y_train_lab, y_test_unlab = train_test_split(X_train, y_train, test_size=0.50, random_state=1, stratify=y_train) # define model model = LogisticRegression() # fit model on labeled dataset model.fit(X_train_lab, y_train_lab) # make predictions on hold out test set yhat = model.predict(X_test) # calculate score for test set score = accuracy_score(y_test, yhat) # summarize score print('Accuracy: %.3f' % (score*100)) Running the algorithm fits the model on the labeled training dataset and evaluates it on the holdout dataset and prints the classification accuracy. Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome. In this case, we can see that the algorithm achieved a classification accuracy of about 84.8 percent. We would expect an effective semi-supervised learning algorithm to achieve a better accuracy than this. Accuracy: 84.800 Next, let\\u2019s explore how to apply the label spreading algorithm to the dataset. Label Spreading for Semi-Supervised Learning The label spreading algorithm is available in the scikit-learn Python machine learning library via the LabelSpreading class. The model can be fit just like any other classification model by calling the fit() function and used to make predictions for new data via the predict() function. ... # define model model = LabelSpreading() # fit model on training dataset model.fit(..., ...) # make predictions on hold out test set yhat = model.predict(...) Importantly, the training dataset provided to the fit() function must include labeled examples that are ordinal encoded (as per normal) and unlabeled examples marked with a label of -1. The model will then determine a label for the unlabeled examples as part of fitting the model. After the model is fit, the estimated labels for the labeled and unlabeled data in the training dataset is available via the \\u201ctransduction_\\u201d attribute on the LabelSpreading class. ... # get labels for entire training dataset data tran_labels = model.transduction_ Now that we are familiar with how to use the label spreading algorithm in scikit-learn, let\\u2019s look at how we might apply it to our semi-supervised learning dataset. First, we must prepare the training dataset. We can concatenate the input data of the training dataset into a single array. ... # create the training dataset input X_train_mixed = concatenate((X_train_lab, X_test_unlab)) We can then create a list of -1 valued (unlabeled) for each row in the unlabeled portion of the training dataset. ... # create \\\"no label\\\" for unlabeled data nolabel = [-1 for _ in range(len(y_test_unlab))] This list can then be concatenated with the labels from the labeled portion of the training dataset to correspond with the input array for the training dataset. ... # recombine training dataset labels y_train_mixed = concatenate((y_train_lab, nolabel)) We can now train the LabelSpreading model on the entire training dataset. ... # define model model = LabelSpreading() # fit model on training dataset model.fit(X_train_mixed, y_train_mixed) Next, we can use the model to make predictions on the holdout dataset and evaluate the model using classification accuracy. ... # make predictions on hold out test set yhat = model.predict(X_test) # calculate score for test set score = accuracy_score(y_test, yhat) # summarize score print('Accuracy: %.3f' % (score*100)) Tying this together, the complete example of evaluating label spreading on the semi-supervised learning dataset is listed below. # evaluate label spreading on the semi-supervised learning dataset from numpy import concatenate from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from sklearn.semi_supervised import LabelSpreading # define dataset X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, random_state=1) # split into train and test X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50, random_state=1, stratify=y) # split train into labeled and unlabeled X_train_lab, X_test_unlab, y_train_lab, y_test_unlab = train_test_split(X_train, y_train, test_size=0.50, random_state=1, stratify=y_train) # create the training dataset input X_train_mixed = concatenate((X_train_lab, X_test_unlab)) # create \\\"no label\\\" for unlabeled data nolabel = [-1 for _ in range(len(y_test_unlab))] # recombine training dataset labels y_train_mixed = concatenate((y_train_lab, nolabel)) # define model model = LabelSpreading() # fit model on training dataset model.fit(X_train_mixed, y_train_mixed) # make predictions on hold out test set yhat = model.predict(X_test) # calculate score for test set score = accuracy_score(y_test, yhat) # summarize score print('Accuracy: %.3f' % (score*100)) Running the algorithm fits the model on the entire training dataset and evaluates it on the holdout dataset and prints the classification accuracy. Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome. In this case, we can see that the label spreading model achieves a classification accuracy of about 85.4 percent, which is slightly higher than a logistic regression fit only on the labeled training dataset that achieved an accuracy of about 84.8 percent. Accuracy: 85.400 So far so good. Another approach we can use with the semi-supervised model is to take the estimated labels for the training dataset and fit a supervised learning model. Recall that we can retrieve the labels for the entire training dataset from the label spreading model as follows: ... # get labels for entire training dataset data tran_labels = model.transduction_ We can then use these labels, along with all of the input data, to train and evaluate a supervised learning algorithm, such as a logistic regression model. The hope is that the supervised learning model fit on the entire training dataset would achieve even better performance than the semi-supervised learning model alone. ... # define supervised learning model model2 = LogisticRegression() # fit supervised learning model on entire training dataset model2.fit(X_train_mixed, tran_labels) # make predictions on hold out test set yhat = model2.predict(X_test) # calculate score for test set score = accuracy_score(y_test, yhat) # summarize score print('Accuracy: %.3f' % (score*100)) Tying this together, the complete example of using the estimated training set labels to train and evaluate a supervised learning model is listed below. # evaluate logistic regression fit on label spreading for semi-supervised learning from numpy import concatenate from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from sklearn.semi_supervised import LabelSpreading from sklearn.linear_model import LogisticRegression # define dataset X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, random_state=1) # split into train and test X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50, random_state=1, stratify=y) # split train into labeled and unlabeled X_train_lab, X_test_unlab, y_train_lab, y_test_unlab = train_test_split(X_train, y_train, test_size=0.50, random_state=1, stratify=y_train) # create the training dataset input X_train_mixed = concatenate((X_train_lab, X_test_unlab)) # create \\\"no label\\\" for unlabeled data nolabel = [-1 for _ in range(len(y_test_unlab))] # recombine training dataset labels y_train_mixed = concatenate((y_train_lab, nolabel)) # define model model = LabelSpreading() # fit model on training dataset model.fit(X_train_mixed, y_train_mixed) # get labels for entire training dataset data tran_labels = model.transduction_ # define supervised learning model model2 = LogisticRegression() # fit supervised learning model on entire training dataset model2.fit(X_train_mixed, tran_labels) # make predictions on hold out test set yhat = model2.predict(X_test) # calculate score for test set score = accuracy_score(y_test, yhat) # summarize score print('Accuracy: %.3f' % (score*100)) Running the algorithm fits the semi-supervised model on the entire training dataset, then fits a supervised learning model on the entire training dataset with inferred labels and evaluates it on the holdout dataset, printing the classification accuracy. Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome. In this case, we can see that this hierarchical approach of semi-supervised model followed by supervised model achieves a classification accuracy of about 85.8 percent on the holdout dataset, slightly better than the semi-supervised learning algorithm used alone that achieved an accuracy of about 85.6 percent. Accuracy: 85.800 Can you achieve better results by tuning the hyperparameters of the LabelSpreading model? Let me know what you discover in the comments below. Further Reading This section provides more resources on the topic if you are looking to go deeper. Books Introduction to Semi-Supervised Learning, 2009. Chapter 11: Label Propagation and Quadratic Criterion, Semi-Supervised Learning, 2006. Papers Learning With Local And Global Consistency, 2003. APIs sklearn.semi_supervised.LabelSpreading API. Section 1.14. Semi-Supervised, Scikit-Learn User Guide. sklearn.model_selection.train_test_split API. sklearn.linear_model.LogisticRegression API. sklearn.datasets.make_classification API. Articles Semi-supervised learning, Wikipedia. Summary In this tutorial, you discovered how to apply the label spreading algorithm to a semi-supervised learning classification dataset. Specifically, you learned: An intuition for how the label spreading semi-supervised learning algorithm works. How to develop a semi-supervised classification dataset and establish a baseline in performance with a supervised learning algorithm. How to develop and evaluate a label spreading algorithm and use the model output to train a supervised learning algorithm. Do you have any questions? Ask your questions in the comments below and I will do my best to answer. Tweet Share Share The post Semi-Supervised Learning With Label Spreading appeared first on Machine Learning Mastery.\",\"4675\":\"Let's say we have MRI and CT data in DICOM format and we want to segment the tumour in it. My current approach is to train a model to recognise high Intensity spots compared to a labelled training data set. From my experience using this method it's possible to segment the edema (puss filled region surrounding the Tumour) region somewhat but getting to the tumour is not working properly. I was wondering if there is any approach I'm missing out or should try Most resources I found online are either proprietary or have some sort of confidential NDA on em. So what do y'all think? [link] [comments]\",\"1140\":\"Alex Bellos discusses how the Incans used knots in string (Quipu) to record numbers. Check out Brilliant (get 20% off their premium service): https:\\/\\/brilliant.org\\/numberphile (sponsor) More links & stuff in full description below \\u2193\\u2193\\u2193 Check out the Language Lover's Puzzle Book) on Amazon: https:\\/\\/amzn.to\\/3oU0wjT Or a signed version from Maths Gear: https:\\/\\/bit.ly\\/Language_Lovers More Alex on Numberphile: http:\\/\\/bit.ly\\/Bellos_Playlist Alex's own website with links to all his stuff: http:\\/\\/www.alexbellos.com See Alex on another forgotten number system: https:\\/\\/youtu.be\\/9p55Qgt7Ciw The Ancient Quipu - L Leland Locke: https:\\/\\/amzn.to\\/37RJfzZ Numberphile is supported by the Mathematical Sciences Research Institute (MSRI): http:\\/\\/bit.ly\\/MSRINumberphile We are also supported by Science Sandbox, a Simons Foundation initiative dedicated to engaging everyone with the process of science. https:\\/\\/www.simonsfoundation.org\\/outreach\\/science-sandbox\\/ And support from Math For America - https:\\/\\/www.mathforamerica.org\\/ NUMBERPHILE Website: http:\\/\\/www.numberphile.com\\/ Numberphile on Facebook: http:\\/\\/www.facebook.com\\/numberphile Numberphile tweets: https:\\/\\/twitter.com\\/numberphile Subscribe: http:\\/\\/bit.ly\\/Numberphile_Sub Video by Brady Haran and Pete McPartlan Patreon: http:\\/\\/www.patreon.com\\/numberphile Numberphile T-Shirts and Merch: https:\\/\\/teespring.com\\/stores\\/numberphile Brady's videos subreddit: http:\\/\\/www.reddit.com\\/r\\/BradyHaran\\/ Brady's latest videos across all channels: http:\\/\\/www.bradyharanblog.com\\/ Thanks to these Patreon supporters: Arjun Chakroborty Ben Delo Jeff Straathof Ken Baron Yana Chernobilsky Andy B James Bissonette Jubal John Jeremy Buchanan Steve Crutchfield Adam Savage Ben White Andrei M Burke RAD Donato Matthew Schuster Nat Tyce Ron Hochsprung Mitch Harding Ubiquity Ventures Mateusz Swiatkowski John Zelinka Tom Marshall Jes\\u00fas Salsero Gnare Jordan W Oja Tracy Parry Ian George Walker Arnas Bernd Sing Valentin Alfred Wallace Charles Southerland Kristian Joensen Bodhisattva Debnath Alex Khein Kermit Norlund That Asymptote Mirik Gogri Sign up for (occasional) emails: http:\\/\\/eepurl.com\\/YdjL9\",\"1347\":\"#ai #technology #poker Daniel Negreanu posted a set of very interesting No-Limit Hold'em situations on Twitter. I try to analyze them from the perspective of a poker bot. See how such bots think about the game and approximate Nash equilibria. https:\\/\\/twitter.com\\/RealKidPoker\\/status\\/1337887509397741568 https:\\/\\/twitter.com\\/RealKidPoker\\/status\\/1337899147337244673 https:\\/\\/twitter.com\\/RealKidPoker\\/status\\/1337904860721606656 Links: YouTube: https:\\/\\/www.youtube.com\\/c\\/yannickilcher Twitter: https:\\/\\/twitter.com\\/ykilcher Discord: https:\\/\\/discord.gg\\/4H8xxDF BitChute: https:\\/\\/www.bitchute.com\\/channel\\/yannic-kilcher BiliBili: https:\\/\\/space.bilibili.com\\/1824646584 Minds: https:\\/\\/www.minds.com\\/ykilcher Parler: https:\\/\\/parler.com\\/profile\\/YannicKilcher LinkedIn: https:\\/\\/www.linkedin.com\\/in\\/yannic-kilcher-488534136\\/ If you want to support me, the best thing to do is to share out the content :) If you want to support me financially (completely optional and voluntary, but a lot of people have asked for this): SubscribeStar: https:\\/\\/www.subscribestar.com\\/yannickilcher Patreon: https:\\/\\/www.patreon.com\\/yannickilcher Bitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq Ethereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2 Litecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m Monero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\",\"4266\":\"Tweet Share Share Regression refers to predictive modeling problems that involve predicting a numeric value. It is different from classification that involves predicting a class label. Unlike classification, you cannot use classification accuracy to evaluate the predictions made by a regression model. Instead, you must use error metrics specifically designed for evaluating predictions made on regression problems. In this tutorial, you will discover how to calculate error metrics for regression predictive modeling projects. After completing this tutorial, you will know: Regression predictive modeling are those problems that involve predicting a numeric value. Metrics for regression involve calculating an error score to summarize the predictive skill of a model. How to calculate and report mean squared error, root mean squared error, and mean absolute error. Let\\u2019s get started. Regression Metrics for Machine Learning Photo by Gael Varoquaux, some rights reserved. Tutorial Overview This tutorial is divided into three parts; they are: Regression Predictive Modeling Evaluating Regression Models Metrics for Regression Mean Squared Error Root Mean Squared Error Mean Absolute Error Regression Predictive Modeling Predictive modeling is the problem of developing a model using historical data to make a prediction on new data where we do not have the answer. Predictive modeling can be described as the mathematical problem of approximating a mapping function (f) from input variables (X) to output variables (y). This is called the problem of function approximation. The job of the modeling algorithm is to find the best mapping function we can given the time and resources available. For more on approximating functions in applied machine learning, see the post: How Machine Learning Algorithms Work Regression predictive modeling is the task of approximating a mapping function (f) from input variables (X) to a continuous output variable (y). Regression is different from classification, which involves predicting a category or class label. For more on the difference between classification and regression, see the tutorial: Difference Between Classification and Regression in Machine Learning A continuous output variable is a real-value, such as an integer or floating point value. These are often quantities, such as amounts and sizes. For example, a house may be predicted to sell for a specific dollar value, perhaps in the range of $100,000 to $200,000. A regression problem requires the prediction of a quantity. A regression can have real-valued or discrete input variables. A problem with multiple input variables is often called a multivariate regression problem. A regression problem where input variables are ordered by time is called a time series forecasting problem. Now that we are familiar with regression predictive modeling, let\\u2019s look at how we might evaluate a regression model. Evaluating Regression Models A common question by beginners to regression predictive modeling projects is: How do I calculate accuracy for my regression model? Accuracy (e.g. classification accuracy) is a measure for classification, not regression. We cannot calculate accuracy for a regression model. The skill or performance of a regression model must be reported as an error in those predictions. This makes sense if you think about it. If you are predicting a numeric value like a height or a dollar amount, you don\\u2019t want to know if the model predicted the value exactly (this might be intractably difficult in practice); instead, we want to know how close the predictions were to the expected values. Error addresses exactly this and summarizes on average how close predictions were to their expected values. There are three error metrics that are commonly used for evaluating and reporting the performance of a regression model; they are: Mean Squared Error (MSE). Root Mean Squared Error (RMSE). Mean Absolute Error (MAE) There are many other metrics for regression, although these are the most commonly used. You can see the full list of regression metrics supported by the scikit-learn Python machine learning library here: Scikit-Learn API: Regression Metrics. In the next section, let\\u2019s take a closer look at each in turn. Metrics for Regression In this section, we will take a closer look at the popular metrics for regression models and how to calculate them for your predictive modeling project. Mean Squared Error Mean Squared Error, or MSE for short, is a popular error metric for regression problems. It is also an important loss function for algorithms fit or optimized using the least squares framing of a regression problem. Here \\u201cleast squares\\u201d refers to minimizing the mean squared error between predictions and expected values. The MSE is calculated as the mean or average of the squared differences between predicted and expected target values in a dataset. MSE = 1 \\/ N * sum for i to N (y_i \\u2013 yhat_i)^2 Where y_i is the i\\u2019th expected value in the dataset and yhat_i is the i\\u2019th predicted value. The difference between these two values is squared, which has the effect of removing the sign, resulting in a positive error value. The squaring also has the effect of inflating or magnifying large errors. That is, the larger the difference between the predicted and expected values, the larger the resulting squared positive error. This has the effect of \\u201cpunishing\\u201d models more for larger errors when MSE is used as a loss function. It also has the effect of \\u201cpunishing\\u201d models by inflating the average error score when used as a metric. We can create a plot to get a feeling for how the change in prediction error impacts the squared error. The example below gives a small contrived dataset of all 1.0 values and predictions that range from perfect (1.0) to wrong (0.0) by 0.1 increments. The squared error between each prediction and expected value is calculated and plotted to show the quadratic increase in squared error. ... # calculate error err = (expected[i] - predicted[i])**2 The complete example is listed below. # example of increase in mean squared error from matplotlib import pyplot from sklearn.metrics import mean_squared_error # real value expected = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0] # predicted value predicted = [1.0, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0.0] # calculate errors errors = list() for i in range(len(expected)): # calculate error err = (expected[i] - predicted[i])**2 # store error errors.append(err) # report error print('\\/prepre class=\\\"urvanov-syntax-highlighter-plain-tag\\\"\",\"5688\":\"So I'm a computer scientist and a Machine Learning enthusiast, and I've faced quite a few problems during my career that I could've never learned from textbooks or courses. Since I've only ever worked on an actual ML project once, I was wondering: What are some technical lessons, tricks\\/tips that you cannot learn from the nowadays oh so popular online courses or even books? And I mean technical, because I myself have faced the usual management issues of \\\"we have great data\\\" (you don't), \\\"we need deep learning\\\" (you don't), \\\"give me deep learning but also explain why it predicts the way it does\\\" (that's...not how it works). But like actual technical stuff, like for example it never occurred to me that in some cases you have to take into account the hardware limitations (duh, I know) that the users are working on - that your model has to fit into the memory of a consumer laptop (not everything can run from the cloud). So yeah, I'd be interested in some lessons learned from you who've been doing this job longer! [link] [comments]\",\"1055\":\"Get free access to over 2500 documentaries on CuriosityStream: https:\\/\\/curiositystream.thld.co\\/zachstaroct5 (use code \\\"zachstar\\\" at sign up) Get the \\\"Don't be a Jerk\\\" shirt here!: https:\\/\\/stemerch.com\\/collections\\/dont-be-a-jerk Support the Channel: https:\\/\\/www.patreon.com\\/zachstar PayPal(one time donation): https:\\/\\/www.paypal.me\\/ZachStarYT Join this channel to get access to perks: https:\\/\\/www.youtube.com\\/channel\\/UCpCSAcbqs-sjEVfk_hMfY9w\\/join \\u25baFollow me Instagram: https:\\/\\/www.instagram.com\\/zachstar\\/ Twitter: https:\\/\\/twitter.com\\/ImZachStar \\u25baOriginal video I'm responding to: https:\\/\\/youtu.be\\/lmcT2mP2bfE \\u25baJames Grime 3 Utilities Problem Explanation: https:\\/\\/www.youtube.com\\/watch?v=ODtwehGzoLM \\u25ba3b1b 3 Utilities on Coffee Mug: https:\\/\\/www.youtube.com\\/watch?v=VvCytJvd4H0 \\u25baWind on Torus Vs Sphere Video: https:\\/\\/youtu.be\\/z-GlM7eTFq8 Animations: Brainup Studios ( http:\\/\\/brainup.in\\/ ) Check out my Spanish channel here: https:\\/\\/www.youtube.com\\/channel\\/UCnkNu2xQBLASpj6cKC8vtpA \\u25baMy Setup: Space Pictures: https:\\/\\/amzn.to\\/2CC4Kqj Magnetic Floating Globe: https:\\/\\/amzn.to\\/2VgPdn0 Camera: https:\\/\\/amzn.to\\/2RivYu5 Mic: https:\\/\\/amzn.to\\/35bKiri Tripod: https:\\/\\/amzn.to\\/2RgMTNL Equilibrium Tube: https:\\/\\/amzn.to\\/2SowDrh \\u25baCheck out my Amazon Store: https:\\/\\/www.amazon.com\\/shop\\/zachstar\",\"1530\":\"David Fravor is a navy pilot of 18 years and a primary witness in one of the most credible UFO sightings in history, video of which has been released by the Pentagon and reported on by the NY Times. Please check out our sponsors to get a discount and to support this podcast: \\u2013 Athletic Greens: https:\\/\\/athleticgreens.com\\/lex \\u2013 ExpressVPN: https:\\/\\/www.expressvpn.com\\/lexpod \\u2013 BetterHelp: https:\\/\\/betterhelp.com\\/lex If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/podcast or connect with @lexfridman on Twitter, LinkedIn, Facebook, Medium, or YouTube where you can watch the video versions of these conversations. If you enjoy the\",\"3137\":\"In conversation with Philipp Singer: A Data Scientist, Kaggle Double Grandmaster, and a Ph.D. in Computer Science. In this series of interviews, I present the stories of established Data Scientists and Kaggle Grandmasters at H2O.ai, who share their journey, inspirations, and accomplishments. These interviews are intended to motivate and encourage others who want to understand what it takes to be a Kaggle Grandmaster. In this interview, I shall be sharing my interaction with Philipp Singer, better known as Psi in Kaggle world. He is a Kaggle Double Grandmaster and a Senior Data Scientist at H2O.ai. Philipp obtained his Ph.D. in computer science with honors at the Technical University of Graz, where he also finished his Master\\u2019s studies in Software Development and Business Management. Philipp has several accomplishments, including multiple winning and top placements on Kaggle and several scientific honors, such as the best paper award at the renowned World Wide Web Conference. He is currently ranked 3rd globally in the Kaggle competitions tier, which is both pretty impressive and inspiring at the same time. One of Philipp\\u2019s most notable achievements has been winning the NFL\\u2019s second annual Big Data Bowl competition by teaming together with a fellow H2O.ai Data Scientist \\u2013Dmitry Gordeev. More than 2,000 data scientists from all over the world competed on Kaggle to predict rushing play outcomes. Philipp Singer and Dmitry Gordeev captured the top prize of $50,000 with their approach. 2019\\u201320 Big Data Bowl winners Philipp Singer and Dmitry Gordeev speaking in Indianapolis. In this interview, we shall know more about his academic background, his passion for Kaggle, and his work as a Data Scientist. Here is an excerpt from my conversation with Philipp: You have a Ph.D. in Computer Science. Why did you opt for Data Science as a career rather than sticking to academia\\u2019s research side? Philipp: I obtained a Ph.D. in Computer Science at the Technical University of Graz in Austria and worked as a postdoctoral researcher in Germany. I touched on many different data science topics during my scientific career and published many papers and articles in renowned conferences and journals. As the next step in that career, I would have had to pursue a professorship, which sounded intriguing. However, even though I love teaching, I also wanted to delve into more applied work, meaning that I wanted my work to have more impact than what is mostly possible in research. This prompted me to take up data science as a career. That said, I thoroughly enjoyed my Ph.D. and learned a lot during that time, but now I am also delighted to be at the forefront of data science and machine learning and have a real maker role at H2O.ai. How did your tryst with Kaggle begin, and what kept you motivated throughout your grandmaster\\u2019s journey? Philipp\\u2019s Kaggle profile Philipp: I signed up on Kaggle around eight years ago, close to my first steps as a Ph.D. because I heard about the platform and wanted to check it out. But I did not do more than a sample submission and then stopped touching Kaggle for six years. Around two years ago, Dmitry (dott1718 on Kaggle and back then and now a work colleague) and I decided to try out a competition together on Kaggle as a side project at work. We went with zero expectations into it but ended up winning the competition, which got me hooked and began my Kaggle journey. My approach on Kaggle has always been to tackle new types of problems to stay motivated, and there are still new interesting problems to solve on a regular basis. I also enjoy meeting and working with talented people on Kaggle and seeing how the community strives. Lately, you have been killing the Kaggle leaderboard with some spectacular results, the latest being NFL 1st and Future \\u2014 Impact Detection, where you finished 2nd. What is your approach towards solving such problems and faring well? Philipp: People often ask me how they can win Kaggle competitions, and I do not think there is a general secret sauce that can be applied. A lot of success on Kaggle is based on experience and the willingness to touch and learn about things that, at first glance, you do not know much about. Over time, I have assembled a particular generic toolbox that incorporates building blocks from each competition that I have tackled. For example, I understand how to set up proper cross-validation, what libraries to use for my models, how to fit models properly, track their performance, and similar things. So I already have more time to focus on new and crucial aspects of recent competitions. I always try to improve my workflow after each competition to become more efficient and competitive. \\u201cA lot of success on Kaggle is based on experience and the willingness to touch and learn about things that at first glance, you do not know much about.\\u201d How do you decide which competitions to participate in? Philipp\\u2019s top achievements on Kaggle Philipp: I mostly try to tackle new types of problems or competitions that sound interesting concerning the data or the problem to solve. Sometimes I also try my luck with more standard competitions to stay informed about the art\\u2019s weekly changing state. How do you typically approach a Kaggle problem? Any favorite ML resources(MOOCS, Blogs, etc.) that you would like to share with the community? Philipp: I try to resort to my repertoire of methods, tools, and experience I have already accumulated and then try to research the specific problem at hand. This means I will study previous solutions to similar problems on Kaggle and read relevant papers. The best way to learn about a problem is to go hands-on and learn along the way. As a Data Scientist at H2O.ai, what are your roles, and in which specific areas do you work? Philipp along with fellow kaggle Grandmasters at H2O.ai Philipp: At H2O.ai, my role is very multi-faceted. I am regularly involved in customer-facing projects where my goal is to support projects with my data science expertise. Furthermore, as Kaggle Grandmasters, we always try to utilize our experience and knowledge about the state-of-the-art to continuously improve our products and develop new bleeding-edge prototypes and solutions. For example, this could mean that we make suggestions for new features in Driverless AI or develop AI applications in Wave demonstrating new techniques or full pipeline data science solutions. What are some of the best things you have learned via Kaggle that you apply in your professional work at H2O.ai? Philipp: One important thing you learn on Kaggle is how to produce robust models that can generalize well and are not subject to strong overfitting. This is crucial on Kaggle as you need to perform well on unseen private data. This means that you learn a lot about robust cross-validation and care about other data facets like feature distribution shifts or certain essential aspects. I can utilize this knowledge well for my work at H2O.ai as this is also an integral part of our products. We want to enable customers to do robust machine learning supported by our expertise and knowledge in the area. The Data Science domain is rapidly evolving. How do you manage to keep up with all the latest developments? Philipp: I mostly use Kaggle to keep up with the latest developments; it is an excellent filter of new techniques that either work on practical and applied problems or do not work. Usually, the robust methods survive, and the marginal techniques that only work occasionally get filtered out. At the same time, I try to keep up-to-date by following well-known researchers and practitioners on Twitter and other platforms. Are there any specific areas or problems where you would want to apply your expertise in ML? Philipp talks at Vienna Data Science Group meetup held on 9 Jan 2020 Philipp: I have nothing specific in mind; I usually try to get surprised by interesting problems popping up either at work or Kaggle. It is quite essential to delve into problems that do not seem that interesting to you at first glance. You can also bring an unbiased view to the problem and probably also apply your experience gained from other issues to the data at hand. A word of advice for the Data Science and Kaggle aspirants who have just started or wish to start their Data Science journey? Philipp: Get your hands dirty, don\\u2019t be afraid to fail, and always be eager to learn new things. Philipp\\u2019s Kaggle journey has been quite remarkable. I\\u2019m sure, his journey, dedication, and achievements will be a source of inspiration for others working to trying to make a career in this field. Read other interviews in this series: Rohan Rao: A Data Scientist\\u2019s journey from Sudoku to Kaggle Shivam Bansal: The Data Scientist who rules the \\u2018Data Science for Good\\u2019 competitions on Kaggle. Meet Yauhen: The first and the only Kaggle Grandmaster from Belarus. Sudalai Rajkumar: How a passion for numbers turned this Mechanical Engineer into a Kaggle Grandmaster Gabor Fodor: The inspiring journey of the \\u2018Beluga\\u2019 of Kaggle World The post Meet the Data Scientist who just cannot stop winning on Kaggle. appeared first on Open Source Leader in AI and ML.\",\"481\":\"Compound interest, e, and how it relates to circles. Full playlist: https:\\/\\/www.youtube.com\\/playlist?list=PLZHQObOWTQDP5CVelJJ1bNDouqrAhVPev Home page: https:\\/\\/www.3blue1brown.com Brought to you by you: https:\\/\\/3b1b.co\\/ldm-thanks Great Mathologer video: https:\\/\\/youtu.be\\/-dhHrg-KbJ0 Beautiful pictorial summary by @ThuyNganVu: https:\\/\\/twitter.com\\/ThuyNganVu\\/status\\/1258221677990703105 https:\\/\\/twitter.com\\/ThuyNganVu\\/status\\/1258222002889875457 My other videos on imaginary exponents: https:\\/\\/youtu.be\\/v0YEaeIClKY https:\\/\\/youtu.be\\/mvmuCPvRoWQ Mistakes: In the off-handed remarks on quaternions, I mentioned rotation in 4d would require 10 degrees of freedom. That's wrong, what I should have said was it requires 6 degrees of freedom, and rotation in 5D is what requires 10 degrees of freedom. ------------------ Video Timeline (Thanks to user \\\"Just TIEriffic\\\") 0:00:00 Welcome 0:00:55 Q1: Prompt (Would you take an imaginary interest rate) 0:02:05 \\\"e to the pi i for dummies\\\" video shoutout 0:02:45 Q1: Results 0:03:30 Q2: Prompt (two banks, two rates) 0:04:55 Ask: Beauty of connections in math 0:06:00 Q2: Results 0:07:05 Desmos for Q2 0:09:10 Q3: Prompt (savings growth rate, 6% every 6mo) 0:10:35 Q3: Results 0:12:35 Desmos graph explored 0:14:45 Breaking down an interest rate 0:18:00 An interesting interest equation 0:19:20 Q4: Prompt (100*(1+0.12\\/n)^2 as n \\u2192 \\u221e) 0:21:05 Ask: Quaternions 0:22:35 Q4: Results 0:24:50 Explaining Q4 0:26:40 Defining e 0:28:40 The definition of e from previous lectures 0:30:45 The imaginary interest rate 0:32:35 Graphing this relationship 0:33:50 The imaginary interest rate animation 0:37:55 Compounding continuously with i 0:40:45 The spring & Hooke's law 0:43:20 Q5: Prompt (\\u0394x & \\u0394v for a spring) 0:44:50 Ask: Rotation in for multiple dimensions 0:47:45 Q5: Results 0:49:50 Rewriting the spring's position 0:55:00 Bringing it all together 0:59:00 Ask: Hints on last lecture's homework 1:03:25 Closing Remarks ------------------ The live question setup with stats on-screen is powered by Itempool. https:\\/\\/itempool.com\\/ Curious about other animations? https:\\/\\/www.3blue1brown.com\\/faq#manim Music by Vincent Rubinetti. Download the music on Bandcamp: https:\\/\\/vincerubinetti.bandcamp.com\\/album\\/the-music-of-3blue1brown Stream the music on Spotify: https:\\/\\/open.spotify.com\\/album\\/1dVyjwS8FBqXhRunaG5W5u If you want to contribute translated subtitles or to help review those that have already been made by others and need approval, you can click the gear icon in the video and go to subtitles\\/cc, then \\\"add subtitles\\/cc\\\". I really appreciate those who do this, as it helps make the lessons accessible to more people. ------------------ 3blue1brown is a channel about animating math, in all senses of the word animate. And you know the drill with YouTube, if you want to stay posted on new videos, subscribe: http:\\/\\/3b1b.co\\/subscribe Various social media stuffs: Website: https:\\/\\/www.3blue1brown.com Twitter: https:\\/\\/twitter.com\\/3blue1brown Reddit: https:\\/\\/www.reddit.com\\/r\\/3blue1brown Instagram: https:\\/\\/www.instagram.com\\/3blue1brown_animations\\/ Patreon: https:\\/\\/patreon.com\\/3blue1brown Facebook: https:\\/\\/www.facebook.com\\/3blue1brown\",\"1633\":\"Read the full story\",\"227\":\"\\ud83c\\udf89 Introducing the new Papers with Code newsletter! Our newsletter helps you manage the firehose of new ML papers by highlighting trending papers, new state-of-the-art results, and community contributions from around PwC. paperswithcode.com\\/newslette\\u2026\",\"1506\":\"Vladimir Vapnik is the co-inventor of support vector machines, support vector clustering, VC theory, and many foundational ideas in statistical learning. He was born in the Soviet Union, worked at the Institute of Control Sciences in Moscow, then in the US, worked at AT&T, NEC Labs, Facebook AI Research, and now is a professor at Columbia University. His work has been cited over 200,000 times. This conversation is part of the Artificial Intelligence podcast. If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman on Twitter, LinkedIn, Facebook, Medium, or YouTube where you\",\"2344\":\"Has anyone heard back for the MILA 2021 Msc\\/PhD program? Supervisor decisions\\/interviews are supposed to be announced by January-end to February-end. Creating this thread for discussion. [link] [comments]\",\"5685\":\"My idea would be Retail with a focus on eCommerce because they generate a lot of data and are tech-savvy. Other opinions? Brief lists of industries I considered, maybe I missed some, so feel free to mention them in the comments. Automative & Transporation Consumer Goods & Manufacturing Financial Services & Insurance Hospitality Travel, & Leisure Media & Entertainment Retail & Restaurants Technology Communications Utilities & Industrials [link] [comments]\",\"1577\":\"Manolis Kellis is a computational biologist at MIT. Please support this podcast by checking out our sponsors: \\u2013 Grammarly: https:\\/\\/grammarly.com\\/lex to get 20% off premium \\u2013 Athletic Greens: https:\\/\\/athleticgreens.com\\/lex and use code LEX to get 1 month of fish oil \\u2013 Cash App: https:\\/\\/cash.app\\/ and use code LexPodcast to get $10 EPISODE LINKS: Manolis Website: http:\\/\\/web.mit.edu\\/manoli\\/ Manolis Twitter: https:\\/\\/twitter.com\\/manoliskellis Manolis YouTube: https:\\/\\/www.youtube.com\\/channel\\/UCkKlJ5LHrE3C7fgbnPA5DGA PODCAST INFO: Podcast website: https:\\/\\/lexfridman.com\\/podcast Apple Podcasts: https:\\/\\/apple.co\\/2lwqZIr Spotify: https:\\/\\/spoti.fi\\/2nEwCF8 RSS: https:\\/\\/lexfridman.com\\/feed\\/podcast\\/ YouTube Full Episodes: https:\\/\\/youtube.com\\/lexfridman YouTube Clips: https:\\/\\/youtube.com\\/lexclips SUPPORT & CONNECT: \\u2013 Check out the sponsors above, it\\u2019s the best way to support this podcast \\u2013 Support\",\"1477\":\"Russ Tedrake is a roboticist and professor at MIT and vice president of robotics research at TRI. He works on control of robots in interesting, complicated, underactuated, stochastic, difficult to model situations. Support this podcast by supporting our sponsors. Click links, get discount: \\u2013 Magic Spoon: https:\\/\\/magicspoon.com\\/lex & use code LEX at checkout \\u2013 BetterHelp: https:\\/\\/betterhelp.com\\/lex \\u2013 ExpressVPN: https:\\/\\/www.expressvpn.com\\/lexpod If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman on Twitter, LinkedIn, Facebook, Medium, or YouTube where you can watch the video versions of these conversations. If you enjoy the podcast, please\",\"1470\":\"Daniel Kahneman is winner of the Nobel Prize in economics for his integration of economic science with the psychology of human behavior, judgment and decision-making. He is the author of the popular book \\u201cThinking, Fast and Slow\\u201d that summarizes in an accessible way his research of several decades, often in collaboration with Amos Tversky, on cognitive biases, prospect theory, and happiness. The central thesis of this work is a dichotomy between two modes of thought: \\u201cSystem 1\\u201d is fast, instinctive and emotional; \\u201cSystem 2\\u201d is slower, more deliberative, and more logical. The book delineates cognitive biases associated with each type\",\"1519\":\"Jack Dorsey is the co-founder and CEO of Twitter and the founder and CEO of Square. Support this podcast by signing up with these sponsors: \\u2013 MasterClass: https:\\/\\/masterclass.com\\/lex EPISODE LINKS: Jack\\u2019s Twitter: https:\\/\\/twitter.com\\/jack Start Small Tracker: https:\\/\\/bit.ly\\/2KxdiBL This conversation is part of the Artificial Intelligence podcast. If you would like to get more information about this podcast go to https:\\/\\/lexfridman.com\\/ai or connect with @lexfridman on Twitter, LinkedIn, Facebook, Medium, or YouTube where you can watch the video versions of these conversations. If you enjoy the podcast, please rate it 5 stars on Apple Podcasts, follow on Spotify, or support it on\",\"1346\":\"#ai #research #engineering Numerical solvers for Partial Differential Equations are notoriously slow. They need to evolve their state by tiny steps in order to stay accurate, and they need to repeat this for each new problem. Neural Fourier Operators, the architecture proposed in this paper, can evolve a PDE in time by a single forward pass, and do so for an entire family of PDEs, as long as the training set covers them well. By performing crucial operations only in Fourier Space, this new architecture is also independent of the discretization or sampling of the underlying signal and has the potential to speed up many scientific applications. OUTLINE: 0:00 - Intro & Overview 6:15 - Navier Stokes Problem Statement 11:00 - Formal Problem Definition 15:00 - Neural Operator 31:30 - Fourier Neural Operator 48:15 - Experimental Examples 50:35 - Code Walkthrough 1:01:00 - Summary & Conclusion Paper: https:\\/\\/arxiv.org\\/abs\\/2010.08895 Blog: https:\\/\\/zongyi-li.github.io\\/blog\\/2020\\/fourier-pde\\/ Code: https:\\/\\/github.com\\/zongyi-li\\/fourier_neural_operator\\/blob\\/master\\/fourier_3d.py MIT Technology Review: https:\\/\\/www.technologyreview.com\\/2020\\/10\\/30\\/1011435\\/ai-fourier-neural-network-cracks-navier-stokes-and-partial-differential-equations\\/ Abstract: The classical development of neural networks has primarily focused on learning mappings between finite-dimensional Euclidean spaces. Recently, this has been generalized to neural operators that learn mappings between function spaces. For partial differential equations (PDEs), neural operators directly learn the mapping from any functional parametric dependence to the solution. Thus, they learn an entire family of PDEs, in contrast to classical methods which solve one instance of the equation. In this work, we formulate a new neural operator by parameterizing the integral kernel directly in Fourier space, allowing for an expressive and efficient architecture. We perform experiments on Burgers' equation, Darcy flow, and the Navier-Stokes equation (including the turbulent regime). Our Fourier neural operator shows state-of-the-art performance compared to existing neural network methodologies and it is up to three orders of magnitude faster compared to traditional PDE solvers. Authors: Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, Anima Anandkumar Links: YouTube: https:\\/\\/www.youtube.com\\/c\\/yannickilcher Twitter: https:\\/\\/twitter.com\\/ykilcher Discord: https:\\/\\/discord.gg\\/4H8xxDF BitChute: https:\\/\\/www.bitchute.com\\/channel\\/yannic-kilcher Minds: https:\\/\\/www.minds.com\\/ykilcher Parler: https:\\/\\/parler.com\\/profile\\/YannicKilcher LinkedIn: https:\\/\\/www.linkedin.com\\/in\\/yannic-kilcher-488534136\\/ If you want to support me, the best thing to do is to share out the content :) If you want to support me financially (completely optional and voluntary, but a lot of people have asked for this): SubscribeStar: https:\\/\\/www.subscribestar.com\\/yannickilcher Patreon: https:\\/\\/www.patreon.com\\/yannickilcher Bitcoin (BTC): bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq Ethereum (ETH): 0x7ad3513E3B8f66799f507Aa7874b1B0eBC7F85e2 Litecoin (LTC): LQW2TRyKYetVC8WjFkhpPhtpbDM4Vw7r9m Monero (XMR): 4ACL8AGrEo5hAir8A9CeVrW8pEauWvnp1WnSDZxW7tziCDLhZAGsgzhRQABDnFy8yuM9fWJDviJPHKRjV4FWt19CJZN9D4n\",\"480\":\"A discovery-oriented introduction to error correction codes. Part 2: https:\\/\\/youtu.be\\/b3NxrZOu_CE Ben Eater:'s take: https:\\/\\/youtu.be\\/h0jloehRKas Many thanks to these supporters: https:\\/\\/3b1b.co\\/hamming-thanks Heavily related is the chessboard puzzle I did with Matt Parker: https:\\/\\/youtu.be\\/as7Gkm7Y7h4 You can read Hamming's own perspective on his discovery of these codes in chapter 12 of \\\"The Art of Doing Science and Engineering\\\". https:\\/\\/amzn.to\\/3lwcnmh The viewer Harry Li made an interactive on this topic: https:\\/\\/harryli0088.github.io\\/hamming-code\\/ ------------------ These animations are largely made using manim, a scrappy open-source python library: https:\\/\\/github.com\\/3b1b\\/manim If you want to check it out, I feel compelled to warn you that it's not the most well-documented tool, and it has many other quirks you might expect in a library someone wrote with only their own use in mind. Music by Vincent Rubinetti. Download the music on Bandcamp: https:\\/\\/vincerubinetti.bandcamp.com\\/album\\/the-music-of-3blue1brown Stream the music on Spotify: https:\\/\\/open.spotify.com\\/album\\/1dVyjwS8FBqXhRunaG5W5u ------------------ 3blue1brown is a channel about animating math, in all senses of the word animate. And you know the drill with YouTube, if you want to stay posted on new videos, subscribe: http:\\/\\/3b1b.co\\/subscribe Various social media links: Website: https:\\/\\/www.3blue1brown.com Twitter: https:\\/\\/twitter.com\\/3blue1brown Reddit: https:\\/\\/www.reddit.com\\/r\\/3blue1brown Instagram: https:\\/\\/www.instagram.com\\/3blue1brown_animations\\/ Patreon: https:\\/\\/patreon.com\\/3blue1brown Facebook: https:\\/\\/www.facebook.com\\/3blue1brown\",\"2331\":\"Paper Highlights: A flow-free and single-shot prediction approach for video frame interpolation. 384x faster than the current most accurate approach and 23x faster than the current fastest on 8x interpolation. [Paper] [Video] [Project] https:\\/\\/preview.redd.it\\/2f4aa60vl1c61.png?width=1110&format=png&auto=webp&s=d5140ffc6378ef74d7d4f142159d007c8c2359d6 Abstract: A majority of approaches solve the problem of video frame interpolation by computing bidirectional optical flow between adjacent frames of a video followed by a suitable warping algorithm to generate the output frames. However, methods relying on optical flow often fail to model occlusions and complex non-linear motions directly from the video and introduce additional bottlenecks unsuitable for real time deployment. To overcome these limitations, we propose a flexible and efficient architecture that makes use of 3D space-time convolutions to enable end to end learning and inference for the task of video frame interpolation. Our method efficiently learns to reason about non-linear motions, complex occlusions and temporal abstractions resulting in improved performance on video interpolation, while requiring no additional inputs in the form of optical flow or depth maps. Due to its simplicity, our proposed method improves the inference speed by 384x compared to the current most accurate method and 23x compared to the current fastest on 8x interpolation. In addition, we evaluate our model on a wide range of challenging settings and consistently demonstrate superior qualitative and quantitative results compared with current methods on various popular benchmarks including Vimeo-90K, UCF101, DAVIS, Adobe, and GoPro. Finally, we demonstrate that video frame interpolation can serve as a useful self-supervised pretext task for action recognition, optical flow estimation, and motion magnification. [link] [comments]\",\"3196\":\"Activation functions are a critical part of the design of a neural network. The choice of activation function in the hidden layer will control how well the network model learns the training dataset. The choice of activation function in the output layer will define the type of predictions the model can make. As such, a careful choice of activation function must be made for each deep learning neural network project. In this tutorial, you will discover how to choose activation functions for neural network models. After completing this tutorial, you will know: Activation functions are a key part of neural network design. The modern default activation function for hidden layers is the ReLU function. The activation function for output layers depends on the type of prediction problem. Let\\u2019s get started. How to Choose an Activation Function for Deep Learning Photo by Peter Dowley, some rights reserved. Tutorial Overview This tutorial is divided into three parts; they are: Activation Functions Activation for Hidden Layers Activation for Output Layers Activation Functions An activation function in a neural network defines how the weighted sum of the input is transformed into an output from a node or nodes in a layer of the network. Sometimes the activation function is called a \\u201ctransfer function.\\u201d If the output range of the activation function is limited, then it may be called a \\u201csquashing function.\\u201d Many activation functions are nonlinear and may be referred to as the \\u201cnonlinearity\\u201d in the layer or the network design. The choice of activation function has a large impact on the capability and performance of the neural network, and different activation functions may be used in different parts of the model. Technically, the activation function is used within or after the internal processing of each node in the network, although networks are designed to use the same activation function for all nodes in a layer. A network may have three types of layers: input layers that take raw input from the domain, hidden layers that take input from another layer and pass output to another layer, and output layers that make a prediction. All hidden layers typically use the same activation function. The output layer will typically use a different activation function from the hidden layers and is dependent upon the type of prediction required by the model. Activation functions are also typically differentiable, meaning the first-order derivative can be calculated for a given input value. This is required given that neural networks are typically trained using the backpropagation of error algorithm that requires the derivative of prediction error in order to update the weights of the model. There are many different types of activation functions used in neural networks, although perhaps only a small number of functions used in practice for hidden and output layers. Let\\u2019s take a look at the activation functions used for each type of layer in turn. Activation for Hidden Layers A hidden layer in a neural network is a layer that receives input from another layer (such as another hidden layer or an input layer) and provides output to another layer (such as another hidden layer or an output layer). A hidden layer does not directly contact input data or produce outputs for a model, at least in general. A neural network may have zero or more hidden layers. Typically, a differentiable nonlinear activation function is used in the hidden layers of a neural network. This allows the model to learn more complex functions than a network trained using a linear activation function. In order to get access to a much richer hypothesis space that would benefit from deep representations, you need a non-linearity, or activation function. \\u2014 Page 72, Deep Learning with Python, 2017. There are perhaps three activation functions you may want to consider for use in hidden layers; they are: Rectified Linear Activation (ReLU) Logistic (Sigmoid) Hyperbolic Tangent (Tanh) This is not an exhaustive list of activation functions used for hidden layers, but they are the most commonly used. Let\\u2019s take a closer look at each in turn. ReLU Hidden Layer Activation Function The rectified linear activation function, or ReLU activation function, is perhaps the most common function used for hidden layers. It is common because it is both simple to implement and effective at overcoming the limitations of other previously popular activation functions, such as Sigmoid and Tanh. Specifically, it is less susceptible to vanishing gradients that prevent deep models from being trained, although it can suffer from other problems like saturated or \\u201cdead\\u201d units. The ReLU function is calculated as follows: max(0.0, x) This means that if the input value (x) is negative, then a value 0.0 is returned, otherwise, the value is returned. You can learn more about the details of the ReLU activation function in this tutorial: A Gentle Introduction to the Rectified Linear Unit (ReLU) We can get an intuition for the shape of this function with the worked example below. # example plot for the relu activation function from matplotlib import pyplot # rectified linear function def rectified(x): return max(0.0, x) # define input data inputs = [x for x in range(-10, 10)] # calculate outputs outputs = [rectified(x) for x in inputs] # plot inputs vs outputs pyplot.plot(inputs, outputs) pyplot.show() Running the example calculates the outputs for a range of values and creates a plot of inputs versus outputs. We can see the familiar kink shape of the ReLU activation function. Plot of Inputs vs. Outputs for the ReLU Activation Function. When using the ReLU function for hidden layers, it is a good practice to use a \\u201cHe Normal\\u201d or \\u201cHe Uniform\\u201d weight initialization and scale input data to the range 0-1 (normalize) prior to training. Sigmoid Hidden Layer Activation Function The sigmoid activation function is also called the logistic function. It is the same function used in the logistic regression classification algorithm. The function takes any real value as input and outputs values in the range 0 to 1. The larger the input (more positive), the closer the output value will be to 1.0, whereas the smaller the input (more negative), the closer the output will be to 0.0. The sigmoid activation function is calculated as follows: 1.0 \\/ (1.0 + e^-x) Where e is a mathematical constant, which is the base of the natural logarithm. We can get an intuition for the shape of this function with the worked example below. # example plot for the sigmoid activation function from math import exp from matplotlib import pyplot # sigmoid activation function def sigmoid(x): return 1.0 \\/ (1.0 + exp(-x)) # define input data inputs = [x for x in range(-10, 10)] # calculate outputs outputs = [sigmoid(x) for x in inputs] # plot inputs vs outputs pyplot.plot(inputs, outputs) pyplot.show() Running the example calculates the outputs for a range of values and creates a plot of inputs versus outputs. We can see the familiar S-shape of the sigmoid activation function. Plot of Inputs vs. Outputs for the Sigmoid Activation Function. When using the Sigmoid function for hidden layers, it is a good practice to use a \\u201cXavier Normal\\u201d or \\u201cXavier Uniform\\u201d weight initialization (also referred to Glorot initialization, named for Xavier Glorot) and scale input data to the range 0-1 (e.g. the range of the activation function) prior to training. Tanh Hidden Layer Activation Function The hyperbolic tangent activation function is also referred to simply as the Tanh (also \\u201ctanh\\u201d and \\u201cTanH\\u201c) function. It is very similar to the sigmoid activation function and even has the same S-shape. The function takes any real value as input and outputs values in the range -1 to 1. The larger the input (more positive), the closer the output value will be to 1.0, whereas the smaller the input (more negative), the closer the output will be to -1.0. The Tanh activation function is calculated as follows: (e^x \\u2013 e^-x) \\/ (e^x + e^-x) Where e is a mathematical constant that is the base of the natural logarithm. We can get an intuition for the shape of this function with the worked example below. # example plot for the tanh activation function from math import exp from matplotlib import pyplot # tanh activation function def tanh(x): return (exp(x) - exp(-x)) \\/ (exp(x) + exp(-x)) # define input data inputs = [x for x in range(-10, 10)] # calculate outputs outputs = [tanh(x) for x in inputs] # plot inputs vs outputs pyplot.plot(inputs, outputs) pyplot.show() Running the example calculates the outputs for a range of values and creates a plot of inputs versus outputs. We can see the familiar S-shape of the Tanh activation function. Plot of Inputs vs. Outputs for the Tanh Activation Function. When using the TanH function for hidden layers, it is a good practice to use a \\u201cXavier Normal\\u201d or \\u201cXavier Uniform\\u201d weight initialization (also referred to Glorot initialization, named for Xavier Glorot) and scale input data to the range -1 to 1 (e.g. the range of the activation function) prior to training. How to Choose a Hidden Layer Activation Function A neural network will almost always have the same activation function in all hidden layers. It is most unusual to vary the activation function through a network model. Traditionally, the sigmoid activation function was the default activation function in the 1990s. Perhaps through the mid to late 1990s to 2010s, the Tanh function was the default activation function for hidden layers. \\u2026 the hyperbolic tangent activation function typically performs better than the logistic sigmoid. \\u2014 Page 195, Deep Learning, 2016. Both the sigmoid and Tanh functions can make the model more susceptible to problems during training, via the so-called vanishing gradients problem. You can learn more about this problem in this tutorial: A Gentle Introduction to the Rectified Linear Unit (ReLU) The activation function used in hidden layers is typically chosen based on the type of neural network architecture. Modern neural network models with common architectures, such as MLP and CNN, will make use of the ReLU activation function, or extensions. In modern neural networks, the default recommendation is to use the rectified linear unit or ReLU \\u2026 \\u2014 Page 174, Deep Learning, 2016. Recurrent networks still commonly use Tanh or sigmoid activation functions, or even both. For example, the LSTM commonly uses the Sigmoid activation for recurrent connections and the Tanh activation for output. Multilayer Perceptron (MLP): ReLU activation function. Convolutional Neural Network (CNN): ReLU activation function. Recurrent Neural Network: Tanh and\\/or Sigmoid activation function. If you\\u2019re unsure which activation function to use for your network, try a few and compare the results. The figure below summarizes how to choose an activation function for the hidden layers of your neural network model. How to Choose a Hidden Layer Activation Function Activation for Output Layers The output layer is the layer in a neural network model that directly outputs a prediction. All feed-forward neural network models have an output layer. There are perhaps three activation functions you may want to consider for use in the output layer; they are: Linear Logistic (Sigmoid) Softmax This is not an exhaustive list of activation functions used for output layers, but they are the most commonly used. Let\\u2019s take a closer look at each in turn. Linear Output Activation Function The linear activation function is also called \\u201cidentity\\u201d (multiplied by 1.0) or \\u201cno activation.\\u201d This is because the linear activation function does not change the weighted sum of the input in any way and instead returns the value directly. We can get an intuition for the shape of this function with the worked example below. # example plot for the linear activation function from matplotlib import pyplot # linear activation function def linear(x): return x # define input data inputs = [x for x in range(-10, 10)] # calculate outputs outputs = [linear(x) for x in inputs] # plot inputs vs outputs pyplot.plot(inputs, outputs) pyplot.show() Running the example calculates the outputs for a range of values and creates a plot of inputs versus outputs. We can see a diagonal line shape where inputs are plotted against identical outputs. Plot of Inputs vs. Outputs for the Linear Activation Function Target values used to train a model with a linear activation function in the output layer are typically scaled prior to modeling using normalization or standardization transforms. Sigmoid Output Activation Function The sigmoid of logistic activation function was described in the previous section. Nevertheless, to add some symmetry, we can review for the shape of this function with the worked example below. # example plot for the sigmoid activation function from math import exp from matplotlib import pyplot # sigmoid activation function def sigmoid(x): return 1.0 \\/ (1.0 + exp(-x)) # define input data inputs = [x for x in range(-10, 10)] # calculate outputs outputs = [sigmoid(x) for x in inputs] # plot inputs vs outputs pyplot.plot(inputs, outputs) pyplot.show() Running the example calculates the outputs for a range of values and creates a plot of inputs versus outputs. We can see the familiar S-shape of the sigmoid activation function. Plot of Inputs vs. Outputs for the Sigmoid Activation Function. Target labels used to train a model with a sigmoid activation function in the output layer will have the values 0 or 1. Softmax Output Activation Function The softmax function outputs a vector of values that sum to 1.0 that can be interpreted as probabilities of class membership. It is related to the argmax function that outputs a 0 for all options and 1 for the chosen option. Softmax is a \\u201csofter\\u201d version of argmax that allows a probability-like output of a winner-take-all function. As such, the input to the function is a vector of real values and the output is a vector of the same length with values that sum to 1.0 like probabilities. The softmax function is calculated as follows: e^x \\/ sum(e^x) Where x is a vector of outputs and e is a mathematical constant that is the base of the natural logarithm. You can learn more about the details of the Softmax function in this tutorial: Softmax Activation Function with Python We cannot plot the softmax function, but we can give an example of calculating it in Python. from numpy import exp # softmax activation function def softmax(x): return exp(x) \\/ exp(x).sum() # define input data inputs = [1.0, 3.0, 2.0] # calculate outputs outputs = softmax(inputs) # report the probabilities print(outputs) # report the sum of the probabilities print(outputs.sum()) Running the example calculates the softmax output for the input vector. We then confirm that the sum of the outputs of the softmax indeed sums to the value 1.0. [0.09003057 0.66524096 0.24472847] 1.0 Target labels used to train a model with the softmax activation function in the output layer will be vectors with 1 for the target class and 0 for all other classes. How to Choose an Output Activation Function You must choose the activation function for your output layer based on the type of prediction problem that you are solving. Specifically, the type of variable that is being predicted. For example, you may divide prediction problems into two main groups, predicting a categorical variable (classification) and predicting a numerical variable (regression). If your problem is a regression problem, you should use a linear activation function. Regression: One node, linear activation. If your problem is a classification problem, then there are three main types of classification problems and each may use a different activation function. Predicting a probability is not a regression problem; it is classification. In all cases of classification, your model will predict the probability of class membership (e.g. probability that an example belongs to each class) that you can convert to a crisp class label by rounding (for sigmoid) or argmax (for softmax). If there are two mutually exclusive classes (binary classification), then your output layer will have one node and a sigmoid activation function should be used. If there are more than two mutually exclusive classes (multiclass classification), then your output layer will have one node per class and a softmax activation should be used. If there are two or more mutually inclusive classes (multilabel classification), then your output layer will have one node for each class and a sigmoid activation function is used. Binary Classification: One node, sigmoid activation. Multiclass Classification: One node per class, softmax activation. Multilabel Classification: One node per class, sigmoid activation. The figure below summarizes how to choose an activation function for the output layer of your neural network model. How to Choose an Output Layer Activation Function Further Reading This section provides more resources on the topic if you are looking to go deeper. Tutorials A Gentle Introduction to the Rectified Linear Unit (ReLU) Softmax Activation Function with Python 4 Types of Classification Tasks in Machine Learning How to Fix the Vanishing Gradients Problem Using the ReLU Books Deep Learning, 2016. Neural Smithing: Supervised Learning in Feedforward Artificial Neural Networks, 1999. Neural Networks for Pattern Recognition, 1996. Deep Learning with Python, 2017. Articles Activation function, Wikipedia. Summary In this tutorial, you discovered how to choose activation functions for neural network models. Specifically, you learned: Activation functions are a key part of neural network design. The modern default activation function for hidden layers is the ReLU function. The activation function for output layers depends on the type of prediction problem. Do you have any questions? Ask your questions in the comments below and I will do my best to answer. The post How to Choose an Activation Function for Deep Learning appeared first on Machine Learning Mastery.\",\"652\":\"Michael Mina is an immunologist, epidemiologist, and physician at Harvard. Please support this podcast by checking out our sponsors: - Brave: https:\\/\\/brave.com\\/lex - Athletic Greens: https:\\/\\/athleticgreens.com\\/lex and use code LEX to get 1 month of fish oil - ExpressVPN: https:\\/\\/expressvpn.com\\/lexpod and use code LexPod to get 3 months free - Cash App: https:\\/\\/cash.app\\/ and use code LexPodcast to get $10 EPISODE LINKS: Michael's Twitter: https:\\/\\/twitter.com\\/michaelmina_lab Michael's Time article: https:\\/\\/time.com\\/5912705\\/covid-19-stop-spread-christmas\\/ Rapid Tests: https:\\/\\/www.rapidtests.org\\/ PODCAST INFO: Podcast website: https:\\/\\/lexfridman.com\\/podcast Apple Podcasts: https:\\/\\/apple.co\\/2lwqZIr Spotify: https:\\/\\/spoti.fi\\/2nEwCF8 RSS: https:\\/\\/lexfridman.com\\/feed\\/podcast\\/ Full episodes playlist: https:\\/\\/www.youtube.com\\/playlist?list=PLrAXtmErZgOdP_8GztsuKi9nrraNbKKp4 Clips playlist: https:\\/\\/www.youtube.com\\/playlist?list=PLrAXtmErZgOeciFP3CBCIEElOJeitOr41 OUTLINE: 0:00 - Introduction 2:32 - Interacting between viruses and bacteria 6:45 - Deadlier viruses 10:17 - Will COVID-19 mutate? 11:51 - Rapid testing 29:15 - PCR vs rapid antigen tests 38:59 - Medical industrial complex 42:51 - Lex takes COVID test 49:35 - FDA and cheap tests 52:21 - Explanation of Elon Musk's positive COVID tests 59:29 - Role of testing during vaccine deployment 1:02:58 - Public health policy 1:12:38 - A weather system for viruses 1:29:30 - Can a virus kill all humans? 1:35:09 - Engineering a deadly virus 1:39:51 - AlphaFold 2 and viruses 1:45:46 - Advice for young people 1:53:54 - Time as a buddhist monk 1:59:58 - Meditation 2:07:36 - Meaning of life CONNECT: - Subscribe to this YouTube channel - Twitter: https:\\/\\/twitter.com\\/lexfridman - LinkedIn: https:\\/\\/www.linkedin.com\\/in\\/lexfridman - Facebook: https:\\/\\/www.facebook.com\\/LexFridmanPage - Instagram: https:\\/\\/www.instagram.com\\/lexfridman - Medium: https:\\/\\/medium.com\\/@lexfridman - Support on Patreon: https:\\/\\/www.patreon.com\\/lexfridman\",\"3766\":\"I've been doing ML in both research and production settings for a few years now, and I've noticed that while there has been a lot of discussion about the reproducibility of ML results in research, there is little to be found in the way of achieving reproducibility in a production setting. While there are many overlaps between the two (production might even be a superset), there are certainly some aspects of reproducibility in production that I feel require a community-wide effort to really get going. I personally found that it was way easier to write reproducible ML code in research than in production. If you break it down, reproducible ML can be achieved by setting up systems to track and version: Code Configuration Environment Data And to get to a point where one can establish a solid system across a fast-moving ML team in the industry is quite difficult and requires more systems and overhead than e.g. writing a reproducible train.py for a paper. I can't be the only one looking at a 6 month old jupyter notebook wondering how the heck a certain model was produced by whom in the company. Even trying to re-deploy a ML model again in production sometimes gives you a different result than expected. I'd be curious to see what other practitioners struggle to reproduce machine learning in production. Any war stories to share or thoughts on what processes\\/tools need to be used to ensure reproducibility in production? Disclaimer: I'm part of a team building ZenML, an open-source MLOps framework whose design centers around enabling reproducibility in machine learning. I gave my two cents in this blog post: I wrote about what I think are the most important aspects in this blog post: https:\\/\\/blog.maiot.io\\/is-your-ml-reproducible\\/ . [link] [comments]\",\"1584\":\"Charles Isbell is the Dean of the College of Computing at Georgia Tech. Please support this podcast by checking out our sponsors: \\u2013 Neuro: https:\\/\\/www.getneuro.com and use code LEX to get 15% off \\u2013 Decoding Digital: https:\\/\\/appdirect.com\\/decoding-digital \\u2013 MasterClass: https:\\/\\/masterclass.com\\/lex to get 15% off annual sub \\u2013 Cash App: https:\\/\\/cash.app\\/ and use code LexPodcast to get $10 EPISODE LINKS: Charles\\u2019s Twitter: https:\\/\\/twitter.com\\/isbellHFh Charles\\u2019s Website: https:\\/\\/www.cc.gatech.edu\\/~isbell\\/ PODCAST INFO: Podcast website: https:\\/\\/lexfridman.com\\/podcast Apple Podcasts: https:\\/\\/apple.co\\/2lwqZIr Spotify: https:\\/\\/spoti.fi\\/2nEwCF8 RSS: https:\\/\\/lexfridman.com\\/feed\\/podcast\\/ YouTube Full Episodes: https:\\/\\/youtube.com\\/lexfridman YouTube Clips: https:\\/\\/youtube.com\\/lexclips SUPPORT & CONNECT: \\u2013 Check out the sponsors above, it\\u2019s the best way to support this\",\"4091\":\"I couldn't find any, but I was wondering if anyone is familiar with those [link] [comments]\",\"5672\":\"If you are still planning a 2021 reading list, here are 10 AI\\/ML books to consider. They are published over the past two years and cover a range of topics from fundamental concepts, to algorithms and applications. And here is a collection of author talks and book reviews that might be helpful to check out before diving deep into the books. Artificial Intelligence: A Guide for Thinking Humans (Melanie Mitchell) Rebooting AI (Gary Marcus and Ernest Davis) Human Compatible: Artificial Intelligence and the Problem of Control (Stuart Russel) You Look Like a Thing and I Love You: How Artificial Intelligence Works and Why It's Making the World a Weirder Place (Janelle Shane) The Hundred-Page Machine Learning Book (Andriy Burkov) Interpretable Machine Learning: A Guide for Making Black Box Models Explainable (Christoph Molnar) Machine Learning Yearning (Andrew Ng) Machine Learning Engineering (Andriy Burkov) Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems (2nd Edition) (Aur\\u00e9lien G\\u00e9ron) Approaching (Almost) Any Machine Learning Problem (Abhishek Thakur) Feel free to comment below and add new book recommendations. [link] [comments]\"},\"Topic\":{\"4311\":\"Machine Learning\",\"499\":\"Machine Learning\",\"486\":\"Machine Learning\",\"1507\":\"Machine Learning\",\"1632\":\"Machine Learning\",\"1547\":\"Machine Learning\",\"4094\":\"Machine Learning\",\"1228\":\"Machine Learning\",\"4084\":\"Machine Learning\",\"1672\":\"Machine Learning\",\"1653\":\"Machine Learning\",\"4090\":\"Machine Learning\",\"1601\":\"Machine Learning\",\"1624\":\"Machine Learning\",\"3187\":\"Machine Learning\",\"1640\":\"Machine Learning\",\"1579\":\"Machine Learning\",\"4092\":\"Machine Learning\",\"1046\":\"Machine Learning\",\"710\":\"Machine Learning\",\"5097\":\"Machine Learning\",\"4270\":\"Machine Learning\",\"1222\":\"Machine Learning\",\"1535\":\"Machine Learning\",\"476\":\"Machine Learning\",\"1461\":\"Machine Learning\",\"646\":\"Machine Learning\",\"259\":\"Machine Learning\",\"1649\":\"Machine Learning\",\"4280\":\"Machine Learning\",\"2320\":\"Machine Learning\",\"709\":\"Machine Learning\",\"5686\":\"Machine Learning\",\"1481\":\"Machine Learning\",\"3746\":\"Machine Learning\",\"1462\":\"Machine Learning\",\"1136\":\"Machine Learning\",\"1647\":\"Machine Learning\",\"650\":\"Machine Learning\",\"4273\":\"Machine Learning\",\"1545\":\"Machine Learning\",\"712\":\"Machine Learning\",\"503\":\"Machine Learning\",\"1660\":\"Machine Learning\",\"3749\":\"Machine Learning\",\"1611\":\"Machine Learning\",\"1635\":\"Machine Learning\",\"1575\":\"Machine Learning\",\"498\":\"Machine Learning\",\"255\":\"Machine Learning\",\"496\":\"Machine Learning\",\"1454\":\"Machine Learning\",\"3872\":\"Machine Learning\",\"1518\":\"Machine Learning\",\"1438\":\"Machine Learning\",\"1348\":\"Machine Learning\",\"1236\":\"Machine Learning\",\"1869\":\"Machine Learning\",\"1873\":\"Machine Learning\",\"4277\":\"Machine Learning\",\"1613\":\"Machine Learning\",\"5684\":\"Machine Learning\",\"1147\":\"Machine Learning\",\"3754\":\"Machine Learning\",\"4282\":\"Machine Learning\",\"4492\":\"Machine Learning\",\"1537\":\"Machine Learning\",\"5837\":\"Machine Learning\",\"4294\":\"Machine Learning\",\"749\":\"Machine Learning\",\"1149\":\"Machine Learning\",\"1516\":\"Machine Learning\",\"1143\":\"Machine Learning\",\"2329\":\"Machine Learning\",\"1655\":\"Machine Learning\",\"1514\":\"Machine Learning\",\"5683\":\"Machine Learning\",\"207\":\"Machine Learning\",\"1145\":\"Machine Learning\",\"2333\":\"Machine Learning\",\"1642\":\"Machine Learning\",\"4298\":\"Machine Learning\",\"1234\":\"Machine Learning\",\"1230\":\"Machine Learning\",\"2328\":\"Machine Learning\",\"1352\":\"Machine Learning\",\"1344\":\"Machine Learning\",\"1656\":\"Machine Learning\",\"3767\":\"Machine Learning\",\"1867\":\"Machine Learning\",\"3752\":\"Machine Learning\",\"1681\":\"Machine Learning\",\"1511\":\"Machine Learning\",\"4286\":\"Machine Learning\",\"1685\":\"Machine Learning\",\"1492\":\"Machine Learning\",\"1631\":\"Machine Learning\",\"1868\":\"Machine Learning\",\"1503\":\"Machine Learning\",\"1583\":\"Machine Learning\",\"1466\":\"Machine Learning\",\"1593\":\"Machine Learning\",\"1880\":\"Machine Learning\",\"3758\":\"Machine Learning\",\"643\":\"Machine Learning\",\"258\":\"Machine Learning\",\"1351\":\"Machine Learning\",\"3140\":\"Machine Learning\",\"1650\":\"Machine Learning\",\"1638\":\"Machine Learning\",\"1625\":\"Machine Learning\",\"245\":\"Machine Learning\",\"1689\":\"Machine Learning\",\"1144\":\"Machine Learning\",\"1555\":\"Machine Learning\",\"3873\":\"Machine Learning\",\"4674\":\"Machine Learning\",\"2339\":\"Machine Learning\",\"1872\":\"Machine Learning\",\"1435\":\"Machine Learning\",\"3756\":\"Machine Learning\",\"4312\":\"Machine Learning\",\"1615\":\"Machine Learning\",\"4900\":\"Machine Learning\",\"1591\":\"Machine Learning\",\"1665\":\"Machine Learning\",\"1050\":\"Machine Learning\",\"5682\":\"Machine Learning\",\"256\":\"Machine Learning\",\"1436\":\"Machine Learning\",\"641\":\"Machine Learning\",\"1687\":\"Machine Learning\",\"3195\":\"Machine Learning\",\"247\":\"Machine Learning\",\"4467\":\"Machine Learning\",\"1525\":\"Machine Learning\",\"2330\":\"Machine Learning\",\"714\":\"Machine Learning\",\"2322\":\"Machine Learning\",\"3748\":\"Machine Learning\",\"645\":\"Machine Learning\",\"1708\":\"Machine Learning\",\"1629\":\"Machine Learning\",\"1571\":\"Machine Learning\",\"1146\":\"Machine Learning\",\"1521\":\"Machine Learning\",\"3855\":\"Machine Learning\",\"1482\":\"Machine Learning\",\"4093\":\"Machine Learning\",\"702\":\"Machine Learning\",\"1549\":\"Machine Learning\",\"1148\":\"Machine Learning\",\"1478\":\"Machine Learning\",\"1699\":\"Machine Learning\",\"1548\":\"Machine Learning\",\"1586\":\"Machine Learning\",\"2332\":\"Machine Learning\",\"3143\":\"Machine Learning\",\"1487\":\"Machine Learning\",\"1707\":\"Machine Learning\",\"1443\":\"Machine Learning\",\"1614\":\"Machine Learning\",\"1618\":\"Machine Learning\",\"1709\":\"Machine Learning\",\"971\":\"Machine Learning\",\"2327\":\"Machine Learning\",\"3138\":\"Machine Learning\",\"4289\":\"Machine Learning\",\"1515\":\"Machine Learning\",\"1694\":\"Machine Learning\",\"1499\":\"Machine Learning\",\"1455\":\"Machine Learning\",\"264\":\"Machine Learning\",\"1488\":\"Machine Learning\",\"640\":\"Machine Learning\",\"1520\":\"Machine Learning\",\"1639\":\"Machine Learning\",\"1472\":\"Machine Learning\",\"1688\":\"Machine Learning\",\"1226\":\"Machine Learning\",\"3191\":\"Machine Learning\",\"2337\":\"Machine Learning\",\"713\":\"Machine Learning\",\"1693\":\"Machine Learning\",\"478\":\"Machine Learning\",\"1568\":\"Machine Learning\",\"653\":\"Machine Learning\",\"1569\":\"Machine Learning\",\"1696\":\"Machine Learning\",\"204\":\"Machine Learning\",\"651\":\"Machine Learning\",\"974\":\"Machine Learning\",\"1440\":\"Machine Learning\",\"3135\":\"Machine Learning\",\"3141\":\"Machine Learning\",\"1523\":\"Machine Learning\",\"2805\":\"Machine Learning\",\"1609\":\"Machine Learning\",\"263\":\"Machine Learning\",\"1139\":\"Machine Learning\",\"504\":\"Machine Learning\",\"1480\":\"Machine Learning\",\"4288\":\"Machine Learning\",\"254\":\"Machine Learning\",\"262\":\"Machine Learning\",\"1546\":\"Machine Learning\",\"1527\":\"Machine Learning\",\"205\":\"Machine Learning\",\"1630\":\"Machine Learning\",\"1675\":\"Machine Learning\",\"1540\":\"Machine Learning\",\"1433\":\"Machine Learning\",\"5690\":\"Machine Learning\",\"1465\":\"Machine Learning\",\"1664\":\"Machine Learning\",\"1442\":\"Machine Learning\",\"489\":\"Machine Learning\",\"4896\":\"Machine Learning\",\"1877\":\"Machine Learning\",\"237\":\"Machine Learning\",\"1651\":\"Machine Learning\",\"1484\":\"Machine Learning\",\"1637\":\"Machine Learning\",\"1451\":\"Machine Learning\",\"1692\":\"Machine Learning\",\"976\":\"Machine Learning\",\"1223\":\"Machine Learning\",\"972\":\"Machine Learning\",\"1619\":\"Machine Learning\",\"253\":\"Machine Learning\",\"1677\":\"Machine Learning\",\"1705\":\"Machine Learning\",\"2325\":\"Machine Learning\",\"1596\":\"Machine Learning\",\"4087\":\"Machine Learning\",\"1679\":\"Machine Learning\",\"1526\":\"Machine Learning\",\"1356\":\"Machine Learning\",\"1052\":\"Machine Learning\",\"3875\":\"Machine Learning\",\"1588\":\"Machine Learning\",\"1508\":\"Machine Learning\",\"3759\":\"Machine Learning\",\"1467\":\"Machine Learning\",\"1233\":\"Machine Learning\",\"1562\":\"Machine Learning\",\"1517\":\"Machine Learning\",\"1448\":\"Machine Learning\",\"2321\":\"Machine Learning\",\"1663\":\"Machine Learning\",\"1560\":\"Machine Learning\",\"1543\":\"Machine Learning\",\"1623\":\"Machine Learning\",\"1550\":\"Machine Learning\",\"644\":\"Machine Learning\",\"973\":\"Machine Learning\",\"1533\":\"Machine Learning\",\"1572\":\"Machine Learning\",\"1469\":\"Machine Learning\",\"1229\":\"Machine Learning\",\"1231\":\"Machine Learning\",\"1559\":\"Machine Learning\",\"1224\":\"Machine Learning\",\"1497\":\"Machine Learning\",\"2334\":\"Machine Learning\",\"1676\":\"Machine Learning\",\"228\":\"Machine Learning\",\"198\":\"Machine Learning\",\"1686\":\"Machine Learning\",\"1058\":\"Machine Learning\",\"1621\":\"Machine Learning\",\"488\":\"Machine Learning\",\"1670\":\"Machine Learning\",\"242\":\"Machine Learning\",\"4302\":\"Machine Learning\",\"3874\":\"Machine Learning\",\"1704\":\"Machine Learning\",\"4284\":\"Machine Learning\",\"1458\":\"Machine Learning\",\"1876\":\"Machine Learning\",\"3753\":\"Machine Learning\",\"1475\":\"Machine Learning\",\"1504\":\"Machine Learning\",\"4297\":\"Machine Learning\",\"1622\":\"Machine Learning\",\"3854\":\"Machine Learning\",\"1563\":\"Machine Learning\",\"701\":\"Machine Learning\",\"4085\":\"Machine Learning\",\"1646\":\"Machine Learning\",\"4276\":\"Machine Learning\",\"1493\":\"Machine Learning\",\"477\":\"Machine Learning\",\"241\":\"Machine Learning\",\"1345\":\"Machine Learning\",\"1491\":\"Machine Learning\",\"479\":\"Machine Learning\",\"244\":\"Machine Learning\",\"1668\":\"Machine Learning\",\"3142\":\"Machine Learning\",\"1698\":\"Machine Learning\",\"234\":\"Machine Learning\",\"4469\":\"Machine Learning\",\"1566\":\"Machine Learning\",\"4086\":\"Machine Learning\",\"232\":\"Machine Learning\",\"1513\":\"Machine Learning\",\"1654\":\"Machine Learning\",\"2682\":\"Machine Learning\",\"1702\":\"Machine Learning\",\"231\":\"Machine Learning\",\"750\":\"Machine Learning\",\"250\":\"Machine Learning\",\"1141\":\"Machine Learning\",\"647\":\"Machine Learning\",\"1489\":\"Machine Learning\",\"1662\":\"Machine Learning\",\"1570\":\"Machine Learning\",\"1573\":\"Machine Learning\",\"505\":\"Machine Learning\",\"4089\":\"Machine Learning\",\"4083\":\"Machine Learning\",\"1592\":\"Machine Learning\",\"203\":\"Machine Learning\",\"1494\":\"Machine Learning\",\"257\":\"Machine Learning\",\"4269\":\"Machine Learning\",\"4088\":\"Machine Learning\",\"711\":\"Machine Learning\",\"1565\":\"Machine Learning\",\"704\":\"Machine Learning\",\"1678\":\"Machine Learning\",\"1342\":\"Machine Learning\",\"1648\":\"Machine Learning\",\"3761\":\"Machine Learning\",\"1496\":\"Machine Learning\",\"1582\":\"Machine Learning\",\"1048\":\"Machine Learning\",\"1449\":\"Machine Learning\",\"1879\":\"Machine Learning\",\"1620\":\"Machine Learning\",\"4971\":\"Machine Learning\",\"1542\":\"Machine Learning\",\"4898\":\"Machine Learning\",\"4279\":\"Machine Learning\",\"249\":\"Machine Learning\",\"1680\":\"Machine Learning\",\"5677\":\"Machine Learning\",\"1599\":\"Machine Learning\",\"2341\":\"Machine Learning\",\"1661\":\"Machine Learning\",\"1691\":\"Machine Learning\",\"1053\":\"Machine Learning\",\"3139\":\"Machine Learning\",\"2335\":\"Machine Learning\",\"4274\":\"Machine Learning\",\"5722\":\"Machine Learning\",\"1871\":\"Machine Learning\",\"229\":\"Machine Learning\",\"1597\":\"Machine Learning\",\"1432\":\"Machine Learning\",\"2837\":\"Machine Learning\",\"1576\":\"Machine Learning\",\"3194\":\"Machine Learning\",\"4673\":\"Machine Learning\",\"4296\":\"Machine Learning\",\"3764\":\"Machine Learning\",\"243\":\"Machine Learning\",\"1464\":\"Machine Learning\",\"5675\":\"Machine Learning\",\"1578\":\"Machine Learning\",\"5667\":\"Machine Learning\",\"201\":\"Machine Learning\",\"500\":\"Machine Learning\",\"1538\":\"Machine Learning\",\"4299\":\"Machine Learning\",\"1138\":\"Machine Learning\",\"3190\":\"Machine Learning\",\"748\":\"Machine Learning\",\"3188\":\"Machine Learning\",\"4295\":\"Machine Learning\",\"3876\":\"Machine Learning\",\"1450\":\"Machine Learning\",\"483\":\"Machine Learning\",\"1580\":\"Machine Learning\",\"4267\":\"Machine Learning\",\"1607\":\"Machine Learning\",\"1446\":\"Machine Learning\",\"1703\":\"Machine Learning\",\"491\":\"Machine Learning\",\"706\":\"Machine Learning\",\"4287\":\"Machine Learning\",\"501\":\"Machine Learning\",\"1501\":\"Machine Learning\",\"240\":\"Machine Learning\",\"492\":\"Machine Learning\",\"703\":\"Machine Learning\",\"4468\":\"Machine Learning\",\"1054\":\"Machine Learning\",\"1553\":\"Machine Learning\",\"4281\":\"Machine Learning\",\"2343\":\"Machine Learning\",\"1643\":\"Machine Learning\",\"1468\":\"Machine Learning\",\"1142\":\"Machine Learning\",\"1479\":\"Machine Learning\",\"1047\":\"Machine Learning\",\"493\":\"Machine Learning\",\"4305\":\"Machine Learning\",\"5680\":\"Machine Learning\",\"4306\":\"Machine Learning\",\"1626\":\"Machine Learning\",\"199\":\"Machine Learning\",\"4879\":\"Machine Learning\",\"1444\":\"Machine Learning\",\"1628\":\"Machine Learning\",\"654\":\"Machine Learning\",\"1608\":\"Machine Learning\",\"202\":\"Machine Learning\",\"1059\":\"Machine Learning\",\"494\":\"Machine Learning\",\"2340\":\"Machine Learning\",\"4880\":\"Machine Learning\",\"260\":\"Machine Learning\",\"1602\":\"Machine Learning\",\"230\":\"Machine Learning\",\"1353\":\"Machine Learning\",\"1457\":\"Machine Learning\",\"5674\":\"Machine Learning\",\"1652\":\"Machine Learning\",\"1498\":\"Machine Learning\",\"4275\":\"Machine Learning\",\"4285\":\"Machine Learning\",\"1539\":\"Machine Learning\",\"1641\":\"Machine Learning\",\"1603\":\"Machine Learning\",\"1866\":\"Machine Learning\",\"1486\":\"Machine Learning\",\"772\":\"Machine Learning\",\"1683\":\"Machine Learning\",\"1604\":\"Machine Learning\",\"1878\":\"Machine Learning\",\"4676\":\"Machine Learning\",\"1051\":\"Machine Learning\",\"4278\":\"Machine Learning\",\"2324\":\"Machine Learning\",\"4308\":\"Machine Learning\",\"3762\":\"Machine Learning\",\"1355\":\"Machine Learning\",\"1606\":\"Machine Learning\",\"1634\":\"Machine Learning\",\"1616\":\"Machine Learning\",\"1617\":\"Machine Learning\",\"251\":\"Machine Learning\",\"1135\":\"Machine Learning\",\"1657\":\"Machine Learning\",\"1350\":\"Machine Learning\",\"1700\":\"Machine Learning\",\"1512\":\"Machine Learning\",\"233\":\"Machine Learning\",\"1045\":\"Machine Learning\",\"3757\":\"Machine Learning\",\"5678\":\"Machine Learning\",\"236\":\"Machine Learning\",\"975\":\"Machine Learning\",\"5669\":\"Machine Learning\",\"1645\":\"Machine Learning\",\"4082\":\"Machine Learning\",\"4293\":\"Machine Learning\",\"484\":\"Machine Learning\",\"1529\":\"Machine Learning\",\"1456\":\"Machine Learning\",\"1495\":\"Machine Learning\",\"1556\":\"Machine Learning\",\"5691\":\"Machine Learning\",\"206\":\"Machine Learning\",\"487\":\"Machine Learning\",\"1636\":\"Machine Learning\",\"1658\":\"Machine Learning\",\"252\":\"Machine Learning\",\"1870\":\"Machine Learning\",\"1532\":\"Machine Learning\",\"4897\":\"Machine Learning\",\"1666\":\"Machine Learning\",\"490\":\"Machine Learning\",\"4310\":\"Machine Learning\",\"2323\":\"Machine Learning\",\"1483\":\"Machine Learning\",\"4264\":\"Machine Learning\",\"248\":\"Machine Learning\",\"649\":\"Machine Learning\",\"1490\":\"Machine Learning\",\"502\":\"Machine Learning\",\"200\":\"Machine Learning\",\"261\":\"Machine Learning\",\"1558\":\"Machine Learning\",\"1510\":\"Machine Learning\",\"1232\":\"Machine Learning\",\"1659\":\"Machine Learning\",\"642\":\"Machine Learning\",\"1057\":\"Machine Learning\",\"2336\":\"Machine Learning\",\"1605\":\"Machine Learning\",\"1674\":\"Machine Learning\",\"4268\":\"Machine Learning\",\"1528\":\"Machine Learning\",\"1471\":\"Machine Learning\",\"4303\":\"Machine Learning\",\"1600\":\"Machine Learning\",\"495\":\"Machine Learning\",\"4877\":\"Machine Learning\",\"705\":\"Machine Learning\",\"4899\":\"Machine Learning\",\"3897\":\"Machine Learning\",\"5670\":\"Machine Learning\",\"1439\":\"Machine Learning\",\"2345\":\"Machine Learning\",\"1541\":\"Machine Learning\",\"482\":\"Machine Learning\",\"5671\":\"Machine Learning\",\"4878\":\"Machine Learning\",\"1554\":\"Machine Learning\",\"4304\":\"Machine Learning\",\"497\":\"Machine Learning\",\"485\":\"Machine Learning\",\"235\":\"Machine Learning\",\"2342\":\"Machine Learning\",\"1706\":\"Machine Learning\",\"4081\":\"Machine Learning\",\"1551\":\"Machine Learning\",\"1673\":\"Machine Learning\",\"2346\":\"Machine Learning\",\"239\":\"Machine Learning\",\"1595\":\"Machine Learning\",\"707\":\"Machine Learning\",\"1552\":\"Machine Learning\",\"1585\":\"Machine Learning\",\"4283\":\"Machine Learning\",\"1589\":\"Machine Learning\",\"708\":\"Machine Learning\",\"1701\":\"Machine Learning\",\"4307\":\"Machine Learning\",\"4300\":\"Machine Learning\",\"1544\":\"Machine Learning\",\"5676\":\"Machine Learning\",\"1690\":\"Machine Learning\",\"1875\":\"Machine Learning\",\"3751\":\"Machine Learning\",\"3136\":\"Machine Learning\",\"4290\":\"Machine Learning\",\"1564\":\"Machine Learning\",\"3763\":\"Machine Learning\",\"3760\":\"Machine Learning\",\"700\":\"Machine Learning\",\"5679\":\"Machine Learning\",\"1502\":\"Machine Learning\",\"5067\":\"Machine Learning\",\"1612\":\"Machine Learning\",\"1534\":\"Machine Learning\",\"1627\":\"Machine Learning\",\"2338\":\"Machine Learning\",\"1349\":\"Machine Learning\",\"5687\":\"Machine Learning\",\"1531\":\"Machine Learning\",\"1610\":\"Machine Learning\",\"1644\":\"Machine Learning\",\"1536\":\"Machine Learning\",\"3750\":\"Machine Learning\",\"1485\":\"Machine Learning\",\"2326\":\"Machine Learning\",\"1463\":\"Machine Learning\",\"3189\":\"Machine Learning\",\"1874\":\"Machine Learning\",\"3193\":\"Machine Learning\",\"1598\":\"Machine Learning\",\"246\":\"Machine Learning\",\"1441\":\"Machine Learning\",\"1434\":\"Machine Learning\",\"1445\":\"Machine Learning\",\"1590\":\"Machine Learning\",\"1225\":\"Machine Learning\",\"4291\":\"Machine Learning\",\"1049\":\"Machine Learning\",\"1557\":\"Machine Learning\",\"3134\":\"Machine Learning\",\"1667\":\"Machine Learning\",\"1594\":\"Machine Learning\",\"3755\":\"Machine Learning\",\"5681\":\"Machine Learning\",\"1447\":\"Machine Learning\",\"1505\":\"Machine Learning\",\"1509\":\"Machine Learning\",\"1473\":\"Machine Learning\",\"4272\":\"Machine Learning\",\"3192\":\"Machine Learning\",\"4954\":\"Machine Learning\",\"4301\":\"Machine Learning\",\"3747\":\"Machine Learning\",\"1453\":\"Machine Learning\",\"1567\":\"Machine Learning\",\"1452\":\"Machine Learning\",\"1574\":\"Machine Learning\",\"1437\":\"Machine Learning\",\"1561\":\"Machine Learning\",\"4309\":\"Machine Learning\",\"1460\":\"Machine Learning\",\"1522\":\"Machine Learning\",\"1587\":\"Machine Learning\",\"1671\":\"Machine Learning\",\"1227\":\"Machine Learning\",\"648\":\"Machine Learning\",\"1459\":\"Machine Learning\",\"5668\":\"Machine Learning\",\"1500\":\"Machine Learning\",\"1682\":\"Machine Learning\",\"1669\":\"Machine Learning\",\"1476\":\"Machine Learning\",\"4265\":\"Machine Learning\",\"238\":\"Machine Learning\",\"1343\":\"Machine Learning\",\"1235\":\"Machine Learning\",\"5673\":\"Machine Learning\",\"5689\":\"Machine Learning\",\"1581\":\"Machine Learning\",\"1354\":\"Machine Learning\",\"1056\":\"Machine Learning\",\"5838\":\"Machine Learning\",\"4292\":\"Machine Learning\",\"1697\":\"Machine Learning\",\"1524\":\"Machine Learning\",\"3765\":\"Machine Learning\",\"1684\":\"Machine Learning\",\"1474\":\"Machine Learning\",\"1137\":\"Machine Learning\",\"1695\":\"Machine Learning\",\"4271\":\"Machine Learning\",\"4675\":\"Machine Learning\",\"1140\":\"Machine Learning\",\"1347\":\"Machine Learning\",\"4266\":\"Machine Learning\",\"5688\":\"Machine Learning\",\"1055\":\"Machine Learning\",\"1530\":\"Machine Learning\",\"3137\":\"Machine Learning\",\"481\":\"Machine Learning\",\"1633\":\"Machine Learning\",\"227\":\"Machine Learning\",\"1506\":\"Machine Learning\",\"2344\":\"Machine Learning\",\"5685\":\"Machine Learning\",\"1577\":\"Machine Learning\",\"1477\":\"Machine Learning\",\"1470\":\"Machine Learning\",\"1519\":\"Machine Learning\",\"1346\":\"Machine Learning\",\"480\":\"Machine Learning\",\"2331\":\"Machine Learning\",\"3196\":\"Machine Learning\",\"652\":\"Machine Learning\",\"3766\":\"Machine Learning\",\"1584\":\"Machine Learning\",\"4091\":\"Machine Learning\",\"5672\":\"Machine Learning\"}}"