title,c1body,Topic
top 10 javascript charting libraries every data visualization need,read full story,Machine Learning
10 great articles python development,read full story,Machine Learning
programming languages software engineering machine learning,week dr. tim scarfe dr. keith duggar yannic `` lightspeed '' kilcher conversation microsoft senior software engineer sachin kundu speak programming languages including favourites functional programming vs oop next speak software engineering intersection software engineering machine learning also talk applications ml finally makes exceptional software engineer tech lead sachin expert field hope enjoy conversation spoiler alert many read mythical man-month frederick p brooks 00:00:00 introduction 00:06:37 programming languages 00:53:41 applications ml 01:55:59 makes exceptional se tech lead 01:22:08 outro,Machine Learning
p persistent anti-muslim bias large language models,highlights authors show gpt-3 contains strong muslim-violence bias authors test different ways including setup generate humans-of-new-york-style captions ones muslims generate violent captions paper https //arxiv.org/pdf/2101.05783.pdf link comments,Machine Learning
love america,love america,Machine Learning
brussels choice numberphile,neil sloane oeis discusses choix de bruxelles check brilliant get 20 premium service https //brilliant.org/numberphile sponsor links stuff full description ↓↓↓ neil sloane founded runs oeis https //oeis.org/ brussels choice oeis https //oeis.org/a323454 neil sloane playlist numberphile http //bit.ly/sloane_numberphile neil sloane numberphile podcast https //youtu.be/mnk_mffknuy numberphile supported mathematical sciences research institute msri http //bit.ly/msrinumberphile also supported science sandbox simons foundation initiative dedicated engaging everyone process science https //www.simonsfoundation.org/outreach/science-sandbox/ support math america https //www.mathforamerica.org/ numberphile website http //www.numberphile.com/ numberphile facebook http //www.facebook.com/numberphile numberphile tweets https //twitter.com/numberphile subscribe http //bit.ly/numberphile_sub video brady haran pete mcpartlan patreon http //www.patreon.com/numberphile numberphile t-shirts merch https //teespring.com/stores/numberphile brady 's videos subreddit http //www.reddit.com/r/bradyharan/ brady 's latest videos across channels http //www.bradyharanblog.com/ sign occasional emails http //eepurl.com/ydjl9,Machine Learning
melanie mitchell concepts analogies common sense future ai,melanie mitchell professor computer science portland state university external professor santa fe institute worked written artificial intelligence fascinating perspectives including adaptive complex systems genetic algorithms copycat cognitive architecture places process analogy making core human cognition doctoral work advisors douglas hofstadter john holland today contributed lot important ideas field ai including recent book simply called artificial intelligence guide thinking humans conversation part,Machine Learning
video -- lena voita nlp,https //youtu.be/q0kn_zhhdqy tim spends first 20 minutes talking kenneth stanley episode open endedness lena voita ph.d. student university edinburgh university amsterdam previously research scientist yandex research worked closely yandex translate team still teaches nlp yandex school data analysis created exciting new nlp course website lena-voita.github.io folks need check one well presented blogs ever seen discusses research easily digestable manner lena investigating many fascinating topics machine learning nlp today going talk three papers corresponding blog articles source target contributions nmt predictions -- talks influential dichotomy source prefix neural translation models https //arxiv.org/pdf/2010.10907.pdf https //lena-voita.github.io/posts/source_target_contributions_to_nmt.html information-theoretic probing mdl -- lena proposes technique evaluating model using minimum description length kolmogorov complexity labels given representations rather something basic like accuracy https //arxiv.org/pdf/2003.12298.pdf https //lena-voita.github.io/posts/mdl_probes.html evolution representations transformer lena investigates evolution representations individual tokens transformers -- trained different training objectives mt lm mlm https //arxiv.org/abs/1909.01380 https //lena-voita.github.io/posts/emnlp19_evolution.html panel dr. tim scarfe yannic kilcher sayak paul link comments,Machine Learning
leaf disease classification using pytorch,video build deep learning model using pytorch classify different types diseases cassava leaf images multi-class imageclassification problem kaggle competition competition link https //www.kaggle.com/c/cassava-leaf-disease-classification using tez small trainer library developed one previous videos https //github.com/abhishekkrthakur/tez tutorial kernel https //www.kaggle.com/abhishek/using-tez-in-leaf-disease-classification training kernel https //www.kaggle.com/abhishek/tez-faster-and-easier-training-for-leaf-detection inference kernel https //www.kaggle.com/abhishek/leaf-disease-inference-using-tez please subscribe like video help keep motivated make awesome videos like one buy book approaching almost machine learning problem please visit https //bit.ly/buyaaml follow twitter https //twitter.com/abhi1thakur linkedin https //www.linkedin.com/in/abhi1thakur/ kaggle https //kaggle.com/abhishek instagram https //instagram.com/abhi4ml,Machine Learning
complex number fundamentals lockdown math ep 3,intro geometry complex numbers full playlist https //www.youtube.com/playlist list=plzhqobowtqdp5cveljj1bndouqrahvpev home page https //www.3blue1brown.com brought https //3b1b.co/ldm-thanks beautiful pictorial summary thuynganvu https //twitter.com/thuynganvu/status/1258219199769440257 errors first sketch complex plane `` 2i '' written instead `` -2i '' end writing angle sum identity last term sin beta instead sin alpha q9 terms parentheses include 1/2 sqrt 3 /2 -- -- -- -- -- -- -- -- -- live question setup stats on-screen powered itempool https //itempool.com/ graphing calculator used desmos https //www.desmos.com/ `` complex slide rule '' came geogebra via ben sparks https //www.geogebra.org/m/mbhbdvkr curious animations https //www.3blue1brown.com/faq manim music vincent rubinetti download music bandcamp https //vincerubinetti.bandcamp.com/album/the-music-of-3blue1brown stream music spotify https //open.spotify.com/album/1dvyjws8fbqxhrunag5w5u want contribute translated subtitles help review already made others need approval click gear icon video go subtitles/cc `` add subtitles/cc '' really appreciate helps make lessons accessible people -- -- -- -- -- -- -- -- -- video timeline thanks user `` tieriffic '' 0:00:30 w3 results 0:01:00 w4 prompt 0:02:00 ask would call 'imaginary numbers 0:06:40 startingpoint assumptions 0:10:25 w4 results 0:11:25 q1 prompt 0:12:20 q1 process 0:14:05 rotatingcoordinates 0:16:40 q1 result 0:17:40 q2 0:18:15 q3 prompt 0:19:40 q3 results 0:21:35 rotationanimation 0:22:35 3 facts multiplication 0:25:40 q4 prompt 0:26:10 ask imaginary vs physics j 0:28:15 q4 result 0:31:00 geogebrademo 0:32:10 q5 prompt 0:33:30 q5 results 0:34:00 q5 solution 0:35:55 rotatingimages example 0:37:10 pythonexample 0:38:25 pythonimage rotation example 0:40:35 ask vectors matrices rotation 0:42:40 q6 prompt 0:46:55 q6 results 0:47:25 q6 solution 0:52:20 redefiningangle addition 0:57:20 q7 prompt 0:57:55 ask without complex numbers 1:00:10 q7 results 1:00:55 q7 solution 1:05:45 q8 prompt 1:06:30 ask sum/difference angles 1:09:25 q8 results 1:10:25 q8 solution 1:12:00 desmosexample 1:15:05 bringing together 1:16:25 `` cis '' shorthand explained 1:18:05 q9 prompt 1:19:35 q9 results 1:20:55 closingremarks -- -- -- -- -- -- -- -- -- 3blue1brown channel animating math senses word animate know drill youtube want stay posted new videos subscribe http //3b1b.co/subscribe various social media stuffs website https //www.3blue1brown.com twitter https //twitter.com/3blue1brown reddit https //www.reddit.com/r/3blue1brown instagram https //www.instagram.com/3blue1brown_animations/ patreon https //patreon.com/3blue1brown facebook https //www.facebook.com/3blue1brown,Machine Learning
569936821221962380720 numberphile,number galactic proportions ... see megafavnumbers playlist https //bit.ly/megafavnumbers read contribute links stuff full description ↓↓↓ see full collection video sum three cubes http //bit.ly/sumofcubes cube depicted quite loosely artistically course would likelihood collapse form black hole incredible mass contained radius megafavnumbers video part megafavnumbers collaboration math youtubers celebrating favourite numbers bigger one million make video favourite mega-number upload videos youtube hashtag megafavnumbers megafavnumbers title video added megafavnumbers playlist https //bit.ly/megafavnumbers submissions close wednesday 2nd september 2020 sum three cubes t-shirt https //teespring.com/42-sum-cubes-numberphile thanks prof mike merrifield discussion topic numberphile supported mathematical sciences research institute msri http //bit.ly/msrinumberphile also supported science sandbox simons foundation initiative dedicated engaging everyone process science https //www.simonsfoundation.org/outreach/science-sandbox/ support math america https //www.mathforamerica.org/ numberphile website http //www.numberphile.com/ numberphile facebook http //www.facebook.com/numberphile numberphile tweets https //twitter.com/numberphile subscribe http //bit.ly/numberphile_sub videos brady haran patreon http //www.patreon.com/numberphile numberphile t-shirts merch https //teespring.com/stores/numberphile brady 's videos subreddit http //www.reddit.com/r/bradyharan/ brady 's latest videos across channels http //www.bradyharanblog.com/ sign occasional emails http //eepurl.com/ydjl9,Machine Learning
engineering students like ... part 2,stemerch store https //stemerch.com/ second channel https //www.youtube.com/zachstarhimself n't jerk https //stemerch.com/collections/dont-be-a-jerk double dot https //stemerch.com/collections/cursed-math-memes-i-double-dot fractal designs https //stemerch.com/collections/fractals engineering clock https //stemerch.com/collections/clocks-watches-1 ►follow instagram https //www.instagram.com/zachstar/ twitter https //twitter.com/imzachstar engineering students like ... part 1 https //youtu.be/d5lkjvmgpve support channel https //www.patreon.com/zachstar paypal one time donation https //www.paypal.me/zachstaryt join channel get access perks https //www.youtube.com/channel/ucpcsacbqs-sjevfk_hmfy9w/join animations brainup studios http //brainup.in/ check spanish channel https //www.youtube.com/channel/ucnknu2xqblaspj6ckc8vtpa ►my setup space pictures https //amzn.to/2cc4kqj magnetic floating globe https //amzn.to/2vgpdn0 camera https //amzn.to/2rivyu5 mic https //amzn.to/35bkiri tripod https //amzn.to/2rgmtnl equilibrium tube https //amzn.to/2sowdrh ►check amazon store https //www.amazon.com/shop/zachstar,Machine Learning
p current sota really detailed image similarity search,example set reference shirts shirt different style wanted reverse image similarity search shirts current sota would achieve best accuracy link comments,Machine Learning
image format impact significantly training inference instance segmentation models,images metallic objects superficial damage labelled goal identify defects deep learning model majority images jpg bmp asked get images sane format e.g. png n't possible 'm stuck say 80 jpg 20 bmp even semantic meaning frequency content jpg images bmp images different thus 'm sure 's good idea train model mixture image formats since jpgs n't access original uncompressed images convert remaining bmp jpg course n't know compression ratio jpg images however given dataset mess 's possible even images already jpg converted compression ratio advice link comments,Machine Learning
max lin finishing second r challenge,participated r package recommendation engine competition kaggle two reasons first use r lot learn statistics without r. competition chance give back community r package recommendation engine second day job engineer behind machine learning service cloud product recommendation one popular applications early adopters want use web service competition opportunity stand users ’ shoes identify pain points associated building recommendation system treat r package recommendation binary classification task classifier latex f u p takes input user latex u package latex p predicts whether user install package final submission team name record men combines four classifiers labeled large dots figure 1 four classifiers share form objective function minimizes loss latex l plus regularizers latex r latex j \theta \sum_i l y_i f u_i p_i \theta \lambda r \theta latex f u v classification model latex \theta parameters classification model latex y_i latex u_i latex p_i class label package installed user package i-th training example respectively latex \lambda controls penalty regularizing function chosen using cross validation model 1 baseline model 1 example_model_2.r competition organizer provides baseline model user encoded dummy variables latex \mathbf u package represented seven features latex \mathbf p p_1 \dots p_7\ e.g logarithmic count dependency see r script complete list classification model linear combination user dummy variables package features weight parameters latex f u p \mathbf \theta_u ^t \mathbf u \mathbf \theta_p ^t \mathbf p parameters latex \theta_u latex \theta_p intercept loss function latex l negative logistic log likelihood regularizer model first model establishes strong baseline achieving auc ~0.94 labeled m1b2 figure 1 variant model omits user feature see example_model_1.r also provided contest organizers achieves noticeable lower auc 0.81 shown figure 1 suggests users likely install r packages others next model explore classification model incorporates user variations also package variations model 2 latent factor models contrast model 1 features derived metadata focus user-package rating matrix model 2 consists two components baseline estimates latent factors baseline estimates linear combinations three parts global one parameter latex \mu user one parameter per user latex \mu_u package one parameter per package latex \mu_p latent factors assumes k latent factors user latex \mathbf \beta _u package latex \mathbf \beta _p inner product two factors captures interaction user package classifier model model 2 latex f u p \mu \mu_u \mu_p \mathbf \beta _u^t \mathbf \beta _p kind latent factor model also known “ singular value decomposition ” reported great success previous collaborative filtering studies netlifx prize choose exponential loss function model 2 latex l f u p \exp f u p latex y_i \in 1 -1\ choose exponential loss squared loss 1 exponential loss matches 0–1 loss better squared loss 2 exponential loss differentiable apply l2 regularizers latex r \cdot ||\mu_u||² ||\mu_p||² ||\beta_u||² ||\beta_p||² minimize objective function using stochastic gradient descent number latent factors k chosen cross validation latent factor model works well r package recommendation data achieving auc ~0.97 see m2 family figure 1 plot performance five latent factor models different ks ranging 10 50 labeled m2k10 m2k20 … m2k50 figure 1 k increases latent factor model becomes expressive fit data better resulting higher auc model 3 package lda topic model 3 explore new features used model 1 2 new feature package ’ topic based lda latent dirichlet allocation lda topic package inferred word counts man pages use topics.csv kindly prepared contest organizers map r package one 25 lda topics see topic_models.r details running lda provided contest organizer classification model model 3 similar model 2 model 3 replaces user factors model 2 lda factors weights latex \mathbf _u t=25 replaces package factors model 3 dummy variables latex \mathbf p classification model latex f u v \mu \mu_u \mu_p \mathbf _u^t \mathbf p loss function model 2 regularizer l2 latex r \cdot ||\mu_u||² ||\mu_p||² ||\mathbf _u||² model 3 achieves better auc model 1 resulting auc ~0.97 labeled m3u figure 1 prior m3u explore simpler model users share weights latex \mathbf parameter space becomes smaller model 3 shared parameters labeled m3b figure 1 performs slightly worse model 3 user-specific parameters makes sense unlikely every r user shares interest set lda topics model 4 package task view r packages organized task views e.g. high-performance computing survival analysis time series see page complete list views possible user interested particular task would install one many even packages task view e.g. using install.views r function could improve recommendation know much user interested particular task view use views.csv provided contest organizers map package task view classification model model 4 similar model 3 except lda topics model 3 replaced task views t=29 28 task views plus 1 unknown view performance model 4 labeled m4u figure 1 model 4 performs well better model 1 similarly experiment variant model 4 users share task view parameters latex \mathbf labeled m4b n figure 1 performs worse model 4 per-user parameters finding consistent model 3 r users least r recommendation data set seem different preferences task views ensemble learning combine four classifiers using logistic regression collect out-of-sample training data ensemble learner divide training data three sets 80 10 10 train individual classifier 80 apply trained classifiers 20 combine scores individual classifiers training data ensemble learner individual classifiers combiner evaluated last 10 data set shown figure 1 figure 2 evaluating one fold shift data conduct another folds data used resulting total 10 ensemble learners output 10 ensemble learners averaged produce final predictions ensemble learning works well shown figure 2 combining m1 m2 achieves auc 0.9721 higher either m1 0.94 m2 0.9702 alone combine models include m3 m4 performance gets even better submission combining four models achieves best performance training set among models test set final model achieves auc 0.983289 post-processing see highest among submissions success ensemble training possibly individual models strong performers models diverse different classification models features complement post-processing apply two post-processing steps submitting entry first label user package association training set memorized user package pairs test set already seen training set labels training set recalled used instead second assume user install package p user also installs packages p depends record packages user installs training set well dependent packages given pair u p test set package p found set packages user u ’ installed packages depend ignore prediction output 1.0 instead although dependency assumption completely hold true known contradictory examples training set two filters combined increases absolute auc ~0.004 matters competition razor-thin margins leading submissions develop classification training programs model 2 3 4 python use r explore data run logistic regression glm stats library calculate auc performance rocr library plot results ggplot ggplot2 library programs developed run commodity pc dual-core 2ghz cpu 4g ram make source codes available github although submissions based data contest organizers provide simple linear models means stop many directions worth exploring examples crawling cran analyze rich graphs depends imports suggests enhances r packages open-end contest like sky limit max lin software engineer google research new york city office tech lead google prediction api prior google published research work video content analysis sentiment analysis machine learning cross-lingual information retrieval phd computer science carnegie mellon university would like thank organizers r package recommendation engine competition hard work efforts putting competition together participates competition individuals writes codes spare time nothing expressed may attached employer originally published blog.kaggle.com february 22 2011 max lin finishing second r challenge originally published kaggle blog medium people continuing conversation highlighting responding story,Machine Learning
need accessible technology,day 1 november 17 2020 theme envisioning future tech inclusion chancey fleet new york public library accessible computer science education fall workshop hosted microsoft university washington create university colorado ’ coleman institute took place november 17-19 2020 consisted three half-days talks discussions planning new research dedicated making computer science education learning experiences accessible people disabilities information workshop found https //www.microsoft.com/en-us/research/event/accessible-cs-education-fall-workshop/,Machine Learning
regression metrics machine learning,"tweet share share regression refers predictive modeling problems involve predicting numeric value different classification involves predicting class label unlike classification use classification accuracy evaluate predictions made regression model instead must use error metrics specifically designed evaluating predictions made regression problems tutorial discover calculate error metrics regression predictive modeling projects completing tutorial know regression predictive modeling problems involve predicting numeric value metrics regression involve calculating error score summarize predictive skill model calculate report mean squared error root mean squared error mean absolute error let ’ get started regression metrics machine learning photo gael varoquaux rights reserved tutorial overview tutorial divided three parts regression predictive modeling evaluating regression models metrics regression mean squared error root mean squared error mean absolute error regression predictive modeling predictive modeling problem developing model using historical data make prediction new data answer predictive modeling described mathematical problem approximating mapping function f input variables x output variables called problem function approximation job modeling algorithm find best mapping function given time resources available approximating functions applied machine learning see post machine learning algorithms work regression predictive modeling task approximating mapping function f input variables x continuous output variable regression different classification involves predicting category class label difference classification regression see tutorial difference classification regression machine learning continuous output variable real-value integer floating point value often quantities amounts sizes example house may predicted sell specific dollar value perhaps range 100,000 200,000 regression problem requires prediction quantity regression real-valued discrete input variables problem multiple input variables often called multivariate regression problem regression problem input variables ordered time called time series forecasting problem familiar regression predictive modeling let ’ look might evaluate regression model evaluating regression models common question beginners regression predictive modeling projects calculate accuracy regression model accuracy e.g classification accuracy measure classification regression calculate accuracy regression model skill performance regression model must reported error predictions makes sense think predicting numeric value like height dollar amount ’ want know model predicted value exactly might intractably difficult practice instead want know close predictions expected values error addresses exactly summarizes average close predictions expected values three error metrics commonly used evaluating reporting performance regression model mean squared error mse root mean squared error rmse mean absolute error mae many metrics regression although commonly used see full list regression metrics supported scikit-learn python machine learning library scikit-learn api regression metrics next section let ’ take closer look turn metrics regression section take closer look popular metrics regression models calculate predictive modeling project mean squared error mean squared error mse short popular error metric regression problems also important loss function algorithms fit optimized using least squares framing regression problem “ least squares ” refers minimizing mean squared error predictions expected values mse calculated mean average squared differences predicted expected target values dataset mse 1 n sum n y_i – yhat_i ^2 y_i ’ th expected value dataset yhat_i ’ th predicted value difference two values squared effect removing sign resulting positive error value squaring also effect inflating magnifying large errors larger difference predicted expected values larger resulting squared positive error effect “ punishing ” models larger errors mse used loss function also effect “ punishing ” models inflating average error score used metric create plot get feeling change prediction error impacts squared error example gives small contrived dataset 1.0 values predictions range perfect 1.0 wrong 0.0 0.1 increments squared error prediction expected value calculated plotted show quadratic increase squared error ... calculate error err expected predicted 2 complete example listed example increase mean squared error matplotlib import pyplot sklearn.metrics import mean_squared_error real value expected 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 predicted value predicted 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 calculate errors errors list range len expected calculate error err expected predicted 2 store error errors.append err report error print '/prepre class= '' urvanov-syntax-highlighter-plain-tag ''",Machine Learning
starting business line,hi 'm working business head small company focused developing bespoke ai ml based solutions want diversify industrial monitoring via computer vision using drones currently small team,Machine Learning
jeff atwood stack overflow coding horror,jeff atwood co-founder stack overflow stack exchange websites visited millions people every day much like wikipedia difficult understate impact global knowledge productivity network sites created jeff also author famed coding horror blog founder discourse open-source software project seeks improve quality online community discussions video version available youtube would like get information podcast go https //lexfridman.com/ai connect lexfridman,Machine Learning
hour minute hand clock look identical always determine time,sign brilliant get 20 annual subscription https //brilliant.org/zachstar/ stemerch store https //stemerch.com/ support channel https //www.patreon.com/zachstar paypal one time donation https //www.paypal.me/zachstaryt ►follow instagram https //www.instagram.com/zachstar/ twitter https //twitter.com/imzachstar animations brainup studios http //brainup.in/ ►my setup space pictures https //amzn.to/2cc4kqj magnetic floating globe https //amzn.to/2vgpdn0 camera https //amzn.to/2rivyu5 mic https //amzn.to/35bkiri tripod https //amzn.to/2rgmtnl equilibrium tube https //amzn.to/2sowdrh ►check amazon store https //www.amazon.com/shop/zachstar spoiler answer always tell time clock time 132 moments/configurations 12 hour period would n't know time moments either hand could hour hand configuration would valid first 132 moments comes 12:05 roughly every 5 minutes get another configuration could n't tell time,Machine Learning
grandmaster series inspiring journey ‘ beluga ’ kaggle world 🐋,conversation gábor fodor data scientist h2o.ai kaggle competitions ’ grandmaster series interviews present stories established data scientists kaggle grandmasters h2o.ai share journey inspirations accomplishments interviews intended motivate encourage others want understand takes kaggle grandmaster interview shall sharing interaction gábor fodor better known beluga kaggle world kaggle competitions grandmaster data scientist h2o.ai gabor hails hungary holds master ’ degree mathematics well computer engineering around ten years experience data science domain joined kaggle nine years ago since made quite mark best global rank 4th competitions 7th notebooks also link gábor ’ recent interview ctds.show discusses 10th place solo gold cornell birdcall competition kaggle excerpt conversation gábor q background mathematics transition academia industry happen gábor master ’ mathematics stochastics major certainly provided strong background discrete math probability theory statistics stochastic processes etc although courses mainly focused theory fortunately free take additional courses result got learn programming data mining well final year chance intern data mining trainee telco industry quite interesting retrain improve old drifted churn models however valuable part direct access data warehouse could learn practice sql real-world data business problems internship stayed company became full-time data analyst since chance work different industries working varied types business problems q get interested machine learning kaggle competition tutorial presented gabor kaggle days china 2020 gábor immensely enjoyed data mining courses first data mining competition 2009 quite fun found kaggle got addicted forever time already full-time job started new master ’ computer science finding time new kaggle challenges always easy learning opportunity enormous could resist trying solve unique data-driven problems q hard become kaggle grandmaster initially attracted kaggle first win come way gábor ’ kaggle profile gábor reaching grandmaster ’ status competitions undoubtedly demanding one needs five gold medals different competitions least one solo gold requires lot effort hard work earn gold every competition instance first competition win came 2013 small research competition 81 teams task recognize bird species audio recordings hundred audio files training time comfortable deep learning tools able win competition template matching spectrograms using random forests competitions became bit difficult since good old days kaggle community grew nowadays hard find competitions less thousand teams q data scientist h2o.ai roles specific areas work gábor along grandmasters h2o.ai gábor joined h2o.ai august like flexibility work different projects besides helping customers using h2o driverless ai pocs also create h2o wave apps test new driverless ai features q best things learned via kaggle apply professional work h2o.ai gábor hear way often kaggle competitions participants fight 4th decimals leaderboard differences significant well much bigger victories e.g. recently finished lyft motion prediction competition philipp team 8 improvement second team even race much closer turn rocks squeeze every possible gain features models experience also teaches get robust baseline model fast criticism hear competitions reward overfitting data leaks agree data leaks could significant issue exploit win competitions overfitting rewarded kaggle quite opposite competition ’ receive feedback final test set saw experienced quite brutal shake-ups best validation strategies stable models survived data leaks quite common real world see — good true- auc result start think immediately cause seeing possible data leaks previous kaggle challenges helps debug machine learning pipeline quicker q team grandmasters h2o.ai would gábor good question recently created membership network kaggle team h2o.ai mostly largely connected ecosystem team directly anyone ’ pick single person many talented kagglers probably team 2021 q data science domain rapidly evolving manage keep latest developments gabor presenting kaggle days paris 2019 gábor think impossible keep everything besides fun like kaggle competitions show tools work best specific problems learn lot reading competition winning solutions trying apply tips tricks next competition teach lot quite stuff catch regarding natural language processing reinforcement learning fortunately team h2o.ai experts every field hand also means tools getting better recent cornell birdcall competition could train models hundred code lines pytorch look driverless ai clicks could solve sorts supervised machine learning problems q word advice data science aspirants started wish start data science journey gábor ’ afraid start prepare long run community enormous willing share already learned basics want get hands dirty recommend participating kaggle competitions personally lot takeaways interaction firstly data science area one needs self-motivated eager learn every stage secondly always much learn every machine learning competition even perform well important thing identify weak points work leveraging strengths end community around always ready help flourishing kaggle community testimony fact originally published post grandmaster series inspiring journey ‘ beluga ’ kaggle world 🐋 appeared first open source leader ai ml,Machine Learning
story flash fill shaped,occasion receiving influential test-of-time paper award popl 2011 paper describes technology behind popular flash fill feature excel sumit shares stories related inspiration problem definition paper solution strategy impact came stories span decade paper paper important turning point research philosophy career stories speak learnings much cultural technical influential test time paper https //www.microsoft.com/en-us/research/publication/automating-string-processing-spreadsheets-using-input-output-examples/ prose research engineering team https //www.microsoft.com/en-us/research/group/prose/,Machine Learning
pamela mccorduck machines think early days ai,pamela mccorduck author written history philosophical significance artificial intelligence future engineering role women technology books include machines think 1979 fifth generation 1983 ed feigenbaum considered father expert systems edge chaos futures women literary work spent lot time seminal figures artificial intelligence includes founding fathers ai 1956 dartmouth summer workshop field launched,Machine Learning
every time,every time,Machine Learning
tensorflow model.predict vs model.predict_on_batch impact predictions result,hello hope y'all well 'm wondering 's difference terms predictions quality using .predict .predict_on_batch aside inner workings processing inputs specified batch size taking one input one batch anything else materially impact predictions using prediction methods theory produce predictions future model prediction workflow consistency 'm curious 's anything watch thanks advance help link comments,Machine Learning
get started recommender systems,recommender systems may common type predictive model average person may encounter provide basis recommendations services amazon spotify youtube recommender systems huge daunting topic ’ getting started myriad data preparation techniques algorithms model evaluation methods techniques relevant fact state-of-the-art ignored likely get good results focusing fundamentals e.g treat straightforward classification regression problem important know basics laid systematic way recommend skimming reading standard books papers topic looking popular libraries tutorial discover resources use get started recommender systems completing tutorial know top review papers recommender systems use quickly understand state field top books recommender systems learn algorithms techniques required developing evaluating recommender systems top python libraries apis use prototype develop recommender systems let ’ get started get started recommender systems photo paul toogood right reserved tutorial overview tutorial divided three parts papers recommender systems books recommender systems recommender systems libraries papers recommender systems research papers recommender systems help quickly get speed state field specifically review papers use precise language define recommender system algorithms used standard datasets metrics comparing algorithms hints state art techniques skimming reading handful review papers recommender systems quickly develop foundation dive deeper start developing systems field change quickly techniques 10 20 years ago give solid results review papers recommender systems recommended establish foundational understanding include amazon.com recommendations item-to-item collaborative filtering 2003 matrix factorization techniques recommender systems 2009 recommender systems 2012 recommender systems survey 2013 advances collaborative filtering 2015 matrix factorization techniques recommender systems questions specific techniques find papers focus techniques dive deeper search papers specific techniques google scholar know additional good review papers recommender systems let know comments books recommender systems books recommender systems provide space lay field take tour techniques give detail need understand breadth detail much shorter review paper given field quite mature older books published decade ago immediately neglected top textbooks published key researchers field include following recommender systems introduction 2010 recommender systems textbook 2016 hard copy “ recommender systems introduction ” recommend highly enough book offers overview approaches developing state-of-the-art recommender systems authors present current algorithmic approaches generating personalized buying proposals collaborative content-based filtering well interactive knowledge- based approaches also discuss measure effectiveness recommender systems illustrate methods practical case studies — recommender systems introduction 2010 table contents book follows chapter 1 introduction chapter 2 collaborative recommendation chapter 3 content-based recommendation chapter 4 knowledge-based recommendation chapter 5 hybrid recommendation approaches chapter 6 explanations recommender systems chapter 7 evaluating recommender systems chapter 8 case study personalized game recommendations mobile internet chapter 9 attacks collaborative recommender systems chapter 10 online consumer decision making chapter 11 recommender systems next-generation web chapter 12 recommendations ubiquitous environments chapter 13 summary outlook recommender systems introduction good get handbook topic chapters written different academics summarizing championing preferred techniques methods recommend handbook recommender systems handbook 2015 looking hands-on book recommend practical recommender systems 2019 read one books know another great book topic let know comments recommender systems libraries probably ’ need dive start art least immediately standard machine learning libraries great place start example develop effective recommender system using matrix factorization methods svd even straight forward k-nearest neighbors model items users recommend starting experiments scikit-learn scikit-learn python machine learning library practice standard recommender system datasets data yet accessible available want get hang things first popular standard datasets recommender systems include movielens yahoo datasets music urls movies etc ready state-of-the-art techniques great place start “ papers code ” lists academic papers links source code methods described paper papers code recommendation systems number proprietary open-source libraries services recommender systems recommend sticking open-source python libraries beginning surprise python scikit building analyzing recommender systems case recommender flexible extensible python framework recommender systems used libraries develop recommender system let know comments summary tutorial discovered resources use get started recommender systems specifically learned top review papers recommender systems use quickly understand state field top books recommender systems learn algorithms techniques required developing evaluating recommender systems top python libraries apis use prototype develop recommender systems questions ask questions comments best answer post get started recommender systems appeared first machine learning mastery,Machine Learning
searching rh counterexamples — deploying docker,"’ ironically searching counterexamples riemann hypothesis setting pytest adding database search strategies unbounded integers article ’ deploy application server search rh counterexamples even close laptop servers containers deploying applications servers reproducibility crucial ’ want application depend details computer ’ running higher-level version principle behind python virtual environments applies collections programs possibly written different languages running different computers case postgres database pgmp extension populate_database program plans web server principle application depending system ’ running called hermeticity noun form hermetic meaning air-tight hermeticity good following reasons server crashes ’ remember set instead run build/install works machine newcomers also ’ guess unknown aspects running server sensitive another benefit test local machine identically run production also allows easily migrate one cloud provider another allows defer expensive commitments information application ’ needs finally multiple applications running server ’ want needs conflict happen easily two applications dependencies transitively depend different versions software called “ dependency hell. ” protect becoming dependent arbitrary choices made knew better one industry-strength approach hermeticity use containers container virtual machine devoted running single program explicitly-defined exposure outside world set three containers one run database one search application later one web server ’ start deploying machine could also deploy different machines docker popular containerization system going stress docker like sacred means decade docker may disappear principle hermeticity need reproducible deployments persist docker allows describe container first starting existing trusted container one operating system postgres already installed extend application includes installing dependencies fetching application code git copying files container host system exposing container ’ network ports launching application save commands accomplish dockerfile special syntax deploy copy dockerfile server say via git run docker commands launch container get dockerfile right test locally work server caveat ’ seen migrate server different processor architecture install script case pip install numba may fail find pre-compiled binary target architecture may fall back compiling source add additional requirements force change os container derived reduces “ set new server ” script operations 1 install docker 2 fetch repository 3 launch docker containers respective dockerfiles experience writing dockerfile small task figuring install stuff awful cases docker gives artifact tracing steps reasonable expectation thankfully dear readers skip head-banging see dockerfiles figured postgres dockerfile commit adds dockerfile database makes small changes project allow run 15 lines took hours figure process similar installing confusing software machine try install see error like `` missing postgres.h “ go hunt around internet figure install get past error repeat let ’ go line dockerfile postgres:12 first line defines container image container starts officially maintained postgres team looking dockerfile starts debian buster-slim debian linux instance “ slimmed ” suitable docker containers meaning packages pre-installed importantly “ debian ” tells us package manager use apt-get dockerfile ’ also worth noting point docker builds container command docker file results new image image serialized copy data docker container started extended easily change line halfway dockerfile docker rebuild images step onward publish images web docker users use base like forking project github exactly happens docker executes postgres:12 env postgres_user docker env postgres_password docker env postgres_db divisor lines declare configuration database base postgres image create container started variable names described “ environment variables ” section postgres image ’ documentation env command tells docker instantiate environment variables like path variable terminal shell running programs access ’ insecurely showing password username server docker containers run ’ yet expose anything outside world later post see pass environment variable docker command line container run would use something close set configuration secrets securely run apt-get update apt-get install -y pgxnclient build-essential libgmp3-dev postgresql-server-dev-12 libmpc-dev run command allows run shell command ’ like case command update apt install dependencies needed build pgmp extension includes gcc make via build-essential gmp-specific libraries run apt-get install -y python3.7 python3-setuptools python3-pip python-pip python3.7-dev pip3 install wheel pip install six next something bit strange install python3.7 pip need pip3 install project ’ requirements.txt also python2 ’ pip ’ ’ going pgmp postgres extension needs built source dependency python2.7 python2-six library first run line installs python-related tools need run pgxn install pgmp install pgmp extension copy /divisor workdir `` /divisor '' next two lines copy current directory host machine container ’ file system sets working directory future commands directory note whenever contents project change docker needs rebuild image step subsequent steps like pip install -r requirements.txt might different outcome run python3 -m pip install -- upgrade pip run pip3 install -r requirements.txt next upgrade pip oddly required numba dependency though ’ re-find github issue discovered install python dependencies project reason required included database schema setup python script riemann/postgres_batabase.py makes container bit complicated absolutely necessary improved later need env pguser=docker env pgpassword=docker env pgdatabase=divisor next lines environment variables used psycopg2 python library infer connect postgres database spec passed would nice shared postgres environment variables duplicating problem copy setup_schema.sh /docker-entrypoint-initdb.d/ last line copies script special directory specified base postgres dockerfile base dockerfile specifies scripts directory run container started case call idempotent command create database normal container might specify command run container started search container defined next postgres base image handles us starting postgres database exposing right ports finally build run container docker build -t divisordb -f divisordb.dockerfile ... lots output ... docker run -d -p 5432:5432 -- name divisordb divisordb latest docker build command—which take while—you able see built images running docker images final image special tag divisordb run command additionally tells docker run container daemon a.k.a background -d -p publish port 5432 host machine map 5432 container allows external programs programs computers talk container hitting 0.0.0.0:5432 also allows containers talk container ’ see shortly requires bit work inside container 0.0.0.0 means container host machine finally one run following code host machine check database accepting connections pg_isready -- host 0.0.0.0 -- username docker -- port 5432 -- dbname divisor want get database run queries run psql flags pg_isready manually enter container docker exec -it divisordb bash run psql psql -- host 0.0.0.0 -- username docker -- port 5432 -- dbname divisor password user docker docker divisor= \d list relations schema name type owner -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -+ -- -- -- -- public riemanndivisorsums table docker public searchmetadata table docker 2 rows look wanted disprove riemann hypothesis running docker containers search container next ’ add container main search application help make main entry point program little bit simpler commit modifies populate_database.py ‘ main routine use argparse sensible defaults run application python -m riemann.populate_database dockerfile search part defined commit ’ copied ’ much simpler database somehow took long build database dockerfile originally chose base image called “ alpine ” unknown time really bad python dependencies compiled c code like numba python:3.7-slim-buster run apt-get update apt-get install -y build-essential libgmp3-dev libmpc-dev copy /divisor workdir `` /divisor '' run pip3 install -r requirements.txt env pguser=docker env pgpassword=docker env pgdatabase=divisor entrypoint `` python3 '' `` -m '' `` riemann.populate_database '' base image debian python3.7 pre-installed build almost run docker build -t divisorsearch -f divisorsearch.dockerfile docker run -d -- name divisorsearch -- env pghost= '' pghost '' divisorsearch latest ’ missing pghost environment variable psycopg2 uses find database problem inside container “ localhost ” 0.0.0.0 interpreted operating system mean container host machine get around problem docker maintains ip addresses docker container uses route network requests containers docker inspect command exposes information ’ sample output docker inspect divisordb `` id '' `` f731a78bde50be3de1d77ae1cff6d23c7fe21d4dbe6a82b31332c3ef3f6bbbb4 '' `` path '' `` docker-entrypoint.sh '' `` args '' `` postgres '' `` state '' `` status '' `` running '' `` running '' true `` paused '' false ... ... `` networksettings '' ... `` ports '' `` 5432/tcp '' `` hostip '' `` 0.0.0.0 '' `` hostport '' `` 5432 '' ... `` ipaddress '' `` 172.17.0.2 '' ... part matters us ip address following extracts environment variable pghost export pghost= docker inspect -f `` .networksettings.ipaddress '' divisordb two containers running—see docker ps running containers docker ps -a see containers killed due error docker logs see container ’ logged output—you check database see ’ populated divisor= select searchmetadata order start_time desc limit 10 start_time end_time search_state_type starting_search_state ending_search_state -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -- -- -- -- 2020-12-27 03:10:01.256996 2020-12-27 03:10:03.594773 superabundantenumerationindex 29,1541 31,1372 2020-12-27 03:09:59.160157 2020-12-27 03:10:01.253247 superabundantenumerationindex 26,705 29,1541 2020-12-27 03:09:52.035991 2020-12-27 03:09:59.156464 superabundantenumerationindex 1,0 26,705 ship aws account let ’ use amazon rather try newfangled beanstalks lightsails whatever aws-specific frameworks ’ trying sell ’ provision single ubuntu ec2 server run everything picked t2.micro testing free ’ bit setup configure launch server—such picking server image downloading ssh key finding ip address ’ skip details since yet relevant engineering process server ssh install docker git clone project run deploy script install docker see get.docker.com curl -fssl https //get.docker.com -o get-docker.sh sudo sh get-docker.sh sudo usermod -ag docker ubuntu log log back git clone https //github.com/j2kun/riemann-divisor-sum cd riemann-divisor-sum bash deploy.sh works sadly within hour divisorsearch container crashes instance runs ram cpu upgrading t2.medium 4 gib ram goes 2 hours exhausting ram could profile find memory hotspots instead let ’ apply theorem due billionaire mathematician jim simons throw money problem upgrading r5.large 16 gib ram runs comfortably day four days later logging back vm notice things sluggish even though docker instance ’ exhausting total available ram cpu docker stats also shows low cpu usage divisorsearch database shows got 75 divisors far got ran docker laptop hours last article something amiss ’ explore happened next time notes notes improvements ’ make article deployment rebuild docker containers time even nothing changes one could instead store built images ’ called container registry pull instead re-building every deploy would save us minutes waiting generally good practice could also use docker compose corresponding configuration file coordinate launching collection containers dependencies case divisorsearch container depended divisordb container startup script added sleep 5 ensure latter running starting former docker compose would automatically handle well configuration naming resource limits etc two containers ’ much convenient given docker compose extra layer indirection learn hides lower-level commands article deployed single database container single “ search ” container time database container sitting idle search container magic wanted scale obvious way would multiple workers would require decent feature work sketch reorganize searchmetadata table contains state attribute like “ started ” “ started ” “ finished ” add functionality worker atomically asks oldest “ started ” block updates row ’ state “ started. ” worker finishes block updates database marks block finished “ started ” blocks found worker proceeds create number new “ started ” blocks details ironed around race conditions multiple workers postgres designed make things straightforward finally could reduce database size keeping track summary search block instead storing data block example could record n witness_value corresponding largest witness_value block instead saving every n every witness_value order usable—i.e. us able say “ checked possible",Machine Learning
dealing new possible outputs data driven recommendation engine,right currently serving recommendations deterministic model eventually want move toward using ml model example consider something like netflix assume want use content based clustering model cluster users personas cluster products clustered based metadata within product cluster ranking based internal data process seems pretty straightforward set possible recommendations fixed size n n grows regularly e.g. movies added set possible recommendations industry standard approach type problem 0 comments link comments,Machine Learning
rebel combining deep reinforcement learning search imperfect-information games explained,ai technology poker paper poker alphazero done chess go combination self-play reinforcement learning tree search tremendous success perfect-information games transferring techniques imperfect information games hard problem rebel solve problem provably converges nash equilibrium delivers superhuman heads no-limit hold'em bot little domain knowledge outline 0:00 intro overview 3:20 rock paper double scissor 10:00 alphazero tree search 18:30 notation setup infostates nash equilibria 31:45 one card poker introducing belief representations 45:00 solving games belief representation 55:20 rebel algorithm 1:04:00 theory experiment results 1:07:00 broader impact 1:10:20 high-level summary paper https //arxiv.org/abs/2007.13544 code https //github.com/facebookresearch/rebel blog https //ai.facebook.com/blog/rebel-a-general-game-playing-ai-bot-that-excels-at-poker-and-more/ errata someone last video pointed best poker algorithm best one uses little expert knowledge abstract combination deep reinforcement learning search training test time powerful paradigm led number successes single-agent settings perfect-information games best exemplified alphazero however prior algorithms form cope imperfect-information games paper presents rebel general framework self-play reinforcement learning search provably converges nash equilibrium two-player zero-sum game simpler setting perfect-information games rebel reduces algorithm similar alphazero results two different imperfect-information games show rebel converges approximate nash equilibrium also show rebel achieves superhuman performance heads-up no-limit texas hold'em poker using far less domain knowledge prior poker ai authors noam brown anton bakhtin adam lerer qucheng gong links youtube https //www.youtube.com/c/yannickilcher twitter https //twitter.com/ykilcher discord https //discord.gg/4h8xxdf bitchute https //www.bitchute.com/channel/yannic-kilcher minds https //www.minds.com/ykilcher parler https //parler.com/profile/yannickilcher linkedin https //www.linkedin.com/in/yannic-kilcher-488534136/ want support best thing share content want support financially completely optional voluntary lot people asked subscribestar https //www.subscribestar.com/yannickilcher patreon https //www.patreon.com/yannickilcher bitcoin btc bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq ethereum eth 0x7ad3513e3b8f66799f507aa7874b1b0ebc7f85e2 litecoin ltc lqw2trykyetvc8wjfkhpphtpbdm4vw7r9m monero xmr 4acl8agreo5hair8a9cevrw8peauwvnp1wnsdzxw7tzicdlhzagsgzhrqabdnfy8yum9fwjdvijphkrjv4fwt19cjzn9d4n,Machine Learning
101 – joscha bach artificial consciousness nature reality,joscha bach vp research ai foundation previously research mit harvard joscha work explores workings human mind intelligence consciousness life earth possibly-simulated fabric universe support podcast supporting sponsors – expressvpn https //www.expressvpn.com/lexpod – cash app – use code “ lexpodcast ” download – cash app app store https //apple.co/2spruhe – cash app google play https //bit.ly/2mlvp5w conversation part artificial intelligence podcast would like get information podcast go https //lexfridman.com/ai connect lexfridman twitter linkedin,Machine Learning
25 favorite data science courses harvard udemy,read full story,Machine Learning
pinned looks like bombard earth photons emit roadster hah,looks like bombard earth photons emit roadster hah,Machine Learning
10 amazing articles python programming machine learning week 3,read full story,Machine Learning
nevermined organizations manage monetize data next level solution,post provides short technical overview nevermined ’ capabilities read full story,Machine Learning
whitney cummings comedy robotics neurology love,whitney cummings stand-up comedian actor producer writer director host new podcast called good recent netflix special called “ touch ” features part robot affectionately named bearclaw designed visually replica whitney ’ exciting see one favorite comedians explore social aspects robotics ai society also fascinating ideas human behavior psychology neurology explores book called “ ’ fine…and lies. ” conversation,Machine Learning
logarithm fundamentals lockdown math ep 6,back basics logarithms full playlist https //www.youtube.com/playlist list=plzhqobowtqdp5cveljj1bndouqrahvpev home page https //www.3blue1brown.com brought https //3b1b.co/ldm-thanks triangle power https //youtu.be/sula9lc4pck beautiful pictorial summary thuynganvu https //twitter.com/thuynganvu/status/1258222677573001219 -- -- -- -- -- -- -- -- -- live question setup stats on-screen powered itempool https //itempool.com/ curious animations https //www.3blue1brown.com/faq manim music vincent rubinetti download music bandcamp https //vincerubinetti.bandcamp.com/album/the-music-of-3blue1brown stream music spotify https //open.spotify.com/album/1dvyjws8fbqxhrunag5w5u want contribute translated subtitles help review already made others need approval click gear icon video go subtitles/cc `` add subtitles/cc '' really appreciate helps make lessons accessible people -- -- -- -- -- -- -- -- -- 3blue1brown channel animating math senses word animate know drill youtube want stay posted new videos subscribe http //3b1b.co/subscribe various social media stuffs website https //www.3blue1brown.com twitter https //twitter.com/3blue1brown reddit https //www.reddit.com/r/3blue1brown instagram https //www.instagram.com/3blue1brown_animations/ patreon https //patreon.com/3blue1brown facebook https //www.facebook.com/3blue1brown,Machine Learning
rt ykilcher temporarily breaking twitter-minimization short thread issues around free speech mass deplatformings last week obviously riots terrible people still supporting dt crazy moving things yet said ...,temporarily breaking twitter-minimization short thread issues around free speech mass deplatformings last week obviously riots terrible people still supporting dt crazy moving things yet said ...,Machine Learning
christof koch consciousness,"conversation christof koch part mit course artificial general intelligence video version available youtube president chief scientific officer allen institute brain science seattle 1986 2013 professor caltech cited 105,000 times author several books including “ consciousness confessions romantic reductionist. ” would like get information podcast go https //lexfridman.com/ai connect lexfridman twitter linkedin facebook youtube watch video versions conversations",Machine Learning
matthew johnson psychedelics lex fridman podcast 145,matthew w. johnson professor psychedelics researcher johns hopkins please support podcast checking sponsors brave https //brave.com/lex neuro https //www.getneuro.com use code lex get 15 four sigmatic https //foursigmatic.com/lex use code lexpod get 60 cash app https //cash.app/ use code lexpodcast get 10 episode links matt 's twitter https //twitter.com/drug_researcher matt 's website https //www.hopkinsmedicine.org/profiles/results/directory/profile/0800020/matthew-johnson study website https //hopkinspsychedelic.org/ podcast info podcast website https //lexfridman.com/podcast apple podcasts https //apple.co/2lwqzir spotify https //spoti.fi/2newcf8 rss https //lexfridman.com/feed/podcast/ full episodes playlist https //www.youtube.com/playlist list=plraxtmerzgodp_8gztsuki9nrranbkkp4 clips playlist https //www.youtube.com/playlist list=plraxtmerzgoecifp3cbcieelojeitor41 outline 0:00 introduction 2:02 introduction psychedelics 18:04 psychedelics expand mind 21:16 priors bring psychedelic experience 25:11 elon musk first principles thinking 35:41 dmt 47:03 joe rogan dmt 53:11 nature drug addiction 1:07:00 economics drug pricing 1:13:15 legalize drugs 1:25:18 dangerous drug 1:27:52 drug prohibition work 1:31:46 cocaine sex 1:38:46 risky sexual decisions 1:49:43 psilocybin helping people quit smoking 1:56:01 young jamie 2:18:09 participating study 2:25:28 psychedelics human mind 2:32:51 future psychedelics 2:35:32 neuralink 2:45:05 consciousness 2:57:46 panpsychism 3:07:51 aliens dmt 3:17:55 mortality 3:27:44 meaning life connect subscribe youtube channel twitter https //twitter.com/lexfridman linkedin https //www.linkedin.com/in/lexfridman facebook https //www.facebook.com/lexfridmanpage instagram https //www.instagram.com/lexfridman medium https //medium.com/ lexfridman support patreon https //www.patreon.com/lexfridman,Machine Learning
mitos e verdades sobre automl,todas revoluções que tivemos até hoje tanto tecnológicas quanto industriais possuem uma semelhança elas estão ligadas à forma como os seres humanos lidam com máquinas antes os processos eram feitos de forma muito manual e com tempo acabaram sofrendo uma evolução natural voltada para automação com aprendizado de máquinas não é diferente início muitas tarefas relacionadas ao aprendizado de máquina eram manuais justamente por estarem em uma fase de experimentação porém quando falarmos em aprendizado de máquina automatizado popularmente conhecido como automl estamos falando de um processo de evolução aprendizado de máquina programador quando bastante sagaz vai dar um jeito de automatizar tarefas repetitivas e manuais para otimizar seu tempo normalmente isso é feito através da criação de scripts templates ou até mesmo bibliotecas que reúnem um compilado de todas principais funções que ele necessita para poder desenvolver uma determinada tarefa e consequentemente facilitar sua vida é assim que funciona com automl como qualquer framework ou linguagem de programação é uma ferramenta que será utilizada em benefício usuário que está desenvolvendo modelo com intuito de desmistificar algumas questões relacionadas ao automl destacamos abaixo alguns pontos importantes e que costumam gerar muitas dúvidas inclusive vale lembrar que desmistificação automl também foi pauta da live que gravamos juntos linkedin e twitter da h2o.ai mês passado vocês podem conferir vídeo na íntegra ao fim deste post automl ameaça ou aliado criou-se um mito uma crença equivocada não só automl como também aprendizado de máquina em geral e da inteligência artificial de que eles iriam em algum momento substituir usuário na realidade não é isso que acontece automl é apenas mais uma ferramenta que veio para auxiliar e facilitar trabalho desenvolvedor de modelos de aprendizado de máquina com objetivo de otimizar seu tempo e não para substituí-lo ao utilizar uma ferramenta de automl muitas das tarefas com quais cientista de dados teria que se preocupar como feature engineering selecionar melhor transformador para extração de características partir de um determinado conjunto de dados selecionar melhor modelo etc acabam sendo definidas pela ferramenta e com isso ele acaba tendo mais tempo para focar que realmente interessa para negócio utilizando os seus conhecimentos matemáticos e estatísticos de análise para gerar melhores insights para empresa alguns desafios um dos principais desafios da utilização automl é cultural com falsa impressão de que automl representa uma ameaça os usuários acabaram se tornando resistentes à ideia da automação outro ponto é que muitas vezes ferramenta de automl mais engessa cientista de dados que lhe dá liberdade por isso é fundamental que usuário considere uma boa ferramenta de automl e possibilidade que ele tem de poder estender e expandir essa plataforma para adequá-la às necessidades com seu caso de uso real por exemplo muitas ferramentas de automl dão um determinado conjunto de modelos “ x ” e um conjunto de feature engineering “ ” cientista de dados então tem que adaptar seu caso de uso para ser aplicado na ferramenta e não é assim que deveria funcionar ferramenta precisa ser flexível e extensível ponto de se adaptar às necessidades usuário e não contrário além disso existem algumas outras limitantes como fato de que os modelos precisam ser supervisionados ou seja criação de modelos partir de um conjunto de dados que possui uma variável-alvo mas isso é algo que tem evoluído e é possível que em breve tenhamos oportunidade de trabalhar com modelos não supervisionados dentro de uma plataforma de automl também inclusive esse é um campo de estudo contínuo e já existem aplicações de técnicas com finalidade de alcançar esse objetivo exemplos de sucesso um caso bem interessante que h2o.ai está trabalhando e que podemos citar é de um modelo de classificação de imagens para detecção de corrosão os cientistas de dados da empresa que nos procurou estava trabalhando nesse conjunto de imagens há uns 2 meses 2 meses e meio nessa atividade e com driverless ai trazendo dataset correto nosso time na h2o.ai conseguiu resolver problema em praticamente um dia e é gratificante ver reação das pessoas quando recebem os resultados muitas vezes mais precisos e gerados em um tempo significativamente mais curto para concluir é fundamental enxergarmos automl como peça-chave processo de democratização da inteligência artificial porque ele empodera todo tipo de usuário – desde analista de negócios que necessita utilizar os conhecimentos aprendizado de máquina e inteligência artificial para gerar novos insights para seus negócios até cientista de dados mais conhecimento mais avançado se interessou e quer assistir à gravação da live na íntegra clique aqui post mitos e verdades sobre automl appeared first open source leader ai ml,Machine Learning
hobbies went hiatus kaggler made fighting covid-19 data mission a…,sports everything else cancelled data scientist decided take covid-19 winner ’ interview david mezzettiwhen hobbies went hiatus kaggler david mezzetti made fighting covid-19 mission.photo clay banks unsplashlet ’ learn david https //www.kaggle.com/davidmezzetti david mezzetti founder neuml data analytics machine learning company develops innovative products backed machine learning previously co-founded built data works 50+ person well-respected software services company august 2019 data works acquired dave worked ensure successful transition david tell us background david technical background etl data extraction data engineering data analytics spent decade career developing large-scale data pipelines transform structured unstructured data formats utilized downstream systems also experience building large-scale distributed text search natural language processing nlp systems prior experience domain knowledge helped succeed competition ’ worked data analytics space 15+ years prior knowledge medical documents medical industry get started competing kaggle ’ participated couple march madness competitions looking forward 2020 tournament model excited way season went perfect strengths model ’ never know would performed made decide enter competition 2020 march madness competition cancelled covid-19 really starting hit hard wanted find way get involved help neuml working real-time sports event tracking application neuspo sports along everything else shut sports track sports life hiatus saw kaggle cord-19 challenge felt background able contribute top everything going mom passed away early march high school biology teacher would happy know involved effort also good distraction everything going way feel like could part help beat covid-19 let ’ get technicaltell us overall architecture approach problem solution consisted two main parts sentence embeddings based search index custom bert qa model extract column based answers known summary tables specific list questions query embeddings query identifies list best matching documents common fields including date title authors reference url stored search result columns custom bert qa model developed add additional columns list search results example given search cord-19 dataset “ hypertension ” additional column question “ risk factor developing severe symptoms patients hypertension ” added separate column past research previous competitions inform approach much search logic based prior project codequestion https //github.com/neuml/codequestion codequestion builds sentence embeddings index coding questions match developers questions previously asked questions/answers given already code base took approach starting cord-19 dataset much code still derived codequestion today preprocessing feature engineering cord-19 dataset metadata csv file full list documents along full-text stored separate json files etl process built take csv find corresponding text articles load data sqlite database text broken sentences per document sentences mapped sentence embeddings using bm25 fasttext method described medium article supervised learning methods use search question-answering unsupervised using fasttext+bm25 bert based model qa important concept discovered early importance study design articles considered equal medical community puts weight behind different study types example studies larger sample size i.e patients systematic reviews held higher regard vs mathematical modeling/forecasting articles random forest classifier built analyze articles determine study design based word tokens named entities within article important insight data cord-19 dataset dynamic growing saw almost everyone including took first approach building search index allowed finding documents based matching tokens additionally summarization seen way also add value thought show researchers data particular term concept building previous point study design documents equal value labeling documents study design proved greatly beneficial allowing researchers review document vs showing documents matching tokens additionally tokens show document important articles reference concept introduction discussion sections article ’ cover concept medical articles methods results sections matches sections important surprised findings little expectations entering competition ’ say surprised anything great see many smart capable people working together try help whatever way could tools use work driven kaggle platform list notebooks cover submissions round 1 round 2 cord-19 challenge notebooks python sentence embeddings notebook round 1 known transmission incubation environmental stability know covid-19 risk factors know virus genetics origin evolution know vaccines therapeutics know non-pharmaceutical interventions published medical care know diagnostics surveillance published information sharing inter-sectoral collaboration published ethical social science considerations round 2 task 1 population task 2 relevant factors task 3 patient descriptions task 4 models open questions task 5 materials task 6 diagnostics task 7 therapeutics task 8 risk factors full task csv export list also separate python project github cord19q cord19q logic etl building embeddings index running custom bert qa model spend time competition early days effort spent eda exchanging ideas members community models could built gaining understanding data strengths weaknesses dataset researchers looking cord-19 dataset needed fortunate enough find like-minded data scientists willing roll sleeves write code help discover want data ’ 1–2 months effort machine learning models feature engineering even considered round 1 focused data extraction parsing requirements analysis building system search documents work round 1 led discovering building summary tables extracted answers series questions would beneficial medical community fortunately team medical experts manually curated dataset could used help build machine learning models round 2 bert based qa model developed able extract answers medical documents required building custom question-answer dataset teach model answer medical questions round 2 majority time spent building model hardware setup look like submissions built kaggle platform cpu notebooks development done quad core laptop 8gb gpu 32gb ram fasttext embeddings study design models custom bert qa models built offline using laptop run time training prediction winning solution given data continually updated recurring job runs update using kernelpipes takes 6 hours fully etl build models run solution notebooks kaggle words wisdomwhat taken away competition challenge unique number reasons first known answer real-world problem like would encounter industry someone large dataset ’ sure approach requires iterative process exploring data sharing feedback experts building workflow solve problem data scientists involved effort extremely fortunate guided savanna reid epidemiologist volunteering time also fortunate kaggle heavily involved paul mooney anthony goldbloom helping guide effort fortunate able bounce ideas data scientists working effort specifically ken miller andy white honor volunteer ’ never know true impact contributions made like think small part help looking back would differently entering competition first instinct use sentence embeddings since existing similar project starting would explored different methods search documents see methods performed better advice getting started data science much time spent data preparation feature engineering best way learn data science solve problem ’ interested sports analytics got started data science engaging way stay focused algorithms data — additional medium posts david mezzetti combating covid-19 data science building analysis pipelines kaggle hobbies went hiatus kaggler made fighting covid-19 data mission a… originally published kaggle blog medium people continuing conversation highlighting responding story,Machine Learning
awesome list equivariance neural networks,hi recently stumbled amazing list papers dealing equivariance neural networks disclaimer list found hosted github everyone add work think missing make pr already gives pretty holistic overview guess far complete seen twitter raised quite interest among different people thought sharing might great people either research field relying methods everyday work happy reading everyone interested link comments,Machine Learning
need apache spark,read full story,Machine Learning
neural networks part 3 relu action,relu activation function one popular activation functions deep learning convolutional neural networks however function deceptively simple statquest walks example step-by-step uses relu activation function see exactly works ⭐ note code use kite free ai-powered coding assistant help code faster smarter kite plugin integrates top editors ides give smart completions documentation ’ typing love https //www.kite.com/get-kite/ utm_medium=referral utm_source=youtube utm_campaign=statquest utm_content=description-only complete index statquest videos check https //statquest.org/video-index/ 'd like support statquest please consider ... patreon https //www.patreon.com/statquest ... ... youtube membership https //www.youtube.com/channel/uctyluttgs3k1fg4y5tahlbw/join ... cool statquest t-shirt sweatshirt usa/europe https //teespring.com/stores/statquest everywhere https //www.redbubble.com/people/starmer/works/40421224-statquest-double-bam asc=u p=t-shirt ... buying one two songs go large get whole album https //joshuastarmer.bandcamp.com/ ... donating statquest https //www.paypal.me/statquest lastly want keep research create new statquests follow twitter https //twitter.com/joshuastarmer 0:00 awesome song introduction 1:45 relu hidden layer 5:35 relu right output 7:38 derivative relu statquest neuralnetworks relu,Machine Learning
dmitri dolgov waymo future self-driving cars lex fridman podcast 147,dmitri dolgov cto waymo autonomous vehicle company please support podcast checking sponsors tryolabs https //tryolabs.com/lex blinkist https //blinkist.com/lex use code lex get 25 premium betterhelp https //betterhelp.com/lex get 10 cash app https //cash.app/ use code lexpodcast get 10 episode links waymo 's twitter https //twitter.com/waymo waymo 's website https //waymo.com podcast info podcast website https //lexfridman.com/podcast apple podcasts https //apple.co/2lwqzir spotify https //spoti.fi/2newcf8 rss https //lexfridman.com/feed/podcast/ full episodes playlist https //www.youtube.com/playlist list=plraxtmerzgodp_8gztsuki9nrranbkkp4 clips playlist https //www.youtube.com/playlist list=plraxtmerzgoecifp3cbcieelojeitor41 outline 0:00 introduction 2:16 computer games 7:23 childhood 9:55 robotics 10:44 moscow institute physics technology 12:56 darpa urban challenge 23:16 waymo origin story 38:58 waymo self-driving hardware 47:31 connected cars 53:23 waymo fully driverless service phoenix 57:45 getting feedback riders 1:05:58 creating product people love 1:11:49 self-driving cars need break rules like humans 1:18:33 waymo trucks 1:24:11 future waymo 1:37:23 role lidar autonomous driving 1:50:23 machine learning essential autonomous driving 1:54:25 pedestrians 2:01:02 trolley problem 2:05:30 book recommendations 2:16:56 meaning life connect subscribe youtube channel twitter https //twitter.com/lexfridman linkedin https //www.linkedin.com/in/lexfridman facebook https //www.facebook.com/lexfridmanpage instagram https //www.instagram.com/lexfridman medium https //medium.com/ lexfridman support patreon https //www.patreon.com/lexfridman,Machine Learning
147 – dmitri dolgov waymo future self-driving cars,dmitri dolgov cto waymo autonomous vehicle company please support podcast checking sponsors – tryolabs https //tryolabs.com/lex – blinkist https //blinkist.com/lex use code lex get 25 premium – betterhelp https //betterhelp.com/lex get 10 – cash app https //cash.app/ use code lexpodcast get 10 episode links waymo ’ twitter https //twitter.com/waymo waymo ’ website https //waymo.com podcast info podcast website https //lexfridman.com/podcast apple podcasts https //apple.co/2lwqzir spotify https //spoti.fi/2newcf8 rss https //lexfridman.com/feed/podcast/ youtube full episodes https //youtube.com/lexfridman youtube clips https //youtube.com/lexclips support connect – check sponsors ’ best way support podcast – support patreon https //www.patreon.com/lexfridman – twitter https //twitter.com/lexfridman – instagram https //www.instagram.com/lexfridman – linkedin https //www.linkedin.com/in/lexfridman – facebook https //www.facebook.com/lexfridmanpage – medium https //medium.com/ lexfridman outline ’ timestamps episode podcast players able click timestamp jump time 00:00 – introduction 07:46 – computer games 12:52 – childhood 15:24 – robotics 16:14 – moscow institute physics technology 18:26 – darpa urban challenge 28:46 – waymo origin story 44:27 – waymo self-driving hardware 53:00 – connected cars 58:53 – waymo fully driverless service phoenix 1:03:14 – getting feedback riders 1:11:28 – creating product people love 1:17:18 – self-driving cars need break rules like humans 1:24:03 – waymo trucks 1:29:41 – future waymo 1:42:53 – role lidar autonomous driving 1:55:53 – machine learning essential autonomous driving 1:59:55 – pedestrians 2:06:32 – trolley problem 2:11:00 – book recommendations 2:22:26 – meaning life,Machine Learning
language models open knowledge graphs paper explained,ai research nlp knowledge graphs structured databases capture real-world entities relations kgs usually built human experts costs considerable amounts time money paper hypothesizes language models increased performance dramatically last years contain enough knowledge use construct knowledge graph given corpus without fine-tuning language model resulting system uncover new unknown relations outperforms baselines automated kg construction even trained ones outline 0:00 intro overview 1:40 tabnine promotion 4:20 title misnomer 6:45 corpus knowledge graph 13:40 paper contributions 15:50 candidate fact finding algorithm 25:50 causal attention confusion 31:25 constraints 35:00 mapping facts schemas 38:40 example constructed knowledge graph 40:10 experimental results 47:25 example discovered facts 50:40 conclusion comments paper https //arxiv.org/abs/2010.11967 abstract paper shows construct knowledge graphs kgs pre-trained language models e.g. bert gpt-2/3 without human supervision popular kgs e.g wikidata nell built either supervised semi-supervised manner requiring humans create knowledge recent deep language models automatically acquire knowledge large-scale corpora via pre-training stored knowledge enabled language models improve downstream nlp tasks e.g. answering questions writing code articles paper propose unsupervised method cast knowledge contained within language models kgs show kgs constructed single forward pass pre-trained language models without fine-tuning corpora demonstrate quality constructed kgs comparing two kgs wikidata tac kbp created humans kgs also provide open factual knowledge new existing kgs code kgs made publicly available authors chenguang wang xiao liu dawn song links youtube https //www.youtube.com/c/yannickilcher twitter https //twitter.com/ykilcher discord https //discord.gg/4h8xxdf bitchute https //www.bitchute.com/channel/yannic-kilcher minds https //www.minds.com/ykilcher parler https //parler.com/profile/yannickilcher linkedin https //www.linkedin.com/in/yannic-kilcher-488534136/ want support best thing share content want support financially completely optional voluntary lot people asked subscribestar https //www.subscribestar.com/yannickilcher patreon https //www.patreon.com/yannickilcher bitcoin btc bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq ethereum eth 0x7ad3513e3b8f66799f507aa7874b1b0ebc7f85e2 litecoin ltc lqw2trykyetvc8wjfkhpphtpbdm4vw7r9m monero xmr 4acl8agreo5hair8a9cevrw8peauwvnp1wnsdzxw7tzicdlhzagsgzhrqabdnfy8yum9fwjdvijphkrjv4fwt19cjzn9d4n,Machine Learning
things get weird infinity,get free access 2500 documentaries curiositystream http //go.thoughtleaders.io/1621720200804 use promo code `` zachstar '' sign stemerch store https //stemerch.com/ support channel https //www.patreon.com/zachstar paypal one time donation https //www.paypal.me/zachstaryt cantor set video https //youtu.be/esgogjyj_uw ►follow instagram https //www.instagram.com/zachstar/ twitter https //twitter.com/imzachstar animations brainup studios http //brainup.in/ ►my setup space pictures https //amzn.to/2cc4kqj magnetic floating globe https //amzn.to/2vgpdn0 camera https //amzn.to/2rivyu5 mic https //amzn.to/35bkiri tripod https //amzn.to/2rgmtnl equilibrium tube https //amzn.to/2sowdrh ►check amazon store https //www.amazon.com/shop/zachstar,Machine Learning
george hotz comma.ai openpilot autonomous vehicles,george hotz founder comma.ai machine learning based vehicle automation company outspoken personality field ai technology general first gained recognition first person carrier-unlock iphone since done quite interesting things intersection hardware software conversation part artificial intelligence podcast would like get information podcast go https //lexfridman.com/ai connect lexfridman twitter linkedin facebook medium youtube watch video versions,Machine Learning
machine learning predict loan defaults,visualize insights discover driving features lending credit risk model loan defaults read full story,Machine Learning
5 companies developing computer vision technology 2020,computer vision technology poster child artificial intelligence sector industry gets media attention tools benefits technology provide autonomous vehicles drones cancer detection augmented reality technologies existed science fiction doorstep read full story,Machine Learning
got chance try nvidia a100 gpu via qblocks.cloud,video show fast training using nvidia a100 gpu got try via qblocks.cloud please subscribe like video help keep motivated make awesome videos like one buy book approaching almost machine learning problem please visit https //bit.ly/buyaaml follow twitter https //twitter.com/abhi1thakur linkedin https //www.linkedin.com/in/abhi1thakur/ kaggle https //kaggle.com/abhishek instagram https //instagram.com/abhi4ml,Machine Learning
data science without statistics,thinking high bar enter field lately everyone needs statistician mathematician computer scientist work data science today edit 'm suggesting people learn stats question direction wider field goes automation abstraction tools evolving rapdily https //medium.com/datadriveninvestor/is-there-data-science-without-statistics-70d671649dc3 link comments,Machine Learning
104 – david patterson computer architecture data storage,david patterson turing award winner professor computer science berkeley known pioneering contributions risc processor architecture used 99 new chips today co-creating raid storage impact two lines research development world immeasurable also one great educators computer science world book john hennessy “ computer architecture quantitative approach ” first learned humbled inner workings machines lowest level support podcast,Machine Learning
interface r using python bioinformatics,learn use rpy2 library way interface r using python bioinformatics purposes read full story,Machine Learning
kai-fu lee ai superpowers – china silicon valley,kai-fu lee chairman ceo sinovation ventures manages 2 billion dollar dual currency investment fund focus developing next generation chinese high-tech companies former president google china founder called microsoft research asia institute trained many ai leaders china including ctos ai execs baidu tencent alibaba lenovo huawei named one 100 influential people world time magazine author seven best-selling books chinese,Machine Learning
marcin pionnier finishing 5th rta competition,"graduated warsaw university technology master thesis text mining topic intelligent web crawling methods work polish consulting company sollers consulting develop design various insurance industry related stuff one insurance fraud detection platform time time try compete data mining contests netflix competitions kaggle tunedit.org — perspective good way get real data mining experience tried far remember basis solution defined beginning create separate predictors individual loop time interval solution required build 61x10=610 regression models playing various regression algorithms quickly chose linear regression — results good computation time short think key get quite good result especially public rmse 🙂 set attributes used used following attributes linear regression individual loop time interval — number minutes 0:00 hours current moment “ ” — average drive time given loop interval — loop times current moment number historical moments number time points loop varied methods — differences “ neighboring ” time moments data differences differences transformed logistic function 1/1+e^-difference use logistic function gave jump public rmse 198 189 idea use sigmoid function intuition inspired differences distribution — “ saturations ” loop except 2 first loops directions introduced simple naive model traffic growth speed given loop 40 km/h — saturation 1 difference previous loop given loop 5 km/h assumed road part partially saturated segment moving 30 km/h second segment speed loop given loop saturation derived proportion first segment whole road part loop detector minimal value rtadata file — regression minimal value used predicted value less minimum use historical data — found useless initial tests maybe hastily source data learning testing rtadata lengths files also weekends holidays weather conditions ended working 610 regression models following 3 models competing models trained data availabe rtadata file model 1 61 loops current 5 times moments 5 simple differences — 675 attributes model 2 10 current next 9 loops available less current 9 times moments 9 simple differences saturations current time moment — 204 404 atrributes model 3 10 current next 9 loops available less current 9 times moments 9 sigmoided differences saturations current time moment 204 404 atrributes model least rmse computed train file selected particular loop good strategy however thought generally linear regression resistant overfitting true — number variable grows variance explained — learnt strategy gave public rmse 189.3 added also 4th model used 15 30 minutes predictions arbitrarily model 4 61 loops current 5 times moments 5 sigmoided differences saturations current time moment — 614 attributes turn gave mi 188.6 public result interesting best private solution however selected since relied much public results 190.819 public 197.979 model 3 described combined model 5 model 5 used 15,30,45,60,90 minutes predictions arbitrarily rest model 3 model 5 like model 3 also loop times “ sigmoided ” differences tools used solution written java application weka linked library always try compete data mining contests since linear regression requires solve matrix equation case quite huge memory allocated program becoming important issue 3,5gb one thread — competition using computer 4 processors 12 gb ram — 3 separate threads building testing models whole computation last attempts took 48 hours computations originally published blog.kaggle.com february 17 2011 marcin pionnier finishing 5th rta competition originally published kaggle blog medium people continuing conversation highlighting responding story",Machine Learning
3090 get,'m finally position buy 3090 tossing brand go manufacturer seems cooling system hard rate ml use major differences different brands 's months since rtx 3090 launch 'd love hear people 's experiences currently 'm looking asus zotac link comments,Machine Learning
gary marcus toward hybrid deep learning symbolic ai,gary marcus professor emeritus nyu founder robust.ai geometric intelligence latter machine learning company acquired uber 2016 author several books natural artificial intelligence including new book rebooting ai building machines trust gary critical voice highlighting limits deep learning discussing challenges ai community must solved order achieve artificial general intelligence conversation part artificial intelligence podcast would like get information podcast go,Machine Learning
deal classes located small area feature space,multi-class classification problem one classes trying predict sensitive variations input data additionally input data wide range values simplicity assume one feature x ranges 0 500 three classes b c. samples class mostly located 0,Machine Learning
joerogan 's podcast yesterday air one special moments life joe took favorite watch gifted one heroes means put words 'm worthy 'll work hard live,joerogan 's podcast yesterday air one special moments life joe took favorite watch gifted one heroes means put words 'm worthy 'll work hard live,Machine Learning
141 – erik brynjolfsson economics ai social networks technology,erik brynjolfsson economist stanford please support podcast checking sponsors – vincero https //vincerowatches.com/lex get 25 free shipping – four sigmatic https //foursigmatic.com/lex use code lexpod get 60 – expressvpn https //expressvpn.com/lexpod use code lexpod get 3 months free – cash app https //cash.app/ use code lexpodcast get 10 episode links erik ’ twitter https //twitter.com/erikbryn erik ’ website https //www.brynjolfsson.com/ second machine age book https //amzn.to/33f1pk2 machine platform crowd book https //amzn.to/3mijz76 podcast info podcast website https //lexfridman.com/podcast apple podcasts https //apple.co/2lwqzir spotify https //spoti.fi/2newcf8 rss https //lexfridman.com/feed/podcast/ youtube full episodes https //youtube.com/lexfridman,Machine Learning
85 – roger penrose physics consciousness infinite universe,roger penrose physicist mathematician philosopher university oxford made fundamental contributions many disciplines mathematical physics general relativity cosmology limitations computational view consciousness support podcast signing sponsors – expressvpn https //www.expressvpn.com/lexpod – cash app – use code “ lexpodcast ” download – cash app app store https //apple.co/2spruhe – cash app google play https //bit.ly/2mlvp5w episode links cycles time book https //amzn.to/39txtpp emperor ’ new mind book https //amzn.to/2yfevkd conversation part artificial intelligence podcast would like get information,Machine Learning
choose activation function deep learning,tweet share share last updated january 22 2021 activation functions critical part design neural network choice activation function hidden layer control well network model learns training dataset choice activation function output layer define type predictions model make careful choice activation function must made deep learning neural network project tutorial discover choose activation functions neural network models completing tutorial know activation functions key part neural network design modern default activation function hidden layers relu function activation function output layers depends type prediction problem let ’ get started choose activation function deep learning photo peter dowley rights reserved tutorial overview tutorial divided three parts activation functions activation hidden layers activation output layers activation functions activation function neural network defines weighted sum input transformed output node nodes layer network sometimes activation function called “ transfer function. ” output range activation function limited may called “ squashing function. ” many activation functions nonlinear may referred “ nonlinearity ” layer network design choice activation function large impact capability performance neural network different activation functions may used different parts model technically activation function used within internal processing node network although networks designed use activation function nodes layer network may three types layers input layers take raw input domain hidden layers take input another layer pass output another layer output layers make prediction hidden layers typically use activation function output layer typically use different activation function hidden layers dependent upon type prediction required model activation functions also typically differentiable meaning first-order derivative calculated given input value required given neural networks typically trained using backpropagation error algorithm requires derivative prediction error order update weights model many different types activation functions used neural networks although perhaps small number functions used practice hidden output layers let ’ take look activation functions used type layer turn activation hidden layers hidden layer neural network layer receives input another layer another hidden layer input layer provides output another layer another hidden layer output layer hidden layer directly contact input data produce outputs model least general neural network may zero hidden layers typically differentiable nonlinear activation function used hidden layers neural network allows model learn complex functions network trained using linear activation function order get access much richer hypothesis space would benefit deep representations need non-linearity activation function — page 72 deep learning python 2017 perhaps three activation functions may want consider use hidden layers rectified linear activation relu logistic sigmoid hyperbolic tangent tanh exhaustive list activation functions used hidden layers commonly used let ’ take closer look turn relu hidden layer activation function rectified linear activation function relu activation function perhaps common function used hidden layers common simple implement effective overcoming limitations previously popular activation functions sigmoid tanh specifically less susceptible vanishing gradients prevent deep models trained although suffer problems like saturated “ dead ” units relu function calculated follows max 0.0 x means input value x negative value 0.0 returned otherwise value returned learn details relu activation function tutorial gentle introduction rectified linear unit relu get intuition shape function worked example example plot relu activation function matplotlib import pyplot rectified linear function def rectified x return max 0.0 x define input data inputs x x range -10 10 calculate outputs outputs rectified x x inputs plot inputs vs outputs pyplot.plot inputs outputs pyplot.show running example calculates outputs range values creates plot inputs versus outputs see familiar kink shape relu activation function plot inputs vs. outputs relu activation function using relu function hidden layers good practice use “ normal ” “ uniform ” weight initialization scale input data range 0-1 normalize prior training sigmoid hidden layer activation function sigmoid activation function also called logistic function function used logistic regression classification algorithm function takes real value input outputs values range 0 1 larger input positive closer output value 1.0 whereas smaller input negative closer output 0.0 sigmoid activation function calculated follows 1.0 1.0 e^-x e mathematical constant base natural logarithm get intuition shape function worked example example plot sigmoid activation function math import exp matplotlib import pyplot sigmoid activation function def sigmoid x return 1.0 1.0 exp -x define input data inputs x x range -10 10 calculate outputs outputs sigmoid x x inputs plot inputs vs outputs pyplot.plot inputs outputs pyplot.show running example calculates outputs range values creates plot inputs versus outputs see familiar s-shape sigmoid activation function plot inputs vs. outputs sigmoid activation function using sigmoid function hidden layers good practice use “ xavier normal ” “ xavier uniform ” weight initialization also referred glorot initialization named xavier glorot scale input data range 0-1 e.g range activation function prior training tanh hidden layer activation function hyperbolic tangent activation function also referred simply tanh also “ tanh ” “ tanh “ function similar sigmoid activation function even s-shape function takes real value input outputs values range -1 1 larger input positive closer output value 1.0 whereas smaller input negative closer output -1.0 tanh activation function calculated follows e^x – e^-x e^x e^-x e mathematical constant base natural logarithm get intuition shape function worked example example plot tanh activation function math import exp matplotlib import pyplot tanh activation function def tanh x return exp x exp -x exp x exp -x define input data inputs x x range -10 10 calculate outputs outputs tanh x x inputs plot inputs vs outputs pyplot.plot inputs outputs pyplot.show running example calculates outputs range values creates plot inputs versus outputs see familiar s-shape tanh activation function plot inputs vs. outputs tanh activation function using tanh function hidden layers good practice use “ xavier normal ” “ xavier uniform ” weight initialization also referred glorot initialization named xavier glorot scale input data range -1 1 e.g range activation function prior training choose hidden layer activation function neural network almost always activation function hidden layers unusual vary activation function network model traditionally sigmoid activation function default activation function 1990s perhaps mid late 1990s 2010s tanh function default activation function hidden layers … hyperbolic tangent activation function typically performs better logistic sigmoid — page 195 deep learning 2016 sigmoid tanh functions make model susceptible problems training via so-called vanishing gradients problem learn problem tutorial gentle introduction rectified linear unit relu activation function used hidden layers typically chosen based type neural network architecture modern neural network models common architectures mlp cnn make use relu activation function extensions modern neural networks default recommendation use rectified linear unit relu … — page 174 deep learning 2016 recurrent networks still commonly use tanh sigmoid activation functions even example lstm commonly uses sigmoid activation recurrent connections tanh activation output multilayer perceptron mlp relu activation function convolutional neural network cnn relu activation function recurrent neural network tanh and/or sigmoid activation function ’ unsure activation function use network try compare results figure summarizes choose activation function hidden layers neural network model choose hidden layer activation function activation output layers output layer layer neural network model directly outputs prediction feed-forward neural network models output layer perhaps three activation functions may want consider use output layer linear logistic sigmoid softmax exhaustive list activation functions used output layers commonly used let ’ take closer look turn linear output activation function linear activation function also called “ identity ” multiplied 1.0 “ activation. ” linear activation function change weighted sum input way instead returns value directly get intuition shape function worked example example plot linear activation function matplotlib import pyplot linear activation function def linear x return x define input data inputs x x range -10 10 calculate outputs outputs linear x x inputs plot inputs vs outputs pyplot.plot inputs outputs pyplot.show running example calculates outputs range values creates plot inputs versus outputs see diagonal line shape inputs plotted identical outputs plot inputs vs. outputs linear activation function target values used train model linear activation function output layer typically scaled prior modeling using normalization standardization transforms sigmoid output activation function sigmoid logistic activation function described previous section nevertheless add symmetry review shape function worked example example plot sigmoid activation function math import exp matplotlib import pyplot sigmoid activation function def sigmoid x return 1.0 1.0 exp -x define input data inputs x x range -10 10 calculate outputs outputs sigmoid x x inputs plot inputs vs outputs pyplot.plot inputs outputs pyplot.show running example calculates outputs range values creates plot inputs versus outputs see familiar s-shape sigmoid activation function plot inputs vs. outputs sigmoid activation function target labels used train model sigmoid activation function output layer values 0 1 softmax output activation function softmax function outputs vector values sum 1.0 interpreted probabilities class membership related argmax function outputs 0 options 1 chosen option softmax “ softer ” version argmax allows probability-like output winner-take-all function input function vector real values output vector length values sum 1.0 like probabilities softmax function calculated follows e^x sum e^x x vector outputs e mathematical constant base natural logarithm learn details softmax function tutorial softmax activation function python plot softmax function give example calculating python numpy import exp softmax activation function def softmax x return exp x exp x .sum define input data inputs 1.0 3.0 2.0 calculate outputs outputs softmax inputs report probabilities print outputs report sum probabilities print outputs.sum running example calculates softmax output input vector confirm sum outputs softmax indeed sums value 1.0 0.09003057 0.66524096 0.24472847 1.0 target labels used train model softmax activation function output layer vectors 1 target class 0 classes choose output activation function must choose activation function output layer based type prediction problem solving specifically type variable predicted example may divide prediction problems two main groups predicting categorical variable classification predicting numerical variable regression problem regression problem use linear activation function regression one node linear activation problem classification problem three main types classification problems may use different activation function predicting probability regression problem classification cases classification model predict probability class membership e.g probability example belongs class convert crisp class label rounding sigmoid argmax softmax two mutually exclusive classes binary classification output layer one node sigmoid activation function used two mutually exclusive classes multiclass classification output layer one node per class softmax activation used two mutually inclusive classes multilabel classification output layer one node class sigmoid activation function used binary classification one node sigmoid activation multiclass classification one node per class softmax activation multilabel classification one node per class sigmoid activation figure summarizes choose activation function output layer neural network model choose output layer activation function reading section provides resources topic looking go deeper tutorials gentle introduction rectified linear unit relu softmax activation function python 4 types classification tasks machine learning fix vanishing gradients problem using relu books deep learning 2016 neural smithing supervised learning feedforward artificial neural networks 1999 neural networks pattern recognition 1996 deep learning python 2017 articles activation function wikipedia summary tutorial discovered choose activation functions neural network models specifically learned activation functions key part neural network design modern default activation function hidden layers relu function activation function output layers depends type prediction problem questions ask questions comments best answer tweet share share post choose activation function deep learning appeared first machine learning mastery,Machine Learning
diana walsh pasulka aliens technology religion nature belief lex fridman podcast 149,diana walsh pasulka professor philosophy religion uncw author american cosmic ufos religion technology please support podcast checking sponsors lmnt https //drinklmnt.com/lex get free shipping grammarly https //grammarly.com/lex get 20 premium business wars https //wondery.com/business-wars/ cash app https //cash.app/ use code lexpodcast get 10 `` many alien civilizations '' video https //www.youtube.com/watch v=jtmxa2mveqk episode links diana 's website https //uncw.edu/par/faculty/faculty-pasulka.html american cosmic book https //amzn.to/3ak2kaj podcast info podcast website https //lexfridman.com/podcast apple podcasts https //apple.co/2lwqzir spotify https //spoti.fi/2newcf8 rss https //lexfridman.com/feed/podcast/ full episodes playlist https //www.youtube.com/playlist list=plraxtmerzgodp_8gztsuki9nrranbkkp4 clips playlist https //www.youtube.com/playlist list=plraxtmerzgoecifp3cbcieelojeitor41 outline 0:00 introduction 2:03 real 7:32 beliefs become reality 12:34 donald hoffman 16:33 immanuel kant 's critique pure reason 20:02 ayn rand 27:00 religions start 42:13 religion evolutionary advantage 47:34 religion used propaganda 52:07 nietzsche mean `` god dead '' 57:34 american cosmic 1:01:20 aliens look like 1:10:03 history space programs 1:13:06 jacques vallee 1:22:31 artificial intelligence 1:28:00 ufology community 1:39:13 psychedelics 1:43:10 tic tac ufo 1:51:44 roswell ufo incident 2:02:49 bob lazar 2:06:25 monoliths desert 2:17:14 humans co-evolve ai 2:20:33 neuralink 2:25:23 singularity 2:35:14 books nietzsche 2:40:20 books hannah arendt 2:45:18 fear death 2:49:46 meaning life connect subscribe youtube channel twitter https //twitter.com/lexfridman linkedin https //www.linkedin.com/in/lexfridman facebook https //www.facebook.com/lexfridmanpage instagram https //www.instagram.com/lexfridman medium https //medium.com/ lexfridman support patreon https //www.patreon.com/lexfridman,Machine Learning
118 – grant sanderson math manim neural networks teaching 3blue1brown,grant sanderson math educator creator 3blue1brown support podcast supporting sponsors – dollar shave club https //dollarshaveclub.com/lex – doordash download app use code lex – cash app download app use code “ lexpodcast ” episode links 3blue1brown http //youtube.com/3blue1brown grant ’ twitter https //twitter.com/3blue1brown would like get information podcast go https //lexfridman.com/podcast connect lexfridman twitter linkedin facebook medium youtube watch video versions conversations enjoy podcast please rate 5 stars apple podcasts follow spotify support,Machine Learning
benchmarking gpus right way,hi fellas n't know wrong impression n't succeed finding systematic way benchmarking gpus 'd need way benchmark raw computational power cards field vision nlp tabular leveraging dense sparse matrices way benchmark good cards communicating cpu ​ offer suggestion thanks link comments,Machine Learning
’ difference data science machine learning,read full story,Machine Learning
116 – sara seager search planets life outside solar system,sara seager planetary scientist mit known work search exoplanets support podcast supporting sponsors click links get discount – public goods https //publicgoods.com/lex use code lex – powerdot https //powerdot.com/lex use code lex – cash app – use code “ lexpodcast ” download – cash app app store https //apple.co/2spruhe – cash app google play https //bit.ly/2mlvp5w episode links sara ’ twitter https //twitter.com/profsaraseager sara ’ website https //www.saraseager.com/ smallest lights universe book https //amzn.to/3g3lfha would like get information podcast go https //lexfridman.com/ai connect lexfridman,Machine Learning
p-values error rates false positives,post interpret p-values emphasize p-values error rate number one misinterpretation p-values probability null hypothesis correct correct interpretation p-values indicate probability observing sample data extreme assume … post p-values error rates false positives appeared first statistics jim,Machine Learning
detecting fraudulent transactions,read full story,Machine Learning
rosalind picard affective computing emotion privacy health,rosalind picard professor mit director affective computing research group mit media lab co-founder two companies affectiva empatica two decades ago launched field affective computing book name book described importance emotion artificial natural intelligence vital role emotion communication relationships people general human-robot interaction would like get information podcast go https //lexfridman.com/ai connect lexfridman twitter linkedin facebook medium youtube,Machine Learning
r prefix-tuning optimizing continuous prompts generation,link comments,Machine Learning
facebook research unsupervised translation programming languages,episode machine learning street talk dr. tim scarfe yannic kilcher connor shorten spoke marie-anne lachaux baptiste roziere dr. guillaume lample facebook research fair paris recently released paper `` unsupervised translation programming languages '' exciting new approach learned translation programming languages learned transcoder using unsupervised encoder trained individual monolingual corpora i.e parallel language data needed trick used significant token overlap using word-piece embeddings incredible talk talented group researchers hope enjoy conversation yannic 's video got watched 120k times check https //www.youtube.com/watch v=xtzfjiknh7e paper https //arxiv.org/abs/2006.03511 marie-anne lachaux baptiste roziere lowik chanussot guillaume lample abstract `` transcompiler also known source-to-source translator system converts source code high-level programming language c++ python another transcompilers primarily used interoperability port codebases written obsolete deprecated language e.g cobol python 2 modern one typically rely handcrafted rewrite rules applied source code abstract syntax tree unfortunately resulting translations often lack readability fail respect target language conventions require manual modifications order work properly overall translation process timeconsuming requires expertise source target languages making code-translation projects expensive although neural models significantly outperform rule-based counterparts context natural language translation applications transcompilation limited due scarcity parallel data domain paper propose leverage recent approaches unsupervised machine translation train fully unsupervised neural transcompiler train model source code open source github projects show translate functions c++ java python high accuracy method relies exclusively monolingual source code requires expertise source target languages easily generalized programming languages also build release test set composed 852 parallel functions along unit tests check correctness translations show model outperforms rule-based commercial baselines significant margin '',Machine Learning
complete guide build machine learning model deployed production using aws sagemaker,read full story,Machine Learning
simple questions thread december 20 2020,please post questions instead creating new thread encourage others create new posts questions post instead thread stay alive next one keep posting date title thanks everyone answering questions previous thread link comments,Machine Learning
site really n't want love true,site really n't want love true,Machine Learning
different approaches pre-training transfer learning machine learning models masking method,transfer learning aims improve target learners performance target domains transferring knowledge contained different related source domains nlp ​ training language model bert done predicting 15 tokens input randomly picked tokens pre-processed follows — 80 replaced “ mask ” token 10 random word 10 use original word intuition led authors pick approach follows thanks jacob devlin google insight used mask 100 time model ’ necessarily produce good token representations non-masked words non-masked tokens still used context model optimized predicting masked words used mask 90 time random words 10 time would teach model observed word never correct used mask 90 time kept word 10 time model could trivially copy non-contextual embedding source apart masking methods wondered methods could used train large model use downstream tasks link comments,Machine Learning
r finding words say hidden state visualizations language models,hi r/machinelearning hidden states transformers interesting place seek clues various layers process input token collectively select output token post demonstrate three ways visualizing hidden state build awesome previous work space find exciting post demo open-source package creating visualizations ecco 'm sure people find interesting ways probe models using tool hope find useful tl dr output tokens tend selected latter half model layers cases even layer 0 certain output token look solid dark pink columns visuals https //jalammar.github.io/hidden-states/ link comments,Machine Learning
approach recommended brain 🧠 tumor segmentation mri scans dicom files,let 's say mri ct data dicom format want segment tumour current approach train model recognise high intensity spots compared labelled training data set experience using method 's possible segment edema puss filled region surrounding tumour region somewhat getting tumour working properly wondering approach 'm missing try resources found online either proprietary sort confidential nda em y'all think link comments,Machine Learning
dialogpt paper walkthrough,paper microsoft presents large tunable neural conversational response generation model dialogpt dialogue generative pre-trained transformer model trained 147m conversation-like exchanges extracted reddit comments researchers show conversational systems leverage dialogpt generate relevant contentful context-consistent responses strong baseline systems paper walkthrough https //youtu.be/zo679myojns ⏩ paper https //www.aclweb.org/anthology/2020.acl-demos.30.pdf link comments,Machine Learning
gustav soderstrom spotify,gustav soderstrom chief research development officer spotify leading product design data technology engineering teams conversation part artificial intelligence podcast would like get information podcast go https //lexfridman.com/ai connect lexfridman twitter linkedin facebook medium youtube watch video versions conversations enjoy podcast please rate 5 stars itunes support patreon,Machine Learning
would see lived hypersphere,sign brilliant get 20 annual subscription https //brilliant.org/zachstar/ support channel https //www.patreon.com/zachstar paypal one time donation https //www.paypal.me/zachstaryt ►follow instagram https //www.instagram.com/zachstar/ twitter https //twitter.com/imzachstar previous video https //youtu.be/jj5ldmaqtuo curved spaces program http //www.geometrygames.org/curvedspaces/index.html.en animations brainup studios http //brainup.in/ ►my setup space pictures https //amzn.to/2cc4kqj magnetic floating globe https //amzn.to/2vgpdn0 camera https //amzn.to/2rivyu5 mic https //amzn.to/35bkiri tripod https //amzn.to/2rgmtnl equilibrium tube https //amzn.to/2sowdrh ►check amazon store https //www.amazon.com/shop/zachstar,Machine Learning
max tegmark ai physics lex fridman podcast 155,max tegmark physicist ai researcher mit please support podcast checking sponsors jordan harbinger show https //www.jordanharbinger.com/lex/ four sigmatic https //foursigmatic.com/lex use code lexpod get 60 betterhelp https //betterhelp.com/lex get 10 expressvpn https //expressvpn.com/lexpod use code lexpod get 3 months free episode links news project explainer video https //www.youtube.com/watch v=prlf17pb6vo news project website https //www.improvethenews.org/ max 's twitter https //twitter.com/tegmark max 's website https //space.mit.edu/home/tegmark/ future life institute https //futureoflife.org/ lex fridman podcast 1 https //www.youtube.com/watch v=gi8lunhp5yu podcast info podcast website https //lexfridman.com/podcast apple podcasts https //apple.co/2lwqzir spotify https //spoti.fi/2newcf8 rss https //lexfridman.com/feed/podcast/ full episodes playlist https //www.youtube.com/playlist list=plraxtmerzgodp_8gztsuki9nrranbkkp4 clips playlist https //www.youtube.com/playlist list=plraxtmerzgoecifp3cbcieelojeitor41 outline 0:00 introduction 2:49 ai physics 16:07 ai discover new laws physics 24:57 ai safety 42:33 extinction human species 53:31 fix fake news misinformation 1:15:05 autonomous weapons 1:30:28 man prevented nuclear war 1:40:36 elon musk ai 1:54:14 ai alignment 2:00:16 consciousness 2:09:20 richard feynman 2:13:30 machine learning computational physics 2:24:28 ai creativity 2:35:42 aliens 2:51:25 mortality connect subscribe youtube channel twitter https //twitter.com/lexfridman linkedin https //www.linkedin.com/in/lexfridman facebook https //www.facebook.com/lexfridmanpage instagram https //www.instagram.com/lexfridman medium https //medium.com/ lexfridman support patreon https //www.patreon.com/lexfridman,Machine Learning
trigonometry fundamentals lockdown math ep 2,intro trig lurking mystery cos x ^2 full playlist https //www.youtube.com/playlist list=plzhqobowtqdp5cveljj1bndouqrahvpev home page https //www.3blue1brown.com brought https //3b1b.co/ldm-thanks -- -- -- -- -- -- -- -- contents introduction 0:00 q1 graph cos θ ² 2:14 q2 translations cos θ cos θ ² 5:34 q3 f 2x f x ² 10:54 intro trig 13:14 q4 sin 3 cos 3 16:44 sohcahtoa 20:44 q5 leaning tower 22:29 compute trig functions 30:59 q6 sin π/6 33:34 q7 cos π/6 36:19 q8 trig -θ 43:34 computing trig functions 47:44 q9 cos π/12 0:49:54 adv trig functions 0:56:44 q10 graph tan θ 1:00:30 json comment 1:02:24 “ exciting part lecture ” –1:05:34 -- -- -- -- -- -- -- -- -- live question setup stats on-screen powered itempool https //itempool.com/ graphing calculator used desmos https //www.desmos.com/ curious animations https //www.3blue1brown.com/faq manim music vincent rubinetti download music bandcamp https //vincerubinetti.bandcamp.com/album/the-music-of-3blue1brown stream music spotify https //open.spotify.com/album/1dvyjws8fbqxhrunag5w5u want contribute translated subtitles help review already made others need approval click gear icon video go subtitles/cc `` add subtitles/cc '' really appreciate helps make lessons accessible people -- -- -- -- -- -- -- -- -- 3blue1brown channel animating math senses word animate know drill youtube want stay posted new videos subscribe http //3b1b.co/subscribe various social media stuffs website https //www.3blue1brown.com twitter https //twitter.com/3blue1brown reddit https //www.reddit.com/r/3blue1brown instagram https //www.instagram.com/3blue1brown_animations/ patreon https //patreon.com/3blue1brown facebook https //www.facebook.com/3blue1brown,Machine Learning
tips better problem solver last lecture lockdown math ep 10,tips problem-solving examples geometry trig probability past episodes integrated quizzes https //itempool.com/c/3b1b full playlist https //www.youtube.com/playlist list=plzhqobowtqdp5cveljj1bndouqrahvpev home page https //www.3blue1brown.com brought https //3b1b.co/ldm-thanks huge huge thanks ben eater https //www.youtube.com/user/eaterbc cam christensen creator itempool https //itempool.com/ notes ngân vũ https //twitter.com/thuynganvu/status/1265480770832855040 mistakes 50:35 dx integral 54:40 notice mistake inclined complain keep watching -- -- -- -- -- -- -- -- -- video timeline thanks user `` noonesperfect '' 0:34 9-problem solving principles/tip 1:15 question 1 probability 2:08 ben eater 4:25 inscribed angle theorem θl=2 θs 5:58 tip-1 7:48 tip-2 8:16 question 2 9:34 answer 2 10:29 tip-3 15:17 tip-4 22:48 question 3 25:56 answer 3 marked incorrectly answer option 26:31 answer 1 27:28 explanation q1 floor function 30:38 tip-5 32:53 tip-6 33:36 question 4 34:43 answer 4 36:37 question 5 38:10 answer 5 41:48 probability graph desmos 44:08 revisiting alternating series sum ln 2 47:29 tip-7 51:08 tip-8 55:23 audience questions tweets 57:28 tip-9 58:29 python programming various probability question 1:04:31 arts created using desmos graph tool mathematical expressions 1:05:54 thank appreciation team -- -- -- -- -- -- -- -- -- live question setup stats on-screen powered itempool https //itempool.com/ curious animations https //www.3blue1brown.com/faq manim music vincent rubinetti download music bandcamp https //vincerubinetti.bandcamp.com/album/the-music-of-3blue1brown stream music spotify https //open.spotify.com/album/1dvyjws8fbqxhrunag5w5u want contribute translated subtitles help review already made others need approval click gear icon video go subtitles/cc `` add subtitles/cc '' really appreciate helps make lessons accessible people -- -- -- -- -- -- -- -- -- 3blue1brown channel animating math senses word animate know drill youtube want stay posted new videos subscribe http //3b1b.co/subscribe various social media stuffs website https //www.3blue1brown.com twitter https //twitter.com/3blue1brown reddit https //www.reddit.com/r/3blue1brown instagram https //www.instagram.com/3blue1brown_animations/ patreon https //patreon.com/3blue1brown facebook https //www.facebook.com/3blue1brown,Machine Learning
elon musk tesla autopilot,elon musk ceo tesla spacex neuralink co-founder several companies video version available youtube would like get information podcast go https //lexfridman.com/ai connect lexfridman twitter linkedin facebook medium youtube watch video versions conversations,Machine Learning
sebastian thrun flying cars autonomous vehicles education,sebastian thrun one greatest roboticists computer scientists educators time led development autonomous vehicles stanford 2005 darpa grand challenge placed second 2007 darpa urban challenge led google self-driving car program launched self-driving revolution taught popular stanford course artificial intelligence 2011 one first moocs experience led co-found udacity online education platform also ceo kitty hawk company working building flying cars technically,Machine Learning
steven pinker ai age reason,steven pinker professor harvard professor mit author many books several big impact way see world better particular better angels nature enlightenment instilled sense optimism grounded data science reason video version available youtube would like get information podcast go https //lexfridman.com/ai connect lexfridman twitter linkedin facebook youtube watch video,Machine Learning
cvpr 2021 reviews,hi today cvpr reviews released got weak accept weak accept borderline thoughts link comments,Machine Learning
97 – sertac karaman robots fly robots drive,sertac karaman professor mit co-founder autonomous vehicle company optimus ride one top roboticists world including robots drive robots fly support podcast signing sponsors – cash app – use code “ lexpodcast ” download – cash app app store https //apple.co/2spruhe – cash app google play https //bit.ly/2mlvp5w episode links sertac ’ website http //sertac.scripts.mit.edu/web/ sertac ’ twitter https //twitter.com/sertackaraman optimus ride https //www.optimusride.com/ conversation part artificial intelligence podcast would like get information podcast go https //lexfridman.com/ai connect lexfridman twitter linkedin,Machine Learning
prevent embarrassment ai,read full story,Machine Learning
predictive coding approximates backprop along arbitrary computation graphs paper explained,ai biology neuroscience backpropagation workhorse modern deep learning core component frameworks long known biologically plausible driving divide neuroscience machine learning paper shows predictive coding much biologically plausible algorithm approximate backpropagation computation graph verify experimentally building training cnns lstms using predictive coding suggests brain deep neural networks could much similar previously believed outline 0:00 intro overview 3:00 backpropagation biology 7:40 experimental results 8:40 predictive coding 29:00 pseudocode 32:10 predictive coding approximates backprop 35:00 hebbian updates 36:35 code walkthrough 46:30 conclusion comments paper https //arxiv.org/abs/2006.04182 code https //github.com/berenmillidge/predictivecodingbackprop abstract backpropagation error backprop powerful algorithm training machine learning architectures end-to-end differentiation however backprop often criticised lacking biological plausibility recently shown backprop multilayer-perceptrons mlps approximated using predictive coding biologically-plausible process theory cortical computation relies local hebbian updates power backprop however lies instantiation mlps rather concept automatic differentiation allows optimisation differentiable program expressed computation graph demonstrate predictive coding converges asymptotically practice rapidly exact backprop gradients arbitrary computation graphs using local learning rules apply result develop straightforward strategy translate core machine learning architectures predictive coding equivalents construct predictive coding cnns rnns complex lstms include non-layer-like branching internal graph structure multiplicative interactions models perform equivalently backprop challenging machine learning benchmarks utilising local mostly hebbian plasticity method raises potential standard machine learning algorithms could principle directly implemented neural circuitry may also contribute development completely distributed neuromorphic architectures authors beren millidge alexander tschantz christopher l. buckley links youtube https //www.youtube.com/c/yannickilcher twitter https //twitter.com/ykilcher discord https //discord.gg/4h8xxdf bitchute https //www.bitchute.com/channel/yannic-kilcher minds https //www.minds.com/ykilcher parler https //parler.com/profile/yannickilcher linkedin https //www.linkedin.com/in/yannic-kilcher-488534136/ want support best thing share content want support financially completely optional voluntary lot people asked subscribestar https //www.subscribestar.com/yannickilcher patreon https //www.patreon.com/yannickilcher bitcoin btc bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq ethereum eth 0x7ad3513e3b8f66799f507aa7874b1b0ebc7f85e2 litecoin ltc lqw2trykyetvc8wjfkhpphtpbdm4vw7r9m monero xmr 4acl8agreo5hair8a9cevrw8peauwvnp1wnsdzxw7tzicdlhzagsgzhrqabdnfy8yum9fwjdvijphkrjv4fwt19cjzn9d4n,Machine Learning
decentralized ai rest us,read full story,Machine Learning
microsoft research sponsors popl 2021,presented calc intelligence team https //aka.ms/calcintel 0:00 – introduction andy gordon 4:08 – lambda action jack williams 8:47 – started collaboration excel simon peyton jones 13:08 – elastic sheet-defined functions gridlets advait sarkar 18:57 – units knowledge base categories types carina negreanu 22:41 – andy gordon conversation brian jones gpm excel microsoft research hiring programming language folks worldwide https //aka.ms/popl,Machine Learning
introducing h2o wave,almost decade h2o.ai worked build open source commercial products leading edge innovation machine learning automl explainable ai thrilled announce release believe future ai applications h2o wave wave open source lightweight python development framework enables developers build beautiful real-time applications powered leading ai technology enabling faster modular development ai powered applications companies build test collaborate deploy ai applications dramatically faster wave easily integrated h2o ai technologies driverless ai h2o-3 python library storytelling python data science data munging model building storytelling crucial part real-world data science projects interactive web applications excellent way visualize results showcase value ai use cases business leaders yet data scientists may essential skills e.g html css javascript modern web application development wave users build rich interactive web apps using tools familiar – python note r also roadmap make ai apps getting right first time hard even seasoned app developers trial error favorite method comes web app development wave ’ live-reload feature allows data science developers preview app live change code dramatically reduces time effort app development show things sometimes need see everything glance wave ’ low-latency server practical broadcasting live information graphics deploy instantly run anywhere likely share apps others point wave apps stored static executables multiple platforms including windows osx linux makes easy sharing get started ready try h2o wave open source download github website follow instructions windows/mac/linux also find links examples api enjoy post introducing h2o wave appeared first open source leader ai ml,Machine Learning
lex plays cyberpunk 2077,couple hours traveling beautiful alternate reality fun please check sponsors theragun https //theragun.com/lex get 30 day trial grammarly https //grammarly.com/lex get 20 premium outline 0:00 introduction 1:28 character creation 10:41 gameplay starts 31:08 tutorial shooting hacking 38:07 rescue sandra dorsett 42:42 going home 49:46 neurosurgery cybernetic implants 57:27 meeting dexter 1:03:06 picking robot dog 1:07:21 closing thoughts connect subscribe youtube channel twitter https //twitter.com/lexfridman linkedin https //www.linkedin.com/in/lexfridman facebook https //www.facebook.com/lexfridmanpage instagram https //www.instagram.com/lexfridman medium https //medium.com/ lexfridman support patreon https //www.patreon.com/lexfridman,Machine Learning
r data movement need case study optimizing transformers,abstract — transformers become widely used language modeling sequence learning tasks one important machine learning workloads today training one compute-intensive task often taking days weeks significant attention given optimizing transformers despite existing implementations efficiently utilize gpus find data movement key bottleneck training due amdahl ’ law massive improvements compute performance training become memory-bound existing frameworks use suboptimal data layouts using insights present recipe globally optimizing data movement transformers reduce data movement 22.91 overall achieve 1.30× performance improvement state-of-the-art frameworks training bert approach applicable broadly optimizing deep neural networks offers insight tackle emerging performance bottlenecks quote — `` gpt-3 transformer model training cost 12m optimizations could save 3.6m 120 mwh energy '' link — https //arxiv.org/abs/2007.00072 great work analyzing dataflow bert encoder architecture using sdfgs dace environment optimizing via cuda kernels https //preview.redd.it/aqetnmob28c61.png width=537 format=png auto=webp s=0dd6d0e9fbcccc0e86d993111ca9701203f97403 also multi-head attention performance extended research domains link comments,Machine Learning
behaviors trees ai ditch event framework,article look shortages event-driven programming suggest behavior trees effective alternative suitable back/front-end application development read full story,Machine Learning
aistats 2021 decisions,n't seen news yet go everyone rebuttal help hopes getting offline conferences back 2022 link comments,Machine Learning
asking friend,asking friend nitter.net/veritasium/status/1352724375036825600,Machine Learning
self-supervised machine learning story far trends 2021,let ’ talk self-supervised machine learning way teach model lot without manual markup well opportunity avoid deep learning setting model solve problem material requires intermediate level preparation many references original publications read full story,Machine Learning
75 – marcus hutter universal artificial intelligence aixi agi,marcus hutter senior research scientist deepmind professor australian national university throughout career research including jürgen schmidhuber shane legg proposed lot interesting ideas around field artificial general intelligence including development aixi model mathematical approach agi incorporates ideas kolmogorov complexity solomonoff induction reinforcement learning episode links hutter prize http //prize.hutter1.net marcus web http //www.hutter1.net books mentioned – universal ai https //amzn.to/2waiauw – ai modern approach https //amzn.to/3camxny – reinforcement learning https //amzn.to/2poanj9 – theory knowledge https //amzn.to/3a6vp7x conversation,Machine Learning
dava newman space exploration space suits life mars,dava newman apollo program professor aeroastro mit former deputy administrator nasa principal investigator four spaceflight missions research interests aerospace biomedical engineering investigating human performance varying gravity environments developed space activity suit namely biosuit would provide pressure compression directly skin via suit ’ textile weave patterning materials rather pressurized gas conversation part artificial intelligence podcast would like get information podcast go https //lexfridman.com/ai connect,Machine Learning
gentle introduction machine learning modeling pipelines,tweet share share applied machine learning typically focused finding single model performs well best given dataset effective use model require appropriate preparation input data hyperparameter tuning model collectively linear sequence steps required prepare data tune model transform predictions called modeling pipeline modern machine learning libraries like scikit-learn python library allow sequence steps defined used correctly without data leakage consistently evaluation prediction nevertheless working modeling pipelines confusing beginners requires shift perspective applied machine learning process tutorial discover modeling pipelines applied machine learning completing tutorial know applied machine learning concerned finding good performing model also requires finding appropriate sequence data preparation steps steps post-processing predictions collectively operations required address predictive modeling problem considered atomic unit called modeling pipeline approaching applied machine learning lens modeling pipelines requires change thinking evaluating specific model configurations sequences transforms algorithms let ’ get started gentle introduction machine learning modeling pipelines photo jay huang rights reserved tutorial overview tutorial divided three parts finding skillful model enough modeling pipeline implications modeling pipeline finding skillful model enough applied machine learning process discovering model performs best given predictive modeling dataset fact ’ addition discovering model performs best dataset must also discover data transforms best expose unknown underlying structure problem learning algorithms model hyperparameters result good best configuration chosen model may also additional considerations techniques transform predictions made model like threshold moving model calibration predicted probabilities common think applied machine learning large combinatorial search problem across data transforms models model configurations quite challenging practice requires sequence one data preparation schemes model model configuration prediction transform schemes must evaluated consistently correctly given test harness although tricky may manageable simple train-test split becomes quite unmanageable using k-fold cross-validation even repeated k-fold cross-validation solution use modeling pipeline keep everything straight modeling pipeline pipeline linear sequence data preparation options modeling operations prediction transform operations allows sequence steps specified evaluated used atomic unit pipeline linear sequence data preparation modeling steps treated atomic unit make idea clear let ’ look two simple examples first example uses data normalization input variables fits logistic regression model input normalization logistic regression predictions second example standardizes input variables applies rfe feature selection fits support vector machine input standardization rfe svm predictions imagine examples modeling pipelines atomic unit pipeline evaluated using preferred resampling scheme train-test split k-fold cross-validation important two main reasons avoid data leakage consistency reproducibility modeling pipeline avoids common type data leakage data preparation techniques scaling input values applied entire dataset data leakage shares knowledge test dataset observations contribute mean maximum known value training dataset turn may result overly optimistic model performance instead data transforms must prepared training dataset applied training dataset test dataset validation dataset datasets require transform prior used model modeling pipeline ensures sequence data preparation operations performed reproducible without modeling pipeline data preparation steps may performed manually twice evaluating model making predictions changes sequence must kept consistent cases otherwise differences impact capability skill model pipeline ensures sequence operations defined consistent used model evaluation making predictions python scikit-learn machine learning library provides machine learning modeling pipeline via pipeline class learn use pipeline api tutorial avoid data leakage performing data preparation implications modeling pipeline modeling pipeline important tool machine learning practitioners nevertheless important implications must considered using main confusion beginners using pipelines comes understanding pipeline learned specific configuration discovered pipeline example pipeline may use data transform configures automatically rfecv technique feature selection evaluating pipeline uses automatically-configured data transform configuration choose fitting pipeline final model making predictions configuration choose answer ’ matter another example use hyperparameter tuning final step pipeline grid search performed data provided prior transform steps pipeline search best combination hyperparameters model using data fit model hyperparameters data evaluating pipeline grid searches model hyperparameters configuration choose fitting pipeline final model making predictions configuration choose answer ’ matter answer applies using threshold moving probability calibration step end pipeline reason reason concerned specific internal structure coefficients chosen model example evaluating logistic regression model ’ need inspect coefficients chosen k-fold cross-validation round order choose model instead focus out-of-fold predictive skill similarly using logistic regression model final model making predictions new data need inspect coefficients chosen fitting model entire dataset making predictions inspect discover coefficients used model exercise analysis impact selection use model answer generalizes considering modeling pipeline concerned features may automatically selected data transform pipeline also concerned hyperparameters chosen model using grid search final step modeling pipeline three cases single model pipeline automatic feature selection pipeline grid search evaluating “ model ” “ modeling pipeline ” atomic unit pipeline allows us machine learning practitioners move one level abstraction less concerned specific outcomes algorithms concerned capability sequence procedures focus evaluating capability algorithms dataset product algorithms i.e model estimate pipeline apply confident get similar performance average shift thinking may take time get used also philosophy behind modern automl automatic machine learning techniques treat applied machine learning large combinatorial search problem reading section provides resources topic looking go deeper avoid data leakage performing data preparation summary tutorial discovered modeling pipelines applied machine learning specifically learned applied machine learning concerned finding good performing model also requires finding appropriate sequence data preparation steps steps post-processing predictions collectively operations required address predictive modeling problem considered atomic unit called modeling pipeline approaching applied machine learning lens modeling pipelines requires change thinking evaluating specific model configurations sequences transforms algorithms questions ask questions comments best answer tweet share share post gentle introduction machine learning modeling pipelines appeared first machine learning mastery,Machine Learning
95 – dawn song adversarial machine learning computer security,dawn song professor computer science uc berkeley research interests security recently focus intersection computer security machine learning support podcast signing sponsors – cash app – use code “ lexpodcast ” download – cash app app store https //apple.co/2spruhe – cash app google play https //bit.ly/2mlvp5w episode links dawn ’ twitter https //twitter.com/dawnsongtweets dawn ’ website https //people.eecs.berkeley.edu/~dawnsong/ oasis labs https //www.oasislabs.com conversation part artificial intelligence podcast would like get information podcast go https //lexfridman.com/ai connect lexfridman twitter linkedin facebook medium,Machine Learning
inca knot numbers numberphile,alex bellos discusses incans used knots string quipu record numbers check brilliant get 20 premium service https //brilliant.org/numberphile sponsor links stuff full description ↓↓↓ check language lover 's puzzle book amazon https //amzn.to/3ou0wjt signed version maths gear https //bit.ly/language_lovers alex numberphile http //bit.ly/bellos_playlist alex 's website links stuff http //www.alexbellos.com see alex another forgotten number system https //youtu.be/9p55qgt7ciw ancient quipu l leland locke https //amzn.to/37rjfzz numberphile supported mathematical sciences research institute msri http //bit.ly/msrinumberphile also supported science sandbox simons foundation initiative dedicated engaging everyone process science https //www.simonsfoundation.org/outreach/science-sandbox/ support math america https //www.mathforamerica.org/ numberphile website http //www.numberphile.com/ numberphile facebook http //www.facebook.com/numberphile numberphile tweets https //twitter.com/numberphile subscribe http //bit.ly/numberphile_sub video brady haran pete mcpartlan patreon http //www.patreon.com/numberphile numberphile t-shirts merch https //teespring.com/stores/numberphile brady 's videos subreddit http //www.reddit.com/r/bradyharan/ brady 's latest videos across channels http //www.bradyharanblog.com/ thanks patreon supporters arjun chakroborty ben delo jeff straathof ken baron yana chernobilsky andy b james bissonette jubal john jeremy buchanan steve crutchfield adam savage ben white andrei burke rad donato matthew schuster nat tyce ron hochsprung mitch harding ubiquity ventures mateusz swiatkowski john zelinka tom marshall jesús salsero gnare jordan w oja tracy parry ian george walker arnas bernd sing valentin alfred wallace charles southerland kristian joensen bodhisattva debnath alex khein kermit norlund asymptote mirik gogri sign occasional emails http //eepurl.com/ydjl9,Machine Learning
hamming codes error correction,discovery-oriented introduction error correction codes part 2 https //youtu.be/b3nxrzou_ce ben eater 's take https //youtu.be/h0jloehrkas many thanks supporters https //3b1b.co/hamming-thanks heavily related chessboard puzzle matt parker https //youtu.be/as7gkm7y7h4 read hamming 's perspective discovery codes chapter 12 `` art science engineering '' https //amzn.to/3lwcnmh viewer harry li made interactive topic https //harryli0088.github.io/hamming-code/ -- -- -- -- -- -- -- -- -- animations largely made using manim scrappy open-source python library https //github.com/3b1b/manim want check feel compelled warn 's well-documented tool many quirks might expect library someone wrote use mind music vincent rubinetti download music bandcamp https //vincerubinetti.bandcamp.com/album/the-music-of-3blue1brown stream music spotify https //open.spotify.com/album/1dvyjws8fbqxhrunag5w5u -- -- -- -- -- -- -- -- -- 3blue1brown channel animating math senses word animate know drill youtube want stay posted new videos subscribe http //3b1b.co/subscribe various social media links website https //www.3blue1brown.com twitter https //twitter.com/3blue1brown reddit https //www.reddit.com/r/3blue1brown instagram https //www.instagram.com/3blue1brown_animations/ patreon https //patreon.com/3blue1brown facebook https //www.facebook.com/3blue1brown,Machine Learning
113 – manolis kellis human genome evolutionary dynamics,manolis kellis professor mit head mit computational biology group interested understanding human genome computational evolutionary biological cross-disciplinary perspectives support podcast supporting sponsors – blinkist https //blinkist.com/lex – eight sleep https //eightsleep.com/lex – masterclass https //masterclass.com/lex would like get information podcast go https //lexfridman.com/ai connect lexfridman twitter linkedin facebook medium youtube watch video versions conversations enjoy podcast please rate 5 stars apple podcasts follow spotify,Machine Learning
michael kearns algorithmic fairness bias privacy ethics machine learning,michael kearns professor university pennsylvania co-author new book ethical algorithm focus much conversation including algorithmic fairness bias privacy ethics general one many fields michael world-class researcher touch quickly including learning theory theoretical foundations machine learning game theory algorithmic trading quantitative finance computational social science conversation part artificial intelligence podcast would like get information podcast go https //lexfridman.com/ai,Machine Learning
donut sphere things one surface,get free access 2500 documentaries curiositystream https //curiositystream.thld.co/zachstaroct5 use code `` zachstar '' sign get `` n't jerk '' shirt https //stemerch.com/collections/dont-be-a-jerk support channel https //www.patreon.com/zachstar paypal one time donation https //www.paypal.me/zachstaryt join channel get access perks https //www.youtube.com/channel/ucpcsacbqs-sjevfk_hmfy9w/join ►follow instagram https //www.instagram.com/zachstar/ twitter https //twitter.com/imzachstar ►original video 'm responding https //youtu.be/lmct2mp2bfe ►james grime 3 utilities problem explanation https //www.youtube.com/watch v=odtwehgzolm ►3b1b 3 utilities coffee mug https //www.youtube.com/watch v=vvcytjvd4h0 ►wind torus vs sphere video https //youtu.be/z-glm7etfq8 animations brainup studios http //brainup.in/ check spanish channel https //www.youtube.com/channel/ucnknu2xqblaspj6ckc8vtpa ►my setup space pictures https //amzn.to/2cc4kqj magnetic floating globe https //amzn.to/2vgpdn0 camera https //amzn.to/2rivyu5 mic https //amzn.to/35bkiri tripod https //amzn.to/2rgmtnl equilibrium tube https //amzn.to/2sowdrh ►check amazon store https //www.amazon.com/shop/zachstar,Machine Learning
r would like study trends word frequency academic papers existing way arxiv instance,would like look statistical frequency certain words arxiv academic papers general year topic etc existing dataset database library would allow example would like look frequency word `` quantum '' last five years math subject areas link comments,Machine Learning
dp-3t algorithm contact tracing via nicky case,though many contact tracing apps involve location tracking ’ video adaptation post nicky case https //ncase.me/contact-tracing/ new post nicky covid-19 https //ncase.me/covid-19/ consider supporting work https //www.patreon.com/ncase/ supporters video https //3b1b.co/ctracing-thanks dp-3t whitepaper https //github.com/dp-3t/documents/blob/master/dp3t 20white 20paper.pdf full repo https //github.com/dp-3t number 60 come https //science.sciencemag.org/content/early/2020/04/09/science.abb6936 specific figure figure 3 last panel showing instant contact tracing app shows combination symptomatic cases isolated pre/a-symptomatic contacts quarantined would contain virus mental health resources copied form john green video since knows 's national suicide prevention lifeline 1-800-273-talk 8255 know scary make phone call people nice promise substance abuse mental health administration helpline 1-800-662-help 4357 trevor project 866-488-7386 international helplines https //togetherweare-strong.tumblr.com/helpline mental health resource list links http //activeminds.org/ anxiety depression association america http //www.adaa.org/ mental health screening tools https //screening.mhanational.org/screening-tools mind http //mind.org.uk/information-support/ -- -- -- -- -- -- -- -- -- music vincent rubinetti download music bandcamp https //vincerubinetti.bandcamp.com/album/the-music-of-3blue1brown stream music spotify https //open.spotify.com/album/1dvyjws8fbqxhrunag5w5u want contribute translated subtitles help review already made others need approval click gear icon video go subtitles/cc `` add subtitles/cc '' really appreciate helps make lessons accessible people -- -- -- -- -- -- -- -- -- 3blue1brown channel animating math senses word animate know drill youtube want stay posted new videos subscribe http //3b1b.co/subscribe various social media stuffs website https //www.3blue1brown.com twitter https //twitter.com/3blue1brown reddit https //www.reddit.com/r/3blue1brown instagram https //www.instagram.com/3blue1brown_animations/ patreon https //patreon.com/3blue1brown facebook https //www.facebook.com/3blue1brown,Machine Learning
r karpathy eg tonight random walk around markets cairo egypt nice background track late night email https //www.youtube.com/watch v=yvydz_cosfy,eg tonight random walk around markets cairo egypt nice background track late night email invidious.snopyta.org/watch v=yvydz_co…,Machine Learning
liqui.do speeds credit scoring fair lending h2o.ai,liqui.do technological innovative company developing platform leasing equipment small medium enterprises part business provide variety credit options companies want finance capital purchases liqui.do needs rapidly accurately assess credit risk scoring customer order offer attractive credit options expanding access credit reducing previous lending biases helps drive freedom flexibility people businesses well business growth liqui.do decided use machine learning meet needs h2o.ai provided automl technology driverless ai enabled liqui.do provide deep insights model help team minimize past biases able provide 20 applicants credit offer right financial products meet needs customers solution helps liqui.do improve credit risk predictions improving lending fairness customer experience using driverless ai less three months company able develop algorithm boosted model performance 20 giving liqui.do ability increase credit approvals without compromising company credit losses liqui.do also aims add ai processes using h2o.ai technology improving efficiency efficacy “ h2o.ai terrific partner helping liqui.do utilize artificial intelligence enhance business increase responsiveness customers ” said sergio nunes liqui.do ceo “ processes much efficient thanks driverless ai solution helps us make right decision right time fair way excited initial results h2o.ai ’ offerings provided looking forward expanding usage ai within company. ” h2o.ai leading open-source ai platform driverless ai leading automatic machine learning automl platform helps customers easily build deploy machine learning models h2o driverless ai automates time-consuming machine learning workflows automatic feature engineering model tuning model selection achieve highest predictive accuracy within shortest amount time “ liqui.do using h2o driverless ai personalize credit towards equipment rentals small-medium enterprises. ” said sri ambati ceo founder h2o.ai “ h2o ’ automl helping liqui.do transform future equipment renting. ” learn liqui.do visit https //liqui.do post liqui.do speeds credit scoring fair lending h2o.ai appeared first open source leader ai ml,Machine Learning
“ would aliens also x ” almost x tickles brain lot x primed stainless steel almost generalization works,“ would aliens also x ” almost x tickles brain lot x primed stainless steel almost generalization works,Machine Learning
154 – avi loeb aliens black holes mystery oumuamua,avi loeb astrophysicist harvard please support podcast checking sponsors – zero fasting https //go.zerofasting.com/s/lex-promo get 30 annual subscription – lmnt https //drinklmnt.com/lex get free sample pack – sun basket https //sunbasket.com/lex use code lex get 35 – pessimists archive https //pessimists.co/ episode links extraterrestrial book https //amzn.to/39xdnkt avi ’ website https //astronomy.fas.harvard.edu/people/avi-loeb podcast info podcast website https //lexfridman.com/podcast apple podcasts https //apple.co/2lwqzir spotify https //spoti.fi/2newcf8 rss https //lexfridman.com/feed/podcast/ youtube full episodes https //youtube.com/lexfridman youtube clips https //youtube.com/lexclips support connect – check sponsors ’ best way support podcast – support patreon https //www.patreon.com/lexfridman – twitter https //twitter.com/lexfridman – instagram https //www.instagram.com/lexfridman – linkedin https //www.linkedin.com/in/lexfridman – facebook https //www.facebook.com/lexfridmanpage – medium https //medium.com/ lexfridman outline ’ timestamps episode podcast players able click timestamp jump time 00:00 – introduction 10:08 – alone universe 14:23 – consciousness 19:01 – sending digital copies humans space 23:38 – oumuamua 45:42 – alien space junk 49:41 – aliens look like 1:06:58 – drake equation 1:08:00 – industrial polution aliens 1:19:52 – ufo sightings 1:27:48 – long human civilization last 1:30:28 – radio signal proxima centauri 1:33:49 – breakthrough starshot project 1:36:49 – space race 1:42:00 – human space exploration 1:47:15 – social media threat society 1:52:04 – humans ready discovering alien civilization 1:56:15 – mayans used astrology wage war 1:57:31 – black holes 2:16:20 – stephen hawking 2:19:59 – grigori perelman 2:24:24 – theory everything 2:31:23 – dark matter 2:34:06 – advice young people 2:37:10 – memories father mother 2:41:38 – existentialism 2:43:52 – mortality 2:46:27 – meaning life,Machine Learning
tour survival analysis,note guest post alexander moreno computer science phd student georgia institute technology blogs www.boostedml.com survival analysis important subfield statistics biostatistics methods involve modeling time first event death post give brief tour … post tour survival analysis appeared first statistics jim,Machine Learning
planning startup data team 's guide 2021,planning startup feel like exercise futility — especially comes data — especially data team small scrappy read full story,Machine Learning
making money organization 's data 's done,traditional sales bounties digital inversion learn extract value data assets via nevermined ’ numerous commercialization models read full story,Machine Learning
independent identically distributed data iid,independent identically distributed iid data common assumption statistical procedures hypothesis tests mouthful words actually mean ’ topic post ’ provide helpful tips determining whether data iid let ’ break components one-by-one talk independent … post independent identically distributed data iid appeared first statistics jim,Machine Learning
discussion good software review annotations csv file,"hi annotating pictures vgg via works great difficult double check annotations softwares color codes etc make easier via software annotate could upload work csv via review easily 18,000 pictures review thanks link comments",Machine Learning
dmitry korkin evolution proteins viruses life ai lex fridman podcast 153,dmitry korkin professor bioinformatics computational biology wpi please support podcast checking sponsors brave https //brave.com/lex netsuite http //netsuite.com/lex get free product tour magic spoon https //magicspoon.com/lex use code lex get 5 eight sleep https //www.eightsleep.com/lex use code lex get special savings episode links dmitry 's website http //korkinlab.org/ dmitry 's twitter https //twitter.com/dmkorkin podcast info podcast website https //lexfridman.com/podcast apple podcasts https //apple.co/2lwqzir spotify https //spoti.fi/2newcf8 rss https //lexfridman.com/feed/podcast/ full episodes playlist https //www.youtube.com/playlist list=plraxtmerzgodp_8gztsuki9nrranbkkp4 clips playlist https //www.youtube.com/playlist list=plraxtmerzgoecifp3cbcieelojeitor41 outline 0:00 introduction 1:57 proteins building blocks life 9:00 spike protein 15:48 coronavirus biological structure explained 20:45 virus mutations 27:16 evolution proteins 37:02 self-replicating computer programs 44:38 origin life 52:11 extraterrestrial life solar system 54:08 joshua lederberg 1:00:07 dendral 1:03:01 expert systems fail 1:05:12 alphafold 2 1:26:50 ai revolutionize art music 1:33:49 multi-protein folding 1:38:16 alphafold 2 result nobel prize 1:40:47 ai used engineer deadly viruses 1:55:54 book recommendations 2:05:37 family 2:08:15 poem russian connect subscribe youtube channel twitter https //twitter.com/lexfridman linkedin https //www.linkedin.com/in/lexfridman facebook https //www.facebook.com/lexfridmanpage instagram https //www.instagram.com/lexfridman medium https //medium.com/ lexfridman support patreon https //www.patreon.com/lexfridman,Machine Learning
knowledge graphs new black year graph newsletter may 2019,read full story,Machine Learning
r karpathy classical robotics computer graphics stacks re-written neural net modules typically building closely classical algorithms whenever possible swapping differentiable versions propagate gradients 's plugged wider system,classical robotics computer graphics stacks re-written neural net modules typically building closely classical algorithms whenever possible swapping differentiable versions propagate gradients 's plugged wider system,Machine Learning
possible train cycle gan two image sets different axes ie one set horizontal image object ground bird 's eye view object,research 've done far cycle gans image sets usually type reference point right two different medical datasets area one pov looking towards pov goal would network translates two different reference points n't really know possible input would nice contrary architecture better cycle gans 'd glad know research topic group people work 'd rather like end wasting everyone 's time making network turns everything black photo link comments,Machine Learning
rt ykilcher epic special edition kenneth0stanley greatness planned abandoning objectives open-endedness joelbot3000 jeffclune ykilcher https //www.youtube.com/watch v=lhygxyemq_e,epic special edition kenneth0stanley greatness planned abandoning objectives open-endedness joelbot3000 jeffclune ykilcher invidious.snopyta.org/watch v=lhygxyem…,Machine Learning
interview data science product manager get job,read full story,Machine Learning
robert lange nn pruning collective intelligence,speak robert lange robert phd student technical university berlin research combines deep multi-agent reinforcement learning cognitive science study learning dynamics large collectives brilliant blog distils explains cutting edge ml research spoke story economics multi-agent rl intelligence agi recent article summarising state art neural network pruning robert 's article pruning nns https //roberttlange.github.io/posts/2020/06/lottery-ticket-hypothesis/ 00:00:00 intro 00:04:17 show start intro robert 00:11:39 economics background 00:27:20 intrinsic motivation 00:33:22 intelligence/consciousness 00:48:16 lottery ticket/pruning article discussion 01:43:21 robert 's advice younger self state deep learning robert 's linkedin https //www.linkedin.com/in/robert-tjarko-lange-19539a12a/ roberttlange machinelearning deeplearning,Machine Learning
bomb blast radius numberphile,featuring tom crawford links stuff full description ↓↓↓ tom crawford website https //tomrocksmaths.com/ videos tom http //bit.ly/crawford_videos atom bombs periodic videos https //youtu.be/qlzmzsrb86e numberphile supported mathematical sciences research institute msri http //bit.ly/msrinumberphile also supported science sandbox simons foundation initiative dedicated engaging everyone process science https //www.simonsfoundation.org/outreach/science-sandbox/ support math america https //www.mathforamerica.org/ numberphile website http //www.numberphile.com/ numberphile facebook http //www.facebook.com/numberphile numberphile tweets https //twitter.com/numberphile subscribe http //bit.ly/numberphile_sub videos brady haran animation pete mcpartlan patreon http //www.patreon.com/numberphile numberphile t-shirts merch https //teespring.com/stores/numberphile brady 's videos subreddit http //www.reddit.com/r/bradyharan/ brady 's latest videos across channels http //www.bradyharanblog.com/ sign occasional emails http //eepurl.com/ydjl9,Machine Learning
150 – michael malice white pill freedom hope happiness amidst chaos,michael malice political thinker podcaster author please support podcast checking sponsors – netsuite http //netsuite.com/strategy get free product tour – athletic greens https //athleticgreens.com/lex use code lex get 1 month fish oil – sun basket https //sunbasket.com/lex use code lex get 35 – cash app https //cash.app/ use code lexpodcast get 10 episode links michael ’ twitter https //twitter.com/michaelmalice michael ’ community https //malice.locals.com/ michael ’ youtube https //www.youtube.com/channel/uc5tj5qcpjkil-kia4gib5xw michael ’ website http //michaelmalice.com/about/ welcome podcast https //bit.ly/30q8oz1 new right book https //amzn.to/34gxlo3 dear reader book https //amzn.to/2hpplhs podcast round 1 https //www.youtube.com/watch v=bik1zuy8ehu podcast info podcast website https //lexfridman.com/podcast apple podcasts https //apple.co/2lwqzir spotify https //spoti.fi/2newcf8 rss https //lexfridman.com/feed/podcast/ youtube full episodes https //youtube.com/lexfridman youtube clips https //youtube.com/lexclips support connect – check sponsors ’ best way support podcast – support patreon https //www.patreon.com/lexfridman – twitter https //twitter.com/lexfridman – instagram https //www.instagram.com/lexfridman – linkedin https //www.linkedin.com/in/lexfridman – facebook https //www.facebook.com/lexfridmanpage – medium https //medium.com/ lexfridman outline ’ timestamps episode podcast players able click timestamp jump time 00:00 – introduction 09:47 – conversation alex jones tim pool 18:31 – michael ’ outfit 26:53 – self-publishing book 36:41 – white pill 48:05 – volcano say true love 49:28 – myth sisyphus 53:09 – journalism failed stop stalin hitler 1:00:53 – good germans 1:04:49 – richard wolff 1:08:20 – could united states stayed world war ii 1:11:12 – trump derangement syndrome 1:12:58 – nazism antisemitism 1:15:40 – knock knock 1:22:20 – putin 1:30:00 – evil kim jong-il north korea 1:38:32 – dark humor 1:43:18 – comedy tragedy plus timing 1:50:34 – interviewing difficult guests 2:00:06 – curtis yarvin mencius moldbug 2:16:24 – violence anarchism 2:31:58 – ayn rand 2:35:07 – secession united states 2:44:46 – politics next 4 years 2:52:14 – mars 2:56:17 – ufos 2:59:12 – psychedelics 3:03:08 – love,Machine Learning
110 – jitendra malik computer vision,"jitendra malik professor berkeley one seminal figures field computer vision kind deep learning revolution kind cited 180,000 times mentored many world-class researchers computer science support podcast supporting sponsors – betterhelp http //betterhelp.com/lex – expressvpn https //www.expressvpn.com/lexpod would like get information podcast go https //lexfridman.com/ai connect lexfridman twitter linkedin facebook medium youtube watch video versions conversations enjoy podcast please",Machine Learning
introductory guide variables data types go,hello today would learning go variables different data types associated go read full story,Machine Learning
iclr 2020 yoshua bengio nature consciousness,episode machine learning street talk tim scarfe connor shorten yannic kilcher react yoshua bengio ’ iclr 2020 keynote “ deep learning priors associated conscious processing ” bengio takes many future directions research deep learning role attention consciousness sparse factor graphs causality study systematic generalization bengio also presents big ideas intelligence border line philosophy practical machine learning includes ideas consciousness machines system 1 system 2 thinking described daniel kahneman ’ book “ thinking fast slow ” similar yann lecun ’ half 2020 iclr keynote talk takes many challenging ideas hopefully video helps get better understanding thanks watching please subscribe videos paper links link talk https //iclr.cc/virtual_2020/speaker_7.html consciousness prior https //arxiv.org/abs/1709.08568 thinking fast slow https //www.amazon.com/thinking-fast-slow-daniel-kahneman/dp/0374533555 systematic generalization https //arxiv.org/abs/1811.12889 closure assessing systematic generalization clevr models https //arxiv.org/abs/1912.05783 neural module networks https //arxiv.org/abs/1511.02799 experience grounds language https //arxiv.org/pdf/2004.10151.pdf benchmarking graph neural networks https //arxiv.org/pdf/2003.00982.pdf measure intelligence https //arxiv.org/abs/1911.01547 please check individual channels well machine learning dojo tim scarfe https //www.youtube.com/channel/ucxvhubmbgjw67i5vrmbboba yannic kilcher https //www.youtube.com/channel/uczhmqk67msjgfcctn7xbfe henry ai labs https //www.youtube.com/channel/uchb9vepy6kyvzjj0bgxnpbw 00:00:00 tim yannics takes 00:01:37 intro bengio 00:03:13 system 2 language chomsky 00:05:58 cristof koch conciousness 00:07:25 francois chollet intelligence consciousness 00:09:29 meditation sam harris consciousness 00:11:35 connor intro 00:13:20 show main intro 00:17:55 priors associated conscious processing 00:26:25 system 1 system 2 00:42:47 implicit verbalized knowledge dont miss 01:08:24 inductive priors dl 2.0 01:27:20 systematic generalization 01:37:53 contrast symbolic ai program 01:54:55 attention 02:00:25 attention consciousness 02:05:31 thoughts consciousness language 02:06:55 sparse factor graph 02:10:52 sparse change abstract latent space 02:15:10 discovering cause effect 02:20:00 factorize joint distribution 02:22:30 rims modular computation 02:24:30 conclusion machinelearning deeplearning,Machine Learning
openai clip connectingtext images paper explained,ai openai technology paper title learning transferable visual models natural language supervision clip trains 400 million images scraped web along text descriptions learn model connect two modalities core idea contrastive objective combined large batch size resulting model turned arbitrary zero-shot classifiers new image text tasks outline 0:00 introduction 3:15 overview 4:40 connecting images text 9:00 building zero-shot classifiers 14:40 clip contrastive training objective 22:25 encoder choices 25:00 zero-shot clip vs linear resnet-50 31:50 zero-shot vs few-shot 35:35 scaling properties 36:35 comparison different tasks 37:40 robustness data shift 44:20 broader impact section 47:00 conclusion comments paper https //cdn.openai.com/papers/learning_transferable_visual_models_from_natural_language_supervision.pdf blog https //openai.com/blog/clip/ code https //github.com/openai/clip abstract state-of-the-art computer vision systems trained predict fixed set predetermined object categories restricted form supervision limits generality usability since additional labeled data needed specify visual concept learning directly raw text images promising alternative leverages much broader source supervision demonstrate simple pre-training task predicting caption goes image efficient scalable way learn sota image representations scratch dataset 400 million image text pairs collected internet pre-training natural language used reference learned visual concepts describe new ones enabling zero-shot transfer model downstream tasks study performance approach benchmarking 30 different existing computer vision datasets spanning tasks ocr action recognition videos geo-localization many types fine-grained object classification model transfers non-trivially tasks often competitive fully supervised baseline without need dataset specific training instance match accuracy original resnet-50 imagenet zero-shot without needing use 1.28 million training examples trained authors alec radford jong wook kim chris hallacy aditya ramesh gabriel goh sandhini agarwal girish sastry amanda askell pamela mishkin jack clark gretchen krueger ilya sutskever links tabnine code completion referral http //bit.ly/tabnine-yannick youtube https //www.youtube.com/c/yannickilcher twitter https //twitter.com/ykilcher discord https //discord.gg/4h8xxdf bitchute https //www.bitchute.com/channel/yannic-kilcher minds https //www.minds.com/ykilcher parler https //parler.com/profile/yannickilcher linkedin https //www.linkedin.com/in/yannic-kilcher-488534136/ bilibili https //space.bilibili.com/1824646584 want support best thing share content want support financially completely optional voluntary lot people asked subscribestar https //www.subscribestar.com/yannickilcher patreon https //www.patreon.com/yannickilcher bitcoin btc bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq ethereum eth 0x7ad3513e3b8f66799f507aa7874b1b0ebc7f85e2 litecoin ltc lqw2trykyetvc8wjfkhpphtpbdm4vw7r9m monero xmr 4acl8agreo5hair8a9cevrw8peauwvnp1wnsdzxw7tzicdlhzagsgzhrqabdnfy8yum9fwjdvijphkrjv4fwt19cjzn9d4n,Machine Learning
applications non-euclidean distance metric spaces,get free access 2500 documentaries curiositystream http //go.thoughtleaders.io/1622620200907 use promo code `` zachstar '' sign stemerch store https //stemerch.com/ support channel https //www.patreon.com/zachstar paypal one time donation https //www.paypal.me/zachstaryt join channel get access perks https //www.youtube.com/channel/ucpcsacbqs-sjevfk_hmfy9w/join ►follow instagram https //www.instagram.com/zachstar/ twitter https //twitter.com/imzachstar mathematics used solve crime https //youtu.be/-cxbghgx5ue hypersphere universe https //youtu.be/iige2x8t6ma lumberjack feynman lectures https //www.youtube.com/watch v=gucaa-pwud8 list=plsuqrd4lfsutmb_7ik7kazxjtu2tpmed3 mathematics universe metric tensor video https //www.youtube.com/watch v=kt5sk-62-pg animations brainup studios http //brainup.in/ check spanish channel https //www.youtube.com/channel/ucnknu2xqblaspj6ckc8vtpa ►my setup space pictures https //amzn.to/2cc4kqj magnetic floating globe https //amzn.to/2vgpdn0 camera https //amzn.to/2rivyu5 mic https //amzn.to/35bkiri tripod https //amzn.to/2rgmtnl equilibrium tube https //amzn.to/2sowdrh ►check amazon store https //www.amazon.com/shop/zachstar,Machine Learning
talks 16 issam laradji build large-scale ml projects manage thousands experiments,title haven-ai build large-scale machine learning projects manage thousands experiments abstract demo briefly describe haven-ai https //github.com/haven-ai/haven-ai useful quickly building large-scale reproducible machine learning benchmarks allows us easily define launch visualize thousands experiments next starting empty project show compare 4 optimizers two datasets obtain visualizations ready presented research paper project report end demo able use haven-ai build deep learning benchmark launch large-scale experiments visualize debug failed experiments generate publishable results final machine learning report bio postdoc mcgill derek nowrouzezahrai elementai david vazquez position supported postdoc mitacs accelerate scholarship graduated phd university british columbia supervision mark schmidt may 2020 focus 1 weakly-supervised computer vision methods 2 new machine learning optimization methods 3 tools help developers define manage visualize large-scale experiments machine learning projects -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- please subscribe like video help keep motivated make awesome videos like one buy book approaching almost machine learning problem please visit https //bit.ly/buyaaml follow twitter https //twitter.com/abhi1thakur linkedin https //www.linkedin.com/in/abhi1thakur/ kaggle https //kaggle.com/abhishek instagram https //instagram.com/abhi4ml,Machine Learning
apply unsupervised learning audio data,read full story,Machine Learning
125 – ryan hall martial arts philosophy violence power grace,ryan hall jiu jitsu black belt ufc fighter philosopher martial arts please check sponsors get discount support podcast – powerdot use code lex https //powerdot.com/lex – babbel https //babbel.com use code lex – cash app download app use code “ lexpodcast ” would like get information podcast go https //lexfridman.com/podcast connect lexfridman twitter linkedin facebook medium youtube watch video versions conversations enjoy podcast please rate 5 stars apple podcasts,Machine Learning
neural network paints depressing russian cityscapes,russian doomer neural network creates paintings music videos tutorial stylegan2 trained thousands images soviet architecture read full story,Machine Learning
independent dependent samples statistics,comparing groups data either independent dependent samples type samples design impacts sample size requirements statistical power proper analysis even study ’ costs understanding implications type sample help design better study example often think … post independent dependent samples statistics appeared first statistics jim,Machine Learning
crypto-economic networks technological enablers scalable tribe-like collaboration,read full story,Machine Learning
10 free python programming courses beginners learn online,read full story,Machine Learning
semi-supervised learning label propagation,"tweet share share semi-supervised learning refers algorithms attempt make use labeled unlabeled training data semi-supervised learning algorithms unlike supervised learning algorithms able learn labeled training data popular approach semi-supervised learning create graph connects examples training dataset propagate known labels edges graph label unlabeled examples example approach semi-supervised learning label propagation algorithm classification predictive modeling tutorial discover apply label propagation algorithm semi-supervised learning classification dataset completing tutorial know intuition label propagation semi-supervised learning algorithm works develop semi-supervised classification dataset establish baseline performance supervised learning algorithm develop evaluate label propagation algorithm use model output train supervised learning algorithm let ’ get started semi-supervised learning label propagation photo thebluesdude rights reserved tutorial overview tutorial divided three parts label propagation algorithm semi-supervised classification dataset label propagation semi-supervised learning label propagation algorithm label propagation semi-supervised learning algorithm algorithm proposed 2002 technical report xiaojin zhu zoubin ghahramani titled “ learning labeled unlabeled data label propagation. ” intuition algorithm graph created connects examples rows dataset based distance euclidean distance nodes graph label soft labels label distribution based labels label distributions examples connected nearby graph many semi-supervised learning algorithms rely geometry data induced labeled unlabeled examples improve supervised methods use labeled data geometry naturally represented empirical graph g v e nodes v 1 … n represent training data edges e represent similarities — page 193 semi-supervised learning 2006 propagation refers iterative nature labels assigned nodes graph propagate along edges graph connected nodes procedure sometimes called label propagation “ propagates ” labels labeled vertices fixed gradually edges unlabeled vertices — page 48 introduction semi-supervised learning 2009 process repeated fixed number iterations strengthen labels assigned unlabeled examples starting nodes 1 2 … l labeled known label 1 −1 nodes l 1 … n labeled 0 node starts propagate label neighbors process repeated convergence — page 194 semi-supervised learning 2006 familiar label propagation algorithm let ’ look might use project first must define semi-supervised classification dataset semi-supervised classification dataset section define dataset semis-supervised learning establish baseline performance dataset first define synthetic classification dataset using make_classification function define dataset two classes binary classification two input variables 1,000 examples ... define dataset x make_classification n_samples=1000 n_features=2 n_informative=2 n_redundant=0 random_state=1 next split dataset train test datasets equal 50-50 split e.g 500 rows ... split train test x_train x_test y_train y_test train_test_split x test_size=0.50 random_state=1 stratify=y finally split training dataset half portion labels portion pretend unlabeled ... split train labeled unlabeled x_train_lab x_test_unlab y_train_lab y_test_unlab train_test_split x_train y_train test_size=0.50 random_state=1 stratify=y_train tying together complete example preparing semi-supervised learning dataset listed prepare semi-supervised learning dataset sklearn.datasets import make_classification sklearn.model_selection import train_test_split define dataset x make_classification n_samples=1000 n_features=2 n_informative=2 n_redundant=0 random_state=1 split train test x_train x_test y_train y_test train_test_split x test_size=0.50 random_state=1 stratify=y split train labeled unlabeled x_train_lab x_test_unlab y_train_lab y_test_unlab train_test_split x_train y_train test_size=0.50 random_state=1 stratify=y_train summarize training set size print 'labeled train set x_train_lab.shape y_train_lab.shape print 'unlabeled train set x_test_unlab.shape y_test_unlab.shape summarize test set size print 'test set x_test.shape y_test.shape running example prepares dataset summarizes shape three portions results confirm test dataset 500 rows labeled training dataset 250 rows 250 rows unlabeled data labeled train set 250 2 250 unlabeled train set 250 2 250 test set 500 2 500 supervised learning algorithm 250 rows train model semi-supervised learning algorithm 250 labeled rows well 250 unlabeled rows could used numerous ways improve labeled training dataset next establish baseline performance semi-supervised learning dataset using supervised learning algorithm fit labeled training data important would expect semi-supervised learning algorithm outperform supervised learning algorithm fit labeled data alone case semi-supervised learning algorithm skill case use logistic regression algorithm fit labeled portion training dataset ... define model model logisticregression fit model labeled dataset model.fit x_train_lab y_train_lab model used make predictions entire hold test dataset evaluated using classification accuracy ... make predictions hold test set yhat model.predict x_test calculate score test set score accuracy_score y_test yhat summarize score print 'accuracy .3f score 100 tying together complete example evaluating supervised learning algorithm semi-supervised learning dataset listed baseline performance semi-supervised learning dataset sklearn.datasets import make_classification sklearn.model_selection import train_test_split sklearn.metrics import accuracy_score sklearn.linear_model import logisticregression define dataset x make_classification n_samples=1000 n_features=2 n_informative=2 n_redundant=0 random_state=1 split train test x_train x_test y_train y_test train_test_split x test_size=0.50 random_state=1 stratify=y split train labeled unlabeled x_train_lab x_test_unlab y_train_lab y_test_unlab train_test_split x_train y_train test_size=0.50 random_state=1 stratify=y_train define model model logisticregression fit model labeled dataset model.fit x_train_lab y_train_lab make predictions hold test set yhat model.predict x_test calculate score test set score accuracy_score y_test yhat summarize score print 'accuracy .3f score 100 running algorithm fits model labeled training dataset evaluates holdout dataset prints classification accuracy note results may vary given stochastic nature algorithm evaluation procedure differences numerical precision consider running example times compare average outcome case see algorithm achieved classification accuracy 84.8 percent would expect effective semi-supervised learning algorithm achieve better accuracy accuracy 84.800 next let ’ explore apply label propagation algorithm dataset label propagation semi-supervised learning label propagation algorithm available scikit-learn python machine learning library via labelpropagation class model fit like classification model calling fit function used make predictions new data via predict function ... define model model labelpropagation fit model training dataset model.fit ... ... make predictions hold test set yhat model.predict ... importantly training dataset provided fit function must include labeled examples integer encoded per normal unlabeled examples marked label -1 model determine label unlabeled examples part fitting model model fit estimated labels labeled unlabeled data training dataset available via “ transduction_ ” attribute labelpropagation class ... get labels entire training dataset data tran_labels model.transduction_ familiar use label propagation algorithm scikit-learn let ’ look might apply semi-supervised learning dataset first must prepare training dataset concatenate input data training dataset single array ... create training dataset input x_train_mixed concatenate x_train_lab x_test_unlab create list -1 valued unlabeled row unlabeled portion training dataset ... create `` label '' unlabeled data nolabel -1 range len y_test_unlab list concatenated labels labeled portion training dataset correspond input array training dataset ... recombine training dataset labels y_train_mixed concatenate y_train_lab nolabel train labelpropagation model entire training dataset ... define model model labelpropagation fit model training dataset model.fit x_train_mixed y_train_mixed next use model make predictions holdout dataset evaluate model using classification accuracy ... make predictions hold test set yhat model.predict x_test calculate score test set score accuracy_score y_test yhat summarize score print 'accuracy .3f score 100 tying together complete example evaluating label propagation semi-supervised learning dataset listed evaluate label propagation semi-supervised learning dataset numpy import concatenate sklearn.datasets import make_classification sklearn.model_selection import train_test_split sklearn.metrics import accuracy_score sklearn.semi_supervised import labelpropagation define dataset x make_classification n_samples=1000 n_features=2 n_informative=2 n_redundant=0 random_state=1 split train test x_train x_test y_train y_test train_test_split x test_size=0.50 random_state=1 stratify=y split train labeled unlabeled x_train_lab x_test_unlab y_train_lab y_test_unlab train_test_split x_train y_train test_size=0.50 random_state=1 stratify=y_train create training dataset input x_train_mixed concatenate x_train_lab x_test_unlab create `` label '' unlabeled data nolabel -1 range len y_test_unlab recombine training dataset labels y_train_mixed concatenate y_train_lab nolabel define model model labelpropagation fit model training dataset model.fit x_train_mixed y_train_mixed make predictions hold test set yhat model.predict x_test calculate score test set score accuracy_score y_test yhat summarize score print 'accuracy .3f score 100 running algorithm fits model entire training dataset evaluates holdout dataset prints classification accuracy note results may vary given stochastic nature algorithm evaluation procedure differences numerical precision consider running example times compare average outcome case see label propagation model achieves classification accuracy 85.6 percent slightly higher logistic regression fit labeled training dataset achieved accuracy 84.8 percent accuracy 85.600 far good another approach use semi-supervised model take estimated labels training dataset fit supervised learning model recall retrieve labels entire training dataset label propagation model follows ... get labels entire training dataset data tran_labels model.transduction_ use labels along input data train evaluate supervised learning algorithm logistic regression model hope supervised learning model fit entire training dataset would achieve even better performance semi-supervised learning model alone ... define supervised learning model model2 logisticregression fit supervised learning model entire training dataset model2.fit x_train_mixed tran_labels make predictions hold test set yhat model2.predict x_test calculate score test set score accuracy_score y_test yhat summarize score print 'accuracy .3f score 100 tying together complete example using estimated training set labels train evaluate supervised learning model listed evaluate logistic regression fit label propagation semi-supervised learning numpy import concatenate sklearn.datasets import make_classification sklearn.model_selection import train_test_split sklearn.metrics import accuracy_score sklearn.semi_supervised import labelpropagation sklearn.linear_model import logisticregression define dataset x make_classification n_samples=1000 n_features=2 n_informative=2 n_redundant=0 random_state=1 split train test x_train x_test y_train y_test train_test_split x test_size=0.50 random_state=1 stratify=y split train labeled unlabeled x_train_lab x_test_unlab y_train_lab y_test_unlab train_test_split x_train y_train test_size=0.50 random_state=1 stratify=y_train create training dataset input x_train_mixed concatenate x_train_lab x_test_unlab create `` label '' unlabeled data nolabel -1 range len y_test_unlab recombine training dataset labels y_train_mixed concatenate y_train_lab nolabel define model model labelpropagation fit model training dataset model.fit x_train_mixed y_train_mixed get labels entire training dataset data tran_labels model.transduction_ define supervised learning model model2 logisticregression fit supervised learning model entire training dataset model2.fit x_train_mixed tran_labels make predictions hold test set yhat model2.predict x_test calculate score test set score accuracy_score y_test yhat summarize score print 'accuracy .3f score 100 running algorithm fits semi-supervised model entire training dataset fits supervised learning model entire training dataset inferred labels evaluates holdout dataset printing classification accuracy note results may vary given stochastic nature algorithm evaluation procedure differences numerical precision consider running example times compare average outcome case see hierarchical approach semi-supervised model followed supervised model achieves classification accuracy 86.2 percent holdout dataset even better semi-supervised learning used alone achieved accuracy 85.6 percent accuracy 86.200 achieve better results tuning hyperparameters labelpropagation model let know discover comments reading section provides resources topic looking go deeper books introduction semi-supervised learning 2009 chapter 11 label propagation quadratic criterion semi-supervised learning 2006 papers learning labeled unlabeled data label propagation 2002 apis sklearn.semi_supervised.labelpropagation api section 1.14 semi-supervised scikit-learn user guide sklearn.model_selection.train_test_split api sklearn.linear_model.logisticregression api sklearn.datasets.make_classification api articles semi-supervised learning wikipedia summary tutorial discovered apply label propagation algorithm semi-supervised learning classification dataset specifically learned intuition label propagation semi-supervised learning algorithm works develop semi-supervised classification dataset establish baseline performance supervised learning algorithm develop evaluate label propagation algorithm use model output train supervised learning algorithm questions ask questions comments best answer tweet share share post semi-supervised learning label propagation appeared first machine learning mastery",Machine Learning
030 multi-armed bandits pure-exploration wouter m. koolen,week dr. tim scarfe dr. keith duggar yannic kilcher discuss multi-arm bandits pure exploration dr. wouter m. koolen senior researcher machine learning group centrum wiskunde informatica wouter specialises machine learning theory game theory information theory statistics optimisation wouter currently interested pure exploration multi-armed bandit models game tree search accelerated learning sequential decision problems research cited 1000 times published neurips number 1 ml conference 14 times well lots exciting publications today going talk two studied settings control decision theory learning unknown environment multi-armed bandit mab reinforcement learning rl approaches agent stop learning start exploiting using knowledge obtained strategy leads minimal learning time 00:00:00 multi-arm bandits/show trailer 00:12:55 show introduction 00:15:50 bandits 00:18:58 taxonomy decision framework approaches 00:25:46 exploration vs exploitation 00:31:43 sharp divide modes 00:34:12 bandit measures success 00:36:44 connections reinforcement learning 00:44:00 apply pure exploration games 00:45:54 bandit lower bounds pure exploration renaissance 00:50:21 pure exploration compiler dreams 00:51:56 would px-compiler dsl look like 00:57:13 long arms bandit 01:00:21 causal models behind curtain arms 01:02:43 adversarial bandits arms trying beat 01:05:12 bandits optimization problem 01:11:39 asymptotic optimality vs practical performance 01:15:38 pitfalls hiding asymptotic cover 01:18:50 adding features bandits 01:27:24 moderate confidence regimes 01:30:33 algorithms choice highly sensitive bounds 01:46:09 post script keith interesting piece n quantum http //wouterkoolen.info https //www.cwi.nl/research-groups/ma ... machinelearning,Machine Learning
gaining sense control covid-19 pandemic winner ’ interview daniel wolffram,one kaggler took top marks across multiple covid-related challenges photo markus spiske unsplash today interview daniel whose notebooks earned top marks kaggle ’ cord-19 challenges kaggle hosted multiple challenges worked kaggle cord-19 dataset daniel 1st place three times including huge margin trec-covid challenge score 0.9 2nd place overall score 0.75 2nd place kaggle score 0.6 let ’ meet daniel daniel tell us bit daniel ’ daniel wolffram graduate student mathematics data science student assistant karlsruhe institute technology kit germany research interests include probabilistic forecasting causal inference machine learning part kaggle cord-19 challenge developed discovid.ai — search engine covid-19 literature right ’ working german covid-19 forecast hub writing master thesis building evaluating forecast ensembles covid-19 death counts well ’ surprise took top marks cord-19 challenge ’ quite relevant daniel indeed ’ also student assistant ’ worked several data science projects last 3 years opportunity work real world data different companies highly diverse domains — predicting waste sawmill analyzing flaws process surface galvanization testing efficiency marketing campaign time student assistant ’ also consulted company works lot text data — ’ gained first experience nlp also came across idea finding similar documents help topic model time client wanted stick another approach never really got try lda approach always stayed back mind get started competing kaggle undergraduate studies joined university group taught basics data science — mostly working kaggle projects titanic instacart challenge ’ also got job student assistant met one now-colleagues made decide enter particular competition friend mine showed competition excited right away remembered lda approach wanted try moreover competition launched covid cases climbing germany live first protective measures flatten curve taken — restaurants shops except supermarkets drugstores leisure facilities closed university closed exams got cancelled shocking numbers italy elsewhere intimidating uncertain atmosphere challenge actually way gain back control facing crisis head simply using skills best aware might biggest impact kept going thought even one medical researcher uses model stumbles upon something useful efforts already worth let ’ get technicalwhat preprocessing feature engineering normalize documents removed stop words performed tokenization lemmatization last step rather critical since cord-19 dataset contains highly technical papers scientific language ’ processed successfully standard packages important use scispacy package specialized processing biomedical scientific clinical text thus could also normalize technical terms chemical elements drug names etc. topic model work properly also necessary perform language detection remove non-english documents details found preprocessing notebook https //www.kaggle.com/danielwolffram/cord-19-create-dataframe augment data also searched article clinical trial ids link document international clinical trials registry platform ictrp required hand crafting several regular expressions — details found https //www.kaggle.com/danielwolffram/cord-19-match-clinical-trials machine learning methods use used latent dirichlet allocation lda unsupervised topic model learns hidden semantic relationships within corpus initially used find relevant articles task cord-19 challenge moved approach website implemented common search engine whoosh allows classical keyword searches complex boolean queries discovid.ai topic model used find related articles — idea article composed set underlying topics find articles similar topic mixture overlap topics might interesting reader could spark new insights topic mixture selected paper related articleclick interactivity https //dwolffram.github.io/cord19_lda_topics/ explore 50 topics model found within corpus — topic distribution words document seen mixture topics important insight data removing non-english articles corpus interestingly following topics discovered topic model topic 46 der die und bei mit von eine ist werden zu für sind oder einer des den nicht das als nach zur auf durch auch ein topic 40 de les des en une est dans du par un ou sont pour plus au que avec chez sur ’ une qui cas être pas ces topic 32 de en el los que se con las por un es para pacientes como más virus son tratamiento su infección puede ha casos enfermedad entre topic 7 un che con sono nel alla più ha tra gli degli come rischio ed pazienti nella nei osteonecrosis ad essere stato studio salute anche see one german french spanish italian encouraging demonstrates powerful lda learning hidden structures actually learns something meaningful surprised findings people first tried search engine became clear search keywords — unlike tasks kaggle composed much text quite problem queries simply short infer topics useful manner ’ decided implement common search engine whoosh initial search https //www.kaggle.com/danielwolffram/whoosh-search topic model used find related articles composed similar topics enables users easily browse corpus discover new insights spend time competition often efforts went data preparation cleaning especially beginning many changes data structure required lot adjustments ’ also read lot forum talked people medical background identify needs community ’ also extracting methodological keywords first quality indicator add cross references clinical trials mentioned papers ’ also spent good amount time learning figuring new things language detection building custom search engine whoosh ’ never done run time training prediction winning solution transforming documents training topic model takes roughly day teamworkhow team form started built widgets kaggle notebook easily explore cord-19 dataset good feedback increasing interest approach wanted make user-friendly could also used without technical background ’ got touch one colleagues ’ hesitate assist assembled small team build website discovid.ai team work together two colleagues working backend frontend another one got running server girlfriend came great design also animated introduction video https //medium.com/media/a3bafdc8ddb3721e6d80e83cd060a088/hrefhow competing team help succeed definitely helped build well-rounded solution user-friendly accessible anyone funwhat dream job ’ really drawn data science medical field wish use analytical skills meaningful project helps others think ’ also kept going throughout cord-19 challenge — never winning using strengths best part global crisis words wisdomwhat taken away competition meaningful project along way got know many interesting inspiring people world great see researchers around globe rushed together search answers global pandemic affects one us different ways paradoxically unites us advice getting started data science get started think ’ important get practical experience learn handle different kinds data easily transform format work math student also say ’ neglect fundamentals probability theory statistics data science science ’ important get intuition uncertainty limitations different approaches also think ’ always important first get clear understanding problem trying solve throwing complex machine learning models … find daniel ’ winning submission cord-19 https //www.kaggle.com/danielwolffram/discovid-ai-a-search-and-recommendation-engine gaining sense control covid-19 pandemic winner ’ interview daniel wolffram originally published kaggle blog medium people continuing conversation highlighting responding story,Machine Learning
remote video game coding camp improved autistic college students ’ self-efficacy comms,day 1 november 17 2020 theme envisioning future tech inclusion andrew begel microsoft accessible computer science education fall workshop hosted microsoft university washington create university colorado ’ coleman institute took place november 17-19 2020 consisted three half-days talks discussions planning new research dedicated making computer science education learning experiences accessible people disabilities information workshop found https //www.microsoft.com/en-us/research/event/accessible-cs-education-fall-workshop/,Machine Learning
extract features ensure sparsity sparse convolutional autoencoders,want build sparse convolutional autoencoder image classification understand extract features encoder output since convolutional autoencoder outputs feature maps classification need full connected layer transform feature maps vector way makes sense course could pool one value feature map could flatten maps put fc layer makes sense another question ensure sparsity training convolutional autoencoder see examples using kullback-leibler divergence dense layers outputs could find example using sparsity feature maps anyone experience link comments,Machine Learning
deepmind 's alphafold 2 explained ai breakthrough protein folding know n't,deepmind biology ai biology 's alexnet moment deepmind solves 50-year old problem protein folding prediction alphafold 2 improves deepmind 's 2018 alphafold system new architecture massively outperforms competition video take look alphafold 1 works gather alphafold 2 little information 's outline 0:00 intro overview 3:10 proteins protein folding 14:20 alphafold 1 overview 18:20 optimizing differentiable geometric model inference 25:40 learning spatial graph distance matrix 31:20 multiple sequence alignment evolutionarily similar sequences 39:40 distance matrix output results 43:45 guessing alphafold 2 's transformers 53:30 conclusion comments alphafold 2 blog https //deepmind.com/blog/article/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology alphafold 1 blog https //deepmind.com/blog/article/alphafold-using-ai-for-scientific-discovery alphafold 1 paper https //www.nature.com/articles/s41586-019-1923-7 msa reference https //arxiv.org/abs/1211.1281 casp14 challenge https //predictioncenter.org/casp14/index.cgi casp14 result bar chart https //www.predictioncenter.org/casp14/zscores_final.cgi paper title high accuracy protein structure prediction using deep learning abstract proteins essential life supporting practically functions large complex molecules made chains amino acids protein largely depends unique 3d structure figuring shapes proteins fold known “ protein folding problem ” stood grand challenge biology past 50 years major scientific advance latest version ai system alphafold recognised solution grand challenge organisers biennial critical assessment protein structure prediction casp breakthrough demonstrates impact ai scientific discovery potential dramatically accelerate progress fundamental fields explain shape world authors john jumper richard evans alexander pritzel tim green michael figurnov kathryn tunyasuvunakool olaf ronneberger russ bates augustin žídek alex bridgland clemens meyer simon kohl anna potapenko andrew j ballard andrew cowie bernardino romera-paredes stanislav nikolov rishub jain jonas adler trevor back stig petersen david reiman martin steinegger michalina pacholska david silver oriol vinyals andrew w senior koray kavukcuoglu pushmeet kohli demis hassabis links youtube https //www.youtube.com/c/yannickilcher twitter https //twitter.com/ykilcher discord https //discord.gg/4h8xxdf bitchute https //www.bitchute.com/channel/yannic-kilcher minds https //www.minds.com/ykilcher parler https //parler.com/profile/yannickilcher linkedin https //www.linkedin.com/in/yannic-kilcher-488534136/ want support best thing share content want support financially completely optional voluntary lot people asked subscribestar https //www.subscribestar.com/yannickilcher patreon https //www.patreon.com/yannickilcher bitcoin btc bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq ethereum eth 0x7ad3513e3b8f66799f507aa7874b1b0ebc7f85e2 litecoin ltc lqw2trykyetvc8wjfkhpphtpbdm4vw7r9m monero xmr 4acl8agreo5hair8a9cevrw8peauwvnp1wnsdzxw7tzicdlhzagsgzhrqabdnfy8yum9fwjdvijphkrjv4fwt19cjzn9d4n,Machine Learning
guide logistic regression sas,read full story,Machine Learning
ray dalio principles economic machine artificial intelligence arc life,ray dalio founder co-chairman co-chief investment officer bridgewater associates one world ’ largest successful investment firms famous principles radical truth transparency underlie culture ray one wealthiest people world ideas extend far beyond specifics made wealth ideas applicable everyone brilliantly summarized book principles conversation part artificial intelligence podcast would like get information podcast go https //lexfridman.com/ai connect lexfridman,Machine Learning
90 – dmitry korkin computational biology coronavirus,dmitry korkin professor bioinformatics computational biology worcester polytechnic institute specializes bioinformatics complex disease computational genomics systems biology biomedical data analytics came across dmitry ’ work february group used viral genome covid-19 reconstruct 3d structure major viral proteins interactions human proteins effect creating structural genomics map coronavirus making data open available researchers everywhere talked biology covid-19 sars viruses general computational methods,Machine Learning
lottery ticket hypothesis jonathan frankle,episode machine learning street talk chat jonathan frankle author lottery ticket hypothesis frankle continued researching sparse neural networks pruning lottery tickets leading really exciting follow-on papers chat discusses papers linear mode connectivity comparing rewinding fine-tuning neural network pruning full list papers linked also chat jonathan got deep learning research information diet work developing technology policy artificial intelligence really fun chat hope enjoy listening learn something thanks watching please subscribe huge thanks everyone r/machinelearning asked questions paper links discussed chat lottery ticket hypothesis finding sparse trainable neural networks https //arxiv.org/abs/1803.03635 linear mode connectivity lottery ticket hypothesis https //arxiv.org/abs/1912.05671 dissecting pruned neural networks https //arxiv.org/abs/1907.00262 training batchnorm batchnorm expressive power random features cnns https //arxiv.org/abs/2003.00152 state neural network pruning https //arxiv.org/abs/2003.03033 early phase neural network training https //arxiv.org/abs/2002.10365 comparing rewinding fine-tuning neural network pruning https //arxiv.org/abs/2003.02389 also mentioned block-sparse gpu kernels https //openai.com/blog/block-sparse-gpu-kernels/ balanced sparsity efficient dnn inference gpu https //arxiv.org/pdf/1811.00206.pdf playing lottery rewards multiple languages lottery tickets rl nlp https //arxiv.org/pdf/1906.02768.pdf r/machinelearning question list https //www.reddit.com/r/machinelearning/comments/g9jqe0/d_lottery_ticket_hypothesis_ask_the_author_a/ edited machinelearning deeplearning,Machine Learning
136 – dan carlin hardcore history,dan carlin historian political thinker podcaster please support podcast checking sponsors – athletic greens https //athleticgreens.com/lex use code lex get free vitamin – simplisafe https //simplisafe.com/lex use code lex get free security camera – magic spoon https //magicspoon.com/lex use code lex get free shipping – cash app https //cash.app/ use code lexpodcast get 10 episode links dan ’ twitter https //twitter.com/hardcorehistory dan ’ website https //www.dancarlin.com/ hardcore history podcast https //apple.co/2hx7haa common sense podcast https //apple.co/3mm6wpz podcast info podcast website https //lexfridman.com/podcast apple podcasts https //apple.co/2lwqzir spotify https //spoti.fi/2newcf8 rss https //lexfridman.com/feed/podcast/ youtube full episodes https //youtube.com/lexfridman youtube,Machine Learning
intuition power lockdown math ep 9,i^i visualized explained full playlist https //www.youtube.com/playlist list=plzhqobowtqdp5cveljj1bndouqrahvpev home page https //www.3blue1brown.com brought https //3b1b.co/ldm-thanks mistakes 1:06:20 changing r equal 0.69 said `` might think 2i ^x '' correct 's 'd think exp ln 2 ^x whatever complex number exp ln 2 beautiful notes ngân vũ https //twitter.com/thuynganvu/status/1263522876403011585 video matt parker https //youtu.be/9tlhqokmhga video red pen black pen https //youtu.be/abk1hk2ar2e -- -- -- -- -- -- -- -- -- video timeline thanks user `` nooonesperfect '' 0:18 exponential function i^i 1:26 question 1 2:27 plug-in imaginary number exp x polynomial 3:38 answer 1 explanation 7:35 really means i^i 9:14 e^it position vector 11:30 question 2 12:39 audience question twitter 13:14 answer 2 14:52 get traveling π/2 units time position vector e^it 19:48 question 3 20:42 audience tweets 23:34 answer 3 35:50 question 4 37:11 answer 4 40:11 exp rx b^x really works 46:28 question 5 47:49 audience tweets 49:26 answer 5 57:06 visualization f x exp r x i.e e^ r x r= unique complex number 1:06:06 questions think 1:08:51 audience tweets 1:09:09 power tower -- -- -- -- -- -- -- -- -- live question setup stats on-screen powered itempool https //itempool.com/ curious animations https //www.3blue1brown.com/faq manim music vincent rubinetti download music bandcamp https //vincerubinetti.bandcamp.com/album/the-music-of-3blue1brown stream music spotify https //open.spotify.com/album/1dvyjws8fbqxhrunag5w5u want contribute translated subtitles help review already made others need approval click gear icon video go subtitles/cc `` add subtitles/cc '' really appreciate helps make lessons accessible people -- -- -- -- -- -- -- -- -- 3blue1brown channel animating math senses word animate know drill youtube want stay posted new videos subscribe http //3b1b.co/subscribe various social media stuffs website https //www.3blue1brown.com twitter https //twitter.com/3blue1brown reddit https //www.reddit.com/r/3blue1brown instagram https //www.instagram.com/3blue1brown_animations/ patreon https //patreon.com/3blue1brown facebook https //www.facebook.com/3blue1brown,Machine Learning
françois chollet keras deep learning progress ai,françois chollet creator keras open source deep learning library designed enable fast user-friendly experimentation deep neural networks serves interface several deep learning libraries popular tensorflow integrated tensorflow main codebase back aside creating exceptionally useful popular library françois also world-class ai researcher software engineer google definitely outspoken controversial personality ai world especially realm ideas around future artificial intelligence,Machine Learning
yoshua bengio deep learning,"yoshua bengio along geoffrey hinton yann lecun considered one three people responsible advancement deep learning 1990s 2000s cited 139,000 times integral biggest breakthroughs ai past 3 decades video version available youtube would like get information podcast go https //lexfridman.com/ai connect lexfridman twitter linkedin facebook youtube watch video versions conversations",Machine Learning
elon musk neuralink ai autopilot pale blue dot,elon musk ceo tesla spacex neuralink co-founder several companies second time elon podcast watch first time youtube listen first time episode page read transcript pdf conversation part artificial intelligence podcast would like get information podcast go https //lexfridman.com/ai connect lexfridman twitter linkedin facebook medium youtube watch video versions conversations enjoy podcast,Machine Learning
87 – richard dawkins evolution intelligence simulation memes,richard dawkins evolutionary biologist author selfish gene blind watchmaker god delusion magic reality greatest show earth latest outgrowing god originator popularizer lot fascinating ideas evolutionary biology science general including funny enough introduction word meme 1976 book selfish gene context gene-centered view evolution exceptionally powerful idea outspoken bold often fearless defense science reason way,Machine Learning
10 great articles data science data engineering,read full story,Machine Learning
coefficient variation statistics,coefficient variation cv relative measure variability indicates size standard deviation relation mean standardized unitless measure allows compare variability disparate groups characteristics also known relative standard deviation rsd post … post coefficient variation statistics appeared first statistics jim,Machine Learning
`` dwell beauty life watch stars see running '' marcus aurelius,`` dwell beauty life watch stars see running '' marcus aurelius,Machine Learning
issue machine ethics robot rights,machine ethics robot rights quickly becoming hot topics artificial intelligence/robotics communities argue attempts allow machines make ethical decisions rights misguided instead propose new science safety engineering intelligent artificial agents particular issue challenge scientific community develop intelligent systems capable proving fact safe even recursive self-improvement read full story,Machine Learning
121 – eugenia kuyda friendship ai companion,eugenia kuyda co-founder replika ai companion please check sponsors get discount support podcast – dollar shave club https //dollarshaveclub.com/lex – doordash download app use code lex – cash app download app use code “ lexpodcast ” episode links eugenia ’ twitter https //twitter.com/ekuyda replika ’ twitter https //twitter.com/myreplika replika ’ website https //replika.ai would like get information podcast go https //lexfridman.com/podcast connect lexfridman twitter linkedin facebook medium youtube watch video versions conversations enjoy podcast please rate 5 stars,Machine Learning
switched signal texting introversion loneliness secure,switched signal texting introversion loneliness secure,Machine Learning
nvidia broadcast app ai-based application streaming broadcasting meeting features,video show features new nvidia broadcast application uses ai powerful features use streaming broadcasting online meetings please subscribe like video help keep motivated make awesome videos like one buy book approaching almost machine learning problem please visit https //bit.ly/buyaaml follow twitter https //twitter.com/abhi1thakur linkedin https //www.linkedin.com/in/abhi1thakur/ kaggle https //kaggle.com/abhishek instagram https //instagram.com/abhi4ml,Machine Learning
rt ykilcher 'm really excited share latest survey paper deep learning applications covid-19 📜 product months research really hope helps people find research projects impact pandemic https //journalofbigdata.springeropen.com/articles/10.1186/s40537-020-00392-9 100daysofmlcode,'m really excited share latest survey paper deep learning applications covid-19 📜 product months research really hope helps people find research projects impact pandemic journalofbigdata.springerope… 100daysofmlcode,Machine Learning
120 – françois chollet measures intelligence,françois chollet ai researcher google creator keras support podcast supporting sponsors get discount – babbel https //babbel.com use code lex – masterclass https //masterclass.com/lex – cash app download app use code “ lexpodcast ” episode links francois ’ twitter https //twitter.com/fchollet francois ’ website https //fchollet.com/ measure intelligence paper https //arxiv.org/abs/1911.01547 would like get information podcast go https //lexfridman.com/podcast connect lexfridman twitter linkedin facebook medium youtube watch video versions conversations enjoy podcast please rate 5 stars,Machine Learning
p dm crowd counting model day 3,using gradio interface crowd counting model try interface link today 's model based paper arxiv link distribution matching crowd counting repo interface shown crowd counting popular research problem applications journalism human traffic management surveillance paper linked proposes novel approach crowd counting existing methods crowd counting detect-then-count method method detects every person image counts number individuals identified technique accurate sensitive occlusion noise density map calculations method divides image regions estimates human density region method accurate especially larger crowds paper uses variation existing density map methods existing methods take annotated data every human identified location image process gaussian generate smoothened density maps loss predicted output annotated data calculated difference density maps dm-count proposes alternative method calculate loss predicted output annotation named `` optimal transport '' loss function takes number humans predicted location predicted output calculates total amount movement needed human human go prediction annotation total movement calculated loss function paper states method tighter error bounds previous methods calculating loss reduces error compared sota models 16 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 've working lot newly researched models lately wanted share interesting models 've worked part series post interesting model along description research purpose interactive interface generated gradio previous post model day 2 link comments,Machine Learning
149 – diana walsh pasulka aliens technology religion nature belief,diana walsh pasulka professor philosophy religion uncw author american cosmic ufos religion technology please support podcast checking sponsors – lmnt https //drinklmnt.com/lex get free shipping – grammarly https //grammarly.com/lex get 20 premium – business wars https //wondery.com/business-wars/ – cash app https //cash.app/ use code lexpodcast get 10 episode links diana ’ website https //uncw.edu/par/faculty/faculty-pasulka.html american cosmic book https //amzn.to/3ak2kaj podcast info podcast website https //lexfridman.com/podcast apple podcasts https //apple.co/2lwqzir spotify https //spoti.fi/2newcf8 rss https //lexfridman.com/feed/podcast/ youtube full episodes https //youtube.com/lexfridman youtube clips https //youtube.com/lexclips support connect – check sponsors ’ best way support podcast – support patreon https //www.patreon.com/lexfridman – twitter https //twitter.com/lexfridman – instagram https //www.instagram.com/lexfridman – linkedin https //www.linkedin.com/in/lexfridman – facebook https //www.facebook.com/lexfridmanpage – medium https //medium.com/ lexfridman outline ’ timestamps episode podcast players able click timestamp jump time 00:00 – introduction 08:28 – real 13:56 – beliefs become reality 18:59 – donald hoffman 22:57 – immanuel kant ’ critique pure reason 26:27 – ayn rand 33:25 – religions start 48:38 – religion evolutionary advantage 53:59 – religion used propaganda 58:32 – nietzsche mean “ god dead ” 1:03:59 – american cosmic 1:07:45 – aliens look like 1:16:28 – history space programs 1:19:30 – jacques vallee 1:28:55 – artificial intelligence 1:34:25 – ufology community 1:45:38 – psychedelics 1:49:35 – tic tac ufo 1:58:09 – roswell ufo incident 2:09:14 – bob lazar 2:12:50 – monoliths desert 2:23:39 – humans co-evolve ai 2:26:58 – neuralink 2:31:48 – singularity 2:41:39 – books nietzsche 2:46:44 – books hannah arendt 2:51:43 – fear death 2:56:11 – meaning life,Machine Learning
cure imposter syndrome data science,imposter syndrome common experience data scientists ways tackle succeed despite read full story,Machine Learning
visiting austin check places still torn austin sf boston went terry black 's bbq 's good everyone says best brisket 've ever,visiting austin check places still torn austin sf boston went terry black 's bbq 's good everyone says best brisket 've ever,Machine Learning
teaching upper level pure math course almost died,get 25 year subscription curiositystream ends jan 3rd 2021 use code `` zachstar '' sign https //curiositystream.thld.co/zachstarnov18 stemerch store https //stemerch.com/ support channel https //www.patreon.com/zachstar paypal one time donation https //www.paypal.me/zachstaryt join channel get access perks https //www.youtube.com/channel/ucpcsacbqs-sjevfk_hmfy9w/join ►previous videos self teaching video 1 https //youtu.be/ev3jf4vjegw video 2 https //youtu.be/pzsd3dfyn7c ►get book associate link https //amzn.to/3exvikn ►follow instagram https //www.instagram.com/zachstar/ twitter https //twitter.com/imzachstar 2d graphing software https //www.desmos.com/calculator check spanish channel https //www.youtube.com/channel/ucnknu2xqblaspj6ckc8vtpa 00:00 intro 2:41 real analysis 5:30 long book take 6:18 approach practice problems 8:08 like course 8:42 quick example 10:53 advice self teaching 15:38 textbook used 17:50 ending/sponsorship ►my setup space pictures https //amzn.to/2cc4kqj magnetic floating globe https //amzn.to/2vgpdn0 camera https //amzn.to/2rivyu5 mic https //amzn.to/35bkiri tripod https //amzn.to/2rgmtnl equilibrium tube https //amzn.to/2sowdrh ►check amazon store https //www.amazon.com/shop/zachstar,Machine Learning
partial dependence plots discover variables influencing model,quick techniques find variables influencing model results much visualize using partial dependence plots read full story,Machine Learning
unboxing installing nvidia rtx 3090 bfgpu,christmas nvidia gifted rtx 3090 gpu decided put good use needed unbox install decided make video around first unboxing video hope like please subscribe like video help keep motivated make awesome videos like one buy book approaching almost machine learning problem please visit https //bit.ly/buyaaml follow twitter https //twitter.com/abhi1thakur linkedin https //www.linkedin.com/in/abhi1thakur/ kaggle https //kaggle.com/abhishek instagram https //instagram.com/abhi4ml,Machine Learning
natural language processing tokenization basic,video learn basics tokenization nlp please note basic video tokenization looking advanced tokenization techniques check advanced video please subscribe like video help keep motivated make awesome videos like one buy book approaching almost machine learning problem please visit https //bit.ly/buyaaml follow twitter https //twitter.com/abhi1thakur linkedin https //www.linkedin.com/in/abhi1thakur/ kaggle https //kaggle.com/abhishek instagram https //instagram.com/abhi4ml,Machine Learning
python panda package tutorial,key methods understanding utilizing pandas read full story,Machine Learning
12 key lessons ml researchers practitioners,read full story,Machine Learning
037 tour de bayesian connor tann,connor tan physicist senior data scientist working multinational energy company co-founded leads data science team holds first-class degree experimental theoretical physics cambridge university master 's particle astrophysics specializes application machine learning models bayesian methods today explore history pratical utility unique capabilities bayesian methods also discuss computational difficulties inherent bayesian methods along modern methods approximate solutions markov chain monte carlo finally discuss bayesian optimization context automl may one day put data scientists like connor work panel dr. keith duggar alex stenlake dr. tim scarfe 00:00:00 duggars philisophical ramblings bayesianism 00:05:10 introduction 00:07:30 small datasets prior scientific knowledge 00:10:37 bayesian methods probability theory 00:14:00 bayesian methods demand hard computations 00:15:46 uncertainty matter estimators 00:19:29 updating combining knowledge key feature 00:25:39 frequency reasonable expectation primary concept 00:30:02 gambling coin flips 00:37:32 rev thomas bayes 's pool table 00:40:37 ignorance priors beautiful yet hard 00:43:49 connections common distributions 00:49:13 curious universe benford 's law 00:55:17 choosing priors tale two factories 01:02:19 integration computational achilles heel 01:35:25 bayesian social context ml community 01:10:24 frequentist methods first approximation 01:13:13 driven bayesian methods small sample size 01:18:46 bayesian optimization automl job killer 01:25:28 different approaches hyper-parameter optimization 01:30:18 advice aspiring bayesians 01:33:59 would connor interview next connor tann https //www.linkedin.com/in/connor-tann-a92906a1/ https //twitter.com/connossor,Machine Learning
rt lexfridman one favorite podcasts ever russian brother lexfridman really great flow available video audio spotify https //www.instagram.com/p/ckzyl4if338/ igshid=67qa2drbtvzg,one favorite podcasts ever russian brother lexfridman really great flow available video audio spotify instagram.com/p/ckzyl4if338/…,Machine Learning
3 books optimization machine learning,tweet share share optimization field mathematics concerned finding good best solution among many candidates important foundational topic required machine learning machine learning algorithms fit historical data using optimization algorithm additionally broader problems model selection hyperparameter tuning also framed optimization problem although background optimization critical machine learning practitioners daunting topic given often described using highly mathematical language post discover top books optimization helpful machine learning practitioners let ’ get started books optimization machine learning photo patrick alexander rights reserved overview field optimization enormous touches many fields study hundreds books topic textbooks filed math proofs fair enough given highly mathematical subject nevertheless books provide approachable description optimization algorithms optimization algorithms relevant machine learning instead useful focus small subset algorithms frankly hard group optimization algorithms many concerns nevertheless important idea optimization underlies simpler algorithms linear regression logistic regression e.g convex optimization least squares newton methods etc neural networks first-order methods gradient descent etc. foundational optimization algorithms covered optimization textbooks optimization problems machine learning well behaved optimization used automl hyperparameter tuning therefore knowledge stochastic optimization algorithms required simulated annealing genetic algorithms particle swarm etc. although optimization algorithms also type learning algorithm referred biologically inspired computation computational intelligence therefore take look books cover classical optimization algorithms well books alternate optimization algorithms fact first book look covers types algorithms much algorithms optimization book written mykel kochenderfer tim wheeler published 2019 algorithms optimization book might one textbooks ’ seen broadly covers field optimization techniques relevant modern machine learning book provides broad introduction optimization focus practical algorithms design engineering systems cover wide variety optimization topics introducing underlying mathematical problem formulations algorithms solving figures examples exercises provided convey intuition behind various approaches — page xiiix algorithms optimization 2019 importantly algorithms range univariate methods bisection line search etc first-order methods gradient descent second-order methods newton ’ method direct methods pattern search stochastic methods simulated annealing population methods genetic algorithms particle swarm much includes technical descriptions algorithms references worked examples algorithms julia ’ shame examples python would make book near perfect eyes complete table contents book listed chapter 01 introduction chapter 02 derivatives gradients chapter 03 bracketing chapter 04 local descent chapter 05 first-order methods chapter 06 second-order methods chapter 07 direct methods chapter 08 stochastic methods chapter 09 population methods chapter 10 constraints chapter 11 linear constrained optimization chapter 12 multiobjective optimization chapter 13 sampling plans chapter 14 surrogate models chapter 15 probabilistic surrogate models chapter 16 surrogate optimization chapter 17 optimization uncertainty chapter 18 uncertainty propagation chapter 19 discrete optimization chapter 20 expression optimization chapter 21 multidisciplinary optimization like book lot full valuable practical advice highly recommend learn algorithms optimization 2019 numerical optimization book written jorge nocedal stephen wright published 2006 numerical optimization book focused math theory optimization algorithms presented cover many foundational techniques used common machine learning algorithms may little heavy average practitioner book intended textbook graduate students mathematical subjects intend book used graduate-level courses optimization offered engineering operations research computer science mathematics departments — page xviii numerical optimization 2006 even though highly mathematical descriptions algorithms precise may provide useful alternative description complement books listed complete table contents book listed chapter 01 introduction chapter 02 fundamentals unconstrained optimization chapter 03 line search methods chapter 04 trust-region methods chapter 05 conjugate gradient methods chapter 06 quasi-newton methods chapter 07 large-scale unconstrained optimization chapter 08 calculating derivatives chapter 09 derivative-free optimization chapter 10 least-squares problems chapter 11 nonlinear equations chapter 12 theory constrained optimization chapter 13 linear programming simplex method chapter 14 linear programming interior-point methods chapter 15 fundamentals algorithms nonlinear constrained optimization chapter 16 quadratic programming chapter 17 penalty augmented lagrangian methods chapter 18 sequential quadratic programming chapter 19 interior-point methods nonlinear programming ’ solid textbook optimization learn numerical optimization 2006 prefer theoretical approach subject another widely used mathematical book optimization “ convex optimization ” written stephen boyd lieven vandenberghe published 2004 computational intelligence introduction book written andries engelbrecht published 2007 computational intelligence introduction book provides excellent overview field nature-inspired optimization algorithms also referred computational intelligence includes fields evolutionary computation swarm intelligence book far less mathematical previous textbooks focused metaphor inspired system configure use specific algorithms lots pseudocode explanations material introductory nature shy away details present mathematical foundations interested reader intention book provide thorough attention computational intelligence paradigms algorithms give overview popular frequently used models — page xxix computational intelligence introduction 2007 algorithms like genetic algorithms genetic programming evolutionary strategies differential evolution particle swarm optimization useful know machine learning model hyperparameter tuning perhaps even model selection also form core many modern automl systems complete table contents book listed part introduction chapter 01 introduction computational intelligence part ii artificial neural networks chapter 02 artificial neuron chapter 03 supervised learning neural networks chapter 04 unsupervised learning neural networks chapter 05 radial basis function networks chapter 06 reinforcement learning chapter 07 performance issues supervised learning part iii evolutionary computation chapter 08 introduction evolutionary computation chapter 09 genetic algorithms chapter 10 genetic programming chapter 11 evolutionary programming chapter 12 evolution strategies chapter 13 differential evolution chapter 14 cultural algorithms chapter 15 coevolution part iv computational swarm intelligence chapter 16 particle swarm optimization chapter 17 ant algorithms part v artificial immune systems chapter 18 natural immune system chapter 19 artificial immune models part vi fuzzy systems chapter 20 fuzzy sets chapter 21 fuzzy logic reasoning ’ fan book recommend learn computational intelligence introduction 2007 summary post discovered books optimization algorithms helpful know applied machine learning miss good book optimization let know comments read books listed let know think comments tweet share share post 3 books optimization machine learning appeared first machine learning mastery,Machine Learning
donald knuth algorithms tex life art computer programming,donald knuth one greatest impactful computer scientists mathematicians ever recipient 1974 turing award considered nobel prize computing author multi-volume work magnum opus art computer programming made several key contributions rigorous analysis computational complexity algorithms popularized asymptotic notation affectionately know big-o notation also created tex typesetting computer scientists physicists mathematicians scientists engineers use write technical papers make look,Machine Learning
p langhuan nlp labeling app ner classify driven dataframe,langhuan simple nlp human tagging app driven pandas dataframe supports classification ner tasks build supervised data machine learning https //github.com/raynardj/langhuan ​ https //preview.redd.it/8tbeany8n0d61.png width=2758 format=png auto=webp s=193b3675e0592a2ac54eaa3f7776f6e004552070 allows change tagged entries simpler user segregations edit options run n users verifying entry k times n k okay order entries score column predictions model rule base score etc deploy strategy taggers jump lowest highest score link comments,Machine Learning
135 – charles isbell computing interactive ai race america,charles isbell dean college computing georgia tech please support podcast checking sponsors – neuro https //www.getneuro.com use code lex get 15 – decoding digital https //appdirect.com/decoding-digital – masterclass https //masterclass.com/lex get 15 annual sub – cash app https //cash.app/ use code lexpodcast get 10 episode links charles ’ twitter https //twitter.com/isbellhfh charles ’ website https //www.cc.gatech.edu/~isbell/ podcast info podcast website https //lexfridman.com/podcast apple podcasts https //apple.co/2lwqzir spotify https //spoti.fi/2newcf8 rss https //lexfridman.com/feed/podcast/ youtube full episodes https //youtube.com/lexfridman youtube clips https //youtube.com/lexclips support connect – check sponsors ’ best way support,Machine Learning
kernels,today yannic lightspeed kilcher spoke alex stenlake kernel methods kernel remember weird kernel things everyone obsessed deep learning representer theorem reproducible kernel hilbert spaces svms kernel ridge regression remember hope enjoy conversation 00:00:00 tim intro 00:01:35 yannic clever insight discussion 00:03:25 street talk alex intro 00:05:06 kernels taught 00:09:20 computational tractability 00:10:32 maths 00:11:50 kernel 00:19:39 kernel latent expansion 00:23:57 overfitting 00:24:50 hilbert spaces 00:30:20 compare dl 00:31:18 back hilbert spaces 00:45:19 computational tractability 2 00:52:23 curse dimensionality 00:55:01 rbf infinite taylor series 00:57:20 margin/svm 01:00:07 krr/dual 01:03:26 complexity compute kernels vs deep learning 01:05:03 good small problems vs deep learning 01:07:50 whats special rbf kernel 01:11:06 another dl comparison 01:14:01 representer theorem 01:20:05 relation back prop 01:25:10 connection nlp/transformers 01:27:31 else kernels good 01:34:34 deep learning vs dual kernel methods 01:33:29 thoughts ai 01:34:35 outro,Machine Learning
032- simon kornblith googleai simclr paper haul,week dr. tim scarfe sayak paul yannic kilcher speak dr. simon kornblith google brain ph.d mit simon trying understand neural nets simon second author seminal google ai simclr paper also cover `` wide deep networks learn things `` `` whats loss function image classification `` `` big self-supervised models strong semi-supervised learners '' simon used neuroscientist also gives us story unique journey ml 00:00:00 show teaser `` short version '' 00:18:34 show intro 00:22:11 relationship neuroscience machine learning 00:29:28 similarity analysis evolution representations neural networks 00:39:55 expressability nns 00:42:33 whats loss function image classification 00:46:52 loss function implications transfer learning 00:50:44 simclr paper 01:00:19 contrast simclr byol 01:01:43 data augmentation 01:06:35 universality image representations 01:09:25 universality augmentations 01:23:04 gpt-3 01:25:09 gans data augmentation 01:26:50 julia language skornblith https //www.linkedin.com/in/simon-kornblith-54b2033a/ https //arxiv.org/abs/2010.15327 wide deep networks learn things uncovering neural network representations vary width depth https //arxiv.org/abs/2010.16402 's loss function image classification https //arxiv.org/abs/2002.05709 simple framework contrastive learning visual representations https //arxiv.org/abs/2006.10029 big self-supervised models strong semi-supervised learners,Machine Learning
find importance weights layers,might stupid question answer would really help lot sort weights parameters layer deep neural net based importance much contribute towards performance model task given evaluation metric link comments,Machine Learning
access mean ​ re-thinking accessibility research problems tackle,day 3 november 19 2020 theme unblocking pipeline education employment cecily morrison microsoft accessible computer science education fall workshop hosted microsoft university washington create university colorado ’ coleman institute took place november 17-19 2020 consisted three half-days talks discussions planning new research dedicated making computer science education learning experiences accessible people disabilities information workshop found https //www.microsoft.com/en-us/research/event/accessible-cs-education-fall-workshop/,Machine Learning
leslie kaelbling reinforcement learning planning robotics,leslie kaelbling roboticist professor mit recognized work reinforcement learning planning robot navigation several topics ai ijcai computers thought award editor-in-chief prestigious journal machine learning research video version available youtube would like get information podcast go https //lexfridman.com/ai connect lexfridman twitter linkedin facebook medium youtube watch video versions conversations,Machine Learning
automation girl scout events,shutdowns brought opportunity daughter participate virtual scouting events united states event registration form changed took chance try new web scraping skills inspiring daughter power code everyday tasks read full story,Machine Learning
rajat monga tensorflow,rajat monga engineering director google leading tensorflow team would like get information podcast go https //lexfridman.com/ai connect lexfridman twitter linkedin facebook medium youtube watch video versions conversations,Machine Learning
10 questions consider setting corporate a.i project,read full story,Machine Learning
stripe cohort analysis python pandas,read full story,Machine Learning
people die suicide forms violence combined people struggling see lot outrage online moment n't see much compassion gandhi said `` eye eye make whole world blind '',people die suicide forms violence combined people struggling see lot outrage online moment n't see much compassion gandhi said `` eye eye make whole world blind '',Machine Learning
big data bring transformative improvements medical care,healthcare landscape providers lawmakers alike faced challenge making best possible decisions patients industry whole choosing best treatments using resources responsible manner medical leaders making decisions daily basis significantly impact health outcomes costs read full story,Machine Learning
data science toolkit concepts code,read full story,Machine Learning
10 insightful practical ai/ml books read 2021,still planning 2021 reading list 10 ai/ml books consider published past two years cover range topics fundamental concepts algorithms applications collection author talks book reviews might helpful check diving deep books artificial intelligence guide thinking humans melanie mitchell rebooting ai gary marcus ernest davis human compatible artificial intelligence problem control stuart russel look like thing love artificial intelligence works 's making world weirder place janelle shane hundred-page machine learning book andriy burkov interpretable machine learning guide making black box models explainable christoph molnar machine learning yearning andrew ng machine learning engineering andriy burkov hands-on machine learning scikit-learn keras tensorflow concepts tools techniques build intelligent systems 2nd edition aurélien géron approaching almost machine learning problem abhishek thakur feel free comment add new book recommendations link comments,Machine Learning
forecasting future customer inflow numbers using arima fbprophet,read full story,Machine Learning
036 max welling quantum manifolds symmetries ml,today fantastic conversation professor max welling vp technology qualcomm technologies netherlands b.v. max strong believer power data computation relevance artificial intelligence fundamental blank slate paradgm machine learning experience data alone currently rule roost max wants build house domain knowledge top blank slate max thinks predictions without assumptions generalization without inductive bias bias-variance tradeoff tells us need use additional human knowledge data insufficient max welling pioneered many sophistocated inductive priors dl models developed recent years allowing us use deep learning non-euclidean data i.e graphs/topology field called `` geometric deep learning '' allowing network architectures recognise new symmetries data example gauge se 3 equivariance max also brought many concepts physics playbook ml example quantum even bayesian approaches episode miss might best yet panel dr. tim scarfe yannic kilcher alex stenlake 00:00:00 show introduction 00:04:37 protein fold deepmind -- use se 3 transformer 00:09:58 machine learning progressed 00:19:57 quantum deformed neural networks paper 00:22:54 probabilistic numeric convolutional neural networks paper 00:27:04 ilia karmanov qualcomm interview mini segment 00:32:04 main show intro 00:35:21 max known community 00:36:35 max nurtures talent freedom relationship key 00:40:30 selecting research directions guidance 00:43:42 priors vs experience bias/variance trade-off 00:48:47 generative models gpt-3 00:51:57 bias/variance trade -- priors hurt us 00:54:48 capsule networks 01:03:09 old ideas whould revive 01:04:36 hardware lottery paper 01:07:50 greatness ca n't planned kenneth stanley reference 01:09:10 new sort peer review originality 01:11:57 quantum computing 01:14:25 quantum deformed neural networks paper 01:21:57 probabalistic numeric convolutional neural networks 01:26:35 matrix exponential 01:28:44 ideas physics i.e chaos holography renormalisation 01:34:25 reddit 01:37:19 open review system ml 01:41:43 outro,Machine Learning
hamming codes part 2 elegance,part 1 https //youtu.be/x8jsijhllia watch ben eater 's video https //youtu.be/h0jloehrkas viewer-supported https //3b1b.co/hamming-thanks cleaner perspective hamming error correction codes read hamming 's perspective discovery codes chapter 12 `` art science engineering '' https //amzn.to/3lwcnmh heavily related chessboard puzzle matt parker https //youtu.be/as7gkm7y7h4 're curious learn bit shannon father information theory take look documentary https //amzn.to/2rhk5hl -- -- -- -- -- -- -- -- -- animations largely made using manim scrappy open-source python library https //github.com/3b1b/manim want check feel compelled warn 's well-documented tool many quirks might expect library someone wrote use mind music vincent rubinetti download music bandcamp https //vincerubinetti.bandcamp.com/album/the-music-of-3blue1brown stream music spotify https //open.spotify.com/album/1dvyjws8fbqxhrunag5w5u -- -- -- -- -- -- -- -- -- 3blue1brown channel animating math senses word animate know drill youtube want stay posted new videos subscribe http //3b1b.co/subscribe various social media links website https //www.3blue1brown.com twitter https //twitter.com/3blue1brown reddit https //www.reddit.com/r/3blue1brown instagram https //www.instagram.com/3blue1brown_animations/ patreon https //patreon.com/3blue1brown facebook https //www.facebook.com/3blue1brown,Machine Learning
top marks student kaggler bengali.ai winner ’ interview linsho kaku,kaggler deoxy takes first-place sets stage next competition please join us congratulating linsho kaku aka deoxy solo first-place win bengali.ai handwritten grapheme classification challenge read winning solution 1st place solution code random ink sankarshan mukhopadhyay flickrlet ’ meet linsho linsho would like share linsho student rio yokota laboratory tokyo institute technology main theme lab high performance computing advanced architectures including gpus also deal deep learning one applications ’ also intern future inc. working ocr task prior experience domain knowledge helped succeed competition experience working ocr tasks intern big advantage ease able pre-process data create models thanks intern experience ’ never specialized few-shot learning major factor scores top teams time around however think knowledge paper shared lab made big difference let ’ get bit technicaldid past research previous competitions inform approach thinking approach went kaggle discussions presented addition often consulted science direct resources achieve few-shot learning preprocessing pre-processing cropping noise reduction done images processes improve recognition accuracy rather tended reduce amount information needed disadvantages cropping smaller area required character area erasing extra characters far outweigh advantages giving clean input important insight data essential task needed classification three types components per se creation model could recognize classes given classification three types components hint solve essential task say division manually determined components appropriate abstracting structures appear character likely improve accuracy classification unknown classes time used method generate font image characters handwritten characters generation model based style style transformation model called cyclegan series models font image classification model connected generative model considered handwritten character classification model view font image generated generative model considered feature middle layer series handwriting classification models ’ highly likely pixel font image intermediate feature generated structure font image observing relatively narrow portion handwriting thought generating features abstract character structure think able build system biggest factor approach tools use used pytorch deep learning framework jupyter notebook ide hardware setup use servers 4 tesla v100 run time winning solution cyclegan takes following time training time 4 tesla v100 2.5 days prediction time 40 min x 2 ensemble 2 model apart took time work usual class classification models words wisdomok walking away anything new result competition gained new skills allow consistent approach future challenges advice getting started data science grandmaster ’ common sense seems obvious always best win funif could run kaggle competition problem would want pose kagglers ’ like propose practical ocr task evaluated end-to-end handwriting detection recognition example something aims transcription digitization handwritten notes feel general-purpose method detection yet established spite sufficient recognition accuracy however field object detection quite variety methods considered think ’ lot hope active discussion development linsho kaku master student tokyo institute technology supervised rio yokota research interests include deep learning image processing optical character recognition top marks student kaggler bengali.ai winner ’ interview linsho kaku originally published kaggle blog medium people continuing conversation highlighting responding story,Machine Learning
introducing datamuni no-nonsense platform machine learning data science articles,unusual video introduce datamuni free platform writing reading good quality articles machinelearning datascience everything else data science like artificial intelligence deep learning python etc visit platform www.datamuni.com please subscribe like video help keep motivated make awesome videos like one buy book approaching almost machine learning problem please visit https //bit.ly/buyaaml follow twitter https //twitter.com/abhi1thakur linkedin https //www.linkedin.com/in/abhi1thakur/ kaggle https //kaggle.com/abhishek instagram https //instagram.com/abhi4ml,Machine Learning
alternative hypotheses main ideas,statistics hypothesis testing supposed two hypotheses primary null hypothesis alternative hypothesis statquest explains need alternative hypothesis even though hypothesis testing tends focus null note statquest follows hypothesis testing null hypothesis https //youtu.be/0oc49dya3hu 'd like learn p-values check ... p-values interpret https //youtu.be/vemztem63gy calculate p-values https //youtu.be/jqc3yx0-q9e lastly would like learn statistical tests see https //www.youtube.com/playlist list=plblh5jkooluizaekcliuxqfjpilapw8nu ⭐ note code use kite free ai-powered coding assistant help code faster smarter kite plugin integrates top editors ides give smart completions documentation ’ typing love https //www.kite.com/get-kite/ utm_medium=referral utm_source=youtube utm_campaign=statquest utm_content=description-only complete index statquest videos check https //statquest.org/video-index/ 'd like support statquest please consider ... patreon https //www.patreon.com/statquest ... ... youtube membership https //www.youtube.com/channel/uctyluttgs3k1fg4y5tahlbw/join ... cool statquest t-shirt sweatshirt usa/europe https //teespring.com/stores/statquest everywhere https //www.redbubble.com/people/starmer/works/40421224-statquest-double-bam asc=u p=t-shirt ... buying one two songs go large get whole album https //joshuastarmer.bandcamp.com/ ... donating statquest https //www.paypal.me/statquest lastly want keep research create new statquests follow twitter https //twitter.com/joshuastarmer 0:00 awesome song introduction 1:49 alternative hypothesis 3:21 testing null vs alternative 2 groups 5:38 testing null vs alternative 3 groups 8:00 summary next steps statquest alternativehypothesis,Machine Learning
assessing covid-19 vaccination experiment results,moderna announced encouraging preliminary results covid-19 vaccine post assess available data explain vaccine ’ effectiveness really means also look moderna ’ experimental design examine incorporates statistical procedures concepts discuss throughout blog posts books concepts include experimental … post assessing covid-19 vaccination experiment results appeared first statistics jim,Machine Learning
recruit ponpare japan ’ leading joint coupon site offering huge discounts everything from…,"halla yang finished 2nd ahead 1,191 data scientists experience working time series data helped use unsupervised… continue reading kaggle blog »",Machine Learning
79 – lee smolin quantum gravity einstein ’ unfinished revolution,lee smolin theoretical physicist co-inventor loop quantum gravity contributor many interesting ideas cosmology quantum field theory foundations quantum mechanics theoretical biology philosophy science author several books including one critiques state physics string theory called trouble physics latest book einstein ’ unfinished revolution search lies beyond quantum episode links books mentioned – einstein ’ unfinished revolution lee smolin https //amzn.to/2tsf5c3 – trouble physics lee smolin https //amzn.to/2v1fmzy – method paul feyerabend,Machine Learning
115 – dileep george brain-inspired ai,dileep george researcher intersection neuroscience artificial intelligence co-founder vicarious formerly co-founder numenta early work hierarchical temporal memory recursive cortical networks today dileep ’ always sought engineer intelligence closely inspired human brain support channel supporting sponsors click links get discount – babbel https //babbel.com use code lex – masterclass https //masterclass.com/lex – raycon https //buyraycon.com/lex would like get information podcast go https //lexfridman.com/ai connect lexfridman twitter linkedin facebook medium youtube,Machine Learning
theoretical papers transformers attention mechanism seq2seq,recently lot interest applying transformers related ideas nlp computer vision theoretically-minded person master student wondering well-understood ideas beyond intuitive justifications attention mechanism generally work done theoretical guarantees sequence-to-sequence image-to-sequence sequence-to-image tasks 've heard guarantees relatively basic classification regression tasks vector inputs outputs `` guarantees '' mean things like pac-learnability sample complexity bounds etc fact even make sense use kinds statistical tools nlp cv possibly performance e.g transformers fields due specific structure data ... sorry obvious anyone field 'm trouble navigating literature -- googling `` transformers '' `` attention mechanism '' even `` sequence-to-sequence '' keeps returning papers proposing new methods extensions link comments,Machine Learning
colin angle irobot,colin angle ceo co-founder irobot robotics company 29 years creating robots operate successfully real world demo scale dozens scale thousands millions year irobot sold 25 million robots consumers including roomba vacuum cleaning robot braava floor mopping robot soon terra lawn mowing robot 25 million robots successfully operating autonomously people ’ homes incredible accomplishment science engineering logistics kinds,Machine Learning
build giant dome numberphile,tom crawford speaks domes curves catenaries check http //kiwico.com/numberphile 50 first month subscription links stuff full description ↓↓↓ tom crawford https //tomrocksmaths.com/ videos tom http //bit.ly/crawford_videos numberphile supported mathematical sciences research institute msri http //bit.ly/msrinumberphile also supported science sandbox simons foundation initiative dedicated engaging everyone process science https //www.simonsfoundation.org/outreach/science-sandbox/ support math america https //www.mathforamerica.org/ numberphile website http //www.numberphile.com/ numberphile facebook http //www.facebook.com/numberphile numberphile tweets https //twitter.com/numberphile subscribe http //bit.ly/numberphile_sub videos brady haran patreon http //www.patreon.com/numberphile numberphile t-shirts merch https //teespring.com/stores/numberphile brady 's videos subreddit http //www.reddit.com/r/bradyharan/ brady 's latest videos across channels http //www.bradyharanblog.com/ sign occasional emails http //eepurl.com/ydjl9,Machine Learning
paul krugman economics innovation automation safety nets universal basic income,paul krugman nobel prize winner economics professor cuny columnist new york times academic work centers around international economics economic geography liquidity traps currency crises conversation part artificial intelligence podcast would like get information podcast go https //lexfridman.com/ai connect lexfridman twitter linkedin facebook medium youtube watch video versions conversations enjoy podcast please rate 5 stars apple podcasts follow spotify support patreon episode,Machine Learning
129 – lisa feldman barrett counterintuitive ideas brain works,lisa feldman barrett neuroscientist psychologist author please support podcast checking sponsors – athletic greens https //athleticgreens.com/lex use code lex get free vitamin d3/k2 – magic spoon https //magicspoon.com/lex use code lex get free shipping – cash app https //cash.app/ use code lexpodcast get 10 episode links seven half lessons brain book https //amzn.to/2sp5ar9 emotions made book https //amzn.to/2gwafg6 lisa ’ twitter https //twitter.com/lfeldmanbarrett lisa ’ website https //lisafeldmanbarrett.com/ podcast info podcast website https //lexfridman.com/podcast apple podcasts https //apple.co/2lwqzir spotify https //spoti.fi/2newcf8 rss https //lexfridman.com/feed/podcast/ youtube full episodes https //youtube.com/lexfridman youtube clips https //youtube.com/lexclips support connect,Machine Learning
jim keller moore ’ law microprocessors abstractions first principles,jim keller legendary microprocessor engineer worked amd apple tesla intel ’ known work amd k7 k8 k12 zen microarchitectures apple a4 a5 processors co-author specifications x86-64 instruction set hypertransport interconnect conversation part artificial intelligence podcast would like get information podcast go https //lexfridman.com/ai connect lexfridman twitter linkedin facebook medium youtube watch video versions conversations enjoy podcast please rate 5 stars,Machine Learning
rt ykilcher fchollet see like new tensorflow honestly quite beautiful therefore 'd curious know think mesh tensorflow personally find much easier use pytorch even keras backend definitions,fchollet see like new tensorflow honestly quite beautiful therefore 'd curious know think mesh tensorflow personally find much easier use pytorch even keras backend definitions,Machine Learning
wow,wow nitter.net/goodwish916/status/1329234124394041345,Machine Learning
2.920050977316 numberphile,dr james grime discussing new prime-generating constant check brilliant get 20 premium service https //brilliant.org/numberphile sponsor links stuff full description ↓↓↓ extra footage interview https //youtu.be/yxphq-36eq4 james grime videos http //bit.ly/grimevideos james grime 's website https //www.singingbanana.com mills constant video https //youtu.be/6ltrpvpewfo prime-representing constant dylan fridman juli garbulsky bruno glecer james grime massi tron florentin https //www.tandfonline.com/doi/abs/10.1080/00029890.2019.1530554 numberphile supported mathematical sciences research institute msri http //bit.ly/msrinumberphile also supported science sandbox simons foundation initiative dedicated engaging everyone process science https //www.simonsfoundation.org/outreach/science-sandbox/ support math america https //www.mathforamerica.org/ numberphile website http //www.numberphile.com/ numberphile facebook http //www.facebook.com/numberphile numberphile tweets https //twitter.com/numberphile subscribe http //bit.ly/numberphile_sub videos brady haran patreon http //www.patreon.com/numberphile numberphile t-shirts merch https //teespring.com/stores/numberphile brady 's videos subreddit http //www.reddit.com/r/bradyharan/ brady 's latest videos across channels http //www.bradyharanblog.com/ sign occasional emails http //eepurl.com/ydjl9,Machine Learning
kevin scott microsoft cto,kevin scott cto microsoft senior vice president engineering operations linkedin oversaw mobile ads engineering google conversation part artificial intelligence podcast would like get information podcast go https //lexfridman.com/ai connect lexfridman twitter linkedin facebook medium youtube watch video versions conversations enjoy podcast please rate 5 stars itunes support patreon,Machine Learning
project inside machine 's dream juan carlos garfias tovar 2021,link comments,Machine Learning
15 best machine learning courses coursera free,read full story,Machine Learning
031 got access gpt-3 gary marcus walid saba connor leahy,special edition dr. tim scarfe yannic kilcher keith duggar speak gary marcus connor leahy gpt-3 significant amount time experiment gpt-3 show demos use considerations note podcast version significantly truncated watch youtube version toc experiments gpt-3 https //www.youtube.com/watch v=iccd86voz3w,Machine Learning
apple ai residency program coding stage invitation,hey guys wanted check anyone received hackerrank link coding assessment 2021 residency program wanted know everyone applied receives already go selection process receive invitation thanks good luck anyone also applied link comments,Machine Learning
talks 14 martin henze knowledge power understanding data eda visualisations,title knowledge power understanding data eda visualisations abstract discuss power exploratory data analysis eda gain deeper understanding data sets machine learning problems particular data visualisation techniques quickly reveal key characteristics promising features important caveats beyond data exploration visuals crucial interpreting communicating findings variety levels kaggle treasure trove eda tools techniques well masterful examples structure analysis discuss outstanding examples underrated gems world kaggle notebooks one notebooks share personal approach investigating new dataset bio data scientist edison software 1st kaggle kernels grandmaster phd astrophysicist data eda visualizations -- please subscribe like video help keep motivated make awesome videos like one buy book approaching almost machine learning problem please visit https //bit.ly/buyaaml follow twitter https //twitter.com/abhi1thakur linkedin https //www.linkedin.com/in/abhi1thakur/ kaggle https //kaggle.com/abhishek instagram https //instagram.com/abhi4ml,Machine Learning
r p random shadows highlights data augmentation,hello first post reddit actually lately released code new paper `` random shadows highlights new data augmentation method extreme lighting conditions '' code available http //bit.do/augrsh also read paper random shadows highlights comments welcome link comments,Machine Learning
empowering k12 students learn computer science,day 3 november 19 2020 theme unblocking pipeline education employment ruthe farmer cs consortium accessible computer science education fall workshop hosted microsoft university washington create university colorado ’ coleman institute took place november 17-19 2020 consisted three half-days talks discussions planning new research dedicated making computer science education learning experiences accessible people disabilities information workshop found https //www.microsoft.com/en-us/research/event/accessible-cs-education-fall-workshop/,Machine Learning
extracting training data large language models paper explained,ai privacy tech paper demonstrates method extract verbatim pieces training data trained language model moreover extracted pieces appear handful times dataset points serious security privacy implications models like gpt-3 authors discuss risks propose mitigation strategies outline 0:00 intro overview 9:15 personal data example 12:30 eidetic memorization language models 19:50 adversary 's objective outlier data 24:45 ethical hedging 26:55 two-step method overview 28:20 perplexity baseline 30:30 improvement via perplexity ratios 37:25 weights patterns weights memorization 43:40 analysis main results 1:00:30 mitigation strategies 1:01:40 conclusion comments paper https //arxiv.org/abs/2012.07805 abstract become common publish large billion parameter language models trained private datasets paper demonstrates settings adversary perform training data extraction attack recover individual training examples querying language model demonstrate attack gpt-2 language model trained scrapes public internet able extract hundreds verbatim text sequences model 's training data extracted examples include public personally identifiable information names phone numbers email addresses irc conversations code 128-bit uuids attack possible even though sequences included one document training data comprehensively evaluate extraction attack understand factors contribute success example find larger models vulnerable smaller models conclude drawing lessons discussing possible safeguards training large language models authors nicholas carlini florian tramer eric wallace matthew jagielski ariel herbert-voss katherine lee adam roberts tom brown dawn song ulfar erlingsson alina oprea colin raffel links youtube https //www.youtube.com/c/yannickilcher twitter https //twitter.com/ykilcher discord https //discord.gg/4h8xxdf bitchute https //www.bitchute.com/channel/yannic-kilcher minds https //www.minds.com/ykilcher parler https //parler.com/profile/yannickilcher linkedin https //www.linkedin.com/in/yannic-kilcher-488534136/ want support best thing share content want support financially completely optional voluntary lot people asked subscribestar https //www.subscribestar.com/yannickilcher patreon https //www.patreon.com/yannickilcher bitcoin btc bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq ethereum eth 0x7ad3513e3b8f66799f507aa7874b1b0ebc7f85e2 litecoin ltc lqw2trykyetvc8wjfkhpphtpbdm4vw7r9m monero xmr 4acl8agreo5hair8a9cevrw8peauwvnp1wnsdzxw7tzicdlhzagsgzhrqabdnfy8yum9fwjdvijphkrjv4fwt19cjzn9d4n,Machine Learning
tool format data nicely ex convert ros bags nuscenes kitti,working bunch ros bags hoping would easy way convert data format accepted machine learning frameworks without write bunch custom data loaders considering modifying nuscenes support custom data means change labels modify bunch stuff data autonomous boats autonomous vehicles go rabbit hole wondering anyone knew existing tools could easily convert data pcd images transformations imu sonar etc different formats 'd also appreciate anyone knew universal data formats n't one seems like good project link comments,Machine Learning
one shot metric learning quadruplet loss machine learning dojo,note episode tim 's machine learning dojo youtube channel join eric craeymeersch wonderful discussion ml engineering computer vision siamese networks contrastive loss one shot learning metric learning 00:00:00 introduction 00:11:47 ml engineering discussion 00:35:59 intro main topic 00:42:13 siamese networks 00:48:36 mining strategies 00:51:15 contrastive loss 00:57:44 trip loss paper 01:09:35 quad loss paper 01:25:49 eric 's quadloss medium article 02:17:32 metric learning reality check 02:21:06 engineering discussion ii 02:26:22 outro second paper review call tess ferrandez covered facenet paper google one-shot siamese network called triplet loss interesting change direction nn architecture i.e using contrastive loss instead fixed number output classes contrastive architectures taking ml landscape recently i.e simclr moco bert eric wrote article time https //medium.com/ crimy/one-shot-learning-siamese-networks-and-triplet-loss-with-keras-2885ed022352 discovered new approach one shot learning vision using quadruplet loss metric learning eric wrote new article several experiments https //medium.com/ crimy/beyond-triplet-loss-one-shot-learning-experiments-with-quadruplet-loss-16671ed51290 source=friends_link sk=bf41673664ad8a52e322380f2a456e8b paper details beyond triplet loss deep quadruplet network person re-identification https //arxiv.org/abs/1704.01719 chen al '17 `` person re-identification reid important task wide area video surveillance focuses identifying people across different cameras recently deep learning networks triplet loss become common framework person reid however triplet loss pays main attentions obtaining correct orders training set still suffers weaker generalization capability training set testing set thus resulting inferior performance paper design quadruplet loss lead model output larger inter-class variation smaller intra-class variation compared triplet loss result model better generalization ability achieve higher performance testing set particular quadruplet deep network using margin-based online hard negative mining proposed based quadruplet loss person reid extensive experiments proposed network outperforms state-of-the-art algorithms representative datasets clearly demonstrates effectiveness proposed method '' original facenet paper https //arxiv.org/abs/1503.03832 deeplearning machinelearning,Machine Learning
's conversation avi loeb astrophysicist harvard argues oumuamua interstellar object passed earth 2017 may alien technology talk aliens black holes space exploration truly fascinating https //www.youtube.com/watch v=plcc6e-e1uu,'s conversation avi loeb astrophysicist harvard argues oumuamua interstellar object passed earth 2017 may alien technology talk aliens black holes space exploration truly fascinating invidious.snopyta.org/watch v=plcc6e-e…,Machine Learning
imaginary interest rates lockdown math ep 5,compound interest e relates circles full playlist https //www.youtube.com/playlist list=plzhqobowtqdp5cveljj1bndouqrahvpev home page https //www.3blue1brown.com brought https //3b1b.co/ldm-thanks great mathologer video https //youtu.be/-dhhrg-kbj0 beautiful pictorial summary thuynganvu https //twitter.com/thuynganvu/status/1258221677990703105 https //twitter.com/thuynganvu/status/1258222002889875457 videos imaginary exponents https //youtu.be/v0yeaeiclky https //youtu.be/mvmucpvrowq mistakes off-handed remarks quaternions mentioned rotation 4d would require 10 degrees freedom 's wrong said requires 6 degrees freedom rotation 5d requires 10 degrees freedom -- -- -- -- -- -- -- -- -- video timeline thanks user `` tieriffic '' 0:00:00 welcome 0:00:55 q1 prompt would take imaginary interest rate 0:02:05 `` e pi dummies '' video shoutout 0:02:45 q1 results 0:03:30 q2 prompt two banks two rates 0:04:55 ask beauty connections math 0:06:00 q2 results 0:07:05 desmos q2 0:09:10 q3 prompt savings growth rate 6 every 6mo 0:10:35 q3 results 0:12:35 desmos graph explored 0:14:45 breaking interest rate 0:18:00 interesting interest equation 0:19:20 q4 prompt 100 1+0.12/n ^2 n → ∞ 0:21:05 ask quaternions 0:22:35 q4 results 0:24:50 explaining q4 0:26:40 defining e 0:28:40 definition e previous lectures 0:30:45 imaginary interest rate 0:32:35 graphing relationship 0:33:50 imaginary interest rate animation 0:37:55 compounding continuously 0:40:45 spring hooke 's law 0:43:20 q5 prompt δx δv spring 0:44:50 ask rotation multiple dimensions 0:47:45 q5 results 0:49:50 rewriting spring 's position 0:55:00 bringing together 0:59:00 ask hints last lecture 's homework 1:03:25 closing remarks -- -- -- -- -- -- -- -- -- live question setup stats on-screen powered itempool https //itempool.com/ curious animations https //www.3blue1brown.com/faq manim music vincent rubinetti download music bandcamp https //vincerubinetti.bandcamp.com/album/the-music-of-3blue1brown stream music spotify https //open.spotify.com/album/1dvyjws8fbqxhrunag5w5u want contribute translated subtitles help review already made others need approval click gear icon video go subtitles/cc `` add subtitles/cc '' really appreciate helps make lessons accessible people -- -- -- -- -- -- -- -- -- 3blue1brown channel animating math senses word animate know drill youtube want stay posted new videos subscribe http //3b1b.co/subscribe various social media stuffs website https //www.3blue1brown.com twitter https //twitter.com/3blue1brown reddit https //www.reddit.com/r/3blue1brown instagram https //www.instagram.com/3blue1brown_animations/ patreon https //patreon.com/3blue1brown facebook https //www.facebook.com/3blue1brown,Machine Learning
`` every moment fresh beginning '' – t.s eliot,`` every moment fresh beginning '' – t.s eliot,Machine Learning
r/woooosh,r/woooosh nitter.net/ylecun/status/1352247446320132096,Machine Learning
searching rh counterexamples — search strategies,’ glibly searching counterexamples riemann hypothesis trick learning software engineering principles first two articles configured testing framework showed hide implementation choices behind interface next ’ improve algorithm ’ core routine ’ link specific git commits final code repository show project evolves superabundant numbers superabundant number one “ maximal relative divisor sums ” following sense,Machine Learning
going able code deep learning hero,detailed plan going able write code deep learning expert advice based personal experience read full story,Machine Learning
daniel kahneman thinking fast slow deep learning ai,daniel kahneman winner nobel prize economics integration economic science psychology human behavior judgment decision-making author popular book “ thinking fast slow ” summarizes accessible way research several decades often collaboration amos tversky cognitive biases prospect theory happiness central thesis work dichotomy two modes thought “ system 1 ” fast instinctive emotional “ system 2 ” slower deliberative logical book delineates cognitive biases associated type,Machine Learning
153 – dmitry korkin evolution proteins viruses life ai,dmitry korkin professor bioinformatics computational biology wpi please support podcast checking sponsors – brave https //brave.com/lex – netsuite http //netsuite.com/lex get free product tour – magic spoon https //magicspoon.com/lex use code lex get 5 – eight sleep https //www.eightsleep.com/lex use code lex get special savings episode links dmitry ’ website http //korkinlab.org/ dmitry ’ twitter https //twitter.com/dmkorkin podcast info podcast website https //lexfridman.com/podcast apple podcasts https //apple.co/2lwqzir spotify https //spoti.fi/2newcf8 rss https //lexfridman.com/feed/podcast/ youtube full episodes https //youtube.com/lexfridman youtube clips https //youtube.com/lexclips support connect – check sponsors ’ best way support podcast – support patreon https //www.patreon.com/lexfridman – twitter https //twitter.com/lexfridman – instagram https //www.instagram.com/lexfridman – linkedin https //www.linkedin.com/in/lexfridman – facebook https //www.facebook.com/lexfridmanpage – medium https //medium.com/ lexfridman outline ’ timestamps episode podcast players able click timestamp jump time 00:00 – introduction 07:21 – proteins building blocks life 14:23 – spike protein 21:11 – coronavirus biological structure explained 26:09 – virus mutations 32:39 – evolution proteins 42:25 – self-replicating computer programs 50:02 – origin life 57:34 – extraterrestrial life solar system 59:31 – joshua lederberg 1:05:30 – dendral 1:08:24 – expert systems fail 1:10:35 – alphafold 2 1:32:13 – ai revolutionize art music 1:39:12 – multi-protein folding 1:43:39 – alphafold 2 result nobel prize 1:46:10 – ai used engineer deadly viruses 2:01:17 – book recommendations 2:11:00 – family 2:13:38 – poem russian,Machine Learning
sean carroll nature universe life intelligence,sean carroll theoretical physicist caltech specializing quantum mechanics gravity cosmology author several popular books one arrow time called eternity one higgs boson called particle end universe one science philosophy called big picture origins life meaning universe upcoming book quantum mechanics preorder called something deeply hidden finally perhaps famously host podcast called mindscape,Machine Learning
anomaly detection quality assurance,hi everyone work financial services company responsible release model output data clients quality assurance thereof particular ds side things key part process comparing data models generated publicly traded companies consecutive quarters particular identifying significant change distribution model outputs large amounts outliers quarters said outputs currently process investigating output data involves great deal data visualization displaying tables showing outliers terms quarter-on-quarter change particular output metrics however many cases large changes outputs often explained factors dataset noted change methodology data vendor substantial increase revenue particular company also part dataset qa 'd wondering people could point direction anomaly detection approach ideally learns unsupervised manner n't attempt find anomalies respect features dataset instead looking anomalies individual features explained changes remaining features dataset looked anomaly detection techniques part seems look outliers across features fit use case particularly interested observing whether bugs modelling code causing undues shifts output values models generate imagine supervised way solving problem would build regression model trained predict values certain output metrics given input data identifying outliers taking delta predicted real values however due changing nature inputs outputs models number models develop would prefer use unsupervised approach possible tldr know anomaly detection approaches consider totality high-dimensional data fed exclusively look anomalies basis idiosyncratic values individual features thanks advance link comments,Machine Learning
big data machine learning fundamentals using gcp data professionals,read full story,Machine Learning
top datasets climate change data science projects,data central piece climate change debate climate change datasets list many data scientists created visualizations models measure track change surface temperatures sea ice levels many datasets made public allow people contribute add valuable insight way climate changing causes read full story,Machine Learning
meet data scientist stop winning kaggle,"conversation philipp singer data scientist kaggle double grandmaster ph.d. computer science series interviews present stories established data scientists kaggle grandmasters h2o.ai share journey inspirations accomplishments interviews intended motivate encourage others want understand takes kaggle grandmaster interview shall sharing interaction philipp singer better known psi kaggle world kaggle double grandmaster senior data scientist h2o.ai philipp obtained ph.d. computer science honors technical university graz also finished master ’ studies software development business management philipp several accomplishments including multiple winning top placements kaggle several scientific honors best paper award renowned world wide web conference currently ranked 3rd globally kaggle competitions tier pretty impressive inspiring time one philipp ’ notable achievements winning nfl ’ second annual big data bowl competition teaming together fellow h2o.ai data scientist –dmitry gordeev 2,000 data scientists world competed kaggle predict rushing play outcomes philipp singer dmitry gordeev captured top prize 50,000 approach 2019–20 big data bowl winners philipp singer dmitry gordeev speaking indianapolis interview shall know academic background passion kaggle work data scientist excerpt conversation philipp ph.d. computer science opt data science career rather sticking academia ’ research side philipp obtained ph.d. computer science technical university graz austria worked postdoctoral researcher germany touched many different data science topics scientific career published many papers articles renowned conferences journals next step career would pursue professorship sounded intriguing however even though love teaching also wanted delve applied work meaning wanted work impact mostly possible research prompted take data science career said thoroughly enjoyed ph.d. learned lot time also delighted forefront data science machine learning real maker role h2o.ai tryst kaggle begin kept motivated throughout grandmaster ’ journey philipp ’ kaggle profile philipp signed kaggle around eight years ago close first steps ph.d. heard platform wanted check sample submission stopped touching kaggle six years around two years ago dmitry dott1718 kaggle back work colleague decided try competition together kaggle side project work went zero expectations ended winning competition got hooked began kaggle journey approach kaggle always tackle new types problems stay motivated still new interesting problems solve regular basis also enjoy meeting working talented people kaggle seeing community strives lately killing kaggle leaderboard spectacular results latest nfl 1st future — impact detection finished 2nd approach towards solving problems faring well philipp people often ask win kaggle competitions think general secret sauce applied lot success kaggle based experience willingness touch learn things first glance know much time assembled particular generic toolbox incorporates building blocks competition tackled example understand set proper cross-validation libraries use models fit models properly track performance similar things already time focus new crucial aspects recent competitions always try improve workflow competition become efficient competitive “ lot success kaggle based experience willingness touch learn things first glance know much about. ” decide competitions participate philipp ’ top achievements kaggle philipp mostly try tackle new types problems competitions sound interesting concerning data problem solve sometimes also try luck standard competitions stay informed art ’ weekly changing state typically approach kaggle problem favorite ml resources moocs blogs etc would like share community philipp try resort repertoire methods tools experience already accumulated try research specific problem hand means study previous solutions similar problems kaggle read relevant papers best way learn problem go hands-on learn along way data scientist h2o.ai roles specific areas work philipp along fellow kaggle grandmasters h2o.ai philipp h2o.ai role multi-faceted regularly involved customer-facing projects goal support projects data science expertise furthermore kaggle grandmasters always try utilize experience knowledge state-of-the-art continuously improve products develop new bleeding-edge prototypes solutions example could mean make suggestions new features driverless ai develop ai applications wave demonstrating new techniques full pipeline data science solutions best things learned via kaggle apply professional work h2o.ai philipp one important thing learn kaggle produce robust models generalize well subject strong overfitting crucial kaggle need perform well unseen private data means learn lot robust cross-validation care data facets like feature distribution shifts certain essential aspects utilize knowledge well work h2o.ai also integral part products want enable customers robust machine learning supported expertise knowledge area data science domain rapidly evolving manage keep latest developments philipp mostly use kaggle keep latest developments excellent filter new techniques either work practical applied problems work usually robust methods survive marginal techniques work occasionally get filtered time try keep up-to-date following well-known researchers practitioners twitter platforms specific areas problems would want apply expertise ml philipp talks vienna data science group meetup held 9 jan 2020 philipp nothing specific mind usually try get surprised interesting problems popping either work kaggle quite essential delve problems seem interesting first glance also bring unbiased view problem probably also apply experience gained issues data hand word advice data science kaggle aspirants started wish start data science journey philipp get hands dirty ’ afraid fail always eager learn new things philipp ’ kaggle journey quite remarkable ’ sure journey dedication achievements source inspiration others working trying make career field read interviews series rohan rao data scientist ’ journey sudoku kaggle shivam bansal data scientist rules ‘ data science good ’ competitions kaggle meet yauhen first kaggle grandmaster belarus sudalai rajkumar passion numbers turned mechanical engineer kaggle grandmaster gabor fodor inspiring journey ‘ beluga ’ kaggle world post meet data scientist stop winning kaggle appeared first open source leader ai ml",Machine Learning
128 – michael malice anarchy democracy libertarianism love trolling,michael malice political thinker podcaster author please support podcast checking sponsors – semrush https //www.semrush.com/partner/lex/ get free month guru – doordash https //doordash.com/ use code lex get 5 – masterclass https //masterclass.com/lex get 15 annual sub episode links michael ’ twitter https //twitter.com/michaelmalice michael ’ community https //malice.locals.com/ michael ’ youtube https //www.youtube.com/channel/uc5tj5qcpjkil-kia4gib5xw michael ’ website http //michaelmalice.com/about/ welcome podcast https //bit.ly/30q8oz1 new right book https //amzn.to/34gxlo3 dear reader book https //amzn.to/2hpplhs podcast info podcast website https //lexfridman.com/podcast apple podcasts https //apple.co/2lwqzir spotify https //spoti.fi/2newcf8 rss https //lexfridman.com/feed/podcast/ youtube full episodes https //youtube.com/lexfridman youtube clips https //lexclips support connect – check,Machine Learning
nlp model able prevent adversarial attack,read full story,Machine Learning
93 – daphne koller biomedicine machine learning,daphne koller professor computer science stanford university co-founder coursera andrew ng founder ceo insitro company intersection machine learning biomedicine support podcast signing sponsors – cash app – use code “ lexpodcast ” download – cash app app store https //apple.co/2spruhe – cash app google play https //bit.ly/2mlvp5w episode links daphne ’ twitter https //twitter.com/daphnekoller daphne ’ website https //ai.stanford.edu/users/koller/index.html insitro http //insitro.com conversation part artificial intelligence podcast would like get information podcast go https //lexfridman.com/ai connect lexfridman twitter,Machine Learning
89 – stephen wolfram cellular automata computation physics,stephen wolfram computer scientist mathematician theoretical physicist founder ceo wolfram research company behind mathematica wolfram alpha wolfram language new wolfram physics project author several books including new kind science personal note one influential books journey computer science artificial intelligence support podcast signing sponsors – expressvpn https //www.expressvpn.com/lexpod – cash app – use code “ lexpodcast ” download – cash app app store https //apple.co/2spruhe – cash app google play,Machine Learning
underspecification presents challenges credibility modern machine learning paper explained,ai research machinelearning deep learning models often overparameterized many degrees freedom leads many local minima perform equally well test set turns even though generalize in-distribution performance models drastically different tested out-of-distribution notably many cases good model actually found among candidates seems impossible select paper describes problem calls underspecification gives several theoretical practical examples outline 0:00 overview 2:00 underspecification ml pipelines 11:15 stress tests 12:40 epidemiological example 20:45 theoretical model 26:55 example medical genomics 34:00 imagenet-c example 36:50 bert models 56:55 conclusion comments paper https //arxiv.org/abs/2011.03395 abstract ml models often exhibit unexpectedly poor behavior deployed real-world domains identify underspecification key reason failures ml pipeline underspecified return many predictors equivalently strong held-out performance training domain underspecification common modern ml pipelines based deep learning predictors returned underspecified pipelines often treated equivalent based training domain performance show predictors behave differently deployment domains ambiguity lead instability poor model behavior practice distinct failure mode previously identified issues arising structural mismatch training deployment domains show problem appears wide variety practical ml pipelines using examples computer vision medical imaging natural language processing clinical risk prediction based electronic health records medical genomics results show need explicitly account underspecification modeling pipelines intended real-world deployment domain authors alexander d'amour katherine heller dan moldovan ben adlam babak alipanahi alex beutel christina chen jonathan deaton jacob eisenstein matthew d. hoffman farhad hormozdiari neil houlsby shaobo hou ghassen jerfel alan karthikesalingam mario lucic yian cory mclean diana mincu akinori mitani andrea montanari zachary nado vivek natarajan christopher nielson thomas f. osborne rajiv raman kim ramasamy rory sayres jessica schrouff martin seneviratne shannon sequeira harini suresh victor veitch max vladymyrov xuezhi wang kellie webster steve yadlowsky taedong yun xiaohua zhai d. sculley links youtube https //www.youtube.com/c/yannickilcher twitter https //twitter.com/ykilcher discord https //discord.gg/4h8xxdf bitchute https //www.bitchute.com/channel/yannic-kilcher minds https //www.minds.com/ykilcher parler https //parler.com/profile/yannickilcher linkedin https //www.linkedin.com/in/yannic-kilcher-488534136/ want support best thing share content want support financially completely optional voluntary lot people asked subscribestar https //www.subscribestar.com/yannickilcher patreon https //www.patreon.com/yannickilcher bitcoin btc bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq ethereum eth 0x7ad3513e3b8f66799f507aa7874b1b0ebc7f85e2 litecoin ltc lqw2trykyetvc8wjfkhpphtpbdm4vw7r9m monero xmr 4acl8agreo5hair8a9cevrw8peauwvnp1wnsdzxw7tzicdlhzagsgzhrqabdnfy8yum9fwjdvijphkrjv4fwt19cjzn9d4n,Machine Learning
binge watching journey microcosmos https //www.youtube.com/c/microcosmos/videos,binge watching journey microcosmos invidious.snopyta.org/microcosmos/vi…,Machine Learning
memes need deep learning meme review episode 2 part 1 2,memes science ai antonio critique creme de la creme deep learning memes music sunshower latashá papov yung logos sunny days anno domini beats trinity jeremy blake memes facebook.com/convolutionalmemes links youtube https //www.youtube.com/c/yannickilcher twitter https //twitter.com/ykilcher discord https //discord.gg/4h8xxdf bitchute https //www.bitchute.com/channel/yannic-kilcher minds https //www.minds.com/ykilcher parler https //parler.com/profile/yannickilcher linkedin https //www.linkedin.com/in/yannic-kilcher-488534136/ want support best thing share content want support financially completely optional voluntary lot people asked subscribestar https //www.subscribestar.com/yannickilcher patreon https //www.patreon.com/yannickilcher bitcoin btc bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq ethereum eth 0x7ad3513e3b8f66799f507aa7874b1b0ebc7f85e2 litecoin ltc lqw2trykyetvc8wjfkhpphtpbdm4vw7r9m monero xmr 4acl8agreo5hair8a9cevrw8peauwvnp1wnsdzxw7tzicdlhzagsgzhrqabdnfy8yum9fwjdvijphkrjv4fwt19cjzn9d4n,Machine Learning
kyle vogt cruise automation,kyle vogt president cto cruise automation leading effort trying solve one biggest robotics challenges time vehicle autonomy co-founder 2 successful companies cruise twitch acquired 1 billion dollars video version available youtube would like get information podcast go https //lexfridman.com/ai connect lexfridman twitter linkedin facebook youtube watch video versions conversations,Machine Learning
fact social media algorithms give rise legitimate problems mean solution `` ban opinions n't like '' also legitimate,fact social media algorithms give rise legitimate problems mean solution `` ban opinions n't like '' also legitimate,Machine Learning
visualization function optimization python,"function optimization involves finding input results optimal value objective function optimization algorithms navigate search space input variables order locate optima shape objective function behavior algorithm search space opaque real-world problems common study optimization algorithms using simple low-dimensional functions easily visualized directly additionally samples input space simple functions made optimization algorithm visualized appropriate context visualization lower-dimensional functions algorithm behavior functions help develop intuitions carry complex higher-dimensional function optimization problems later tutorial discover create visualizations function optimization python completing tutorial know visualization important tool studying function optimization algorithms visualize one-dimensional functions samples using line plots visualize two-dimensional functions samples using contour surface plots let ’ get started visualization function optimization python photo nathalie rights reserved tutorial overview tutorial divided three parts visualization function optimization visualize 1d function optimization test function sample test function line plot test function scatter plot test function line plot marked optima line plot samples visualize 2d function optimization test function sample test function contour plot test function filled contour plot test function filled contour plot test function samples surface plot test function visualization function optimization function optimization field mathematics concerned finding inputs function result optimal output function typically minimum maximum value optimization may straightforward simple differential functions solution calculated analytically however functions ’ interested solving applied machine learning may may well behaved may complex nonlinear multivariate non-differentiable important understanding wide range different algorithms used address function optimization problems important aspect studying function optimization understanding objective function optimized understanding behavior optimization algorithm time visualization plays important role getting started function optimization select simple well-understood test functions study optimization algorithms simple functions plotted understand relationship input objective function output objective function highlighting hills valleys optima addition samples selected search space optimization algorithm also plotted top plots objective function plots algorithm behavior provide insight intuition specific optimization algorithms work navigate search space generalize new problems future typically one-dimensional two-dimensional functions chosen study optimization algorithms easy visualize using standard plots like line plots surface plots explore tutorial first let ’ explore might visualize one-dimensional function optimization visualize 1d function optimization one-dimensional function takes single input variable outputs evaluation input variable input variables typically continuous represented real-valued floating-point value often input domain unconstrained although test problems impose domain interest test function case explore function visualization simple x^2 objective function f x x^2 optimal value input x=0.0 equals 0.0 example implements objective function evaluates single input example 1d objective function objective function def objective x return x 2.0 evaluate inputs objective function x 4.0 result objective x print f .3f .3f x result running example evaluates value 4.0 objective function equals 16.0. f 4.000 16.000 sample test function first thing might want new function define input range interest sample domain interest using uniform grid sample provide basis generating plot later case define domain interest around optima x=0.0 x=-5.0 x=5.0 sample grid values range 0.1 increments -5.0 -4.9 -4.8 etc ... define range input r_min r_max -5.0 5.0 sample input range uniformly 0.1 increments inputs arange r_min r_max 0.1 summarize input domain print inputs :5 evaluate x values sample ... compute targets results objective inputs summarize results print results :5 finally check input corresponding outputs ... create mapping inputs results range 5 print f .3f .3f inputs results tying together complete example sampling input space evaluating points sample listed sample 1d objective function numpy import arange objective function def objective x return x 2.0 define range input r_min r_max -5.0 5.0 sample input range uniformly 0.1 increments inputs arange r_min r_max 0.1 summarize input domain print inputs :5 compute targets results objective inputs summarize results print results :5 create mapping inputs results range 5 print f .3f .3f inputs results running example first generates uniform sample input points expected input points evaluated using objective function finally see simple mapping inputs outputs objective function -5 -4.9 -4.8 -4.7 -4.6 25 24.01 23.04 22.09 21.16 f -5.000 25.000 f -4.900 24.010 f -4.800 23.040 f -4.700 22.090 f -4.600 21.160 confidence generating sample inputs evaluating objective function look generating plots function line plot test function could sample input space randomly benefit uniform line grid points used generate smooth plot smooth points input space ordered smallest largest ordering important expect hope output objective function similar smooth relationship values e.g small changes input result locally consistent smooth changes output function case use samples generate line plot objective function input points x x-axis plot objective function output results y-axis plot ... create line plot input vs result pyplot.plot inputs results show plot pyplot.show tying together complete example listed line plot input vs result 1d objective function numpy import arange matplotlib import pyplot objective function def objective x return x 2.0 define range input r_min r_max -5.0 5.0 sample input range uniformly 0.1 increments inputs arange r_min r_max 0.1 compute targets results objective inputs create line plot input vs result pyplot.plot inputs results show plot pyplot.show running example creates line plot objective function see function large u-shape called parabola common shape studying curves e.g study calculus line plot one-dimensional function scatter plot test function line construct really function smooth summary function always keep mind recall fact generated sample points input space corresponding evaluation points would accurate create scatter plot points example scatter plot input vs result 1d objective function numpy import arange matplotlib import pyplot objective function def objective x return x 2.0 define range input r_min r_max -5.0 5.0 sample input range uniformly 0.1 increments inputs arange r_min r_max 0.1 compute targets results objective inputs create scatter plot input vs result pyplot.scatter inputs results show plot pyplot.show running example creates scatter plot objective function see familiar shape function ’ gain anything plotting points directly line smooth interpolation points provides useful draw points top line location optima points sampled optimization algorithm scatter plot one-dimensional function line plot marked optima next let ’ draw line plot time draw point known optima function located helpful studying optimization algorithm might want see close optimization algorithm get optima first must define input optima evaluate point give x-axis y-axis values plotting ... define known function optima optima_x 0.0 optima_y objective optima_x plot point shape color like case red square ... draw function optima red square pyplot.plot optima_x optima_y 's color= r tying together complete example creating line plot function optima highlighted point listed line plot input vs result 1d objective function show optima numpy import arange matplotlib import pyplot objective function def objective x return x 2.0 define range input r_min r_max -5.0 5.0 sample input range uniformly 0.1 increments inputs arange r_min r_max 0.1 compute targets results objective inputs create line plot input vs result pyplot.plot inputs results define known function optima optima_x 0.0 optima_y objective optima_x draw function optima red square pyplot.plot optima_x optima_y 's color= r show plot pyplot.show running example creates familiar line plot function time optima function e.g input results minimum output function marked red square line plot one-dimensional function optima marked red square simple function red square optima easy see sometimes function might complex lots hills valleys might want make optima visible case draw vertical line across whole plot ... draw vertical line optimal input pyplot.axvline x=optima_x ls= -- color='red tying together complete example listed line plot input vs result 1d objective function show optima line numpy import arange matplotlib import pyplot objective function def objective x return x 2.0 define range input r_min r_max -5.0 5.0 sample input range uniformly 0.1 increments inputs arange r_min r_max 0.1 compute targets results objective inputs create line plot input vs result pyplot.plot inputs results define known function optima optima_x 0.0 draw vertical line optimal input pyplot.axvline x=optima_x ls= -- color='red show plot pyplot.show running example creates plot time draws red line clearly marking point input space marks optima line plot one-dimensional function optima marked red line line plot samples finally might want draw samples input space selected optimization algorithm simulate samples random points drawn input domain ... simulate sample made optimization algorithm seed 1 sample r_min rand 10 r_max r_min evaluate sample sample_eval objective sample plot sample case using small black circles ... plot sample black circles pyplot.plot sample sample_eval color='black complete example creating line plot function optima marked red line algorithm sample drawn small black dots listed line plot domain 1d function optima algorithm sample numpy import arange numpy.random import seed numpy.random import rand matplotlib import pyplot objective function def objective x return x 2.0 define range input r_min r_max -5.0 5.0 sample input range uniformly 0.1 increments inputs arange r_min r_max 0.1 compute targets results objective inputs simulate sample made optimization algorithm seed 1 sample r_min rand 10 r_max r_min evaluate sample sample_eval objective sample create line plot input vs result pyplot.plot inputs results define known function optima optima_x 0.0 draw vertical line optimal input pyplot.axvline x=optima_x ls= -- color='red plot sample black circles pyplot.plot sample sample_eval color='black show plot pyplot.show running example creates line plot domain marks optima red line time sample domain selected algorithm really random sample points drawn black dots imagine real optimization algorithm show points narrowing domain searches down-hill starting point line plot one-dimensional function optima marked red line samples shown black dots next let ’ look might perform similar visualizations optimization two-dimensional function visualize 2d function optimization two-dimensional function function takes two input variables e.g x test function use x^2 function scale two-dimensional function example f x x^2 y^2 optimal value input x=0.0 y=0.0 equals 0.0 example implements objective function evaluates single input example 2d objective function objective function def objective x return x 2.0 2.0 evaluate inputs objective function x 4.0 4.0 result objective x print f .3f .3f .3f x result running example evaluates point x=4 y=4 equals 32. f 4.000 4.000 32.000 next need way sample domain turn sample objective function sample test function common way sampling two-dimensional function first generate uniform sample along variable x use two uniform samples create grid samples called mesh grid two-dimensional array across input space instead two two-dimensional arrays used together define grid across two input variables achieved duplicating entire x sample array sample point similarly duplicating entire sample array x sample point achieved using meshgrid numpy function example ... define range input r_min r_max -5.0 5.0 sample input range uniformly 0.1 increments xaxis arange r_min r_max 0.1 yaxis arange r_min r_max 0.1 create mesh axis x meshgrid xaxis yaxis summarize input domain print x :5 :5 evaluate pair points using objective function ... compute targets results objective x summarize results print results :5 :5 finally review mapping inputs corresponding output values ... create mapping inputs results range 5 print f .3f .3f .3f x i,0 i,0 results i,0 example demonstrates create uniform sample grid across two-dimensional input space objective function sample 2d objective function numpy import arange numpy import meshgrid objective function def objective x return x 2.0 2.0 define range input r_min r_max -5.0 5.0 sample input range uniformly 0.1 increments xaxis arange r_min r_max 0.1 yaxis arange r_min r_max 0.1 create mesh axis x meshgrid xaxis yaxis summarize input domain print x :5 :5 compute targets results objective x summarize results print results :5 :5 create mapping inputs results range 5 print f .3f .3f .3f x i,0 i,0 results i,0 running example first summarizes points mesh grid objective function evaluation points finally enumerate coordinates two-dimensional input space corresponding function evaluation -5 -4.9 -4.8 -4.7 -4.6 -5 -4.9 -4.8 -4.7 -4.6 -5 -4.9 -4.8 -4.7 -4.6 -5 -4.9 -4.8 -4.7 -4.6 -5 -4.9 -4.8 -4.7 -4.6 50 49.01 48.04 47.09 46.16 49.01 48.02 47.05 46.1 45.17 48.04 47.05 46.08 45.13 44.2 47.09 46.1 45.13 44.18 43.25 46.16 45.17 44.2 43.25 42.32 f -5.000 -5.000 50.000 f -5.000 -4.900 49.010 f -5.000 -4.800 48.040 f -5.000 -4.700 47.090 f -5.000 -4.600 46.160 familiar sample input space evaluate points let ’ look might plot function contour plot test function popular plot two-dimensional functions contour plot plot creates flat representation objective function outputs x coordinate color contour lines indicate relative value height output objective function like contour map landscape mountains distinguished valleys achieved using contour matplotlib function takes mesh grid evaluation mesh grid input directly specify number levels draw contour color scheme use case use 50 levels popular “ jet ” color scheme low-levels use cold color scheme blue high-levels use hot color scheme red ... create contour plot 50 levels jet color scheme pyplot.contour x results 50 alpha=1.0 cmap='jet show plot pyplot.show tying together complete example creating contour plot two-dimensional objective function listed create contour plot 50 levels jet color scheme pyplot.contour x results 50 alpha=1.0 cmap='jet show plot pyplot.show tying together complete example creating contour plot two-dimensional objective function listed contour plot 2d objective function numpy import arange numpy import meshgrid matplotlib import pyplot objective function def objective x return x 2.0 2.0 define range input r_min r_max -5.0 5.0 sample input range uniformly 0.1 increments xaxis arange r_min r_max 0.1 yaxis arange r_min r_max 0.1 create mesh axis x meshgrid xaxis yaxis compute targets results objective x create contour plot 50 levels jet color scheme pyplot.contour x results 50 alpha=1.0 cmap='jet show plot pyplot.show running example creates contour plot see curved parts surface around edges contours show detail less curved parts surface middle fewer contours see lowest part domain middle expected contour plot two-dimensional objective function filled contour plot test function also helpful color plot contours show complete surface colors simple linear interpolation true function evaluation must kept mind complex functions fine detail shown fill contour plot using contourf version function takes arguments ... create filled contour plot 50 levels jet color scheme pyplot.contourf x results levels=50 cmap='jet also show optima plot case white star stand blue background color lowest part plot ... define known function optima optima_x 0.0 0.0 draw function optima white star pyplot.plot optima_x 0 optima_x 1 color='white tying together complete example filled contour plot optima marked listed filled contour plot 2d objective function show optima numpy import arange numpy import meshgrid matplotlib import pyplot objective function def objective x return x 2.0 2.0 define range input r_min r_max -5.0 5.0 sample input range uniformly 0.1 increments xaxis arange r_min r_max 0.1 yaxis arange r_min r_max 0.1 create mesh axis x meshgrid xaxis yaxis compute targets results objective x create filled contour plot 50 levels jet color scheme pyplot.contourf x results levels=50 cmap='jet define known function optima optima_x 0.0 0.0 draw function optima white star pyplot.plot optima_x 0 optima_x 1 color='white show plot pyplot.show running example creates filled contour plot gives better idea shape objective function optima x=0 y=0 marked clearly white star filled contour plot two-dimensional objective function optima marked white star filled contour plot test function samples may want show progress optimization algorithm get idea behavior context shape objective function case simulate points chosen optimization algorithm random coordinates input space ... simulate sample made optimization algorithm seed 1 sample_x r_min rand 10 r_max r_min sample_y r_min rand 10 r_max r_min points plotted directly black circles context color give idea relative quality ... plot sample black circles pyplot.plot sample_x sample_y color='black tying together complete example filled contour plot optimal input sample plotted listed filled contour plot 2d objective function show optima sample numpy import arange numpy import meshgrid numpy.random import seed numpy.random import rand matplotlib import pyplot objective function def objective x return x 2.0 2.0 define range input r_min r_max -5.0 5.0 sample input range uniformly 0.1 increments xaxis arange r_min r_max 0.1 yaxis arange r_min r_max 0.1 create mesh axis x meshgrid xaxis yaxis compute targets results objective x simulate sample made optimization algorithm seed 1 sample_x r_min rand 10 r_max r_min sample_y r_min rand 10 r_max r_min create filled contour plot 50 levels jet color scheme pyplot.contourf x results levels=50 cmap='jet define known function optima optima_x 0.0 0.0 draw function optima white star pyplot.plot optima_x 0 optima_x 1 color='white plot sample black circles pyplot.plot sample_x sample_y color='black show plot pyplot.show running example see filled contour plot optima marked see sample drawn black dots surrounding color relative distance optima gives idea close algorithm random points case got solving problem filled contour plot two-dimensional objective function optima input sample marked surface plot test function finally may want create three-dimensional plot objective function get fuller idea curvature function achieved using plot_surface matplotlib function like contour plot takes mesh grid function evaluation directly ... create surface plot jet color scheme figure pyplot.figure axis figure.gca projection='3d axis.plot_surface x results cmap='jet complete example creating surface plot listed surface plot 2d objective function numpy import arange numpy import meshgrid matplotlib import pyplot mpl_toolkits.mplot3d import axes3d objective function def objective x return x 2.0 2.0 define range input r_min r_max -5.0 5.0 sample input range uniformly 0.1 increments xaxis arange r_min r_max 0.1 yaxis arange r_min r_max 0.1 create mesh axis x meshgrid xaxis yaxis compute targets results objective x create surface plot jet color scheme figure pyplot.figure axis figure.gca projection='3d axis.plot_surface x results cmap='jet show plot pyplot.show running example creates three-dimensional surface plot objective function surface plot two-dimensional objective function additionally plot interactive meaning use mouse drag perspective surface around view different angles surface plot different angle two-dimensional objective function reading section provides resources topic looking go deeper apis optimization root finding scipy.optimize optimization scipy.optimize numpy.meshgrid api matplotlib.pyplot.contour api matplotlib.pyplot.contourf api mpl_toolkits.mplot3d.axes3d.plot_surface api articles mathematical optimization wikipedia parabola wikipedia summary tutorial discovered create visualizations function optimization python specifically learned visualization important tool studying function optimization algorithms visualize one-dimensional functions samples using line plots visualize two-dimensional functions samples using contour surface plots questions ask questions comments best answer post visualization function optimization python appeared first machine learning mastery",Machine Learning
makes natural log `` natural '' lockdown math ep 7,ln x full playlist https //www.youtube.com/playlist list=plzhqobowtqdp5cveljj1bndouqrahvpev home page https //www.3blue1brown.com brought https //3b1b.co/ldm-thanks beautiful pictorial summary thuynganvu https //twitter.com/thuynganvu/status/1259288683489849344 errors minute 16 sum written `` ... '' indicate going infinity minute 38 exponent 1/ 2s^2 instead 1/s^2 represent standard deviation minute 54 equal sign mistakenly used taking derivative x^3 3 end pointed alternating series x^n terms converges values x -1 1 values one ca n't considered proven values x outside range everything argument fine deals convergent input fact still mentioned related videos calculus series https //www.youtube.com/playlist list=plzhqobowtqdmsr9k-rj53dwvrmyo3t5yr sum giving pi^2 6 https //youtu.be/d-o3eb9sfls sum giving pi 4 https //youtu.be/nal_cb42wyy https //youtu.be/00w8gu2al-w mathologer -- -- -- -- -- -- -- -- -- video timeline thanks user `` noonesperfect '' 0:00:14 question 1 0:02:29 answer 1 0:06:27 prime nos infinite geometric series basel problem relationship natural logarithm 0:12:01 examples prime numbers infinite series relationship ln 0:17:25 question 2 0:19:20 answer 2 explanation using ln 0:22:25 question 3 families curves 0:26:37 answer 3 explanation 0:28:50 imaginary exponential 0:30:57 derivatives exponential terms 0:37:21 derivative e^t e^t 0:41:21 question 4 0:44:12 answer 4 explanation using python 0:46:02 taylor series e^x 0:48:29 derivatives polynomial terms/derivatives e^x 0:50:56 derivative natural logarithm using graph 0:56:07 question 5 0:57:37 answer 5 explanation 1:02:15 euler–mascheroni constant 1:08:37 question 6 1:12:41 connecting dots familiarity different expression math -- -- -- -- -- -- -- -- -- live question setup stats on-screen powered itempool https //itempool.com/ curious animations https //www.3blue1brown.com/faq manim music vincent rubinetti download music bandcamp https //vincerubinetti.bandcamp.com/album/the-music-of-3blue1brown stream music spotify https //open.spotify.com/album/1dvyjws8fbqxhrunag5w5u want contribute translated subtitles help review already made others need approval click gear icon video go subtitles/cc `` add subtitles/cc '' really appreciate helps make lessons accessible people -- -- -- -- -- -- -- -- -- 3blue1brown channel animating math senses word animate know drill youtube want stay posted new videos subscribe http //3b1b.co/subscribe various social media stuffs website https //www.3blue1brown.com twitter https //twitter.com/3blue1brown reddit https //www.reddit.com/r/3blue1brown instagram https //www.instagram.com/3blue1brown_animations/ patreon https //patreon.com/3blue1brown facebook https //www.facebook.com/3blue1brown,Machine Learning
random things,get free access 2500 documentaries curiositystream https //curiositystream.thld.co/zachstardec16 use code `` zachstar '' sign stemerch store https //stemerch.com/ support channel https //www.patreon.com/zachstar paypal one time donation https //www.paypal.me/zachstaryt join channel get access perks https //www.youtube.com/channel/ucpcsacbqs-sjevfk_hmfy9w/join ►follow instagram https //www.instagram.com/zachstar/ twitter https //twitter.com/imzachstar 2d graphing software https //www.desmos.com/calculator animations brainup studios http //brainup.in/ check spanish channel https //www.youtube.com/channel/ucnknu2xqblaspj6ckc8vtpa ►my setup space pictures https //amzn.to/2cc4kqj magnetic floating globe https //amzn.to/2vgpdn0 camera https //amzn.to/2rivyu5 mic https //amzn.to/35bkiri tripod https //amzn.to/2rgmtnl equilibrium tube https //amzn.to/2sowdrh ►check amazon store https //www.amazon.com/shop/zachstar,Machine Learning
social dilemma part 1,first part three part series social dilemma netflix film dr. tim scarfe yannic `` lightspeed '' kilcher zak jost gang cybersecurity expert andy smith give take film super excited get feedback one hope enjoy 00:00:00 introduction 00:06:11 moral hypocrisy 00:12:38 road hell paved good intentions attention economy 00:15:04 know everything 00:18:02 addiction 00:21:22 differential realities 00:26:12 self determination monetisation 00:29:08 ai overwhelm human strengths undermine human vulnerabilities 00:31:51 conspiracy theory fake news 00:34:23 overton window polarisation 00:39:12 short attention span convergent behaviour 00:41:26 social media good 00:45:17 attention time linear things pay attention volume anonymity 00:51:32 andy question security social engineering 00:56:32 security risk information social media 00:58:02 retrospective judgement 01:03:06 free speech censorship 01:06:06 technology accelerator,Machine Learning
ai alignment agi fire alarm connor leahy,week dr. tim scarfe alex stenlake yannic kilcher speak agi ai alignment specialist connor leahy machine learning engineer aleph alpha founder eleutherai connor believes ai alignment philosophy deadline precipice stakes astronomical ai important go wrong default connor thinks singularity intelligence explosion near connor says agi like climate change worse even harder problems even shorter deadline even worse consequences future problems hard nobody knows 00:00:00 introduction ai alignment agi fire alarm 00:15:16 main show intro 00:18:38 different schools thought ai safety 00:24:03 intelligence 00:25:48 ai alignment 00:27:39 humans dont coherent utility function 00:28:13 newcomb 's paradox advanced decision problems 00:34:01 incentives behavioural economics 00:37:19 prisoner 's dilemma 00:40:24 ayn rand game theory politics business 00:44:04 instrumental convergence orthogonality thesis 00:46:14 utility functions stop button problem 00:55:24 ai corrigibality self alignment 00:56:16 decision theory stability wireheading robust delegation 00:59:30 stop button problem 01:00:40 making world better place 01:03:43 intelligence search problem 01:04:39 mesa optimisation humans misaligned ai 01:06:04 inner vs outer alignment faulty reward functions 01:07:31 large corporations intelligent stop function 01:10:21 dutch booking rationality decision theory 01:16:32 understanding powerful ais 01:18:03 kolmogorov complexity 01:19:52 gpt-3 intelligent humans even intelligent 01:28:40 scaling hypothesis 01:29:30 connor thought dl dead 2017 01:37:54 gpt-3 intelligent human 01:44:43 jeff hawkins intelligence compression great lookup table 01:50:28 ai ethics related ai alignment 01:53:26 interpretability 01:56:27 regulation 01:57:54 intelligence explosion discord https //discord.com/invite/vtrgjbm eleutherai https //www.eleuther.ai twitter https //twitter.com/npcollapse linkedin https //www.linkedin.com/in/connor-j-leahy/,Machine Learning
objects classification using cnn-based model,— images plots generated modified author read full story,Machine Learning
yann lecun deep learning convolutional neural networks self-supervised learning,yann lecun one fathers deep learning recent revolution ai captivated world possibility machines learn data professor new york university vice president chief ai scientist facebook co-recipient turing award work deep learning probably best known founder convolutional neural networks particular early application optical character recognition conversation part artificial intelligence podcast would like get information podcast go,Machine Learning
probability machine learning,read full story,Machine Learning
p building new machine learning competition platform would love get feedback,hi /r/machinelearning community couple friends spent last year building machine learning competition platform https //telesto.ai slowly getting started working filling competitions would love get feedback might ask building another platform kaggle aicrowd drivendata many others available let explain tl dr crowdsourcing suitable production-ready models crowdsourcing process part machine learning development cycle want change deploying production-like environments rewarding top solutions weekly crowdsourcing development cycle let 's take look development machine learning solution done software tools development workflow cycle well roughly main steps following data collection problem formulated data collected provide training test sets cases data collected even problems clear model training part exciting data scientists coming architecture solution training machine learning model however takes small percentage total workload deployment production model ready prepared production use involves quality assurance packaging api securely deploying production servers etc testing validation model used production new unexpected problems might arise suppose train classifier recognizes brand model car manufacturer issues new model classifier longer works correctly go back beginning collect data train new model know machine learning competition covers small part process model training happens unrelated competition done internal team instead thousands participants effectively crowdsourcing covers small portion work want change crowdsourcing production involve participants challenge entire process opinion several opportunities instance participated competitions participants noticed problems training data labels right away yet nothing changed entire duration others participants cheated way victory downloading private test dataset overfitting model best result proof concept worst useless 've come idea making make entire process dynamic involving competitors challenge hosts entire development cycle simply speaking instead rewarding top 3-5 solutions end three-month period conclude competition would reward best solutions every week deploy production-like environments currently ask weekly winners package solution docker image provide template heavy development issues arise like mislabeled data fixed next round solution keep improving constantly hand believe would rewarding participants well working hard competition staying top leaderboard months get outcompeted final hours receive reward think unfair consistency rewarded idea issue rewards every week brief goals connect machine learning community industry exciting problems provide rewarding competition platform developers integrate crowdsourcing entire development life cycle think asking feedback far beginning journey set early release version test https //telesto.ai currently testing platform competition diagnose covid-19 based cell microscopy challenges coming soon asking feedback idea execution far building platform machine learning community aim provide best experience link comments,Machine Learning
108 – sergey levine robotics machine learning,sergey levine professor berkeley world-class researcher deep learning reinforcement learning robotics computer vision including development algorithms end-to-end training neural network policies combine perception control scalable algorithms inverse reinforcement learning deep rl algorithms support podcast supporting sponsors – expressvpn https //www.expressvpn.com/lexpod – cash app – use code “ lexpodcast ” download – cash app app store https //apple.co/2spruhe – cash app google play https //bit.ly/2mlvp5w would like get information podcast go https //lexfridman.com/ai connect lexfridman twitter linkedin facebook,Machine Learning
77 – alex garland ex machina devs annihilation poetry science,alex garland writer director many imaginative philosophical films dreamlike exploration human self-destruction movie annihilation deep questions consciousness intelligence raised movie ex machina one greatest movies artificial intelligence ever made ’ releasing podcast coincide release new series called devs premiere thursday march 5 hulu episode links devs https //hulu.tv/2x35hah annihilation https //hulu.tv/3ai9eqk ex machina https //www.netflix.com/title/80023689 alex imdb https //www.imdb.com/name/nm0307497/ alex wiki https //en.wikipedia.org/wiki/alex_garland conversation part artificial intelligence podcast,Machine Learning
harri valpola system 2 ai planning model-based reinforcement learning,episode machine learning street talk tim scarfe yannic kilcher connor shorten interviewed harri valpola ceo founder curious ai continued discussion system 1 system 2 thinking deep learning well miscellaneous topics around model-based reinforcement learning dr. valpola describes challenges modelling industrial control processes water sewage filters paper mills use model-based rl dr. valpola collaborators recently published “ regularizing trajectory optimization denoising autoencoders ” addresses concerns planning algorithms exploit inaccuracies world models 00:00:00 intro harri curious ai system1/system 2 00:04:50 background model-based rl challenges tim 00:06:26 interesting research papers model-based rl connor 00:08:36 intro curious ai recent neurips paper model-based rl denoising autoencoders yannic 00:21:00 main show kick system 1/2 00:31:50 simulator come 00:33:59 evolutionary priors 00:37:17 consciousness 00:40:37 one build company like curious ai 00:46:42 deep q networks 00:49:04 planning model based rl 00:53:04 learning good representations 00:55:55 typical problem curious ai might solve industry 01:00:56 exploration 01:08:00 paper regularizing trajectory optimization denoising 01:13:47 epistemic uncertainty 01:16:44 would curious develop models 01:18:00 explainability simulations 01:22:33 system 2 works humans 01:26:11 planning 01:27:04 advice starting ai company 01:31:31 real world implementation planning models 01:33:49 publishing research openness really hope enjoy episode please subscribe regularizing trajectory optimization denoising autoencoders https //papers.nips.cc/paper/8552-regularizing-trajectory-optimization-with-denoising-autoencoders.pdf pulp paper packaging future transformed deep learning https //thecuriousaicompany.com/pulp-paper-packaging-a-future-transformed-through-deep-learning/ curious ai https //thecuriousaicompany.com/ harri valpola publications https //scholar.google.com/citations user=1ut7-84aaaaj hl=en oi=ao interesting papers around model-based rl gamegan https //cdn.arstechnica.net/wp-content/uploads/2020/05/nvidia_gamegan_research.pdf plan2explore https //ramanans1.github.io/plan2explore/ world models https //worldmodels.github.io/ muzero https //arxiv.org/pdf/1911.08265.pdf planet deep planning network rl https //ai.googleblog.com/2019/02/introducing-planet-deep-planning.html dreamer scalable rl using world models https //ai.googleblog.com/2020/03/introducing-dreamer-scalable.html model based rl atari https //arxiv.org/pdf/1903.00374.pdf,Machine Learning
happens infinity cantor set,get free access 2500 documentaries curiositystream http //go.thoughtleaders.io/1622720200820 use promo code `` zachstar '' sign stemerch store https //stemerch.com/ support channel https //www.patreon.com/zachstar paypal one time donation https //www.paypal.me/zachstaryt ►follow instagram https //www.instagram.com/zachstar/ twitter https //twitter.com/imzachstar 3b1b fractal video https //www.youtube.com/watch v=gb9n2ghshn4 numberphile infinity https //www.youtube.com/watch v=elvozm0d4h0 vsauce banach-tarski paradox https //www.youtube.com/watch v=s86-z-cbaha epic song release fire formantx animations brainup studios http //brainup.in/ ►my setup space pictures https //amzn.to/2cc4kqj magnetic floating globe https //amzn.to/2vgpdn0 camera https //amzn.to/2rivyu5 mic https //amzn.to/35bkiri tripod https //amzn.to/2rgmtnl equilibrium tube https //amzn.to/2sowdrh ►check amazon store https //www.amazon.com/shop/zachstar,Machine Learning
computation bayesian model selection interactive articles,week dr. keith duggar alex stenlake dr. tim scarfe discuss theory computation intelligence bayesian model selection intelligence explosion phenomenon `` interactive articles '' 00:00:00 intro 00:01:27 kernels context-free grammars 00:06:04 theory computation 00:18:41 intelligence 00:22:03 bayesian model selection 00:44:05 ai-iq measure intelligence explosion 00:52:09 interactive articles 01:12:32 outro,Machine Learning
accurate,accurate,Machine Learning
decision trees python start finish,note support statquest purchasing jupyter notebook python code seen video https //statquest.org/product/jupyter-notebook-classification-trees-in-python-from-start-to-finish/ ⭐ note code use kite free ai-powered coding assistant help code faster smarter kite plugin integrates top editors ides give smart completions documentation ’ typing love https //www.kite.com/get-kite/ utm_medium=referral utm_source=youtube utm_campaign=statquest utm_content=description-only webinar recorded 20200528 11:00am new york time note statquest assumes already familiar decision trees https //youtu.be/7veupufgjhk cross validation https //youtu.be/fsytzgwwbvw confusion matrices https //youtu.be/kdsp6soqa7o cost complexity pruning https //youtu.be/d0efhejsfho bias variance overfitting https //youtu.be/eubbz3bi-aa complete index statquest videos check https //statquest.org/video-index/ 'd like support statquest please consider ... patreon https //www.patreon.com/statquest ... ... youtube membership https //www.youtube.com/channel/uctyluttgs3k1fg4y5tahlbw/join ... cool statquest t-shirt sweatshirt usa/europe https //teespring.com/stores/statquest everywhere https //www.redbubble.com/people/starmer/works/40421224-statquest-double-bam asc=u p=t-shirt ... buying one two songs go large get whole album https //joshuastarmer.bandcamp.com/ ... donating statquest https //www.paypal.me/statquest lastly want keep research create new statquests follow twitter https //twitter.com/joshuastarmer 0:00 awesome song introduction 5:23 import modules 7:40 import data 11:18 missing data part 1 identifying 15:57 missing data part 2 dealing 21:16 format data part 1 x 23:33 format data part 2 one-hot encoding 37:29 build preliminary tree 46:31 pruning part 1 visualize alpha 51:22 pruning part 2 cross validation 56:46 build draw final tree statquest ml classificationtrees,Machine Learning
charles isbell michael littman machine learning education lex fridman podcast 148,charles isbell dean college computing georgia tech michael littman computer scientist brown university please support podcast checking sponsors athletic greens https //athleticgreens.com/lex use code lex get 1 month fish oil eight sleep https //www.eightsleep.com/lex use code lex get special savings masterclass https //masterclass.com/lex get 2 price 1 cash app https //cash.app/ use code lexpodcast get 10 episode links charles 's twitter https //twitter.com/isbellhfh charles 's website https //www.cc.gatech.edu/~isbell/ michael 's twitter https //twitter.com/mlittmancs michael 's website https //www.littmania.com/ michael 's youtube https //www.youtube.com/user/mlittman podcast info podcast website https //lexfridman.com/podcast apple podcasts https //apple.co/2lwqzir spotify https //spoti.fi/2newcf8 rss https //lexfridman.com/feed/podcast/ full episodes playlist https //www.youtube.com/playlist list=plraxtmerzgodp_8gztsuki9nrranbkkp4 clips playlist https //www.youtube.com/playlist list=plraxtmerzgoecifp3cbcieelojeitor41 outline 0:00 introduction 2:27 machine learning statistics 6:49 neurips vs icml 9:05 data important algorithm 14:49 role hardship education 23:33 charles michael met 28:05 key success never satisfied 31:23 bell labs 42:50 teaching machine learning 53:01 westworld ex machina 1:01:00 simulation 1:07:49 college experience times covid 1:36:27 advice young people 1:43:19 learn program 1:54:43 friendship connect subscribe youtube channel twitter https //twitter.com/lexfridman linkedin https //www.linkedin.com/in/lexfridman facebook https //www.facebook.com/lexfridmanpage instagram https //www.instagram.com/lexfridman medium https //medium.com/ lexfridman support patreon https //www.patreon.com/lexfridman,Machine Learning
3 books optimization machine learning,optimization field mathematics concerned finding good best solution among many candidates important foundational topic required machine learning machine learning algorithms fit historical data using optimization algorithm additionally broader problems model selection hyperparameter tuning also framed optimization problem although background optimization critical machine learning practitioners daunting topic given often described using highly mathematical language post discover top books optimization helpful machine learning practitioners let ’ get started books optimization machine learning photo patrick alexander rights reserved overview field optimization enormous touches many fields study hundreds books topic textbooks filed math proofs fair enough given highly mathematical subject nevertheless books provide approachable description optimization algorithms optimization algorithms relevant machine learning instead useful focus small subset algorithms frankly hard group optimization algorithms many concerns nevertheless important idea optimization underlies simpler algorithms linear regression logistic regression e.g convex optimization least squares newton methods etc neural networks first-order methods gradient descent etc. foundational optimization algorithms covered optimization textbooks optimization problems machine learning well behaved optimization used automl hyperparameter tuning therefore knowledge stochastic optimization algorithms required simulated annealing genetic algorithms particle swarm etc. although optimization algorithms also type learning algorithm referred biologically inspired computation computational intelligence therefore take look books cover classical optimization algorithms well books alternate optimization algorithms fact first book look covers types algorithms much algorithms optimization book written mykel kochenderfer tim wheeler published 2019 algorithms optimization book might one textbooks ’ seen broadly covers field optimization techniques relevant modern machine learning book provides broad introduction optimization focus practical algorithms design engineering systems cover wide variety optimization topics introducing underlying mathematical problem formulations algorithms solving figures examples exercises provided convey intuition behind various approaches — page xiiix algorithms optimization 2019 importantly algorithms range univariate methods bisection line search etc first-order methods gradient descent second-order methods newton ’ method direct methods pattern search stochastic methods simulated annealing population methods genetic algorithms particle swarm much includes technical descriptions algorithms references worked examples algorithms julia ’ shame examples python would make book near perfect eyes complete table contents book listed chapter 01 introduction chapter 02 derivatives gradients chapter 03 bracketing chapter 04 local descent chapter 05 first-order methods chapter 06 second-order methods chapter 07 direct methods chapter 08 stochastic methods chapter 09 population methods chapter 10 constraints chapter 11 linear constrained optimization chapter 12 multiobjective optimization chapter 13 sampling plans chapter 14 surrogate models chapter 15 probabilistic surrogate models chapter 16 surrogate optimization chapter 17 optimization uncertainty chapter 18 uncertainty propagation chapter 19 discrete optimization chapter 20 expression optimization chapter 21 multidisciplinary optimization like book lot full valuable practical advice highly recommend learn algorithms optimization 2019 numerical optimization book written jorge nocedal stephen wright published 2006 numerical optimization book focused math theory optimization algorithms presented cover many foundational techniques used common machine learning algorithms may little heavy average practitioner book intended textbook graduate students mathematical subjects intend book used graduate-level courses optimization offered engineering operations research computer science mathematics departments — page xviii numerical optimization 2006 even though highly mathematical descriptions algorithms precise may provide useful alternative description complement books listed complete table contents book listed chapter 01 introduction chapter 02 fundamentals unconstrained optimization chapter 03 line search methods chapter 04 trust-region methods chapter 05 conjugate gradient methods chapter 06 quasi-newton methods chapter 07 large-scale unconstrained optimization chapter 08 calculating derivatives chapter 09 derivative-free optimization chapter 10 least-squares problems chapter 11 nonlinear equations chapter 12 theory constrained optimization chapter 13 linear programming simplex method chapter 14 linear programming interior-point methods chapter 15 fundamentals algorithms nonlinear constrained optimization chapter 16 quadratic programming chapter 17 penalty augmented lagrangian methods chapter 18 sequential quadratic programming chapter 19 interior-point methods nonlinear programming ’ solid textbook optimization learn numerical optimization 2006 prefer theoretical approach subject another widely used mathematical book optimization “ convex optimization ” written stephen boyd lieven vandenberghe published 2004 computational intelligence introduction book written andries engelbrecht published 2007 computational intelligence introduction book provides excellent overview field nature-inspired optimization algorithms also referred computational intelligence includes fields evolutionary computation swarm intelligence book far less mathematical previous textbooks focused metaphor inspired system configure use specific algorithms lots pseudocode explanations material introductory nature shy away details present mathematical foundations interested reader intention book provide thorough attention computational intelligence paradigms algorithms give overview popular frequently used models — page xxix computational intelligence introduction 2007 algorithms like genetic algorithms genetic programming evolutionary strategies differential evolution particle swarm optimization useful know machine learning model hyperparameter tuning perhaps even model selection also form core many modern automl systems complete table contents book listed part introduction chapter 01 introduction computational intelligence part ii artificial neural networks chapter 02 artificial neuron chapter 03 supervised learning neural networks chapter 04 unsupervised learning neural networks chapter 05 radial basis function networks chapter 06 reinforcement learning chapter 07 performance issues supervised learning part iii evolutionary computation chapter 08 introduction evolutionary computation chapter 09 genetic algorithms chapter 10 genetic programming chapter 11 evolutionary programming chapter 12 evolution strategies chapter 13 differential evolution chapter 14 cultural algorithms chapter 15 coevolution part iv computational swarm intelligence chapter 16 particle swarm optimization chapter 17 ant algorithms part v artificial immune systems chapter 18 natural immune system chapter 19 artificial immune models part vi fuzzy systems chapter 20 fuzzy sets chapter 21 fuzzy logic reasoning ’ fan book recommend learn computational intelligence introduction 2007 summary post discovered books optimization algorithms helpful know applied machine learning miss good book optimization let know comments read books listed let know think comments post 3 books optimization machine learning appeared first machine learning mastery,Machine Learning
n john leonard `` marine robotics simultaneous localization mapping slam '',hello guys ieee soft robotics podcast would like thank great questions sent john really humble person great roboticist enjoyed discussion hopefully would helpful students acknowledge open problems challenges also interesting anecdotes john hope enjoy listening apologize post sounds self-promotional feel free close subreddit rules ​ episode audio podcast souncloud https //soundcloud.com/ieeeras-softrobotics/john-leonard-marine-robotics-simultaneous-localization-and-mapping-slam spotify https //open.spotify.com/show/3f19ovcbn05f9r9duy15tk itunes https //podcasts.apple.com/us/podcast/id1475793741 ​ video format https //youtu.be/juqsuxf3lam link comments,Machine Learning
p stylegan ~5k images,hi learned quite bit machine learning university never really actually deployed machine learning generated code want change machine learning part first project collection around 5k square images sizes href= '' https //github.com/lucidrains/stylegan2-pytorch '' /a -- sc_on -- href= '' https //teddit.net/r/machinelearning/comments/kzseha/p_stylegan_on_5k_images/ '' /aa href= '' https //teddit.net/r/machinelearning/comments/kzseha/p_stylegan_on_5k_images/ '' /a,Machine Learning
multinomial logistic regression python,"multinomial logistic regression extension logistic regression adds native support multi-class classification problems logistic regression default limited two-class classification problems extensions like one-vs-rest allow logistic regression used multi-class classification problems although require classification problem first transformed multiple binary classification problems instead multinomial logistic regression algorithm extension logistic regression model involves changing loss function cross-entropy loss predict probability distribution multinomial probability distribution natively support multi-class classification problems tutorial discover develop multinomial logistic regression models python completing tutorial know multinomial logistic regression extension logistic regression multi-class classification develop evaluate multinomial logistic regression develop final model making predictions new data tune penalty hyperparameter multinomial logistic regression model let ’ get started multinomial logistic regression python photo nicolas rénac rights reserved tutorial overview tutorial divided three parts multinomial logistic regression evaluate multinomial logistic regression model tune penalty multinomial logistic regression multinomial logistic regression logistic regression classification algorithm intended datasets numerical input variables categorical target variable two values classes problems type referred binary classification problems logistic regression designed two-class problems modeling target using binomial probability distribution function class labels mapped 1 positive class outcome 0 negative class outcome fit model predicts probability example belongs class 1 default logistic regression used classification tasks two class labels so-called multi-class classification instead requires modification support multi-class classification problems one popular approach adapting logistic regression multi-class classification problems split multi-class classification problem multiple binary classification problems fit standard logistic regression model subproblem techniques type include one-vs-rest one-vs-one wrapper models alternate approach involves changing logistic regression model support prediction multiple class labels directly specifically predict probability input example belongs known class label probability distribution defines multi-class probabilities called multinomial probability distribution logistic regression model adapted learn predict multinomial probability distribution referred multinomial logistic regression similarly might refer default standard logistic regression binomial logistic regression binomial logistic regression standard logistic regression predicts binomial probability i.e two classes input example multinomial logistic regression modified version logistic regression predicts multinomial probability i.e two classes input example new binomial multinomial probability distributions may want read tutorial discrete probability distributions machine learning changing logistic regression binomial multinomial probability requires change loss function used train model e.g log loss cross-entropy loss change output single probability value one probability class label familiar multinomial logistic regression let ’ look might develop evaluate multinomial logistic regression models python evaluate multinomial logistic regression model section develop evaluate multinomial logistic regression model using scikit-learn python machine learning library first define synthetic multi-class classification dataset use basis investigation generic dataset easily replace loaded dataset later make_classification function used generate dataset given number rows columns classes case generate dataset 1,000 rows 10 input variables columns 3 classes example generates dataset summarizes shape arrays distribution examples across three classes test classification dataset collections import counter sklearn.datasets import make_classification define dataset x make_classification n_samples=1000 n_features=10 n_informative=5 n_redundant=5 n_classes=3 random_state=1 summarize dataset print x.shape y.shape print counter running example confirms dataset 1,000 rows 10 columns expected rows distributed approximately evenly across three classes 334 examples class 1000 10 1000 counter 1 334 2 334 0 332 logistic regression supported scikit-learn library via logisticregression class logisticregression class configured multinomial logistic regression setting “ multi_class ” argument “ multinomial ” “ solver ” argument solver supports multinomial logistic regression “ lbfgs “ ... define multinomial logistic regression model model logisticregression multi_class='multinomial solver='lbfgs multinomial logistic regression model fit using cross-entropy loss predict integer value integer encoded class label familiar multinomial logistic regression api look might evaluate multinomial logistic regression model synthetic multi-class classification dataset good practice evaluate classification models using repeated stratified k-fold cross-validation stratification ensures cross-validation fold approximately distribution examples class whole training dataset use three repeats 10 folds good default evaluate model performance using classification accuracy given classes balanced complete example evaluating multinomial logistic regression multi-class classification listed evaluate multinomial logistic regression model numpy import mean numpy import std sklearn.datasets import make_classification sklearn.model_selection import cross_val_score sklearn.model_selection import repeatedstratifiedkfold sklearn.linear_model import logisticregression define dataset x make_classification n_samples=1000 n_features=10 n_informative=5 n_redundant=5 n_classes=3 random_state=1 define multinomial logistic regression model model logisticregression multi_class='multinomial solver='lbfgs define model evaluation procedure cv repeatedstratifiedkfold n_splits=10 n_repeats=3 random_state=1 evaluate model collect scores n_scores cross_val_score model x scoring='accuracy cv=cv n_jobs=-1 report model performance print 'mean accuracy .3f .3f mean n_scores std n_scores running example reports mean classification accuracy across folds repeats evaluation procedure note results may vary given stochastic nature algorithm evaluation procedure differences numerical precision consider running example times compare average outcome case see multinomial logistic regression model default penalty achieved mean classification accuracy 68.1 percent synthetic classification dataset mean accuracy 0.681 0.042 may decide use multinomial logistic regression model final model make predictions new data achieved first fitting model available data calling predict function make prediction new data example demonstrates make prediction new data using multinomial logistic regression model make prediction multinomial logistic regression model sklearn.datasets import make_classification sklearn.linear_model import logisticregression define dataset x make_classification n_samples=1000 n_features=10 n_informative=5 n_redundant=5 n_classes=3 random_state=1 define multinomial logistic regression model model logisticregression multi_class='multinomial solver='lbfgs fit model whole dataset model.fit x define single row input data row 1.89149379 -0.39847585 1.63856893 0.01647165 1.51892395 -3.52651223 1.80998823 0.58810926 -0.02542177 -0.52835426 predict class label yhat model.predict row summarize predicted class print 'predicted class yhat 0 running example first fits model available data defines row data provided model order make prediction case see model predicted class “ 1 ” single row data predicted class 1 benefit multinomial logistic regression predict calibrated probabilities across known class labels dataset achieved calling predict_proba function model example demonstrates predict multinomial probability distribution new example using multinomial logistic regression model predict probabilities multinomial logistic regression model sklearn.datasets import make_classification sklearn.linear_model import logisticregression define dataset x make_classification n_samples=1000 n_features=10 n_informative=5 n_redundant=5 n_classes=3 random_state=1 define multinomial logistic regression model model logisticregression multi_class='multinomial solver='lbfgs fit model whole dataset model.fit x define single row input data row 1.89149379 -0.39847585 1.63856893 0.01647165 1.51892395 -3.52651223 1.80998823 0.58810926 -0.02542177 -0.52835426 predict multinomial probability distribution yhat model.predict_proba row summarize predicted probabilities print 'predicted probabilities yhat 0 running example first fits model available data defines row data provided model order predict class probabilities note results may vary given stochastic nature algorithm evaluation procedure differences numerical precision consider running example times compare average outcome case see class 1 e.g array index mapped class integer value largest predicted probability 0.50 predicted probabilities 0.16470456 0.50297138 0.33232406 familiar evaluating using multinomial logistic regression models let ’ explore might tune model hyperparameters tune penalty multinomial logistic regression important hyperparameter tune multinomial logistic regression penalty term term imposes pressure model seek smaller model weights achieved adding weighted sum model coefficients loss function encouraging model reduce size weights along error fitting model popular type penalty l2 penalty adds weighted sum squared coefficients loss function weighting coefficients used reduces strength penalty full penalty slight penalty default logisticregression class uses l2 penalty weighting coefficients set 1.0 type penalty set via “ penalty ” argument values “ l1 “ “ l2 “ “ elasticnet ” e.g although solvers support penalty types weighting coefficients penalty set via “ c ” argument ... define multinomial logistic regression model default penalty logisticregression multi_class='multinomial solver='lbfgs penalty='l2 c=1.0 weighting penalty actually inverse weighting perhaps penalty 1 – c. documentation c float default=1.0 inverse regularization strength must positive float like support vector machines smaller values specify stronger regularization means values close 1.0 indicate little penalty values close zero indicate strong penalty c value 1.0 may indicate penalty c close 1.0 light penalty c close 0.0 strong penalty penalty disabled setting “ penalty ” argument string “ none “ ... define multinomial logistic regression model without penalty logisticregression multi_class='multinomial solver='lbfgs penalty='none familiar penalty let ’ look might explore effect different penalty values performance multinomial logistic regression model common test penalty values log scale order quickly discover scale penalty works well model found tuning scale may beneficial explore l2 penalty weighting values range 0.0001 1.0 log scale addition penalty 0.0 complete example evaluating l2 penalty values multinomial logistic regression listed tune regularization multinomial logistic regression numpy import mean numpy import std sklearn.datasets import make_classification sklearn.model_selection import cross_val_score sklearn.model_selection import repeatedstratifiedkfold sklearn.linear_model import logisticregression matplotlib import pyplot get dataset def get_dataset x make_classification n_samples=1000 n_features=20 n_informative=15 n_redundant=5 random_state=1 n_classes=3 return x get list models evaluate def get_models models dict p 0.0 0.0001 0.001 0.01 0.1 1.0 create name model key .4f p turn penalty cases p == 0.0 penalty case models key logisticregression multi_class='multinomial solver='lbfgs penalty='none else models key logisticregression multi_class='multinomial solver='lbfgs penalty='l2 c=p return models evaluate give model using cross-validation def evaluate_model model x define evaluation procedure cv repeatedstratifiedkfold n_splits=10 n_repeats=3 random_state=1 evaluate model scores cross_val_score model x scoring='accuracy cv=cv n_jobs=-1 return scores define dataset x get_dataset get models evaluate models get_models evaluate models store results results names list list name model models.items evaluate model collect scores scores evaluate_model model x store results results.append scores names.append name summarize progress along way print '/prestrong/stronga href= '' https //machinelearningmastery.com/different-results-each-time-in-machine-learning/ '' /apre class= '' urvanov-syntax-highlighter-plain-tag ''",Machine Learning
brains count numberphile,professor brian butterworth neuroscientist specialises numbers mathematics interview https //youtu.be/fcs4b3ojvjm earlier videos https //bit.ly/brian_butterworth links stuff full description ↓↓↓ brian 's website https //www.mathematicalbrain.com brian butterworth playlist https //bit.ly/brian_butterworth discussed papers ... auditory midbrain neurons count https //pubmed.ncbi.nlm.nih.gov/12219094/ mapping human temporal parietal neuronal population activity functional coupling mathematical cognition https //www.pnas.org/content/113/46/e7277 numberphile supported mathematical sciences research institute msri http //bit.ly/msrinumberphile also supported science sandbox simons foundation initiative dedicated engaging everyone process science https //www.simonsfoundation.org/outreach/science-sandbox/ support math america https //www.mathforamerica.org/ numberphile website http //www.numberphile.com/ numberphile facebook http //www.facebook.com/numberphile numberphile tweets https //twitter.com/numberphile subscribe http //bit.ly/numberphile_sub videos brady haran animation pete mcpartlan patreon http //www.patreon.com/numberphile numberphile t-shirts merch https //teespring.com/stores/numberphile brady 's videos subreddit http //www.reddit.com/r/bradyharan/ brady 's latest videos across channels http //www.bradyharanblog.com/ sign occasional emails http //eepurl.com/ydjl9,Machine Learning
127 – joe rogan conversations ideas love freedom joe rogan experience,joe rogan comedian ufc commentator host joe rogan experience please check sponsors get discount support podcast – neuro https //www.getneuro.com use code lex – eight sleep https //eightsleep.com/lex use code lex – dollar shave club https //dollarshaveclub.com/lex episode links joe ’ instagram https //www.instagram.com/joerogan joe ’ twitter http //twitter.com/joerogan jre spotify https //open.spotify.com/show/4rooj6egrf8k2irywzwomk jre apple https //podcasts.apple.com/us/podcast/the-joe-rogan-experience/id360084272 jre youtube https //www.youtube.com/channel/uczqup1qowdoebmsqxvdjxgq would like get information podcast go https //lexfridman.com/podcast connect lexfridman twitter linkedin facebook medium youtube watch video versions conversations,Machine Learning
master ’ data science amount cumulated data,read full story,Machine Learning
100 – alexander fridman dad plasma physicist,alexander fridman professor drexel university director nyheim plasma institute one top plasma physicists plasma chemists world importantly dad support podcast supporting sponsors jordan harbinger show https //www.jordanharbinger.com/subscribe magic spoon https //magicspoon.com/lex use code lex checkout conversation part artificial intelligence podcast would like get information podcast go https //lexfridman.com/ai connect lexfridman twitter linkedin facebook medium youtube watch video versions,Machine Learning
video -- kenneth stanley greatness planned ml street talk,https //www.youtube.com/watch v=lhygxyemq_e special edition professor kenneth stanley currently research science manager openai san fransisco 've dreaming getting kenneth show since beginning machine learning street talk might recall first ever show enhanced poet paper course kenneth hands interests neuroevolution open-endedness nns artificial life ai invented concept novelty search key idea tyranny objectives prevailing every aspect lives society indeed algorithms crucially objectives produce convergent behaviour thinking distract us discovering stepping stones lead greatness thinks monotonic objective obsession idea need continue improve benchmarks every year dangerous wrote detail recent book `` greatness planned '' main topic discussion show also cover ideas open endedness machine learning also audio version https //anchor.fm/machinelearningstreettalk/episodes/038 -- -professor-kenneth-stanley -- -why-greatness- -be-planned-ep7116 link comments,Machine Learning
discussion much data turns useless,recently working project found good chunk dataset might useless useful originally thought 40-60 luckily ton data still able something wanted know common link comments,Machine Learning
hacking marketing campaigns data science,read full story,Machine Learning
rt karpathy 🎉 introducing new papers code newsletter newsletter helps manage firehose new ml papers highlighting trending papers new state-of-the-art results community contributions around pwc https //paperswithcode.com/newsletter/1,🎉 introducing new papers code newsletter newsletter helps manage firehose new ml papers highlighting trending papers new state-of-the-art results community contributions around pwc paperswithcode.com/newslette…,Machine Learning
jordan edwards ml engineering devops azureml,week super insightful conversation jordan edwards principal program manager azureml team jordan coalface turning machine learning software engineering reality microsoft 's largest customers ml devops increasing velocity of- orchastrating non-interactive phase of- software deployments ml cover ml devops microsoft azure ml discuss model governance testing intepretability tooling cover age-old discussion dichotomy science engineering bridge gap ml devops cover jordan 's maturity model ml devops also cover exciting ml announcments recent microsoft build conference i.e fairlearn intepretml seal whitenoise openai code generation openai gpt-3 00:00:04 introduction ml devops microsoft build ml announcements 00:10:29 main show kick-off 00:11:06 jordan 's story 00:14:36 typical ml devops workflow 00:17:38 tim 's articulation ml devops 00:19:31 intepretability fairness 00:24:31 testing robustness 00:28:10 using gans generate testing data 00:30:26 gratuitous dl 00:33:46 challenges making ml devops framework iaas 00:38:48 cultural battles ml devops 00:43:04 maturity model ml devops 00:49:19 ml high interest credit card technical debt paper 00:50:19 ml engineering microsoft 01:01:20 ml flow 01:03:05 company-wide governance 01:08:15 's coming next 01:12:10 jordan 's hillarious piece advice younger self super happy turned one miss folks deeplearning machinelearning devops mldevops,Machine Learning
thank god machine learning 12 none others least 12 🙃,thank god machine learning 12 none others least 12 🙃 nitter.net/eigenbros/status/1348269634240327682,Machine Learning
drag equation empire state building v eiffel tower numberphile,tom crawford discusses drag drop empire state building eiffel tower ocean ... check brilliant get 20 premium service https //brilliant.org/numberphile sponsor links stuff full description ↓↓↓ find tom crawford https //tomrocksmaths.com/about/ tom videos numberphile http //bit.ly/crawford_videos check raw slow motions filming video https //youtu.be/slr-2stu20g numberphile supported mathematical sciences research institute msri http //bit.ly/msrinumberphile also supported science sandbox simons foundation initiative dedicated engaging everyone process science https //www.simonsfoundation.org/outreach/science-sandbox/ support math america https //www.mathforamerica.org/ numberphile website http //www.numberphile.com/ numberphile facebook http //www.facebook.com/numberphile numberphile tweets https //twitter.com/numberphile subscribe http //bit.ly/numberphile_sub videos brady haran patreon http //www.patreon.com/numberphile numberphile t-shirts merch https //teespring.com/stores/numberphile brady 's videos subreddit http //www.reddit.com/r/bradyharan/ brady 's latest videos across channels http //www.bradyharanblog.com/ sign occasional emails http //eepurl.com/ydjl9,Machine Learning
130 – scott aaronson computational complexity consciousness,scott aaronson quantum computer scientist please support podcast checking sponsors – simplisafe https //simplisafe.com/lex use code lex get free security camera – eight sleep https //www.eightsleep.com/lex use code lex get 200 – expressvpn https //expressvpn.com/lexpod use code lexpod get 3 months free – betterhelp https //betterhelp.com/lex use code lex get 10 episode links scott ’ blog https //www.scottaaronson.com/blog/ previous episode https //www.youtube.com/watch v=ux5t8eivcam podcast info podcast website https //lexfridman.com/podcast apple podcasts https //apple.co/2lwqzir spotify https //spoti.fi/2newcf8 rss https //lexfridman.com/feed/podcast/ youtube full episodes https //youtube.com/lexfridman youtube clips https //youtube.com/lexclips support connect – check sponsors,Machine Learning
recent advances metric monocular depth estimation,hi everyone regarding monocular depth estimation seems lots landmark papers advances direction unsupervised/self-supervised learning depth however occurred models regress metric depth maps 's always `` bananas '' 'm wondering deal lack distance unit autonomous vehicles surely useful know depth meters determine velocity cars far pedestrian us believe also easier gather data indoor since use rgb-d cameras kinect imagenet-like dataset bigger nyu depth train current state-of-the-art terms accuracy thanks advance best regards kostozard link comments,Machine Learning
tool track relate key ideas papers readen,'m wondering tool researchers use log key ideas papers read sorry vague question guess thinking kind digital whiteboard boxes representing papers bullet points main contributions kind personal comments relations among papers anything similar purpose would recommend link comments,Machine Learning
human brain work neurobio recommendations thread,human mind decent ai brain nice dedicated hardware without faults maybe learn things ml researcher practitioner books articles topic would recommend recommendations jeff hawkins 2004 intelligence book engineer 's perspective high-level functioning brain presented falsifiable scientific theory open source code used commercial applications eliasmith et al 2012 large-scale model functioning brain article describes world ’ largest functional brain model mostly handcrafted open source code link comments,Machine Learning
docker dev workflow apache spark,benefits come using docker containers well known provide consistent isolated environments applications deployed anywhere locally dev testing prod environments across cloud providers on-premise repeatable way read full story,Machine Learning
034 eray özkural- agi simulations safety,dr. eray ozkural agi researcher turkey founder celestial intellect cybernetics eray extremely critical max tegmark nick bostrom miri founder elizier yodokovsky views ai safety eray thinks views represent form neoludditism capturing valuable research budgets doomsday fear-mongering effectively want prevent ai developed n't agree eray also sceptical intelligence explosion hypothesis argument simulation panel -- dr. keith duggar dr. tim scarfe yannic kilcher 00:00:00 show teaser intro added nuggets commentary 00:48:39 main show introduction 00:53:14 doomsaying control 00:56:39 fear basilisk 01:08:00 intelligence explosion ethics 01:09:45 fear automous drone ... spam 01:11:25 infinity point hypothesis 01:15:26 meat level intelligence 01:21:25 defining intelligence ... yet 01:27:34 'll make brains shoot 01:31:00 universe likes deep learning 01:33:16 nns glorified hash tables 01:38:44 radical behaviorists 01:41:29 omega architecture possible agi 01:53:33 simulation hypothesis 02:09:44 one cometh unto simulation jesus christ 02:16:47 agendas motivations mind projections 02:23:38 computable universe bulk automata 02:30:31 self-organized post-show coda 02:31:29 investigating intelligent agency science 02:36:56 goodbye cheers https //www.youtube.com/watch v=pzshzda9tju,Machine Learning
ai dungeon ai-generated adventure game nick walton,original ai dungeon made year ago result curious gamer hackathon gpt-2 text transformer fast forward present day ai dungeon expanded unique example creative ai technology game boasts 1.5 million players multiple genres stories even multiplayer adventures read full story,Machine Learning
subword tokenization byte pair encoding,video learn byte pair encoding works look motivation see character level byte pair encoding works also touch byte-level bpe wordpiece tokenization bpe bytepairencoding nlp please subscribe like video help keep motivated make awesome videos like one buy book approaching almost machine learning problem please visit https //bit.ly/buyaaml follow twitter https //twitter.com/abhi1thakur linkedin https //www.linkedin.com/in/abhi1thakur/ kaggle https //kaggle.com/abhishek instagram https //instagram.com/abhi4ml,Machine Learning
machine learning wayr reading week 104,place share machine learning research papers journals articles 're reading week relates 're researching means elaborate give us insight otherwise could interesting paper 've read please try provide insight understanding please n't post things present wiki preferably link arxiv page pdf easily access pdf summary page way around pertinent links previous weeks 1-10 11-20 21-30 31-40 41-50 51-60 61-70 71-80 81-90 91-100 101-110 week 1 week 11 week 21 week 31 week 41 week 51 week 61 week 71 week 81 week 91 week 101 week 2 week 12 week 22 week 32 week 42 week 52 week 62 week 72 week 82 week 92 week 102 week 3 week 13 week 23 week 33 week 43 week 53 week 63 week 73 week 83 week 93 week 103 week 4 week 14 week 24 week 34 week 44 week 54 week 64 week 74 week 84 week 94 week 5 week 15 week 25 week 35 week 45 week 55 week 65 week 75 week 85 week 95 week 6 week 16 week 26 week 36 week 46 week 56 week 66 week 76 week 86 week 96 week 7 week 17 week 27 week 37 week 47 week 57 week 67 week 77 week 87 week 97 week 8 week 18 week 28 week 38 week 48 week 58 week 68 week 78 week 88 week 98 week 9 week 19 week 29 week 39 week 49 week 59 week 69 week 79 week 89 week 99 week 10 week 20 week 30 week 40 week 50 week 60 week 70 week 80 week 90 week 100 upvoted papers two weeks ago /u/big_temporary_3449 /u/arminbazzaa pdf link /u/captain_flashheart machine learning design patterns besides rules fun link comments,Machine Learning
perform regression analysis using excel,excel perform various statistical analyses including regression analysis great option nearly everyone access excel post excellent introduction performing interpreting regression analysis even excel ’ primary statistical software package post provide step-by-step instructions using excel perform multiple regression … post perform regression analysis using excel appeared first statistics jim,Machine Learning
switch transformers scaling trillion parameter models simple efficient sparsity,ai technology switchtransformer scale next frontier ai google brain uses sparsity hard routing massively increase model 's parameters keeping flops per forward pass constant switch transformer compares favorably dense counterparts terms speed sample efficiency breaks next magic number one trillion parameters outline 0:00 intro overview 4:30 performance gains scale 8:30 switch transformer architecture 17:00 model- data- expert-parallelism 25:30 experimental results 29:00 stabilizing training 32:20 distillation dense models 33:30 final comments paper https //arxiv.org/abs/2101.03961 codebase t5 https //github.com/google-research/text-to-text-transfer-transformer abstract deep learning models typically reuse parameters inputs mixture experts moe defies instead selects different parameters incoming example result sparsely-activated model -- outrageous numbers parameters -- constant computational cost however despite several notable successes moe widespread adoption hindered complexity communication costs training instability -- address switch transformer simplify moe routing algorithm design intuitive improved models reduced communication computational costs proposed training techniques help wrangle instabilities show large sparse models may trained first time lower precision bfloat16 formats design models based t5-base t5-large obtain 7x increases pre-training speed computational resources improvements extend multilingual settings measure gains mt5-base version across 101 languages finally advance current scale language models pre-training trillion parameter models `` colossal clean crawled corpus '' achieve 4x speedup t5-xxl model authors william fedus barret zoph noam shazeer links tabnine code completion referral http //bit.ly/tabnine-yannick youtube https //www.youtube.com/c/yannickilcher twitter https //twitter.com/ykilcher discord https //discord.gg/4h8xxdf bitchute https //www.bitchute.com/channel/yannic-kilcher minds https //www.minds.com/ykilcher parler https //parler.com/profile/yannickilcher linkedin https //www.linkedin.com/in/yannic-kilcher-488534136/ bilibili https //space.bilibili.com/1824646584 want support best thing share content want support financially completely optional voluntary lot people asked subscribestar https //www.subscribestar.com/yannickilcher patreon https //www.patreon.com/yannickilcher bitcoin btc bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq ethereum eth 0x7ad3513e3b8f66799f507aa7874b1b0ebc7f85e2 litecoin ltc lqw2trykyetvc8wjfkhpphtpbdm4vw7r9m monero xmr 4acl8agreo5hair8a9cevrw8peauwvnp1wnsdzxw7tzicdlhzagsgzhrqabdnfy8yum9fwjdvijphkrjv4fwt19cjzn9d4n,Machine Learning
140 – lisa feldman barrett love evolution human brain,lisa feldman barrett neuroscientist psychologist author please support podcast checking sponsors – athletic greens https //athleticgreens.com/lex use code lex get 1 month fish oil – eight sleep https //www.eightsleep.com/lex use code lex get 200 – masterclass https //masterclass.com/lex get 15 annual sub – betterhelp https //betterhelp.com/lex get 10 episode links seven half lessons brain book https //amzn.to/2sp5ar9 emotions made book https //amzn.to/2gwafg6 lisa ’ twitter https //twitter.com/lfeldmanbarrett lisa ’ website https //lisafeldmanbarrett.com/ podcast info podcast website https //lexfridman.com/podcast apple podcasts https //apple.co/2lwqzir spotify https //spoti.fi/2newcf8 rss https //lexfridman.com/feed/podcast/ youtube full episodes,Machine Learning
presidential debate sentiment analysis lstm onevsrest linearsvc nlp step-by-step guide,read full story,Machine Learning
reinforcement learning rl open source fest day 2 demos,three students present research programming project microsoft research real world reinforcement learning team online 0:00 intro day 2 1:12 pushing limits vw flatbuffers speaker sharad chitlangia 20:30 contextual bandits data visualization jupyter notebooks speaker milind agarwal 38:37 vw onnx speaker harish kamath learn microsoft research 's rl open source fest https //www.microsoft.com/en-us/research/academic-program/rl-open-source-fest/,Machine Learning
peter norvig artificial intelligence modern approach,peter norvig research director google co-author stuart russell book artificial intelligence modern approach educated inspired whole generation researchers including get field conversation part artificial intelligence podcast would like get information podcast go https //lexfridman.com/ai connect lexfridman twitter linkedin facebook medium youtube watch video versions conversations enjoy podcast please rate 5 stars itunes support patreon ’ outline,Machine Learning
accountable algorithms get wrong,read full story,Machine Learning
rt ykilcher go catch ykilcher latest deep learning video,go catch ykilcher latest deep learning video nitter.net/ykilcher/status/1352704316080091139,Machine Learning
different career paths academia industry panel microsoft phd summit 2020,panel four researchers across academia industry discuss different career paths research microsoft 's third phd summit two-day virtual workshop opportunity top phd students enhance skills build network discuss research within community peers notable microsoft researchers speakers gonzalo ramos principal researcher microsoft discussion lead valerie taylor ceo president cmd-it director mathematics computer science division argonne national laboratory armando solar-lezama professor eecs associate director coo csail kristin lauter principal researcher partner research manager microsoft information workshop found https //www.microsoft.com/en-us/research/event/phd-summit-2020/,Machine Learning
rt ykilcher how.rl.works/,how.rl.works/,Machine Learning
ai studies quantum computer systems consequences,roger penrose 's theory consciousness correct 's good chance 's true quantum events microtubuls show normal computer systems enough make general artificial intelligence 'd like ask possible create machane learning deep learning general ai systems quantum computers say super quantum computers think answer logically yes also wonder 's mathematical technical basis link comments,Machine Learning
best python tutorial ever need watch,read full story,Machine Learning
take good data scientist,read full story,Machine Learning
rt lexfridman tegmark exceptionally smart good human,tegmark exceptionally smart good human,Machine Learning
elements statquest webinar,webinar q brought xp inc. information xp inc see https //www.xpinc.com/ ⭐ note code use kite free ai-powered coding assistant help code faster smarter kite plugin integrates top editors ides give smart completions documentation ’ typing love https //www.kite.com/get-kite/ utm_medium=referral utm_source=youtube utm_campaign=statquest utm_content=description-only complete index statquest videos check https //statquest.org/video-index/ 'd like support statquest please consider ... patreon https //www.patreon.com/statquest ... ... youtube membership https //www.youtube.com/channel/uctyluttgs3k1fg4y5tahlbw/join ... cool statquest t-shirt sweatshirt usa/europe https //teespring.com/stores/statquest everywhere https //www.redbubble.com/people/starmer/works/40421224-statquest-double-bam asc=u p=t-shirt ... buying one two songs go large get whole album https //joshuastarmer.bandcamp.com/ ... donating statquest https //www.paypal.me/statquest lastly want keep research create new statquests follow twitter https //twitter.com/joshuastarmer 0:00 introduction 1:16 history statquest 4:55 rule 1 focus main ideas 9:59 rule 2 empathy audience 11:31 rule 3 use pictures 14:17 rules 4 5 repetition helpful math 21:01 rule 6 n't repeat existing explanations 23:08 rule 7 limit presentation 3 main ideas 24:50 n't use laser pointer 27:27 dare look stupid 29:16 summary 32:58 q communicate 34:32 q future big data 39:58 q automl 43:54 prepare future data science 48:03 academics vs industry 52:11 data science vs powerpoint 57:00 neural nets overrated 1:00:17 story behind silly songs bam 1:02:35 favorite ml algorithm 1:03:32 decide become full time youtuber 1:07:18 think 5 cutoff p-values 1:12:50 silly song statquest,Machine Learning
r best research papers look ml bias,hello everyone looking recommendations papers covering different ml biases suggestive approaches mitigate biases pros cons limited knowledge topic would like learn could n't find suitable thread query appreciate responses link comments,Machine Learning
rt ykilcher wanted give openai 's newest text-to-image transformer dall·e try pytorch thanks lucidrains 🐍🔥 breathtaking possibilities generating high-quality images arbitrary descriptions ▶️ https //github.com/lucidrains/dalle-pytorch h/t andfanilo ykilcher,wanted give openai 's newest text-to-image transformer dall·e try pytorch thanks lucidrains 🐍🔥 breathtaking possibilities generating high-quality images arbitrary descriptions ▶️ github.com/lucidrains/dalle-… h/t andfanilo ykilcher,Machine Learning
francois chollet measure intelligence,cover francois chollet 's recent paper abstract make deliberate progress towards intelligent human-like artificial systems need following appropriate feedback signal need able define evaluate intelligence way enables comparisons two systems well comparisons humans past hundred years abundance attempts define measure intelligence across fields psychology ai summarize critically assess definitions evaluation approaches making apparent two historical conceptions intelligence implicitly guided note practice contemporary ai community still gravitates towards benchmarking intelligence comparing skill exhibited ais humans specific tasks board games video games argue solely measuring skill given task falls short measuring intelligence skill heavily modulated prior knowledge experience unlimited priors unlimited training data allow experimenters `` buy '' arbitrary levels skills system way masks system 's generalization power articulate new formal definition intelligence based algorithmic information theory describing intelligence skill-acquisition efficiency highlighting concepts scope generalization difficulty priors experience using definition propose set guidelines general ai benchmark look like finally present benchmark closely following guidelines abstraction reasoning corpus arc built upon explicit set priors designed close possible innate human priors argue arc used measure human-like form general fluid intelligence enables fair general intelligence comparisons ai systems humans,Machine Learning
technical lessons learn job,'m computer scientist machine learning enthusiast 've faced quite problems career could 've never learned textbooks courses since 've ever worked actual ml project wondering technical lessons tricks/tips learn nowadays oh popular online courses even books mean technical faced usual management issues `` great data '' n't `` need deep learning '' n't `` give deep learning also explain predicts way '' 's ... works like actual technical stuff like example never occurred cases take account hardware limitations duh know users working model fit memory consumer laptop everything run cloud yeah 'd interested lessons learned 've job longer link comments,Machine Learning
134 – eric weinstein nature good evil genius madness,eric weinstein mathematical physicist podcaster intellectual please support podcast checking sponsors – grammarly https //grammarly.com/lex get 20 premium – sun basket https //sunbasket.com/lex use code lex get 35 – semrush https //www.semrush.com/partner/lex/ get free month guru – expressvpn https //expressvpn.com/lexpod use code lexpod get 3 months free episode links eric ’ twitter https //twitter.com/ericrweinstein eric ’ youtube https //www.youtube.com/ericweinsteinphd portal podcast https //podcasts.apple.com/us/podcast/the-portal/id1469999563 podcast info podcast website https //lexfridman.com/podcast apple podcasts https //apple.co/2lwqzir spotify https //spoti.fi/2newcf8 rss https //lexfridman.com/feed/podcast/ youtube full episodes https //youtube.com/lexfridman youtube clips https //youtube.com/lexclips support connect – check sponsors,Machine Learning
michael stevens vsauce,michael stevens creator vsauce one popular educational youtube channel world 15 million subscribers 1.7 billion views videos often ask answer questions profound entertaining spanning topics physics psychology part channel created 3 seasons mind field series explored human behavior conversation part artificial intelligence podcast would like get information podcast go https //lexfridman.com/ai connect lexfridman twitter linkedin facebook medium youtube,Machine Learning
spotify audio features time series additive spotify analyzer,many articles analyzing spotify data many applications well one-time analysis individual 's music library app specific purpose app different one thing meant grow provide place add analysis article audio features time series created read full story,Machine Learning
iclr 2020 yann lecun energy-based models,week connor shorten yannic kilcher tim scarfe reacted yann lecun 's keynote speech year 's iclr conference passed iclr number two ml conference completely open year sessions publicly accessible via internet yann spent talk speaking self-supervised learning energy-based models ebms manifold learning n't worry n't heard ebms neither thanks watching please subscribe paper links iclr 2020 keynote talk https //iclr.cc/virtual_2020/speaker_7.html tutorial energy-based learning http //yann.lecun.com/exdb/publis/pdf/lecun-06.pdf concept learning energy-based models yannic 's explanation https //www.youtube.com/watch v=cs_j-onwggg concept learning energy-based models paper https //arxiv.org/pdf/1811.02486.pdf concept learning energy-based models openai blog post https //openai.com/blog/learning-concepts-with-energy-functions/ deeplearning machinelearning iclr iclr2020 yannlecun,Machine Learning
build pytorch trainer,video show easy simple build trainer training pytorch models help make kinds customizations want video also intro tez small simple to-the-point no-nonsense trainer/wrapper built find https //github.com/abhishekkrthakur/tez using tez future videos keep videos short precise still things like tez star repository please subscribe like video help keep motivated make awesome videos like one buy book approaching almost machine learning problem please visit https //bit.ly/buyaaml follow twitter https //twitter.com/abhi1thakur linkedin https //www.linkedin.com/in/abhi1thakur/ kaggle https //kaggle.com/abhishek instagram https //instagram.com/abhi4ml,Machine Learning
semi-supervised learning label propagation,"semi-supervised learning refers algorithms attempt make use labeled unlabeled training data semi-supervised learning algorithms unlike supervised learning algorithms able learn labeled training data popular approach semi-supervised learning create graph connects examples training dataset propagate known labels edges graph label unlabeled examples example approach semi-supervised learning label propagation algorithm classification predictive modeling tutorial discover apply label propagation algorithm semi-supervised learning classification dataset completing tutorial know intuition label propagation semi-supervised learning algorithm works develop semi-supervised classification dataset establish baseline performance supervised learning algorithm develop evaluate label propagation algorithm use model output train supervised learning algorithm let ’ get started semi-supervised learning label propagation photo thebluesdude rights reserved tutorial overview tutorial divided three parts label propagation algorithm semi-supervised classification dataset label propagation semi-supervised learning label propagation algorithm label propagation semi-supervised learning algorithm algorithm proposed 2002 technical report xiaojin zhu zoubin ghahramani titled “ learning labeled unlabeled data label propagation. ” intuition algorithm graph created connects examples rows dataset based distance euclidean distance nodes graph label soft labels label distribution based labels label distributions examples connected nearby graph many semi-supervised learning algorithms rely geometry data induced labeled unlabeled examples improve supervised methods use labeled data geometry naturally represented empirical graph g v e nodes v 1 … n represent training data edges e represent similarities — page 193 semi-supervised learning 2006 propagation refers iterative nature labels assigned nodes graph propagate along edges graph connected nodes procedure sometimes called label propagation “ propagates ” labels labeled vertices fixed gradually edges unlabeled vertices — page 48 introduction semi-supervised learning 2009 process repeated fixed number iterations strengthen labels assigned unlabeled examples starting nodes 1 2 … l labeled known label 1 −1 nodes l 1 … n labeled 0 node starts propagate label neighbors process repeated convergence — page 194 semi-supervised learning 2006 familiar label propagation algorithm let ’ look might use project first must define semi-supervised classification dataset semi-supervised classification dataset section define dataset semis-supervised learning establish baseline performance dataset first define synthetic classification dataset using make_classification function define dataset two classes binary classification two input variables 1,000 examples ... define dataset x make_classification n_samples=1000 n_features=2 n_informative=2 n_redundant=0 random_state=1 next split dataset train test datasets equal 50-50 split e.g 500 rows ... split train test x_train x_test y_train y_test train_test_split x test_size=0.50 random_state=1 stratify=y finally split training dataset half portion labels portion pretend unlabeled ... split train labeled unlabeled x_train_lab x_test_unlab y_train_lab y_test_unlab train_test_split x_train y_train test_size=0.50 random_state=1 stratify=y_train tying together complete example preparing semi-supervised learning dataset listed prepare semi-supervised learning dataset sklearn.datasets import make_classification sklearn.model_selection import train_test_split define dataset x make_classification n_samples=1000 n_features=2 n_informative=2 n_redundant=0 random_state=1 split train test x_train x_test y_train y_test train_test_split x test_size=0.50 random_state=1 stratify=y split train labeled unlabeled x_train_lab x_test_unlab y_train_lab y_test_unlab train_test_split x_train y_train test_size=0.50 random_state=1 stratify=y_train summarize training set size print 'labeled train set x_train_lab.shape y_train_lab.shape print 'unlabeled train set x_test_unlab.shape y_test_unlab.shape summarize test set size print 'test set x_test.shape y_test.shape running example prepares dataset summarizes shape three portions results confirm test dataset 500 rows labeled training dataset 250 rows 250 rows unlabeled data labeled train set 250 2 250 unlabeled train set 250 2 250 test set 500 2 500 supervised learning algorithm 250 rows train model semi-supervised learning algorithm 250 labeled rows well 250 unlabeled rows could used numerous ways improve labeled training dataset next establish baseline performance semi-supervised learning dataset using supervised learning algorithm fit labeled training data important would expect semi-supervised learning algorithm outperform supervised learning algorithm fit labeled data alone case semi-supervised learning algorithm skill case use logistic regression algorithm fit labeled portion training dataset ... define model model logisticregression fit model labeled dataset model.fit x_train_lab y_train_lab model used make predictions entire hold test dataset evaluated using classification accuracy ... make predictions hold test set yhat model.predict x_test calculate score test set score accuracy_score y_test yhat summarize score print 'accuracy .3f score 100 tying together complete example evaluating supervised learning algorithm semi-supervised learning dataset listed baseline performance semi-supervised learning dataset sklearn.datasets import make_classification sklearn.model_selection import train_test_split sklearn.metrics import accuracy_score sklearn.linear_model import logisticregression define dataset x make_classification n_samples=1000 n_features=2 n_informative=2 n_redundant=0 random_state=1 split train test x_train x_test y_train y_test train_test_split x test_size=0.50 random_state=1 stratify=y split train labeled unlabeled x_train_lab x_test_unlab y_train_lab y_test_unlab train_test_split x_train y_train test_size=0.50 random_state=1 stratify=y_train define model model logisticregression fit model labeled dataset model.fit x_train_lab y_train_lab make predictions hold test set yhat model.predict x_test calculate score test set score accuracy_score y_test yhat summarize score print 'accuracy .3f score 100 running algorithm fits model labeled training dataset evaluates holdout dataset prints classification accuracy note results may vary given stochastic nature algorithm evaluation procedure differences numerical precision consider running example times compare average outcome case see algorithm achieved classification accuracy 84.8 percent would expect effective semi-supervised learning algorithm achieve better accuracy accuracy 84.800 next let ’ explore apply label propagation algorithm dataset label propagation semi-supervised learning label propagation algorithm available scikit-learn python machine learning library via labelpropagation class model fit like classification model calling fit function used make predictions new data via predict function ... define model model labelpropagation fit model training dataset model.fit ... ... make predictions hold test set yhat model.predict ... importantly training dataset provided fit function must include labeled examples integer encoded per normal unlabeled examples marked label -1 model determine label unlabeled examples part fitting model model fit estimated labels labeled unlabeled data training dataset available via “ transduction_ ” attribute labelpropagation class ... get labels entire training dataset data tran_labels model.transduction_ familiar use label propagation algorithm scikit-learn let ’ look might apply semi-supervised learning dataset first must prepare training dataset concatenate input data training dataset single array ... create training dataset input x_train_mixed concatenate x_train_lab x_test_unlab create list -1 valued unlabeled row unlabeled portion training dataset ... create `` label '' unlabeled data nolabel -1 range len y_test_unlab list concatenated labels labeled portion training dataset correspond input array training dataset ... recombine training dataset labels y_train_mixed concatenate y_train_lab nolabel train labelpropagation model entire training dataset ... define model model labelpropagation fit model training dataset model.fit x_train_mixed y_train_mixed next use model make predictions holdout dataset evaluate model using classification accuracy ... make predictions hold test set yhat model.predict x_test calculate score test set score accuracy_score y_test yhat summarize score print 'accuracy .3f score 100 tying together complete example evaluating label propagation semi-supervised learning dataset listed evaluate label propagation semi-supervised learning dataset numpy import concatenate sklearn.datasets import make_classification sklearn.model_selection import train_test_split sklearn.metrics import accuracy_score sklearn.semi_supervised import labelpropagation define dataset x make_classification n_samples=1000 n_features=2 n_informative=2 n_redundant=0 random_state=1 split train test x_train x_test y_train y_test train_test_split x test_size=0.50 random_state=1 stratify=y split train labeled unlabeled x_train_lab x_test_unlab y_train_lab y_test_unlab train_test_split x_train y_train test_size=0.50 random_state=1 stratify=y_train create training dataset input x_train_mixed concatenate x_train_lab x_test_unlab create `` label '' unlabeled data nolabel -1 range len y_test_unlab recombine training dataset labels y_train_mixed concatenate y_train_lab nolabel define model model labelpropagation fit model training dataset model.fit x_train_mixed y_train_mixed make predictions hold test set yhat model.predict x_test calculate score test set score accuracy_score y_test yhat summarize score print 'accuracy .3f score 100 running algorithm fits model entire training dataset evaluates holdout dataset prints classification accuracy note results may vary given stochastic nature algorithm evaluation procedure differences numerical precision consider running example times compare average outcome case see label propagation model achieves classification accuracy 85.6 percent slightly higher logistic regression fit labeled training dataset achieved accuracy 84.8 percent accuracy 85.600 far good another approach use semi-supervised model take estimated labels training dataset fit supervised learning model recall retrieve labels entire training dataset label propagation model follows ... get labels entire training dataset data tran_labels model.transduction_ use labels along input data train evaluate supervised learning algorithm logistic regression model hope supervised learning model fit entire training dataset would achieve even better performance semi-supervised learning model alone ... define supervised learning model model2 logisticregression fit supervised learning model entire training dataset model2.fit x_train_mixed tran_labels make predictions hold test set yhat model2.predict x_test calculate score test set score accuracy_score y_test yhat summarize score print 'accuracy .3f score 100 tying together complete example using estimated training set labels train evaluate supervised learning model listed evaluate logistic regression fit label propagation semi-supervised learning numpy import concatenate sklearn.datasets import make_classification sklearn.model_selection import train_test_split sklearn.metrics import accuracy_score sklearn.semi_supervised import labelpropagation sklearn.linear_model import logisticregression define dataset x make_classification n_samples=1000 n_features=2 n_informative=2 n_redundant=0 random_state=1 split train test x_train x_test y_train y_test train_test_split x test_size=0.50 random_state=1 stratify=y split train labeled unlabeled x_train_lab x_test_unlab y_train_lab y_test_unlab train_test_split x_train y_train test_size=0.50 random_state=1 stratify=y_train create training dataset input x_train_mixed concatenate x_train_lab x_test_unlab create `` label '' unlabeled data nolabel -1 range len y_test_unlab recombine training dataset labels y_train_mixed concatenate y_train_lab nolabel define model model labelpropagation fit model training dataset model.fit x_train_mixed y_train_mixed get labels entire training dataset data tran_labels model.transduction_ define supervised learning model model2 logisticregression fit supervised learning model entire training dataset model2.fit x_train_mixed tran_labels make predictions hold test set yhat model2.predict x_test calculate score test set score accuracy_score y_test yhat summarize score print 'accuracy .3f score 100 running algorithm fits semi-supervised model entire training dataset fits supervised learning model entire training dataset inferred labels evaluates holdout dataset printing classification accuracy note results may vary given stochastic nature algorithm evaluation procedure differences numerical precision consider running example times compare average outcome case see hierarchical approach semi-supervised model followed supervised model achieves classification accuracy 86.2 percent holdout dataset even better semi-supervised learning used alone achieved accuracy 85.6 percent accuracy 86.200 achieve better results tuning hyperparameters labelpropagation model let know discover comments reading section provides resources topic looking go deeper books introduction semi-supervised learning 2009 chapter 11 label propagation quadratic criterion semi-supervised learning 2006 papers learning labeled unlabeled data label propagation 2002 apis sklearn.semi_supervised.labelpropagation api section 1.14 semi-supervised scikit-learn user guide sklearn.model_selection.train_test_split api sklearn.linear_model.logisticregression api sklearn.datasets.make_classification api articles semi-supervised learning wikipedia summary tutorial discovered apply label propagation algorithm semi-supervised learning classification dataset specifically learned intuition label propagation semi-supervised learning algorithm works develop semi-supervised classification dataset establish baseline performance supervised learning algorithm develop evaluate label propagation algorithm use model output train supervised learning algorithm questions ask questions comments best answer post semi-supervised learning label propagation appeared first machine learning mastery",Machine Learning
max tegmark life 3.0,conversation max tegmark part mit course artificial general intelligence video version available youtube physics professor mit co-founder future life institute author “ life 3.0 human age artificial intelligence. ” would like get information podcast go https //lexfridman.com/ai connect lexfridman twitter linkedin facebook youtube watch video versions conversations,Machine Learning
brett lantz shows manage data r,read full story,Machine Learning
n't seen much bs legal statements google co since fired timnitgebru attacks free speech wrong matter dangerous done monopolies,n't seen much bs legal statements google co since fired timnitgebru attacks free speech wrong matter dangerous done monopolies nitter.net/not_the_bee/status/1348081057309077506,Machine Learning
vladimir vapnik predicates invariants essence intelligence,"vladimir vapnik co-inventor support vector machines support vector clustering vc theory many foundational ideas statistical learning born soviet union worked institute control sciences moscow us worked nec labs facebook ai research professor columbia university work cited 200,000 times conversation part artificial intelligence podcast would like get information podcast go https //lexfridman.com/ai connect lexfridman twitter linkedin facebook medium youtube",Machine Learning
train simsiam cifar10 91 accuracy,wanted share small example training simsiam paper exploring simple siamese representation learning https //arxiv.org/pdf/2011.10566.pdf cifar10 knn callback mechanism knn callback creates features training set end every epoch uses weighted knn make predictions test set mechanism knn pretty simple thanks pytorch lightning simsiam implementation lightly training 91 takes 800 epochs around 8h v100 gpu code found https //github.com/igorsusmelj/simsiam-cifar10 link comments,Machine Learning
sayak paul,week spoke sayak paul extremely active machine learning community discussed ai landscape india unsupervised representation learning data augmentation contrastive learning explainability abstract scene representations finally pruning recent super positions paper really enjoyed conversation hope folks 00:00:00 intro sayak 00:17:50 ai landscape india 00:24:20 unsupervised representation learning 00:26:11 data augmentation/contrastive learning 00:59:20 explainability 01:12:10 abstract scene representations 01:14:50 pruning super position paper,Machine Learning
r karpathy impressiveness judged distribution prompt/output likely e.g `` collection glasses table '' giving generic images nice rendering arbitrary text prompt textures rare/specific prompts 💥,impressiveness judged distribution prompt/output likely e.g `` collection glasses table '' giving generic images nice rendering arbitrary text prompt textures rare/specific prompts 💥,Machine Learning
cvpr2021 review met unqualified reviewer,read review cvpr 2021 think one reviewers totally unqualified he/she strongly rejected manuscript he/she ca n't understand method he/she claimed nothing attractive refused evaluate experiment results he/she said explanation algorithm -- -in fact used section introduce since used interdisciplinary acknowledge perhaps he/she overlooked deliberately he/she ca n't understand fact two reviewers understood method clearly also brought good advice critics well n't know face reviewer decided drawback paper think cvpr reviewers professional according past experience think reviewer really qualified cvpr link comments,Machine Learning
video tutorial mocking neural networks,https //youtu.be/_kvv9jxszvo hi filmed video tutorial unit testing deep learning code specifically focused mocking picked topic think many resources hope like feel free leave comment cheers link comments,Machine Learning
data scientists know multi-output multi-label training,multi-output machine learning — mixedrandomforest read full story,Machine Learning
paola arlotta brain development stem cell organoid,paola arlotta professor stem cell regenerative biology harvard university interested understanding molecular laws govern birth differentiation assembly human brain ’ cerebral cortex explores complexity brain studying engineering elements brain develops conversation part artificial intelligence podcast would like get information podcast go https //lexfridman.com/ai connect lexfridman twitter linkedin facebook medium youtube watch video versions conversations enjoy podcast,Machine Learning
michio kaku future humans aliens space travel physics,michio kaku theoretical physicist futurist professor city college new york author many fascinating books nature reality future civilization conversation part artificial intelligence podcast would like get information podcast go https //lexfridman.com/ai connect lexfridman twitter linkedin facebook medium youtube watch video versions conversations enjoy podcast please rate 5 stars apple podcasts support patreon ’ outline,Machine Learning
maximizing value ai,organizations already identified benefits gained artificial intelligence data science bringing talented resources enable build ai models solutions often business ’ understand capabilities huge potential ai well enough investments required tooling people make benefits sustainable long-term gap understanding often leads tactical exercises proofs-of-concept end going nowhere session andrew burgess strategic adviser ai – founder ceo greenhouse intelligence explained data scientists business executives work together maximize value ai check key takeaways session talk ai business person 1 explain ai simple terms business people usually ’ want hear algorithms python libraries highly unlikely understand aspects still want understand ai means business important explain capabilities ai simple terms focus ai rather works explain ai andrew uses framework 8 ai capabilities split three main areas capturing information explaining happening finally explaining happening 2 focus positives… ai bring huge value organization important business understands full range benefits available include customer satisfaction risk mitigation cost reduction loss mitigation revenue leakage mitigation revenue generation terms definitely resonate business people especially align strategic goals 3 …without getting carried away… ’ promise deliver understanding business ’ ai maturity key defining starting point help build successful ai journey great way assessing business ’ ai maturity talk different business lines understand want terms implementing ai strategy ‘ ai ambition ’ gap current situation ambition scope ai setting expectations ai avoid teams getting excitement help achieve realistic goals 4 …and without ignoring risks risks specific ai ’ important keep mind problems like black box models bias ai ’ naivety over-excitement… happen business people understand risks associated ai journey also solutions overcome 5 get strategy getting ai strategy place important define business challenges within business well objectives ai help deliver tool brings business data teams together creating ai strategy crucial identify opportunities ai align business ’ objectives solve specific challenges 6 think big act small strategy vision key ensure get important start things practical achievable short period time try identify activities going give value easiest implement start proofs-of-concept highest priority opportunities list also work data cleansing change management education 7 look options many business people become paralyzed look plethora ai vendors vendors one option used requirement exactly matches standard capabilities end scale unique data set and/or major project may need code scratch opportunities likely involve use configuration data science platforms 8 keep measuring checking get continuous improvement optimization activities program starting build momentum make sure ’ got regular meetings program team monitor progress demonstrate clear return investment simple dashboards display benefits following eight points data scientists analysts developers engage effectively business deliver ai projects valuable beneficial whole organization interested watching recording session learning click post maximizing value ai appeared first open source leader ai ml,Machine Learning
industry would categorize low-hanging fruit provide value use ml,idea would retail focus ecommerce generate lot data tech-savvy opinions brief lists industries considered maybe missed feel free mention comments automative transporation consumer goods manufacturing financial services insurance hospitality travel leisure media entertainment retail restaurants technology communications utilities industrials link comments,Machine Learning
somehow missed tenet new nolan movie back august watched last night bracing disappointment mediocre reviews disorientation settled realized may one favorite movies ever certain yet watch times,somehow missed tenet new nolan movie back august watched last night bracing disappointment mediocre reviews disorientation settled realized may one favorite movies ever certain yet watch times,Machine Learning
openai dall·e creating images text blog post explained,openai science gpt3 openai 's newest model dall·e shows absolutely amazing abilities generating high-quality images arbitrary text descriptions like gpt-3 range applications diversity outputs astonishing given single model trained purely autoregressive task model significant step towards combination text images future ai applications outline 0:00 introduction 2:45 overview 4:20 dataset 5:35 comparison gpt-3 7:00 model architecture 13:20 vq-vae 21:00 combining vq-vae gpt-3 27:30 pre-training relaxation 32:15 experimental results 33:00 hypothesis dall·e 's inner workings 36:15 sparse attention patterns 38:00 dall·e ca n't count 39:35 dall·e ca n't global order 40:10 dall·e renders different views 41:10 dall·e good texture 41:40 dall·e complete bust 43:30 dall·e reflections others 44:15 dall·e cross-sections objects 45:50 dall·e amazing style 46:30 dall·e generate logos 47:40 dall·e generate bedrooms 48:35 dall·e combine unusual concepts 49:25 dall·e generate illustrations 50:15 dall·e sometimes understands complicated prompts 50:55 dall·e pass part iq test 51:40 dall·e probably geographical temporal knowledge 53:10 reranking dramatically improves quality 53:50 conclusions comments blog https //openai.com/blog/dall-e/ links tabnine code completion referral http //bit.ly/tabnine-yannick youtube https //www.youtube.com/c/yannickilcher twitter https //twitter.com/ykilcher discord https //discord.gg/4h8xxdf bitchute https //www.bitchute.com/channel/yannic-kilcher minds https //www.minds.com/ykilcher parler https //parler.com/profile/yannickilcher linkedin https //www.linkedin.com/in/yannic-kilcher-488534136/ want support best thing share content want support financially completely optional voluntary lot people asked subscribestar https //www.subscribestar.com/yannickilcher patreon https //www.patreon.com/yannickilcher bitcoin btc bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq ethereum eth 0x7ad3513e3b8f66799f507aa7874b1b0ebc7f85e2 litecoin ltc lqw2trykyetvc8wjfkhpphtpbdm4vw7r9m monero xmr 4acl8agreo5hair8a9cevrw8peauwvnp1wnsdzxw7tzicdlhzagsgzhrqabdnfy8yum9fwjdvijphkrjv4fwt19cjzn9d4n,Machine Learning
131 – chris lattner future computing programming languages,chris lattner world-class software hardware engineer leading projects apple tesla google sifive please support podcast checking sponsors – blinkist https //blinkist.com/lex use code lex get free week premium – neuro https //www.getneuro.com use code lex get 15 – masterclass https //masterclass.com/lex get 15 annual sub – cash app https //cash.app/ use code lexpodcast get 10 episode links chris ’ twitter https //twitter.com/clattner_llvm chris ’ website http //nondot.org/sabre/ podcast info podcast website https //lexfridman.com/podcast apple podcasts https //apple.co/2lwqzir spotify https //spoti.fi/2newcf8 rss https //lexfridman.com/feed/podcast/ youtube full episodes https //youtube.com/lexfridman youtube clips https //youtube.com/lexclips support,Machine Learning
94 – ilya sutskever deep learning,"ilya sutskever co-founder openai one cited computer scientist history 165,000 citations one brilliant insightful minds ever field deep learning people world would rather talk brainstorm deep learning intelligence life ilya mic support podcast signing sponsors – cash app – use code “ lexpodcast ” download – cash app app store https //apple.co/2spruhe – cash app google play https //bit.ly/2mlvp5w episode links",Machine Learning
deep learning almost text classification problem binary multi-class multi-label,video show build model almost text classification problem i.e. binary classification multi-class classification multi-label classification expand language type deep learning model lstm gru transformers etc video using bert model transformers tez find tez https //github.com/abhishekkrthakur/tez nlp deeplearning pytorch please subscribe like video help keep motivated make awesome videos like one buy book approaching almost machine learning problem please visit https //bit.ly/buyaaml follow twitter https //twitter.com/abhi1thakur linkedin https //www.linkedin.com/in/abhi1thakur/ kaggle https //kaggle.com/abhishek instagram https //instagram.com/abhi4ml,Machine Learning
151 – dan kokotov speech recognition ai humans,dan kokotov vp engineering rev.ai automatic speech recognition company please support podcast checking sponsors – athletic greens https //athleticgreens.com/lex use code lex get 1 month fish oil – blinkist https //blinkist.com/lex use code lex get 25 premium – business wars https //wondery.com/business-wars/ – cash app https //cash.app/ use code lexpodcast get 10 episode links rev https //www.rev.com rev.ai https //www.rev.ai podcast info podcast website https //lexfridman.com/podcast apple podcasts https //apple.co/2lwqzir spotify https //spoti.fi/2newcf8 rss https //lexfridman.com/feed/podcast/ youtube full episodes https //youtube.com/lexfridman youtube clips https //youtube.com/lexclips support connect – check sponsors ’ best way support podcast – support patreon https //www.patreon.com/lexfridman – twitter https //twitter.com/lexfridman – instagram https //www.instagram.com/lexfridman – linkedin https //www.linkedin.com/in/lexfridman – facebook https //www.facebook.com/lexfridmanpage – medium https //medium.com/ lexfridman outline ’ timestamps episode podcast players able click timestamp jump time 00:00 – introduction 09:17 – dune 12:34 – rev 18:33 – translation 25:22 – gig economy 34:02 – automatic speech recognition 44:53 – create products people love 53:02 – future podcasts spotify 1:14:41 – book recommendations 1:16:02 – stories dystopian future 1:19:45 – movies stalin hitler 1:24:59 – interviewing putin 1:30:56 – meaning life,Machine Learning
thoughts optimal hyperparams gpt-2 used mathematical functions,'ve got pet project 've working using gpt-2 integer factoring current model dataset loader minimal gpt-2 trained simple sequences random 24-bit integers multiplied together product dataloader generates sequences multiplicand multiplier product multiplicand multiplier masked goal trained model consume product inference produce multiplicand multiplier training get ~0.8 loss eventually training gets stuck fails converge training test loss close thoughts ideal hyperparams gpt-2 used mathematical function approximation number layers heads embeddings warmup tokens final tokens etc tia link comments,Machine Learning
backpropagation details part 1 optimizing 3 parameters simultaneously,main ideas behind backpropagation super simple tons details comes time implementing video shows optimize three parameters neural network simultaneously introduces fancy notation ⭐ note code use kite free ai-powered coding assistant help code faster smarter kite plugin integrates top editors ides give smart completions documentation ’ typing love https //www.kite.com/get-kite/ utm_medium=referral utm_source=youtube utm_campaign=statquest utm_content=description-only note statquest assumes already know main ideas behind backpropagation https //youtu.be/in2xmbhilt4 ... also means familiar ... neural networks https //youtu.be/cqofi41lfdw chain rule https //youtu.be/wl1myxrtqhq gradient descent https //youtu.be/sdv4f4s2sb8 last note researching 'quest found page sebastian raschka helpful https //sebastianraschka.com/faq/docs/backprop-arbitrary.html complete index statquest videos check https //statquest.org/video-index/ 'd like support statquest please consider ... patreon https //www.patreon.com/statquest ... ... youtube membership https //www.youtube.com/channel/uctyluttgs3k1fg4y5tahlbw/join ... cool statquest t-shirt sweatshirt usa/europe https //teespring.com/stores/statquest everywhere https //www.redbubble.com/people/starmer/works/40421224-statquest-double-bam asc=u p=t-shirt ... buying one two songs go large get whole album https //joshuastarmer.bandcamp.com/ ... donating statquest https //www.paypal.me/statquest lastly want keep research create new statquests follow twitter https //twitter.com/joshuastarmer 0:00 awesome song introduction 3:01 derivatives change optimize multiple parameters 6:28 fancy notation 10:51 derivatives respect two different weights 15:02 gradient descent three parameters 17:19 fancy gradient descent animation statquest neuralnetworks backpropagation,Machine Learning
univariate function optimization python,optimize function one variable univariate function optimization involves finding input function results optimal output objective function common procedure machine learning fitting model one parameter tuning model single hyperparameter efficient algorithm required solve optimization problems type find best solution minimum number evaluations objective function given evaluation objective function could computationally expensive fitting evaluating model dataset excludes expensive grid search random search algorithms favor efficient algorithms like brent ’ method tutorial discover perform univariate function optimization python completing tutorial know univariate function optimization involves finding optimal input objective function takes single continuous argument perform univariate function optimization unconstrained convex function perform univariate function optimization unconstrained non-convex function let ’ get started univariate function optimization python photo robert haandrikman rights reserved tutorial overview tutorial divided three parts univariate function optimization convex univariate function optimization non-convex univariate function optimization univariate function optimization may need find optimal value function takes single parameter machine learning may occur many situations finding coefficient model fit training dataset finding value single hyperparameter results best model performance called univariate function optimization may interested minimum outcome maximum outcome function although simplified minimization maximizing function made minimizing adding negative sign outcomes function may may limits inputs function so-called unconstrained constrained optimization assume small changes input correspond small changes output function e.g smooth function may may single optima although prefer single optima shape function looks like large basin case know sample function one point find path minima function technically referred convex function minimization concave maximization functions ’ basin shape referred non-convex convex target function single optima shape target function leads optima nevertheless target function sufficiently complex ’ know derivative meaning use calculus analytically compute minimum maximum function gradient zero referred function non-differentiable although might able sample function candidate values ’ know input result best outcome may many reasons expensive evaluate candidate solutions therefore require algorithm efficiently samples input values function one approach solving univariate function optimization problems use brent ’ method brent ’ method optimization algorithm combines bisecting algorithm dekker ’ method inverse quadratic interpolation used constrained unconstrained univariate function optimization brent-dekker method extension bisection method root-finding algorithm combines elements secant method inverse quadratic interpolation reliable fast convergence properties univariate optimization algorithm choice many popular numerical optimization packages — pages 49-51 algorithms optimization 2019 bisecting algorithms use bracket lower upper input values split input domain bisecting order locate domain optima located much like binary search dekker ’ method one way achieved efficiently continuous domain dekker ’ method gets stuck non-convex problems brent ’ method modifies dekker ’ method avoid getting stuck also approximates second derivative objective function called secant method effort accelerate search brent ’ method univariate function optimization generally preferred univariate function optimization algorithms given efficiency brent ’ method available python via minimize_scalar scipy function takes name function minimized target function constrained range specified via “ bounds ” argument returns optimizeresult object dictionary containing solution importantly ‘ x ‘ key summarizes input optima ‘ fun ‘ key summarizes function output optima ‘ nfev ‘ summarizes number evaluations target function performed ... minimize function result minimize_scalar objective method='brent know perform univariate function optimization python let ’ look examples convex univariate function optimization section explore solve convex univariate function optimization problem first define function implements function case use simple offset version x^2 function e.g simple parabola u-shape function minimization objective function optima -5.0 objective function def objective x return 5.0 x 2.0 plot coarse grid function input values -10 10 get idea shape target function complete example listed plot convex target function numpy import arange matplotlib import pyplot objective function def objective x return 5.0 x 2.0 define range r_min r_max -10.0 10.0 prepare inputs inputs arange r_min r_max 0.1 compute targets targets objective x x inputs plot inputs vs target pyplot.plot inputs targets -- pyplot.show running example evaluates input values specified range using target function creates plot function inputs function outputs see u-shape function objective -5.0 line plot convex objective function note real optimization problem would able perform many evaluations objective function easily simple function used demonstration purposes learn use optimization algorithm next use optimization algorithm find optima ... minimize function result minimize_scalar objective method='brent optimized summarize result including input evaluation optima number function evaluations required locate optima ... summarize result opt_x opt_y result x result 'fun print 'optimal input x .6f opt_x print 'optimal output f x .6f opt_y print 'total evaluations n result 'nfev finally plot function mark optima confirm located place expected function ... define range r_min r_max -10.0 10.0 prepare inputs inputs arange r_min r_max 0.1 compute targets targets objective x x inputs plot inputs vs target pyplot.plot inputs targets -- plot optima pyplot.plot opt_x opt_y 's color= r show plot pyplot.show complete example optimizing unconstrained convex univariate function listed optimize convex objective function numpy import arange scipy.optimize import minimize_scalar matplotlib import pyplot objective function def objective x return 5.0 x 2.0 minimize function result minimize_scalar objective method='brent summarize result opt_x opt_y result x result 'fun print 'optimal input x .6f opt_x print 'optimal output f x .6f opt_y print 'total evaluations n result 'nfev define range r_min r_max -10.0 10.0 prepare inputs inputs arange r_min r_max 0.1 compute targets targets objective x x inputs plot inputs vs target pyplot.plot inputs targets -- plot optima pyplot.plot opt_x opt_y 's color= r show plot pyplot.show running example first solves optimization problem reports result note results may vary given stochastic nature algorithm evaluation procedure differences numerical precision consider running example times compare average outcome case see optima located 10 evaluations objective function input -5.0 achieving objective function value 0.0 optimal input x -5.000000 optimal output f x 0.000000 total evaluations n 10 plot function created time optima marked red square line plot convex objective function optima marked non-convex univariate function optimization convex function one resemble basin meaning may one hill valley make challenging locate global optima multiple hills valleys cause search get stuck report false local optima instead define non-convex univariate function follows objective function def objective x return x 2.0 x x 2.0 2.0 sample function create line plot input values objective values complete example listed plot non-convex univariate function numpy import arange matplotlib import pyplot objective function def objective x return x 2.0 x x 2.0 2.0 define range r_min r_max -3.0 2.5 prepare inputs inputs arange r_min r_max 0.1 compute targets targets objective x x inputs plot inputs vs target pyplot.plot inputs targets -- pyplot.show running example evaluates input values specified range using target function creates plot function inputs function outputs see function one false optima around -2.0 global optima around 1.2 line plot non-convex objective function note real optimization problem would able perform many evaluations objective function easily simple function used demonstration purposes learn use optimization algorithm next use optimization algorithm find optima call minimize_scalar function optimize function summarize result plot optima line plot complete example optimization unconstrained non-convex univariate function listed optimize non-convex objective function numpy import arange scipy.optimize import minimize_scalar matplotlib import pyplot objective function def objective x return x 2.0 x x 2.0 2.0 minimize function result minimize_scalar objective method='brent summarize result opt_x opt_y result x result 'fun print 'optimal input x .6f opt_x print 'optimal output f x .6f opt_y print 'total evaluations n result 'nfev define range r_min r_max -3.0 2.5 prepare inputs inputs arange r_min r_max 0.1 compute targets targets objective x x inputs plot inputs vs target pyplot.plot inputs targets -- plot optima pyplot.plot opt_x opt_y 's color= r show plot pyplot.show running example first solves optimization problem reports result want get started ensemble learning take free 7-day email crash course sample code click sign-up also get free pdf ebook version course download free mini-course case see optima located 15 evaluations objective function input 1.28 achieving objective function value -9.91 optimal input x 1.280776 optimal output f x -9.914950 total evaluations n 15 plot function created time optima marked red square see optimization deceived false optima successfully located global optima line plot non-convex objective function optima marked reading section provides resources topic looking go deeper books algorithms optimization 2019 apis optimization scipy.optimize optimization root finding scipy.optimize scipy.optimize.minimize_scalar api articles brent ’ method wikipedia secant method wikipedia summary tutorial discovered perform univariate function optimization python specifically learned univariate function optimization involves finding optimal input objective function takes single continuous argument perform univariate function optimization unconstrained convex function perform univariate function optimization unconstrained non-convex function questions ask questions comments best answer post univariate function optimization python appeared first machine learning mastery,Machine Learning
precision recall different loss functions,’ trying understand precision recall changes different loss functions ’ especially looking cross entropy dice loss focal loss object detection far ’ researched loss functions know dice loss focal loss good data imbalance know improve cross entropy loss calculation ’ struggling analyse would differ terms precision recall ’ appreciate someone could explain using different loss functions affect precision recall cheers link comments,Machine Learning
social dilemma part 3 dr. rebecca roache,week join dr. tim scarfe yannic kilcher keith duggar conversation dr. rebecca roache last 3-part series social dilemma netflix film rebecca senior lecturer philosophy royal holloway university london written extensively future friendship people claim friendships used people always staring phones even public social media turned us narcissists always managing pr rather present anxiety negative effects technology old written word technology bad friendships friends screens social media cause polarization bad thing promote quantity quality rebecca thinks social media echo chambers less ominous friendship closer inspection 00:00:32 teaser clip rebecca new manuscript friendship 00:02:52 introduction 00:04:56 memorisation vs reasoning technology enhancing friendships 00:09:29 word warcraft gaming communities echo chambers polarisation 00:12:34 horizontal vs vertical social attributes 00:17:18 exclusion others opinions 00:20:36 power silence others truth verification 00:23:58 misinformation 00:27:28 norms memes political terms co-opting bullying 00:31:57 redefinition political terms i.e racism 00:36:13 virtue signalling 00:38:57 many friends spread thin dunbars 150 00:42:54 morally objectionable believe contemplate objectionable ideas punishment 00:50:52 speaking thing acting 00:52:24 punishment deterrence vs retribution historical 00:53:59 yannic contemplating form speaking 00:57:32 silencing/blocking intellectual laziness ideas allowed talk 01:04:53 corporate ai ethics frameworks 01:09:14 autonomous vehicles 01:10:51 eternal facebook world online vs offline friendships 01:14:05 get best online friendships,Machine Learning
stochastic meme descent deep learning meme review episode 2 part 2 2,memes science ai part 2 antonio examining latest greatest deep learning memes music sunshower latashá papov yung logos sunny days anno domini beats trinity jeremy blake memes facebook.com/convolutionalmemes links youtube https //www.youtube.com/c/yannickilcher twitter https //twitter.com/ykilcher discord https //discord.gg/4h8xxdf bitchute https //www.bitchute.com/channel/yannic-kilcher minds https //www.minds.com/ykilcher parler https //parler.com/profile/yannickilcher linkedin https //www.linkedin.com/in/yannic-kilcher-488534136/ bilibili https //space.bilibili.com/1824646584 want support best thing share content want support financially completely optional voluntary lot people asked subscribestar https //www.subscribestar.com/yannickilcher patreon https //www.patreon.com/yannickilcher bitcoin btc bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq ethereum eth 0x7ad3513e3b8f66799f507aa7874b1b0ebc7f85e2 litecoin ltc lqw2trykyetvc8wjfkhpphtpbdm4vw7r9m monero xmr 4acl8agreo5hair8a9cevrw8peauwvnp1wnsdzxw7tzicdlhzagsgzhrqabdnfy8yum9fwjdvijphkrjv4fwt19cjzn9d4n,Machine Learning
114 – russ tedrake underactuated robotics control dynamics touch,russ tedrake roboticist professor mit vice president robotics research tri works control robots interesting complicated underactuated stochastic difficult model situations support podcast supporting sponsors click links get discount – magic spoon https //magicspoon.com/lex use code lex checkout – betterhelp https //betterhelp.com/lex – expressvpn https //www.expressvpn.com/lexpod would like get information podcast go https //lexfridman.com/ai connect lexfridman twitter linkedin facebook medium youtube watch video versions conversations enjoy podcast please,Machine Learning
talks 15 shubhadeep roychowdhury applying machine learning source code,recent times machine learning especially deep learning shown enormous potential learn latent patterns data practically breaking one benchmark another main ingredients continuing success massive amount open data gpu accelerated high-performance computing last least clever algorithmic tricks self attention graph neural networks one argue similar techniques methods applied diverse fields source code written computer softwares fact 2018 seminal paper `` survey machine learning big code naturalness '' https //arxiv.org/pdf/1709.06182.pdf miltiadis allamanis et al came call naturalness hypothesis states `` software form human communication software corpora similar statistical properties natural language corpora properties exploited build better software engineering tools '' post surge new research papers either explore already proven techniques discipline machine learning transformers nlp graph convolution networks etc devise novel techniques gated graph neural network nueral symbolic methods deep problog etc. new kind companies kite diffblue ponicode codist use advance methods produce new set tools helps developers write source code faster safer easier today 's talk look field discuss recent advancements new tools came thanks advancements finally write code explore one problem among many researchers trying solve discuss present best approaches drawbacks research directions -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- shubhadeep roychowdhury cto co-founder codist paris based start-up created docly http //thedocly.io/ automated source code summarization tools developers help maintain better code documentation src 17 years experience diverse areas game programming large scale data engineering cutting edge deep learning based systems co-authored `` data wrangling python '' 2019 massive 500+ pages treasure trove anything related data handling wrangling descriptive statistics active blogger medium father two kids src lives works paris france -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- please subscribe like video help keep motivated make awesome videos like one buy book approaching almost machine learning problem please visit https //bit.ly/buyaaml follow twitter https //twitter.com/abhi1thakur linkedin https //www.linkedin.com/in/abhi1thakur/ kaggle https //kaggle.com/abhishek instagram https //instagram.com/abhi4ml,Machine Learning
statistics storks babies numberphile,author economist tim harford talking statistics interview https //youtu.be/notn2fsduhq video links book stuff full description ↓↓↓ buy tim 's new book `` make world add '' signed copies extra cost maths gear http //bit.ly/world_add_up tim harford books amazon https //amzn.to/3lbt2vt tim 's website https //timharford.com also catch tim recent standupmaths video https //youtu.be/esc4hb-ajgi numberphile supported mathematical sciences research institute msri http //bit.ly/msrinumberphile also supported science sandbox simons foundation initiative dedicated engaging everyone process science https //www.simonsfoundation.org/outreach/science-sandbox/ support math america https //www.mathforamerica.org/ numberphile website http //www.numberphile.com/ numberphile facebook http //www.facebook.com/numberphile numberphile tweets https //twitter.com/numberphile subscribe http //bit.ly/numberphile_sub videos brady haran animation pete mcpartlan patreon http //www.patreon.com/numberphile numberphile t-shirts merch https //teespring.com/stores/numberphile brady 's videos subreddit http //www.reddit.com/r/bradyharan/ brady 's latest videos across channels http //www.bradyharanblog.com/ sign occasional emails http //eepurl.com/ydjl9,Machine Learning
medical test paradox redesigning bayes rule help,likelihood ratios also sometimes called bayes factors viewer-supported https //3b1b.co/bayes-factor-thanks home page https //www.3blue1brown.com book friend matt cook paradoxes mentioned end https //amzn.to/3abrezg topic ca n't help also mentioning another paradox book 'm rather fond bunch https //amzn.to/3mbdske another video bayes theorem https //youtu.be/hzgcovf3yvm mentioned on-screen note end terms `` bayes factor '' `` likelihood ratio '' refer ratio setting bayes rule used probability event two possible outcomes either disease n't take divergent meanings general contexts namely continuous parameter trying estimate two terms reflect two alternate approaches use comparing hypotheses fact people take phrase `` bayes factor '' _specifically_ refer use continuous context want details wikipedia actually really nice example discussing difference https //en.wikipedia.org/wiki/bayes_factor example post nice discussion distinction https //stats.stackexchange.com/questions/27345/likelihood-ratio-vs-bayes-factor -- -- -- -- -- -- -- -- -- animations largely made using manim scrappy open source python library https //github.com/3b1b/manim want check feel compelled warn 's well-documented tool many quirks might expect library someone wrote use mind music vincent rubinetti download music bandcamp https //vincerubinetti.bandcamp.com/album/the-music-of-3blue1brown stream music spotify https //open.spotify.com/album/1dvyjws8fbqxhrunag5w5u want contribute translated subtitles help review already made others need approval click gear icon video go subtitles/cc `` add subtitles/cc '' really appreciate helps make lessons accessible people -- -- -- -- -- -- -- -- -- 3blue1brown channel animating math senses word animate know drill youtube want stay posted new videos subscribe http //3b1b.co/subscribe various social media stuffs website https //www.3blue1brown.com twitter https //twitter.com/3blue1brown reddit https //www.reddit.com/r/3blue1brown instagram https //www.instagram.com/3blue1brown_animations/ patreon https //patreon.com/3blue1brown facebook https //www.facebook.com/3blue1brown,Machine Learning
definitely awesome kenneth0stanley 's tutorial icml kickoff made put work channel 's great honor talk,definitely awesome kenneth0stanley 's tutorial icml kickoff made put work channel 's great honor talk nitter.net/mlstreettalk/status/1351709386612363267,Machine Learning
bolzano–weierstrass theorem proof real analysis,get 25 year subscription curiositystream ends jan 3rd 2021 use code `` zachstar '' sign https //curiositystream.thld.co/zachstardec2 stemerch store https //stemerch.com/ support channel https //www.patreon.com/zachstar paypal one time donation https //www.paypal.me/zachstaryt join channel get access perks https //www.youtube.com/channel/ucpcsacbqs-sjevfk_hmfy9w/join ►follow instagram https //www.instagram.com/zachstar/ twitter https //twitter.com/imzachstar animations brainup studios http //brainup.in/ check spanish channel https //www.youtube.com/channel/ucnknu2xqblaspj6ckc8vtpa ►my setup space pictures https //amzn.to/2cc4kqj magnetic floating globe https //amzn.to/2vgpdn0 camera https //amzn.to/2rivyu5 mic https //amzn.to/35bkiri tripod https //amzn.to/2rgmtnl equilibrium tube https //amzn.to/2sowdrh ►check amazon store https //www.amazon.com/shop/zachstar,Machine Learning
photo earlier discarded version t-800 buggy lacked personality kept talking dostoevksy later repurposed human-like podcast host thanks ethos_pictures instagram drawing https //www.instagram.com/ethos_pictures/,photo earlier discarded version t-800 buggy lacked personality kept talking dostoevksy later repurposed human-like podcast host thanks ethos_pictures instagram drawing instagram.com/ethos_pictures…,Machine Learning
multinomial logistic regression python,"tweet share share multinomial logistic regression extension logistic regression adds native support multi-class classification problems logistic regression default limited two-class classification problems extensions like one-vs-rest allow logistic regression used multi-class classification problems although require classification problem first transformed multiple binary classification problems instead multinomial logistic regression algorithm extension logistic regression model involves changing loss function cross-entropy loss predict probability distribution multinomial probability distribution natively support multi-class classification problems tutorial discover develop multinomial logistic regression models python completing tutorial know multinomial logistic regression extension logistic regression multi-class classification develop evaluate multinomial logistic regression develop final model making predictions new data tune penalty hyperparameter multinomial logistic regression model let ’ get started multinomial logistic regression python photo nicolas rénac rights reserved tutorial overview tutorial divided three parts multinomial logistic regression evaluate multinomial logistic regression model tune penalty multinomial logistic regression multinomial logistic regression logistic regression classification algorithm intended datasets numerical input variables categorical target variable two values classes problems type referred binary classification problems logistic regression designed two-class problems modeling target using binomial probability distribution function class labels mapped 1 positive class outcome 0 negative class outcome fit model predicts probability example belongs class 1 default logistic regression used classification tasks two class labels so-called multi-class classification instead requires modification support multi-class classification problems one popular approach adapting logistic regression multi-class classification problems split multi-class classification problem multiple binary classification problems fit standard logistic regression model subproblem techniques type include one-vs-rest one-vs-one wrapper models alternate approach involves changing logistic regression model support prediction multiple class labels directly specifically predict probability input example belongs known class label probability distribution defines multi-class probabilities called multinomial probability distribution logistic regression model adapted learn predict multinomial probability distribution referred multinomial logistic regression similarly might refer default standard logistic regression binomial logistic regression binomial logistic regression standard logistic regression predicts binomial probability i.e two classes input example multinomial logistic regression modified version logistic regression predicts multinomial probability i.e two classes input example new binomial multinomial probability distributions may want read tutorial discrete probability distributions machine learning changing logistic regression binomial multinomial probability requires change loss function used train model e.g log loss cross-entropy loss change output single probability value one probability class label familiar multinomial logistic regression let ’ look might develop evaluate multinomial logistic regression models python evaluate multinomial logistic regression model section develop evaluate multinomial logistic regression model using scikit-learn python machine learning library first define synthetic multi-class classification dataset use basis investigation generic dataset easily replace loaded dataset later make_classification function used generate dataset given number rows columns classes case generate dataset 1,000 rows 10 input variables columns 3 classes example generates dataset summarizes shape arrays distribution examples across three classes test classification dataset collections import counter sklearn.datasets import make_classification define dataset x make_classification n_samples=1000 n_features=10 n_informative=5 n_redundant=5 n_classes=3 random_state=1 summarize dataset print x.shape y.shape print counter running example confirms dataset 1,000 rows 10 columns expected rows distributed approximately evenly across three classes 334 examples class 1000 10 1000 counter 1 334 2 334 0 332 logistic regression supported scikit-learn library via logisticregression class logisticregression class configured multinomial logistic regression setting “ multi_class ” argument “ multinomial ” “ solver ” argument solver supports multinomial logistic regression “ lbfgs “ ... define multinomial logistic regression model model logisticregression multi_class='multinomial solver='lbfgs multinomial logistic regression model fit using cross-entropy loss predict integer value integer encoded class label familiar multinomial logistic regression api look might evaluate multinomial logistic regression model synthetic multi-class classification dataset good practice evaluate classification models using repeated stratified k-fold cross-validation stratification ensures cross-validation fold approximately distribution examples class whole training dataset use three repeats 10 folds good default evaluate model performance using classification accuracy given classes balanced complete example evaluating multinomial logistic regression multi-class classification listed evaluate multinomial logistic regression model numpy import mean numpy import std sklearn.datasets import make_classification sklearn.model_selection import cross_val_score sklearn.model_selection import repeatedstratifiedkfold sklearn.linear_model import logisticregression define dataset x make_classification n_samples=1000 n_features=10 n_informative=5 n_redundant=5 n_classes=3 random_state=1 define multinomial logistic regression model model logisticregression multi_class='multinomial solver='lbfgs define model evaluation procedure cv repeatedstratifiedkfold n_splits=10 n_repeats=3 random_state=1 evaluate model collect scores n_scores cross_val_score model x scoring='accuracy cv=cv n_jobs=-1 report model performance print 'mean accuracy .3f .3f mean n_scores std n_scores running example reports mean classification accuracy across folds repeats evaluation procedure note results may vary given stochastic nature algorithm evaluation procedure differences numerical precision consider running example times compare average outcome case see multinomial logistic regression model default penalty achieved mean classification accuracy 68.1 percent synthetic classification dataset mean accuracy 0.681 0.042 may decide use multinomial logistic regression model final model make predictions new data achieved first fitting model available data calling predict function make prediction new data example demonstrates make prediction new data using multinomial logistic regression model make prediction multinomial logistic regression model sklearn.datasets import make_classification sklearn.linear_model import logisticregression define dataset x make_classification n_samples=1000 n_features=10 n_informative=5 n_redundant=5 n_classes=3 random_state=1 define multinomial logistic regression model model logisticregression multi_class='multinomial solver='lbfgs fit model whole dataset model.fit x define single row input data row 1.89149379 -0.39847585 1.63856893 0.01647165 1.51892395 -3.52651223 1.80998823 0.58810926 -0.02542177 -0.52835426 predict class label yhat model.predict row summarize predicted class print 'predicted class yhat 0 running example first fits model available data defines row data provided model order make prediction case see model predicted class “ 1 ” single row data predicted class 1 benefit multinomial logistic regression predict calibrated probabilities across known class labels dataset achieved calling predict_proba function model example demonstrates predict multinomial probability distribution new example using multinomial logistic regression model predict probabilities multinomial logistic regression model sklearn.datasets import make_classification sklearn.linear_model import logisticregression define dataset x make_classification n_samples=1000 n_features=10 n_informative=5 n_redundant=5 n_classes=3 random_state=1 define multinomial logistic regression model model logisticregression multi_class='multinomial solver='lbfgs fit model whole dataset model.fit x define single row input data row 1.89149379 -0.39847585 1.63856893 0.01647165 1.51892395 -3.52651223 1.80998823 0.58810926 -0.02542177 -0.52835426 predict multinomial probability distribution yhat model.predict_proba row summarize predicted probabilities print 'predicted probabilities yhat 0 running example first fits model available data defines row data provided model order predict class probabilities note results may vary given stochastic nature algorithm evaluation procedure differences numerical precision consider running example times compare average outcome case see class 1 e.g array index mapped class integer value largest predicted probability 0.50 predicted probabilities 0.16470456 0.50297138 0.33232406 familiar evaluating using multinomial logistic regression models let ’ explore might tune model hyperparameters tune penalty multinomial logistic regression important hyperparameter tune multinomial logistic regression penalty term term imposes pressure model seek smaller model weights achieved adding weighted sum model coefficients loss function encouraging model reduce size weights along error fitting model popular type penalty l2 penalty adds weighted sum squared coefficients loss function weighting coefficients used reduces strength penalty full penalty slight penalty default logisticregression class uses l2 penalty weighting coefficients set 1.0 type penalty set via “ penalty ” argument values “ l1 “ “ l2 “ “ elasticnet ” e.g although solvers support penalty types weighting coefficients penalty set via “ c ” argument ... define multinomial logistic regression model default penalty logisticregression multi_class='multinomial solver='lbfgs penalty='l2 c=1.0 weighting penalty actually inverse weighting perhaps penalty 1 – c. documentation c float default=1.0 inverse regularization strength must positive float like support vector machines smaller values specify stronger regularization means values close 1.0 indicate little penalty values close zero indicate strong penalty c value 1.0 may indicate penalty c close 1.0 light penalty c close 0.0 strong penalty penalty disabled setting “ penalty ” argument string “ none “ ... define multinomial logistic regression model without penalty logisticregression multi_class='multinomial solver='lbfgs penalty='none familiar penalty let ’ look might explore effect different penalty values performance multinomial logistic regression model common test penalty values log scale order quickly discover scale penalty works well model found tuning scale may beneficial explore l2 penalty weighting values range 0.0001 1.0 log scale addition penalty 0.0 complete example evaluating l2 penalty values multinomial logistic regression listed tune regularization multinomial logistic regression numpy import mean numpy import std sklearn.datasets import make_classification sklearn.model_selection import cross_val_score sklearn.model_selection import repeatedstratifiedkfold sklearn.linear_model import logisticregression matplotlib import pyplot get dataset def get_dataset x make_classification n_samples=1000 n_features=20 n_informative=15 n_redundant=5 random_state=1 n_classes=3 return x get list models evaluate def get_models models dict p 0.0 0.0001 0.001 0.01 0.1 1.0 create name model key .4f p turn penalty cases p == 0.0 penalty case models key logisticregression multi_class='multinomial solver='lbfgs penalty='none else models key logisticregression multi_class='multinomial solver='lbfgs penalty='l2 c=p return models evaluate give model using cross-validation def evaluate_model model x define evaluation procedure cv repeatedstratifiedkfold n_splits=10 n_repeats=3 random_state=1 evaluate model scores cross_val_score model x scoring='accuracy cv=cv n_jobs=-1 return scores define dataset x get_dataset get models evaluate models get_models evaluate models store results results names list list name model models.items evaluate model collect scores scores evaluate_model model x store results results.append scores names.append name summarize progress along way print '/prestrong/stronga href= '' https //machinelearningmastery.com/different-results-each-time-in-machine-learning/ '' /apre class= '' urvanov-syntax-highlighter-plain-tag ''",Machine Learning
range prediction general approaches,'m looking general thoughts would go building predictions output type range set ranges inputs best guesses number models somewhat known accuracies individual model tends good almost always least one model tends get right mostly right ranges tend continuous possible cases may discontinuous e.g usually truth data single range may many 2-3 probably example let 's say want predict time action valid range 0 3600 e.g minutes day model 1 known accuracy 60 say valid range 0 1200 model 2 known accuracy 70 say valid range 0 1100 also 1300 1500 maybe truth data valid range 0 1200 1300 1500 would combination models naively could approach discretizing ranges individual predictions e.g could predict 0 15 30 ... would probably featurize model 's binary prediction validity moment well scores including attributes however feel like might issues could result discontinuity e.g model predicts series y/n/y/n/y/n range continuous could always attempt smooth discontinuity post-processing seems coarse best feel like abstractly sort 1d edge/object detection literature see seems apply images also issue limited truth data tens thousands samples maybe lot model data maybe best approach would heuristic-driven statistical model instead 'd love thoughts might approach 're aware literature dealing link comments,Machine Learning
npc character chaotic good understand accept pre-programmed storyline ethical alignment amor fati,npc character chaotic good understand accept pre-programmed storyline ethical alignment amor fati,Machine Learning
david chalmers hard problem consciousness,david chalmers philosopher cognitive scientist specializing philosophy mind philosophy language consciousness perhaps best known formulating hard problem consciousness could stated “ feeling accompanies awareness sensory information exist ” conversation part artificial intelligence podcast would like get information podcast go https //lexfridman.com/ai connect lexfridman twitter linkedin facebook medium youtube watch video versions conversations enjoy podcast please rate 5,Machine Learning
pinned social media press currently incentivized drastically exaggerate narratives division turn creates division downward spiral continues hope build tech changes incentives believe much love hate world,social media press currently incentivized drastically exaggerate narratives division turn creates division downward spiral continues hope build tech changes incentives believe much love hate world,Machine Learning
capsule networks education targets,today 's episode dr. keith duggar alex stenlake dr. tim scarfe chat education chapter kenneth stanley 's `` greatness planned '' book relate algoshambes conversation weeks ago debate whether objectives education good thing whether cause perverse incentives stifle creativity innovation next dissect capsule networks top finish talking fast algorithms quantum computing 00:00:00 introduction 00:01:13 greatness planned education 00:12:03 perverse incentives 00:19:25 treasure hunting 00:30:28 capsule networks 00:46:08 capsules compositional networks 00:52:45 capsule routing 00:57:10 loss warps 01:09:55 fast algorithms quantum computing,Machine Learning
rt ykilcher bit late party 💃new video🕺 switch transformers googleai hard routing selective dropout mixed precision achieve 🔥one trillion parameters🔥 language model watch learn 's done🧙💪 https //youtu.be/iar8lkkmmim liamfedus barret_zoph,bit late party 💃new video🕺 switch transformers googleai hard routing selective dropout mixed precision achieve 🔥one trillion parameters🔥 language model watch learn 's done🧙💪 invidious.snopyta.org/iar8lkkmmim liamfedus barret_zoph,Machine Learning
guido van rossum python,guido van rossum creator python one popular impactful programming languages world video version available youtube would like get information podcast go https //lexfridman.com/ai connect lexfridman twitter linkedin facebook youtube watch video versions conversations,Machine Learning
hope 2021 let 's make year amazing let engineers scientists small businesses best solve problems,hope 2021 let 's make year amazing let engineers scientists small businesses best solve problems,Machine Learning
81 – anca dragan human-robot interaction reward engineering,anca dragan professor berkeley working human-robot interaction — algorithms look beyond robot ’ function isolation generate robot behavior accounts interaction coordination human beings support podcast supporting sponsors using special code – download cash app app store google play use code “ lexpodcast ” episode links anca ’ twitter https //twitter.com/ancadianadragan anca ’ website https //people.eecs.berkeley.edu/~anca/ conversation part artificial intelligence podcast would like get information podcast go https //lexfridman.com/ai connect lexfridman twitter linkedin facebook medium,Machine Learning
best data science overviews,read full story,Machine Learning
84 – william macaskill effective altruism,william macaskill philosopher ethicist one originators effective altruism movement research focuses fundamentals effective altruism – use evidence reason help others much possible time money particular concentration act given moral uncertainty author good better – effective altruism radical new way make difference co-founder president centre effective altruism cea encourages people commit donate least 10,Machine Learning
impossible chessboard puzzle,information puzzle interesting twist solution stand-up maths https //youtu.be/as7gkm7y7h4 paid viewers like https //3b1b.co/chess-thanks home page https //www.3blue1brown.com -- -- -- -- -- -- -- -- -- 0:00 introduction 3:58 visualizing two-square case 5:46 visualizing three-square case 12:19 proof 's impossible 16:22 explicit painting hypercube -- -- -- -- -- -- -- -- -- thanks everyone endured probing puzzle provided helpful discussion especially cam christensen matt parker mike sklar mike way deserves credit coming particularly clean way see 's impossible n power 2 animations largely made using manim scrappy open-source python library https //github.com/3b1b/manim want check feel compelled warn 's well-documented tool many quirks might expect library someone wrote use mind music vincent rubinetti download music bandcamp https //vincerubinetti.bandcamp.com/album/the-music-of-3blue1brown stream music spotify https //open.spotify.com/album/1dvyjws8fbqxhrunag5w5u want contribute translated subtitles help review already made others need approval click gear icon video go subtitles/cc `` add subtitles/cc '' really appreciate helps make lessons accessible people -- -- -- -- -- -- -- -- -- 3blue1brown channel animating math senses word animate know drill youtube want stay posted new videos subscribe http //3b1b.co/subscribe various social media stuffs website https //www.3blue1brown.com twitter https //twitter.com/3blue1brown reddit https //www.reddit.com/r/3blue1brown instagram https //www.instagram.com/3blue1brown_animations/ patreon https //patreon.com/3blue1brown facebook https //www.facebook.com/3blue1brown,Machine Learning
podcast machine learning meets privacy,first episode podcast series machine learning data privacy read full story,Machine Learning
fireside chat susan dumais microsoft phd summit 2020,day 1 december 1 2020 fireside chat susan dumais technical fellow managing director microsoft research new england new york city montreal microsoft 's third phd summit two-day virtual workshop opportunity top phd students enhance skills build network discuss research within community peers notable microsoft researchers information workshop found https //www.microsoft.com/en-us/research/event/phd-summit-2020/,Machine Learning
mila 2021 msc/phd program supervision request,anyone heard back mila 2021 msc/phd program supervisor decisions/interviews supposed announced january-end february-end creating thread discussion link comments,Machine Learning
112 – ian hutchinson nuclear fusion plasma physics religion,ian hutchinson nuclear engineer plasma physicist mit made number important contributions plasma physics including magnetic confinement plasmas seeking enable fusion reactions energy source stars used practical energy production current nuclear reactors based fission discuss ian also written philosophy science relationship science religion support podcast supporting sponsors – sun basket use code lex https //sunbasket.com/lex – powerdot use code lex https //powerdot.com/lex would like get,Machine Learning
ai financial industry 8 key takeaways bill.com h2o.ai fireside chat,"current global pandemic crisis presents various challenges businesses industries including financial services institutions monitoring dealing effects covid-19 across world time pandemic important teams get together share insights experience goal inspiring helping contain mitigate negative impact mind h2o.ai hosted virtual fireside chat bill.com business leaders companies discussed latest ai key factors building recruiting ai-first engineering team impact covid-19 smbs lessons learned pandemic finance industry check key takeaways sri ambati ’ conversation vinay pai svp engineering bill.com manasa murthy vp engineering bill.com prakruthi narasimha senior accountant h2o.ai thrive challenging times role ai shaping future best 1 state smb sri ambati kicked fireside chat offering deeper look state small medium business smb data gathered salesforce research report analyzed responses 2,300 small medium business smb owners leaders around world determine impact covid-19 pandemic role digital transformation terms enhancing business resiliency smb leaders planning recovery growth post-pandemic according report world smbs radically changed early 2020 due covid-19 pandemic significant reduction revenue smb leaders say struggling keep businesses afloat businesses consumed-focused micro-businesses affected also besides challenge getting access capital cash-flow smbs seeing reduced customer demand supply chain disruptions challenges public health mandates 2 bringing teams together times covid-19 vinay pai one thing feel really lucky tech kind work remotely took whole company remote beginning march started four principles stay healthy safe stay connected teammates keep bill.com running customers focus outcomes actually week-long innovation week hackathon ’ making work ’ growing business even since march probably hired 30 engineers manasa murthy think lot challenges us feel going remote uncertainties 2020 presented us ’ lot embracing change need look wholeheartedly bring whole selves work kids running around conference calls tend probably person things day think ’ really listening ’ working ’ 3 building gender-balanced team vinay pai ceo rene lacerte believes diversity inclusion ’ reflected culture teams reflect composition customers end making better products creating diverse inclusive culture something solve right away ’ long game 4 ai journey bill.com vinay pai goes back three years ago joined second generation ai bill.com engineering team already invested building models know get lot invoices customers upload challenge extract data invoices bring system minimize data entry also risk fraud started using term “ democratizing data science ” ’ like take data make accessible everyone company whether ’ product management marketing sales analytics ’ much learn data kind two three-step process one started building ml models around invoices risk fraud ran h2o.ai money2020 looked guys one competitors bake-off internally guys decided get access early adopters product management data-driven get trained along engineers driverless ai let ’ expand time wanted build ai capabilities part hiring manasa lead team next day hiring several data scientists ’ rich collaboration toolset brought h2o bunch folks company ’ using day-to-day lives powerful manasa murthy think product like ’ much opportunity apply ai use cases across board feel like bringing h2o really useful accelerating business use cases model building went months days fast development ’ also able expand many new use cases early team smaller incredibly useful tool like expand build expertise sri ambati core vision democratize ai course pride trying make customers ai companies 5 ai bill.com helping customers end-users day-to-day responsibilities prakruthi narasimha ’ using bill.com two three years much regular basis whole ai functionality game-changer end-user perspective fact information coded bill uploaded could pdf word excel file could something took picture phone would still capture 90 important information need terms data entry ’ little data entry ’ verifying information gives lot confidence accuracy bills entered also lot things bill.com offers set apart lot ap companies keeping whole end-to-end ap cloud giving us ability job remotely without need physical space without compromising things like segregation duties good approval integrated workflow 6 recommendations business value ai vinay pai think ai one things gets lot bad press ’ lot use cases ’ misused used inappropriately things get news look ai problems ’ solving feel like pandemic recession fact help businesses manage business remotely continue pay vendors continue get paid customers ’ great mission help manage business actually grow business ’ coming place solving real customer problems manasa murthy seen success start customer problem becomes real issue see benefit clearly otherwise ai buzzword ’ tech play “ care ” ai especially high initial cost really need start thinking actually balance startup boosted cost versus benefit ’ going get overtime recommendation would focus value focus gathering data foundation first 7 building ai-first engineering team vinay pai think biggest challenge leader building team ’ going first deliver today need deliver today well build future help grow scale ’ always trying bring people smarter want average upon iq team bring skill sets time want bring along team ’ scars battle stories know customer base ’ fun engineering problem ’ really factors recruit build 8 future ai sri ambati next years innovation thanks ai part discovery ’ see dramatic acceleration innovation time experiment cost experiment dramatically reduced say democratize ai ’ really meaning faster cheaper easier ways experiments making experience inexpensive ’ almost natural fail learn failures quickly reinvent general think human redefined ai making things much intelligent things become intelligent focus human general think probably see lot empathy gratitude click watch recording panel post ai financial industry 8 key takeaways bill.com h2o.ai fireside chat appeared first open source leader ai ml",Machine Learning
experience training cloud tracking experiments feedback wanted,'ve worked various machine learning experiments academic industry roles consistently bugged hard train models gpu boxes set cloud compute account ssh box move files around git figure log track results training run ... yet find good options market looked anyscale determined databricks etc wrote software wanted decided turn venture want open thread discussion issues machine learning versioning tracking training general maybe see could valid solution https //latch.ai/ would love get substantive conversations going link comments,Machine Learning
148 – charles isbell michael littman machine learning education,charles isbell dean college computing georgia tech michael littman computer scientist brown university please support podcast checking sponsors – athletic greens https //athleticgreens.com/lex use code lex get 1 month fish oil – eight sleep https //www.eightsleep.com/lex use code lex get special savings – masterclass https //masterclass.com/lex get 2 price 1 – cash app https //cash.app/ use code lexpodcast get 10 episode links charles ’ twitter https //twitter.com/isbellhfh charles ’ website https //www.cc.gatech.edu/~isbell/ michael ’ twitter https //twitter.com/mlittmancs michael ’ website https //www.littmania.com/ michael ’ youtube https //www.youtube.com/user/mlittman podcast info podcast website https //lexfridman.com/podcast apple podcasts https //apple.co/2lwqzir spotify https //spoti.fi/2newcf8 rss https //lexfridman.com/feed/podcast/ youtube full episodes https //youtube.com/lexfridman youtube clips https //youtube.com/lexclips support connect – check sponsors ’ best way support podcast – support patreon https //www.patreon.com/lexfridman – twitter https //twitter.com/lexfridman – instagram https //www.instagram.com/lexfridman – linkedin https //www.linkedin.com/in/lexfridman – facebook https //www.facebook.com/lexfridmanpage – medium https //medium.com/ lexfridman outline ’ timestamps episode podcast players able click timestamp jump time 00:00 – introduction 07:51 – machine learning statistics 12:14 – neurips vs icml 14:30 – data important algorithm 20:14 – role hardship education 28:57 – charles michael met 33:30 – key success never satisfied 36:47 – bell labs 48:15 – teaching machine learning 58:25 – westworld ex machina 1:06:24 – simulation 1:13:14 – college experience times covid 1:41:52 – advice young people 1:48:44 – learn program 2:00:07 – friendship,Machine Learning
155 – max tegmark ai physics,max tegmark physicist ai researcher mit please support podcast checking sponsors – jordan harbinger show https //www.jordanharbinger.com/lex/ – four sigmatic https //foursigmatic.com/lex use code lexpod get 60 – betterhelp https //betterhelp.com/lex get 10 – expressvpn https //expressvpn.com/lexpod use code lexpod get 3 months free episode links news project explainer video https //www.youtube.com/watch v=prlf17pb6vo news project website https //www.improvethenews.org/ max ’ twitter https //twitter.com/tegmark max ’ website https //space.mit.edu/home/tegmark/ future life institute https //futureoflife.org/ lex fridman podcast 1 https //www.youtube.com/watch v=gi8lunhp5yu podcast info podcast website https //lexfridman.com/podcast apple podcasts https //apple.co/2lwqzir spotify https //spoti.fi/2newcf8 rss https //lexfridman.com/feed/podcast/ youtube full episodes https //youtube.com/lexfridman youtube clips https //youtube.com/lexclips support connect – check sponsors ’ best way support podcast – support patreon https //www.patreon.com/lexfridman – twitter https //twitter.com/lexfridman – instagram https //www.instagram.com/lexfridman – linkedin https //www.linkedin.com/in/lexfridman – facebook https //www.facebook.com/lexfridmanpage – medium https //medium.com/ lexfridman outline ’ timestamps episode podcast players able click timestamp jump time 00:00 – introduction 08:15 – ai physics 21:32 – ai discover new laws physics 30:22 – ai safety 47:59 – extinction human species 58:57 – fix fake news misinformation 1:20:30 – autonomous weapons 1:35:54 – man prevented nuclear war 1:46:02 – elon musk ai 1:59:39 – ai alignment 2:05:42 – consciousness 2:14:45 – richard feynman 2:18:56 – machine learning computational physics 2:29:53 – ai creativity 2:41:08 – aliens 2:56:51 – mortality,Machine Learning
use google api save cat ’ photo cloud,read full story,Machine Learning
anyone knows ml algorithms behind ads accurate,anyone working big tech companies google fb amazon etc knows algorithms behind ads surfing accurate 'm curious link comments,Machine Learning
going phase obsessively trying evaluating flavors philz today ’ “ silken splendor ” allegedly claimed “ dark cocoa citrus butterscotch ” wonder determine,going phase obsessively trying evaluating flavors philz today ’ “ silken splendor ” allegedly claimed “ dark cocoa citrus butterscotch ” wonder determine,Machine Learning
know happens data,read full story,Machine Learning
support vector machines python start finish,note support statquest purchasing jupyter notebook python code seen video https //statquest.org/product/jupyter-notebook-support-vector-machines-in-python/ ⭐ note code use kite free ai-powered coding assistant help code faster smarter kite plugin integrates top editors ides give smart completions documentation ’ typing love https //www.kite.com/get-kite/ utm_medium=referral utm_source=youtube utm_campaign=statquest utm_content=description-only webinar recorded 20200609 11:00am new york time note statquest assumes already familiar support vector machines https //youtu.be/efr1c6cvhme radial basis function https //youtu.be/qc5iylw_hns regularization https //youtu.be/q81rr3ykn30 cross validation https //youtu.be/fsytzgwwbvw confusion matrices https //youtu.be/kdsp6soqa7o complete index statquest videos check https //statquest.org/video-index/ 'd like support statquest please consider ... patreon https //www.patreon.com/statquest ... ... youtube membership https //www.youtube.com/channel/uctyluttgs3k1fg4y5tahlbw/join ... cool statquest t-shirt sweatshirt usa/europe https //teespring.com/stores/statquest everywhere https //www.redbubble.com/people/starmer/works/40421224-statquest-double-bam asc=u p=t-shirt ... buying one two songs go large get whole album https //joshuastarmer.bandcamp.com/ ... donating statquest https //www.paypal.me/statquest lastly want keep research create new statquests follow twitter https //twitter.com/joshuastarmer 0:00 awesome song introduction 4:16 import modules 6:36 import data 11:27 missing data part 1 identifying 16:57 missing data part 2 dealing 21:04 downsampling data 24:35 format data part 1 x 26:35 format data part 2 one-hot encoding 31:25 format data part 3 centering scaling 32:45 build preliminary svm 34:55 optimize parameters cross validation gridsearchcv 37:58 build draw final svm statquest ml svm,Machine Learning
maybe 's travel starved really getting enjoying growing genre 4k walking videos around world e.g https //www.openculture.com/2020/03/explore-the-entire-world-from-the-comfort-of-quarantine-with-4k-walking-tours.html examples interesting leave running tv background unscripted samples human condition,maybe 's travel starved really getting enjoying growing genre 4k walking videos around world e.g openculture.com/2020/03/expl… examples interesting leave running tv background unscripted samples human condition,Machine Learning
103 – ben goertzel artificial general intelligence,ben goertzel one interesting minds artificial intelligence community founder singularitynet designer opencog ai framework formerly director machine intelligence research institute chief scientist hanson robotics company created sophia robot central figure agi community many years including conference artificial general intelligence support podcast supporting sponsors – jordan harbinger show https //jordanharbinger.com/lex/ – masterclass https //masterclass.com/lex conversation part artificial intelligence podcast would like get information,Machine Learning
list novel ml hardware companies january 2021,'ve heard mentioned several times paper lot companies working next gen ml hardware big companies like intel ibm working neurochips 've heard cerberas graphcore lot trying assemble list intention applying companies afterwards recently 've finished new version spiral programming language want show would good could make list thread think would interest know kind hardware coming pipe link comments,Machine Learning
judea pearl causal reasoning counterfactuals bayesian networks path agi,judea pearl professor ucla winner turing award ’ generally recognized nobel prize computing one seminal figures field artificial intelligence computer science statistics developed championed probabilistic approaches ai including bayesian networks profound ideas causality general ideas important ai understanding practice science field ai idea causality cause effect many lies core currently missing,Machine Learning
neural networks part 1 inside black box,neural networks one popular machine learning algorithms also one poorly understood everyone says neural networks `` black boxes '' 's true video break piece show works step-by-step using simple mathematics still true algorithm end video deep understanding neural networks ⭐ note code use kite free ai-powered coding assistant help code faster smarter kite plugin integrates top editors ides give smart completions documentation ’ typing love https //www.kite.com/get-kite/ utm_medium=referral utm_source=youtube utm_campaign=statquest utm_content=description-only complete index statquest videos check https //statquest.org/video-index/ 'd like support statquest please consider ... patreon https //www.patreon.com/statquest ... ... youtube membership https //www.youtube.com/channel/uctyluttgs3k1fg4y5tahlbw/join ... cool statquest t-shirt sweatshirt usa/europe https //teespring.com/stores/statquest everywhere https //www.redbubble.com/people/starmer/works/40421224-statquest-double-bam asc=u p=t-shirt ... buying one two songs go large get whole album https //joshuastarmer.bandcamp.com/ ... donating statquest https //www.paypal.me/statquest lastly want keep research create new statquests follow twitter https //twitter.com/joshuastarmer 0:00 awesome song introduction 2:01 simple dataset problem 3:37 description neural networks 7:54 creating squiggle curved lines 15:25 using neural network make prediction 16:38 neural network terminology statquest neuralnetworks,Machine Learning
102 – steven pressfield war art,steven pressfield historian author war art book big impact life life millions whose passion create art science business sport everywhere else highly recommend others books topic including turning pro work nobody wants read shit warrior ethos also books gates fire spartans battle thermopylae lion ’ gate tides war others best historical fiction novels ever written,Machine Learning
forgotten number system numberphile,featuring author alex bellos check books including language lover 's puzzle book amazon https //amzn.to/3ou0wjt links stuff full description ↓↓↓ alex numberphile http //bit.ly/bellos_playlist alex 's website links stuff http //www.alexbellos.com numberphile supported mathematical sciences research institute msri http //bit.ly/msrinumberphile also supported science sandbox simons foundation initiative dedicated engaging everyone process science https //www.simonsfoundation.org/outreach/science-sandbox/ support math america https //www.mathforamerica.org/ numberphile website http //www.numberphile.com/ numberphile facebook http //www.facebook.com/numberphile numberphile tweets https //twitter.com/numberphile subscribe http //bit.ly/numberphile_sub videos brady haran patreon http //www.patreon.com/numberphile numberphile t-shirts merch https //teespring.com/stores/numberphile brady 's videos subreddit http //www.reddit.com/r/bradyharan/ brady 's latest videos across channels http //www.bradyharanblog.com/ sign occasional emails http //eepurl.com/ydjl9 thanks patreon supporters including arjun chakroborty ben delo jeff straathof ken baron yana chernobilsky andy b james bissonette jubal john jeremy buchanan steve crutchfield adam savage ben white andrei burke rad donato matthew schuster nat tyce ron hochsprung mitch harding ubiquity ventures mateusz swiatkowski john zelinka gnare tom marshall jesús salsero jordan w oja tracy parry ian george walker arnas bernd sing valentin alfred wallace charles southerland kristian joensen bodhisattva debnath alex khein kermit norlund asymptote mirik gogri,Machine Learning
god bless ml reddit 😂,god bless ml reddit 😂,Machine Learning
033 prof. karl friston free energy principle,week dr. tim scarfe dr. keith duggar connor leahy chat prof. karl friston professor friston british neuroscientist university college london authority brain imaging 2016 ranked influential neuroscientist semantic scholar main contribution theoretical neurobiology variational free energy principle also known active inference bayesian brain fep formal statement existential imperative system survives changing world cast inference problem bayesian brain hypothesis states brain confronted ambiguous sensory evidence interprets making inferences hidden states caused sensory data brain inference engine key concept separating friston 's idea traditional stochastic reinforcement learning methods even bayesian reinforcement learning moving away goal-directed optimisation remember subscribe enjoy show 00:00:00 show teaser intro 00:16:24 main formalism fep 00:28:29 path integral 00:30:52 feel talking friston 00:34:06 skit cultures checked maybe make shorter 00:36:02 friston joins 00:36:33 main show introduction 00:40:51 prediction takes intelligence 00:48:21 balancing accuracy flexibility 00:57:36 belief-free vs belief-based beliefs crucial 01:04:53 fuzzy markov blankets wandering sets 01:12:37 free energy principle conforms 01:14:50 useful false beliefs 01:19:14 complexity minimization heart free energy 01:19:14 keith 01:23:25 alpha tip scales absoute absolutely yes 01:28:47 fep applied brain anatomy 01:36:28 multiple non-fep forms brain 01:43:11 positive conneciton backpropagation 01:47:12 fep explain origin fep systems 01:49:32 post-show banter https //www.fil.ion.ucl.ac.uk/~karl/ machinelearning,Machine Learning
using data social graphs clinical trials,building social graph — knowledge graph — improve clinical trials processes reduce costs providing better clarity access heterogeneous datasets read full story,Machine Learning
pinned 🔥deep learning meme review episode 2 here🔥 antonio go state art memes 2020 💪 check https //youtu.be/7dglelsvygo,🔥deep learning meme review episode 2 here🔥 antonio go state art memes 2020 💪 check invidious.snopyta.org/7dglelsvygo,Machine Learning
engineering metrics dashboards part 1,read full story,Machine Learning
2020 world university ranking ai safety,top 10 ranking produced dr. roman v. yampolskiy university louisville based solely biased opinion reduce bias university louisville ranked certain degree ranking also based perceived reputation google scholar listings ai safety quality quantity papers google search rankings impact publications number scholars working area full time many universities work ai safety ranked year definition list excludes industry labs read full story,Machine Learning
scanta named finalist datatribe 2020 cybersecurity startup challenge,world ’ preeminent cyber startup foundry datatribe selects scanta one three companies worldwide compete chance 2m seed capital read full story,Machine Learning
feels like amazon removing parler aws created dangerous world less dangerous one may wrong 'm listening learning thinking hope,feels like amazon removing parler aws created dangerous world less dangerous one may wrong 'm listening learning thinking hope,Machine Learning
towards understanding ensemble knowledge distillation self-distillation deep learning microsoft research,link comments,Machine Learning
91 – jack dorsey square cryptocurrency artificial intelligence,jack dorsey co-founder ceo twitter founder ceo square support podcast signing sponsors – masterclass https //masterclass.com/lex episode links jack ’ twitter https //twitter.com/jack start small tracker https //bit.ly/2kxdibl conversation part artificial intelligence podcast would like get information podcast go https //lexfridman.com/ai connect lexfridman twitter linkedin facebook medium youtube watch video versions conversations enjoy podcast please rate 5 stars apple podcasts follow spotify support,Machine Learning
careers microsoft research panel discussion microsoft phd summit 2020,day 2 december 2 2020 panel four researchers microsoft discuss experience working microsoft phds career journeys since microsoft 's third phd summit two-day virtual workshop opportunity top phd students enhance skills build network discuss research within community peers notable microsoft researchers speakers denae ford robinson senior researcher microsoft discussion lead bita darvish rouhani senior researcher manager microsoft lukas maas senior research sde microsoft martez mott senior researcher microsoft information workshop found https //www.microsoft.com/en-us/research/event/phd-summit-2020/,Machine Learning
r counterfactual latent dance,recently really enjoyed gan music videos i.e. latent walks synced music track spent bit time creating one using counterfactual generative network cgn https //arxiv.org/abs/2101.06046 showcasing cgn 's shape/texture/background disentanglement video https //www.youtube.com/watch v=jdwaluer35u code cgn available soon https //github.com/autonomousvision/counterfactual_generative_networks link comments,Machine Learning
p made nlprule library fast grammatical error correction,hi nlprule fast grammatical error correction english german checking thousands rules written rust bindings python repository https //github.com/bminixhofer/nlprule synopsis nlprule import tokenizer rules spliton tokenizer tokenizer.load `` en '' rules rules.load `` en '' tokenizer spliton `` `` `` `` `` '' rules.correct `` wants send email '' returns 'he wants send email rules.correct `` due homework '' returns homework rules.correct `` enough intensive purposes '' returns 'it enough intents purposes suggestions rules.suggest `` since monday '' suggestions print s.start s.end s.replacements s.source s.message prints 4 16 'was 'has was_been.1 mean background 've interested grammatical error correction came across languagetool based thousands rules error correction xml file think rule syntax restricted form regex atoms words annotated lemmas part-of-speech tags chunks 'm big fan java wanted improve rust interested rules parsed made proof concept reverse-engineering languagetool logic rust lying around quite time decided finish make usable library holidays relation sophisticated gec approaches 's lots research using neural networks grammatical error correction exciting recent approaches capture many errors rule-based approach could still two reasons use rules speed machine 8th gen intel cpu takes less 1ms correct sentence dealing extreme data sparsity errors example `` enough intensive purposes '' contains well known error yet would surprised current ml model corrects error unless specifically accounted since almost never appeared training data even true similarly rare errors languages less data available english believe rules especially useful conjunction powerful ml model think nlprule kind `` sanity-check '' text rule-based postprocessing nlg two areas nlprule might interesting preprocessing nlp postprocessing nlg 've tried latter texts generated gpt2 applying nlprule yields significant amount suggestions generated 192300 tokens misspelling 35 suggestions 0.18 per 1000 tokens style 53 suggestions 0.28 per 1000 tokens typographical 112 suggestions 0.58 per 1000 tokens grammar 29 suggestions 0.15 per 1000 tokens none 3 suggestions 0.02 per 1000 tokens inconsistency 2 suggestions 0.01 per 1000 tokens errors suggestions improvement information although strictly speaking project focus machine learning thought people might interested 'm happy discuss anything comments edit sorry attribute get replacements example .replacements .text fixed link comments,Machine Learning
lex solo 3 – memory grandmother,attempt find words honor grandmother amazing woman responsible much taught man taught strength wisdom compassion love would like get information podcast go https //lexfridman.com/podcast connect lexfridman twitter linkedin facebook medium youtube watch video versions conversations enjoy podcast please rate 5 stars apple podcasts follow spotify support patreon ’ outline,Machine Learning
quasiperfect numbers eric lander numberphile,eric lander discusses quasiperfect numbers gave start ... links stuff full description ↓↓↓ interview filmed 2015 remained unedited unpublished ... main interview day `` basic research '' found https //youtu.be/6gnsqjpcc78 westinghouse science talent search since become regeneron science talent search https //en.wikipedia.org/wiki/regeneron_science_talent_search numberphile supported mathematical sciences research institute msri http //bit.ly/msrinumberphile also supported science sandbox simons foundation initiative dedicated engaging everyone process science https //www.simonsfoundation.org/outreach/science-sandbox/ support math america https //www.mathforamerica.org/ numberphile website http //www.numberphile.com/ numberphile facebook http //www.facebook.com/numberphile numberphile tweets https //twitter.com/numberphile subscribe http //bit.ly/numberphile_sub videos brady haran patreon http //www.patreon.com/numberphile numberphile t-shirts merch https //teespring.com/stores/numberphile brady 's videos subreddit http //www.reddit.com/r/bradyharan/ brady 's latest videos across channels http //www.bradyharanblog.com/ sign occasional emails http //eepurl.com/ydjl9,Machine Learning
kaggle ml community engineering sanyam bhutani,join dr tim scarfe sayak paul yannic kilcher alex stenlake conversation mr. chai time data science sanyam bhutani 00:00:00 introduction 00:03:42 show kick 00:06:34 sanyam get started ml 00:07:46 content creator 00:09:01 self taught without formal education ml 00:22:54 kaggle 00:33:41 h20 product job 00:40:58 intepretability bias engineering skills 00:43:22 get first job ds 00:46:29 aws ml ops architecture ml engineering 01:14:19 patterns 01:18:09 testability 01:20:54 adversarial examples sanyam 's blog -- https //sanyambhutani.com/tag/chaitimedatascience/ chai time data science -- https //www.youtube.com/c/chaitimedatascience,Machine Learning
jeff hawkins thousand brains theory intelligence,jeff hawkins founder redwood center theoretical neuroscience 2002 numenta 2005 2004 book titled intelligence research team worked reverse-engineer neocortex propose artificial intelligence architectures approaches ideas inspired human brain ideas include hierarchical temporal memory htm 2004 thousand brains theory intelligence 2017 would like get information podcast go https //lexfridman.com/ai connect lexfridman twitter linkedin facebook medium youtube,Machine Learning
searching rh counterexamples — adding database,last article set pytest simple application computes divisor sums tries disprove riemann hypothesis post ’ show extend application add database dependency database stores computed sums analyze application finishes previous post ’ link specific git commits final code repository show project evolves browse checkout repository commit see works interface implementation approach ’ take one highlights principle good testing good software design separate components thin interfaces implementations interfaces change later without needing update lots client code ’ take extreme implementing testing logic application ever decide sort database plan use words choice database last choice making inherently flexible change first iron minimal interface application needs choose right database based needs useful software engineers often ’ understand choice dependency especially database dependency work long term particularly prototype starts scale hit application-specific bottlenecks couple industry ’ trend chasing hot new fads eventually realize choice sacred interface separation software engineer ’ defense potent tool flexibility side note tom gamon summarizes attitude well recent article borrowing analogy 1975 investment essay winner ’ game charles ellis articles reinforce idea important decisions made late possible since time know enough make decisions well application two parts far adding new divisor sums database loading divisor sums analysis since ’ adding database time may also prudent summarize contents database e.g say ’ largest computed integer suggests following first-pass interface implemented commit class divisordb abc abstractmethod def load,Machine Learning
124 – stephen wolfram fundamental theory physics life universe,stephen wolfram computer scientist mathematician theoretical physicist second conversation podcast please check sponsors get discount support podcast – simplisafe https //simplisafe.com/lex – sun basket use code lex https //sunbasket.com/lex – masterclass https //masterclass.com/lex would like get information podcast go https //lexfridman.com/podcast connect lexfridman twitter linkedin facebook medium youtube watch video versions conversations enjoy podcast please rate 5 stars apple podcasts follow spotify support patreon,Machine Learning
pieter abbeel deep reinforcement learning,pieter abbeel professor uc berkeley director berkeley robot learning lab one top researchers world working make robots understand interact world around especially imitation deep reinforcement learning video version available youtube would like get information podcast go https //lexfridman.com/ai connect lexfridman twitter linkedin facebook youtube watch video versions conversations,Machine Learning
python first step data science,read full story,Machine Learning
garry kasparov chess deep blue ai putin,garry kasparov considered many greatest chess player time 1986 retirement 2005 dominated chess world ranking world number 1 19 years many historic matches human chess players long arc history may remembered match machine ibm ’ deep blue initial victories eventual loss deep blue captivated imagination world role artificial intelligence systems may play civilization ’ future excitement inspired entire generation,Machine Learning
full article svm classification kernel selection outlier detection code r…,read full story,Machine Learning
one solve coding interviews stats/ml background,stuff completely head always ask data science ml positions feel like know principles ml/stats get shafted section never taken cs class programming done r/julia focuses numerical computing almost never ask implement glm via gradient descent irls would comfortable always data structure general algo related question never statistical/ml algorithms questions asked instead asking analyze dataset something value leetcode style questions field never seen things come actual stats/ml work yet many ds/ml coding challenges include coding parts never ask demonstrate actual knowledge ml code find questions lot harder “ real ” ml mind ’ work way ’ go cs reason like want gatekeep stat/math/sciences people getting ml link comments,Machine Learning
using ai unearth unconscious bias job descriptions,“ diversity collective strength successful organization unconscious bias job descriptions unconscious bias affects us one way defined prejudice unsupported judgments favor one thing person group compared another way usually considered unfair unconscious bias discussed colleges universities across big small workplaces today one prominent examples unconscious bias observed hiring process adopted companies often job bulletins put forth contain elements favor particular gender group biased job descriptions limits candidate pool also diversity workplace therefore companies need check biases job descriptions eliminate create healthy fair culture organization one detect biased job descriptions first place well artificial intelligence technologies come rescue artificial intelligence provide answer today machine learning artificial intelligence made possible analyze data various sources lot accuracy precision whether structured unstructured data ai-backed technologies provide superior results compared manual processing hence ’ great idea ai applications infused platform enables business users directly interact data without lot hassle complexities well h2o wave created precisely article ’ clearly see implement idea go let ’ quickly understand h2o wave advantages h2o wave h2o wave open-source python development framework makes fast easy data scientists machine learning engineers software developers develop real-time interactive ai apps sophisticated visualizations h2o wave accelerates development wide variety user-interface components charts including dashboard templates dialogs themes widgets many thousands potential use cases using wave ’ ‘ hiring bias ’ app detect unconscious bias job description dataset let ’ see ai help us detect unconscious bias dataset containing job descriptions article shall working preprocessed version los angeles job description dataset kaggle includes following attributes job detect whether text description_text column contains unconscious bias methodology hiring bias analysis app dataset first loaded app ‘ hiring app ’ consists several machine learning algorithms models analyze different parts job description text perform analysis based word choices text structure tone sentiment etc app generates detailed report containing multiple insights findings form dashboard detailed findings grouped following headings i.e. word choice text structure tone sentiment 1 analysis word choice unconscious bias job descriptions towards specific gender limit candidate pool diversity figure shows choice words lead bias let ’ see insights presented app use gendered keywords job descriptions using gender-specific words job description isolate specific gender applying certain jobs observed words “ aggressive ” “ assertive ” “ independent ” typically put women applying specific roles plots show male female-specific keywords identified dataset higher use superlatives following example shows certain job descriptions highly superlative keywords specifically “ master ” “ expert ” used keywords strong masculine tone hence show preference towards particular gender use gendered pronouns wrong practice specify two genders job descriptions form “ he/she. ” usage another form unconscious gender bias commonly present descriptions today 2 analysis text structures several studies linkedin forbes glassdoor suggested quality quantity applicant ’ pool significantly influenced job description written “ well-written complete insightful job description result attracting top diverse talents role. ” hand description lacks vital features example — optimal word limit choice words language used overall tone may result attracting fewer candidates text structure job description analyzed several ways namely analysis difficulty level keywords job descriptions since readability paramount importance good idea focus words difficult read people eliminate graph shows high usage difficult words descriptions various jobs analysis readability job descriptions similarly companies refrain making job descriptions complex challenging read instance analysis found following job descriptions following posts least readable 3 analysis tone sentiment sentiment analysis sub-field natural language processing nlp tries identify extract opinions given text sentiment analysis job descriptions help companies gauge tone overall sentiment sentiments tone conveyed job descriptions negative demanding resulting fewer people applying job overall sentiment job descriptions use sentences words convey moderate high negative sentiments avoided following plot shows distribution negative sentiments job descriptions tone job descriptions companies must ensure tone job descriptions ’ negative instance keywords containing negative sentiments automatically highlighted avoided use strict keywords job descriptions excessive use demanding keywords nature also desirable job description phrases “ fail ” “ considered ” “ must-have ” etc. avoided replaced positive encouraging words like “ good ” “ add-on ” etc analysis clearly demonstrated idea job descriptions encourage wider group people apply thus choice language words job descriptions plays crucial role promoting applicant diversity hence factors like content tone language format directly indirectly influence hiring process company create apps h2o wave ‘ hiring bias ’ app created using h2o wave made easy others also create similar interactive visual ai applications best part done using python without learn html css javascript wave comes robust documentation numerous examples get started immediately waiting see create wave post using ai unearth unconscious bias job descriptions appeared first open source leader ai ml,Machine Learning
exploring open-ended algorithms poet,three youtubers tim scarfe machine learning dojo https //www.youtube.com/channel/ucxvhubmbgjw67i5vrmbboba connor shorten henry ai labs https //www.youtube.com/channel/uchb9vepy6kyvzjj0bgxnpbw yannic kilcher https //www.youtube.com/channel/uczhmqk67msjgfcctn7xbfew made new youtube channel called machine learning street talk every week talk latest greatest ai subscribe special guests week dr. mathew salvaris https //www.linkedin.com/in/drmathewsalvaris/ eric craeymeersch https //www.linkedin.com/in/ericcraeymeersch/ dr. keith duggar https //www.linkedin.com/in/dr-keith-duggar/ dmitri soshnikov https //www.linkedin.com/in/shwars/ discuss new concept open-ended `` ai-generating '' algorithm open-endedness class algorithms generate problems solutions increasingly complex diverse tasks algorithms create curriculum learning complex tasks become tractable final stepping stone lineage progressions many respects 's better trust machine develop learning curriculum best curriculum might counter-intuitive algorithms generate radiating tree evolving challenges solutions like natural evolution evolution produced eternity diversity complexity even produced human intelligence side-effect could ai-generating algorithms next big thing machine learning wang rui et al `` enhanced poet open-ended reinforcement learning unbounded invention learning challenges solutions '' arxiv preprint arxiv:2003.08536 2020 https //arxiv.org/abs/2003.08536 wang rui et al `` paired open-ended trailblazer poet endlessly generating increasingly complex diverse learning environments solutions '' arxiv preprint arxiv:1901.01753 2019 https //arxiv.org/abs/1901.01753 watch yannic ’ video poet https //www.youtube.com/watch v=8wkgdnnxivs extended poet https //youtu.be/gbg1x8xq-t8 watch connor ’ video https //www.youtube.com/watch v=jxikpxkn10u uberai labs video https //www.youtube.com/watch v=rx0skdrq400 reinforcementlearning machinelearning uber deeplearning rl timscarfe connorshorten yannickilcher,Machine Learning
stephen kotkin stalin putin nature power,stephen kotkin professor history princeton university one great historians time specializing russian soviet history written many books stalin soviet union including first 2 3 volume work stalin currently working volume 3 conversation part artificial intelligence podcast would like get information podcast go https //lexfridman.com/ai connect lexfridman twitter linkedin facebook medium youtube watch video versions conversations,Machine Learning
paper explained switch transformers scaling trillion parameter models simple efficient sparsity full video analysis,https //youtu.be/iar8lkkmmim scale next frontier ai google brain uses sparsity hard routing massively increase model 's parameters keeping flops per forward pass constant switch transformer compares favorably dense counterparts terms speed sample efficiency breaks next magic number one trillion parameters ​ outline 0:00 intro overview 4:30 performance gains scale 8:30 switch transformer architecture 17:00 model- data- expert-parallelism 25:30 experimental results 29:00 stabilizing training 32:20 distillation dense models 33:30 final comments ​ paper https //arxiv.org/abs/2101.03961 link comments,Machine Learning
132 – george hotz hacking simulation learning drive neural nets,george hotz geohot programmer hacker founder comma.ai please support podcast checking sponsors – four sigmatic https //foursigmatic.com/lex use code lexpod get 40 free shipping – decoding digital https //appdirect.com/decoding-digital – expressvpn https //expressvpn.com/lexpod use code lexpod get 3 months free episode links comma.ai ’ twitter https //twitter.com/comma_ai comma.ai ’ website https //comma.ai/ george ’ instagram https //www.instagram.com/georgehotz george ’ twitch https //www.twitch.tv/georgehotz george ’ twitter https //twitter.com/realgeorgehotz comma.ai youtube unofficial https //www.youtube.com/channel/ucwgkmjm4zjqrj-u5njvr2dg podcast info podcast website https //lexfridman.com/podcast apple podcasts https //apple.co/2lwqzir spotify https //spoti.fi/2newcf8 rss https //lexfridman.com/feed/podcast/ youtube full episodes https //youtube.com/lexfridman youtube clips https //youtube.com/lexclips support connect – check,Machine Learning
deep learning empirical success large extent proportional raw experimental throughput ability babysit large number experiments staring plots tweaking/re-launching works necessary sufficient,deep learning empirical success large extent proportional raw experimental throughput ability babysit large number experiments staring plots tweaking/re-launching works necessary sufficient,Machine Learning
exploring limits transfer learning unified text-to-text transformer,episode machine learning street talk tim scarfe yannic kilcher connor shorten chat large-scale transfer learning natural language processing text-to-text transfer transformer t5 model google ai exhaustive survey ’ important transfer learning nlp ’ conversation go key takeaways paper text-to-text input/output format architecture choice dataset size composition fine-tuning strategy best use computation beginning topics diverge exciting ideas embodied cognition meta-learning measure intelligence still beginning podcast journey really appreciate feedback listeners chat technical prefer group discussions interviewing experts chats three us thanks watching ’ already please subscribe paper links discussed chat text-to-text transfer transformer https //arxiv.org/abs/1910.10683 experience grounds language relevant divergent discussion embodied cognition https //arxiv.org/pdf/2004.10151.pdf measure intelligence https //arxiv.org/abs/1911.01547 train large compress https //arxiv.org/pdf/2002.11794.pdf scaling laws neural language models https //arxiv.org/pdf/2001.08361.pdf illustrated transformer http //jalammar.github.io/illustrated ... electra https //arxiv.org/pdf/2003.10555.pdf transformer-xl https //arxiv.org/pdf/1901.02860.pdf reformer efficient transformer https //openreview.net/pdf id=rkgnkkhtvb evolved transformer https //arxiv.org/pdf/1901.11117.pdf distilbert https //arxiv.org/pdf/1910.01108.pdf generate text highly recommend https //huggingface.co/blog/how-to-ge ... tokenizers https //blog.floydhub.com/tokenization-nlp/,Machine Learning
avi loeb aliens black holes mystery oumuamua lex fridman podcast 154,avi loeb astrophysicist harvard please support podcast checking sponsors zero fasting https //go.zerofasting.com/s/lex-promo get 30 annual subscription lmnt https //drinklmnt.com/lex get free sample pack sun basket https //sunbasket.com/lex use code lex get 35 pessimists archive https //pessimists.co/ episode links extraterrestrial book https //amzn.to/39xdnkt avi 's website https //astronomy.fas.harvard.edu/people/avi-loeb podcast info podcast website https //lexfridman.com/podcast apple podcasts https //apple.co/2lwqzir spotify https //spoti.fi/2newcf8 rss https //lexfridman.com/feed/podcast/ full episodes playlist https //www.youtube.com/playlist list=plraxtmerzgodp_8gztsuki9nrranbkkp4 clips playlist https //www.youtube.com/playlist list=plraxtmerzgoecifp3cbcieelojeitor41 outline 0:00 introduction 2:31 alone universe 6:46 consciousness 11:23 sending digital copies humans space 16:01 oumuamua 38:04 alien space junk 42:03 aliens look like 59:21 drake equation 1:00:23 industrial polution aliens 1:12:15 ufo sightings 1:20:11 long human civilization last 1:22:51 radio signal proxima centauri 1:26:12 breakthrough starshot project 1:29:11 space race 1:34:22 human space exploration 1:39:38 social media threat society 1:44:26 humans ready discovering alien civilization 1:48:38 mayans used astrology wage war 1:49:53 black holes 2:08:43 stephen hawking 2:12:21 grigori perelman 2:16:46 theory everything 2:23:45 dark matter 2:26:28 advice young people 2:29:32 memories father mother 2:34:01 existentialism 2:36:15 mortality 2:38:49 meaning life connect subscribe youtube channel twitter https //twitter.com/lexfridman linkedin https //www.linkedin.com/in/lexfridman facebook https //www.facebook.com/lexfridmanpage instagram https //www.instagram.com/lexfridman medium https //medium.com/ lexfridman support patreon https //www.patreon.com/lexfridman,Machine Learning
research ml research permutations combinations ol existing models datasets,'m slightly new field ml research 1 published conference paper 2 journal papers review 've seen far vast majority papers field permutations combinations existing datasets existing methods worked research group computational biology work something along lines taking biology dataset statistical analysis done training random ml/deep learning network publishing `` novel '' contribution low hanging fruit like everywhere astronomy healthcare 've seen friends trying 100s different models dataset see one model ensemble models would beat existing sota even 0.5 popular datasets like nslkdd intrusion detection dataset 100s neural network models paper even though less performance better accuracy lower fpr lesser training cost others ensembles sure interesting novel ideas coming vast majority people seem throwing random models random fields random datasets hoping 's faster/better/less memory usage/anything used claim `` novelty '' `` research '' like even useful link comments,Machine Learning
visualization function optimization python,"tweet share share function optimization involves finding input results optimal value objective function optimization algorithms navigate search space input variables order locate optima shape objective function behavior algorithm search space opaque real-world problems common study optimization algorithms using simple low-dimensional functions easily visualized directly additionally samples input space simple functions made optimization algorithm visualized appropriate context visualization lower-dimensional functions algorithm behavior functions help develop intuitions carry complex higher-dimensional function optimization problems later tutorial discover create visualizations function optimization python completing tutorial know visualization important tool studying function optimization algorithms visualize one-dimensional functions samples using line plots visualize two-dimensional functions samples using contour surface plots let ’ get started visualization function optimization python photo nathalie rights reserved tutorial overview tutorial divided three parts visualization function optimization visualize 1d function optimization test function sample test function line plot test function scatter plot test function line plot marked optima line plot samples visualize 2d function optimization test function sample test function contour plot test function filled contour plot test function filled contour plot test function samples surface plot test function visualization function optimization function optimization field mathematics concerned finding inputs function result optimal output function typically minimum maximum value optimization may straightforward simple differential functions solution calculated analytically however functions ’ interested solving applied machine learning may may well behaved may complex nonlinear multivariate non-differentiable important understanding wide range different algorithms used address function optimization problems important aspect studying function optimization understanding objective function optimized understanding behavior optimization algorithm time visualization plays important role getting started function optimization select simple well-understood test functions study optimization algorithms simple functions plotted understand relationship input objective function output objective function highlighting hills valleys optima addition samples selected search space optimization algorithm also plotted top plots objective function plots algorithm behavior provide insight intuition specific optimization algorithms work navigate search space generalize new problems future typically one-dimensional two-dimensional functions chosen study optimization algorithms easy visualize using standard plots like line plots surface plots explore tutorial first let ’ explore might visualize one-dimensional function optimization visualize 1d function optimization one-dimensional function takes single input variable outputs evaluation input variable input variables typically continuous represented real-valued floating-point value often input domain unconstrained although test problems impose domain interest test function case explore function visualization simple x^2 objective function f x x^2 optimal value input x=0.0 equals 0.0 example implements objective function evaluates single input example 1d objective function objective function def objective x return x 2.0 evaluate inputs objective function x 4.0 result objective x print f .3f .3f x result running example evaluates value 4.0 objective function equals 16.0. f 4.000 16.000 sample test function first thing might want new function define input range interest sample domain interest using uniform grid sample provide basis generating plot later case define domain interest around optima x=0.0 x=-5.0 x=5.0 sample grid values range 0.1 increments -5.0 -4.9 -4.8 etc ... define range input r_min r_max -5.0 5.0 sample input range uniformly 0.1 increments inputs arange r_min r_max 0.1 summarize input domain print inputs :5 evaluate x values sample ... compute targets results objective inputs summarize results print results :5 finally check input corresponding outputs ... create mapping inputs results range 5 print f .3f .3f inputs results tying together complete example sampling input space evaluating points sample listed sample 1d objective function numpy import arange objective function def objective x return x 2.0 define range input r_min r_max -5.0 5.0 sample input range uniformly 0.1 increments inputs arange r_min r_max 0.1 summarize input domain print inputs :5 compute targets results objective inputs summarize results print results :5 create mapping inputs results range 5 print f .3f .3f inputs results running example first generates uniform sample input points expected input points evaluated using objective function finally see simple mapping inputs outputs objective function -5 -4.9 -4.8 -4.7 -4.6 25 24.01 23.04 22.09 21.16 f -5.000 25.000 f -4.900 24.010 f -4.800 23.040 f -4.700 22.090 f -4.600 21.160 confidence generating sample inputs evaluating objective function look generating plots function line plot test function could sample input space randomly benefit uniform line grid points used generate smooth plot smooth points input space ordered smallest largest ordering important expect hope output objective function similar smooth relationship values e.g small changes input result locally consistent smooth changes output function case use samples generate line plot objective function input points x x-axis plot objective function output results y-axis plot ... create line plot input vs result pyplot.plot inputs results show plot pyplot.show tying together complete example listed line plot input vs result 1d objective function numpy import arange matplotlib import pyplot objective function def objective x return x 2.0 define range input r_min r_max -5.0 5.0 sample input range uniformly 0.1 increments inputs arange r_min r_max 0.1 compute targets results objective inputs create line plot input vs result pyplot.plot inputs results show plot pyplot.show running example creates line plot objective function see function large u-shape called parabola common shape studying curves e.g study calculus line plot one-dimensional function scatter plot test function line construct really function smooth summary function always keep mind recall fact generated sample points input space corresponding evaluation points would accurate create scatter plot points example scatter plot input vs result 1d objective function numpy import arange matplotlib import pyplot objective function def objective x return x 2.0 define range input r_min r_max -5.0 5.0 sample input range uniformly 0.1 increments inputs arange r_min r_max 0.1 compute targets results objective inputs create scatter plot input vs result pyplot.scatter inputs results show plot pyplot.show running example creates scatter plot objective function see familiar shape function ’ gain anything plotting points directly line smooth interpolation points provides useful draw points top line location optima points sampled optimization algorithm scatter plot one-dimensional function line plot marked optima next let ’ draw line plot time draw point known optima function located helpful studying optimization algorithm might want see close optimization algorithm get optima first must define input optima evaluate point give x-axis y-axis values plotting ... define known function optima optima_x 0.0 optima_y objective optima_x plot point shape color like case red square ... draw function optima red square pyplot.plot optima_x optima_y 's color= r tying together complete example creating line plot function optima highlighted point listed line plot input vs result 1d objective function show optima numpy import arange matplotlib import pyplot objective function def objective x return x 2.0 define range input r_min r_max -5.0 5.0 sample input range uniformly 0.1 increments inputs arange r_min r_max 0.1 compute targets results objective inputs create line plot input vs result pyplot.plot inputs results define known function optima optima_x 0.0 optima_y objective optima_x draw function optima red square pyplot.plot optima_x optima_y 's color= r show plot pyplot.show running example creates familiar line plot function time optima function e.g input results minimum output function marked red square line plot one-dimensional function optima marked red square simple function red square optima easy see sometimes function might complex lots hills valleys might want make optima visible case draw vertical line across whole plot ... draw vertical line optimal input pyplot.axvline x=optima_x ls= -- color='red tying together complete example listed line plot input vs result 1d objective function show optima line numpy import arange matplotlib import pyplot objective function def objective x return x 2.0 define range input r_min r_max -5.0 5.0 sample input range uniformly 0.1 increments inputs arange r_min r_max 0.1 compute targets results objective inputs create line plot input vs result pyplot.plot inputs results define known function optima optima_x 0.0 draw vertical line optimal input pyplot.axvline x=optima_x ls= -- color='red show plot pyplot.show running example creates plot time draws red line clearly marking point input space marks optima line plot one-dimensional function optima marked red line line plot samples finally might want draw samples input space selected optimization algorithm simulate samples random points drawn input domain ... simulate sample made optimization algorithm seed 1 sample r_min rand 10 r_max r_min evaluate sample sample_eval objective sample plot sample case using small black circles ... plot sample black circles pyplot.plot sample sample_eval color='black complete example creating line plot function optima marked red line algorithm sample drawn small black dots listed line plot domain 1d function optima algorithm sample numpy import arange numpy.random import seed numpy.random import rand matplotlib import pyplot objective function def objective x return x 2.0 define range input r_min r_max -5.0 5.0 sample input range uniformly 0.1 increments inputs arange r_min r_max 0.1 compute targets results objective inputs simulate sample made optimization algorithm seed 1 sample r_min rand 10 r_max r_min evaluate sample sample_eval objective sample create line plot input vs result pyplot.plot inputs results define known function optima optima_x 0.0 draw vertical line optimal input pyplot.axvline x=optima_x ls= -- color='red plot sample black circles pyplot.plot sample sample_eval color='black show plot pyplot.show running example creates line plot domain marks optima red line time sample domain selected algorithm really random sample points drawn black dots imagine real optimization algorithm show points narrowing domain searches down-hill starting point line plot one-dimensional function optima marked red line samples shown black dots next let ’ look might perform similar visualizations optimization two-dimensional function visualize 2d function optimization two-dimensional function function takes two input variables e.g x test function use x^2 function scale two-dimensional function example f x x^2 y^2 optimal value input x=0.0 y=0.0 equals 0.0 example implements objective function evaluates single input example 2d objective function objective function def objective x return x 2.0 2.0 evaluate inputs objective function x 4.0 4.0 result objective x print f .3f .3f .3f x result running example evaluates point x=4 y=4 equals 32. f 4.000 4.000 32.000 next need way sample domain turn sample objective function sample test function common way sampling two-dimensional function first generate uniform sample along variable x use two uniform samples create grid samples called mesh grid two-dimensional array across input space instead two two-dimensional arrays used together define grid across two input variables achieved duplicating entire x sample array sample point similarly duplicating entire sample array x sample point achieved using meshgrid numpy function example ... define range input r_min r_max -5.0 5.0 sample input range uniformly 0.1 increments xaxis arange r_min r_max 0.1 yaxis arange r_min r_max 0.1 create mesh axis x meshgrid xaxis yaxis summarize input domain print x :5 :5 evaluate pair points using objective function ... compute targets results objective x summarize results print results :5 :5 finally review mapping inputs corresponding output values ... create mapping inputs results range 5 print f .3f .3f .3f x i,0 i,0 results i,0 example demonstrates create uniform sample grid across two-dimensional input space objective function sample 2d objective function numpy import arange numpy import meshgrid objective function def objective x return x 2.0 2.0 define range input r_min r_max -5.0 5.0 sample input range uniformly 0.1 increments xaxis arange r_min r_max 0.1 yaxis arange r_min r_max 0.1 create mesh axis x meshgrid xaxis yaxis summarize input domain print x :5 :5 compute targets results objective x summarize results print results :5 :5 create mapping inputs results range 5 print f .3f .3f .3f x i,0 i,0 results i,0 running example first summarizes points mesh grid objective function evaluation points finally enumerate coordinates two-dimensional input space corresponding function evaluation -5 -4.9 -4.8 -4.7 -4.6 -5 -4.9 -4.8 -4.7 -4.6 -5 -4.9 -4.8 -4.7 -4.6 -5 -4.9 -4.8 -4.7 -4.6 -5 -4.9 -4.8 -4.7 -4.6 50 49.01 48.04 47.09 46.16 49.01 48.02 47.05 46.1 45.17 48.04 47.05 46.08 45.13 44.2 47.09 46.1 45.13 44.18 43.25 46.16 45.17 44.2 43.25 42.32 f -5.000 -5.000 50.000 f -5.000 -4.900 49.010 f -5.000 -4.800 48.040 f -5.000 -4.700 47.090 f -5.000 -4.600 46.160 familiar sample input space evaluate points let ’ look might plot function contour plot test function popular plot two-dimensional functions contour plot plot creates flat representation objective function outputs x coordinate color contour lines indicate relative value height output objective function like contour map landscape mountains distinguished valleys achieved using contour matplotlib function takes mesh grid evaluation mesh grid input directly specify number levels draw contour color scheme use case use 50 levels popular “ jet ” color scheme low-levels use cold color scheme blue high-levels use hot color scheme red ... create contour plot 50 levels jet color scheme pyplot.contour x results 50 alpha=1.0 cmap='jet show plot pyplot.show tying together complete example creating contour plot two-dimensional objective function listed create contour plot 50 levels jet color scheme pyplot.contour x results 50 alpha=1.0 cmap='jet show plot pyplot.show tying together complete example creating contour plot two-dimensional objective function listed contour plot 2d objective function numpy import arange numpy import meshgrid matplotlib import pyplot objective function def objective x return x 2.0 2.0 define range input r_min r_max -5.0 5.0 sample input range uniformly 0.1 increments xaxis arange r_min r_max 0.1 yaxis arange r_min r_max 0.1 create mesh axis x meshgrid xaxis yaxis compute targets results objective x create contour plot 50 levels jet color scheme pyplot.contour x results 50 alpha=1.0 cmap='jet show plot pyplot.show running example creates contour plot see curved parts surface around edges contours show detail less curved parts surface middle fewer contours see lowest part domain middle expected contour plot two-dimensional objective function filled contour plot test function also helpful color plot contours show complete surface colors simple linear interpolation true function evaluation must kept mind complex functions fine detail shown fill contour plot using contourf version function takes arguments ... create filled contour plot 50 levels jet color scheme pyplot.contourf x results levels=50 cmap='jet also show optima plot case white star stand blue background color lowest part plot ... define known function optima optima_x 0.0 0.0 draw function optima white star pyplot.plot optima_x 0 optima_x 1 color='white tying together complete example filled contour plot optima marked listed filled contour plot 2d objective function show optima numpy import arange numpy import meshgrid matplotlib import pyplot objective function def objective x return x 2.0 2.0 define range input r_min r_max -5.0 5.0 sample input range uniformly 0.1 increments xaxis arange r_min r_max 0.1 yaxis arange r_min r_max 0.1 create mesh axis x meshgrid xaxis yaxis compute targets results objective x create filled contour plot 50 levels jet color scheme pyplot.contourf x results levels=50 cmap='jet define known function optima optima_x 0.0 0.0 draw function optima white star pyplot.plot optima_x 0 optima_x 1 color='white show plot pyplot.show running example creates filled contour plot gives better idea shape objective function optima x=0 y=0 marked clearly white star filled contour plot two-dimensional objective function optima marked white star filled contour plot test function samples may want show progress optimization algorithm get idea behavior context shape objective function case simulate points chosen optimization algorithm random coordinates input space ... simulate sample made optimization algorithm seed 1 sample_x r_min rand 10 r_max r_min sample_y r_min rand 10 r_max r_min points plotted directly black circles context color give idea relative quality ... plot sample black circles pyplot.plot sample_x sample_y color='black tying together complete example filled contour plot optimal input sample plotted listed filled contour plot 2d objective function show optima sample numpy import arange numpy import meshgrid numpy.random import seed numpy.random import rand matplotlib import pyplot objective function def objective x return x 2.0 2.0 define range input r_min r_max -5.0 5.0 sample input range uniformly 0.1 increments xaxis arange r_min r_max 0.1 yaxis arange r_min r_max 0.1 create mesh axis x meshgrid xaxis yaxis compute targets results objective x simulate sample made optimization algorithm seed 1 sample_x r_min rand 10 r_max r_min sample_y r_min rand 10 r_max r_min create filled contour plot 50 levels jet color scheme pyplot.contourf x results levels=50 cmap='jet define known function optima optima_x 0.0 0.0 draw function optima white star pyplot.plot optima_x 0 optima_x 1 color='white plot sample black circles pyplot.plot sample_x sample_y color='black show plot pyplot.show running example see filled contour plot optima marked see sample drawn black dots surrounding color relative distance optima gives idea close algorithm random points case got solving problem filled contour plot two-dimensional objective function optima input sample marked surface plot test function finally may want create three-dimensional plot objective function get fuller idea curvature function achieved using plot_surface matplotlib function like contour plot takes mesh grid function evaluation directly ... create surface plot jet color scheme figure pyplot.figure axis figure.gca projection='3d axis.plot_surface x results cmap='jet complete example creating surface plot listed surface plot 2d objective function numpy import arange numpy import meshgrid matplotlib import pyplot mpl_toolkits.mplot3d import axes3d objective function def objective x return x 2.0 2.0 define range input r_min r_max -5.0 5.0 sample input range uniformly 0.1 increments xaxis arange r_min r_max 0.1 yaxis arange r_min r_max 0.1 create mesh axis x meshgrid xaxis yaxis compute targets results objective x create surface plot jet color scheme figure pyplot.figure axis figure.gca projection='3d axis.plot_surface x results cmap='jet show plot pyplot.show running example creates three-dimensional surface plot objective function surface plot two-dimensional objective function additionally plot interactive meaning use mouse drag perspective surface around view different angles surface plot different angle two-dimensional objective function reading section provides resources topic looking go deeper apis optimization root finding scipy.optimize optimization scipy.optimize numpy.meshgrid api matplotlib.pyplot.contour api matplotlib.pyplot.contourf api mpl_toolkits.mplot3d.axes3d.plot_surface api articles mathematical optimization wikipedia parabola wikipedia summary tutorial discovered create visualizations function optimization python specifically learned visualization important tool studying function optimization algorithms visualize one-dimensional functions samples using line plots visualize two-dimensional functions samples using contour surface plots questions ask questions comments best answer tweet share share post visualization function optimization python appeared first machine learning mastery",Machine Learning
becoming data scientist easier think,read full story,Machine Learning
14 open datasets text classification machine learning,text classification datasets used categorize natural language texts according content example think classifying news articles topic classifying book reviews based positive negative response text classification also helpful language detection organizing customer feedback fraud detection though time consuming done manually process automated machine learning models result saves companies time also providing valuable data insights read full story,Machine Learning
78 – ann druyan cosmos carl sagan voyager beauty science,ann druyan writer producer director one important impactful communicators science time co-wrote 1980 science documentary series cosmos hosted carl sagan married 1981 love help nasa recorded brain waves golden record along things civilization offer launched space voyager 1 voyager 2 spacecraft 42 years later still active reaching farther deep space human-made object ever,Machine Learning
stuart russell long-term future ai,stuart russell professor computer science uc berkeley co-author book introduced millions people ai called artificial intelligence modern approach video version available youtube would like get information podcast go https //lexfridman.com/ai connect lexfridman twitter linkedin facebook youtube watch video versions conversations,Machine Learning
road become python ninja — handling exceptions,read full story,Machine Learning
pretty sure `` guy '' took couple pictures dad wig 😂,pretty sure `` guy '' took couple pictures dad wig 😂 nitter.net/not_the_bee/status/1349035604160487429,Machine Learning
2m all-in 5 pot wwyd daniel negreanu 's no-limit hold'em challenge poker hand analysis,ai technology poker daniel negreanu posted set interesting no-limit hold'em situations twitter try analyze perspective poker bot see bots think game approximate nash equilibria https //twitter.com/realkidpoker/status/1337887509397741568 https //twitter.com/realkidpoker/status/1337899147337244673 https //twitter.com/realkidpoker/status/1337904860721606656 links youtube https //www.youtube.com/c/yannickilcher twitter https //twitter.com/ykilcher discord https //discord.gg/4h8xxdf bitchute https //www.bitchute.com/channel/yannic-kilcher bilibili https //space.bilibili.com/1824646584 minds https //www.minds.com/ykilcher parler https //parler.com/profile/yannickilcher linkedin https //www.linkedin.com/in/yannic-kilcher-488534136/ want support best thing share content want support financially completely optional voluntary lot people asked subscribestar https //www.subscribestar.com/yannickilcher patreon https //www.patreon.com/yannickilcher bitcoin btc bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq ethereum eth 0x7ad3513e3b8f66799f507aa7874b1b0ebc7f85e2 litecoin ltc lqw2trykyetvc8wjfkhpphtpbdm4vw7r9m monero xmr 4acl8agreo5hair8a9cevrw8peauwvnp1wnsdzxw7tzicdlhzagsgzhrqabdnfy8yum9fwjdvijphkrjv4fwt19cjzn9d4n,Machine Learning
5 papers face recognition every data scientist read,facial recognition one largest areas research within computer vision article introduce 5 face recognition papers data scientists read full story,Machine Learning
r characterizing signal propagation close performance gap unnormalized resnets,link comments,Machine Learning
power tower puzzle lockdown math ep 8,fun puzzle stemming repeated exponentiation full playlist https //www.youtube.com/playlist list=plzhqobowtqdp5cveljj1bndouqrahvpev home page https //www.3blue1brown.com brought https //3b1b.co/ldm-thanks notes ngân vũ https //twitter.com/thuynganvu/status/1261014161464516608 s=20 play along desmos https //www.desmos.com/calculator/nul32eaaa9 related videos calculus series https //www.youtube.com/playlist list=plzhqobowtqdmsr9k-rj53dwvrmyo3t5yr particular look https //youtu.be/cfw845lnobm numberphile grahm 's constant https //youtu.be/xtej64kd5cg -- -- -- -- -- -- -- -- -- video timeline thanks user `` noonesperfect '' 0:36 question 1 1:13 answer 1 1:29 introduction tetration 3:37 exponentiation works tetration 6:10 python program power tower iterations 8:40 question 2 9:32 python program regarding question 2 10:37 answer 2 explanation 13:18 power tower infinite size converges thumbnail question 15:21 question 3 16:28 footage grant 's setup arrangement problem due construction-work back home 16:49 answer 3 explanation 17:40 checking logic behind 2 different problems power towers whose answer converges value even possible 19:42 checking logic using desmos graph tool i.e cobweb graph desmos graph link description 28:12 question 4 28:51 questions audience tweets 29:15 knuth 's arrow notation graham 's number check numberphile video description 32:32 answer 4 explanation 37:29 homework/challenge puzzle 39:20 thumbnail question power tower logic 40:55 audience questions twitter 41:45 power tower complex numbers/fractal set 45:19 brainteaser 48:06 questions tweets 53:17 notes lock-down series grant 's tweeter -- -- -- -- -- -- -- -- -- live question setup stats on-screen powered itempool https //itempool.com/ curious animations https //www.3blue1brown.com/faq manim music vincent rubinetti download music bandcamp https //vincerubinetti.bandcamp.com/album/the-music-of-3blue1brown stream music spotify https //open.spotify.com/album/1dvyjws8fbqxhrunag5w5u want contribute translated subtitles help review already made others need approval click gear icon video go subtitles/cc `` add subtitles/cc '' really appreciate helps make lessons accessible people -- -- -- -- -- -- -- -- -- 3blue1brown channel animating math senses word animate know drill youtube want stay posted new videos subscribe http //3b1b.co/subscribe various social media stuffs website https //www.3blue1brown.com twitter https //twitter.com/3blue1brown reddit https //www.reddit.com/r/3blue1brown instagram https //www.instagram.com/3blue1brown_animations/ patreon https //patreon.com/3blue1brown facebook https //www.facebook.com/3blue1brown,Machine Learning
backpropagation details part 2 going bonkers chain rule,statquest picks right part 1 left time 're going go totally bonkers chain rule optimize every single parameter simple neural network bam ⭐ note code use kite free ai-powered coding assistant help code faster smarter kite plugin integrates top editors ides give smart completions documentation ’ typing love https //www.kite.com/get-kite/ utm_medium=referral utm_source=youtube utm_campaign=statquest utm_content=description-only note statquest assumes already know main ideas behind backpropagation https //youtu.be/in2xmbhilt4 ... also means familiar ... neural networks https //youtu.be/cqofi41lfdw chain rule https //youtu.be/wl1myxrtqhq gradient descent https //youtu.be/sdv4f4s2sb8 last note researching 'quest found page sebastian raschka helpful https //sebastianraschka.com/faq/docs/backprop-arbitrary.html complete index statquest videos check https //statquest.org/video-index/ 'd like support statquest please consider ... patreon https //www.patreon.com/statquest ... ... youtube membership https //www.youtube.com/channel/uctyluttgs3k1fg4y5tahlbw/join ... cool statquest t-shirt sweatshirt usa/europe https //teespring.com/stores/statquest everywhere https //www.redbubble.com/people/starmer/works/40421224-statquest-double-bam asc=u p=t-shirt ... buying one two songs go large get whole album https //joshuastarmer.bandcamp.com/ ... donating statquest https //www.paypal.me/statquest lastly want keep research create new statquests follow twitter https //twitter.com/joshuastarmer 0:00 awesome song introduction 1:28 derivative weight w1 5:58 derivative bias b1 7:39 derivatives w2 b2 9:21 gradient descent parameters 11:18 fancy gradient descent animation statquest neuralnetworks backpropagation,Machine Learning
juergen schmidhuber godel machines meta-learning lstms,juergen schmidhuber co-creator long short-term memory networks lstms used billions devices today speech recognition translation much 30 years proposed lot interesting out-of-the-box ideas artificial intelligence including formal theory creativity video version available youtube would like get information podcast go https //lexfridman.com/ai connect lexfridman twitter linkedin facebook youtube watch video versions conversations,Machine Learning
best computer vision competitions kaggle beginners,list best computer vision competitions kaggle beginners hope like full list discussed video https //www.kaggle.com/c/dogs-vs-cats https //www.kaggle.com/c/flower-classification-with-tpus https //www.kaggle.com/c/cdiscount-image-classification-challenge/ https //www.kaggle.com/c/rsna-pneumonia-detection-challenge/ https //www.kaggle.com/c/facial-keypoints-detection/ https //www.kaggle.com/c/noaa-right-whale-recognition/ https //www.kaggle.com/c/tgs-salt-identification-challenge https //www.kaggle.com/c/carvana-image-masking-challenge/ https //www.kaggle.com/c/imaterialist-fashion-2019-fgvc6 https //www.kaggle.com/c/global-wheat-detection https //www.kaggle.com/c/generative-dog-images/ please subscribe like video help keep motivated make awesome videos like one buy book approaching almost machine learning problem please visit https //bit.ly/buyaaml follow twitter https //twitter.com/abhi1thakur linkedin https //www.linkedin.com/in/abhi1thakur/ kaggle https //kaggle.com/abhishek instagram https //instagram.com/abhi4ml,Machine Learning
gilbert strang linear algebra deep learning teaching mit opencourseware,gilbert strang professor mathematics mit perhaps one famous impactful teachers math world mit opencourseware lectures linear algebra viewed millions times conversation part artificial intelligence podcast would like get information podcast go https //lexfridman.com/ai connect lexfridman twitter linkedin facebook medium youtube watch video versions conversations enjoy podcast please rate 5 stars apple podcasts support patreon episode,Machine Learning
145 – matthew johnson psychedelics,matthew w. johnson professor psychedelics researcher johns hopkins please support podcast checking sponsors – brave https //brave.com/lex – neuro https //www.getneuro.com use code lex get 15 – four sigmatic https //foursigmatic.com/lex use code lexpod get 60 – cash app https //cash.app/ use code lexpodcast get 10 episode links matt ’ twitter https //twitter.com/drug_researcher matt ’ website https //www.hopkinsmedicine.org/profiles/results/directory/profile/0800020/matthew-johnson study website https //hopkinspsychedelic.org/ podcast info podcast website https //lexfridman.com/podcast apple podcasts https //apple.co/2lwqzir spotify https //spoti.fi/2newcf8 rss https //lexfridman.com/feed/podcast/ youtube full episodes https //youtube.com/lexfridman youtube clips https //youtube.com/lexclips support connect – check sponsors ’,Machine Learning
imaginary numbers needed understand radius convergence,get free access 2500 documentaries curiositystream https //curiositystream.thld.co/zachstarsep24 use code `` zachstar '' sign stemerch store https //stemerch.com/ support channel https //www.patreon.com/zachstar paypal one time donation https //www.paypal.me/zachstaryt join channel get access perks https //www.youtube.com/channel/ucpcsacbqs-sjevfk_hmfy9w/join 2d plot software https //www.desmos.com/calculator 3d plot software https //runiter.com/ ►follow instagram https //www.instagram.com/zachstar/ twitter https //twitter.com/imzachstar animations brainup studios http //brainup.in/ check spanish channel https //www.youtube.com/channel/ucnknu2xqblaspj6ckc8vtpa ►my setup space pictures https //amzn.to/2cc4kqj magnetic floating globe https //amzn.to/2vgpdn0 camera https //amzn.to/2rivyu5 mic https //amzn.to/35bkiri tripod https //amzn.to/2rgmtnl equilibrium tube https //amzn.to/2sowdrh ►check amazon store https //www.amazon.com/shop/zachstar,Machine Learning
teaching learning neurodiverse students,day 3 november 19 2020 theme unblocking pipeline education employment beth rosenberg tech kids unlimited ability project/nyu accessible computer science education fall workshop hosted microsoft university washington create university colorado ’ coleman institute took place november 17-19 2020 consisted three half-days talks discussions planning new research dedicated making computer science education learning experiences accessible people disabilities information workshop found https //www.microsoft.com/en-us/research/event/accessible-cs-education-fall-workshop/,Machine Learning
news soccer ai fails mixes ball referee 's bald head,ai tech news soccer camera operated ai track ball however ai interesting failure mode repeatedly mixes ball bald head referee raises interesting questions role ethics ai research footage spfl championship ictfc 1 v 1 ayr 24/10/2020 links youtube https //www.youtube.com/c/yannickilcher twitter https //twitter.com/ykilcher discord https //discord.gg/4h8xxdf bitchute https //www.bitchute.com/channel/yannic-kilcher minds https //www.minds.com/ykilcher parler https //parler.com/profile/yannickilcher linkedin https //www.linkedin.com/in/yannic-kilcher-488534136/ want support best thing share content want support financially completely optional voluntary lot people asked subscribestar https //www.subscribestar.com/yannickilcher patreon https //www.patreon.com/yannickilcher bitcoin btc bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq ethereum eth 0x7ad3513e3b8f66799f507aa7874b1b0ebc7f85e2 litecoin ltc lqw2trykyetvc8wjfkhpphtpbdm4vw7r9m monero xmr 4acl8agreo5hair8a9cevrw8peauwvnp1wnsdzxw7tzicdlhzagsgzhrqabdnfy8yum9fwjdvijphkrjv4fwt19cjzn9d4n,Machine Learning
computational journalism data behind stories jonathan stray,read full story,Machine Learning
use plaidml machine learning macos amd gpu,want train machine learning models mac ’ integrated amd gpu external graphics card look plaidml read full story,Machine Learning
many alien civilizations,thoughts based drake equation please check sponsors brave https //brave.com/lex neuro https //www.getneuro.com use code lex get 15 outline 0:00 introduction 2:22 new stars 2:45 planets 3:23 habitable planets 4:52 life 6:54 intelligence 8:20 communication 9:47 civilization lifetime 12:35 civilization rebirth 13:02 estimated number intelligent alien civilizations 14:44 view takeaway connect subscribe youtube channel twitter https //twitter.com/lexfridman linkedin https //www.linkedin.com/in/lexfridman facebook https //www.facebook.com/lexfridmanpage instagram https //www.instagram.com/lexfridman medium https //medium.com/ lexfridman support patreon https //www.patreon.com/lexfridman,Machine Learning
122 – david fravor ufos aliens fighter jets aerospace engineering,david fravor navy pilot 18 years primary witness one credible ufo sightings history video released pentagon reported ny times please check sponsors get discount support podcast – athletic greens https //athleticgreens.com/lex – expressvpn https //www.expressvpn.com/lexpod – betterhelp https //betterhelp.com/lex would like get information podcast go https //lexfridman.com/podcast connect lexfridman twitter linkedin facebook medium youtube watch video versions conversations enjoy,Machine Learning
choose clone data github,would anyone choose clone continuously maintain perfect clone data github debricked answer read full story,Machine Learning
openai code dangerous complex mere mortals lucidrains hold beer,openai code dangerous complex mere mortals lucidrains hold beer nitter.net/lucidrains/status/1346871881366847488,Machine Learning
stem students non stem classes like ...,stemerch store https //stemerch.com/ second channel skits https //www.youtube.com/zachstarhimself support channel https //www.patreon.com/zachstar paypal one time donation https //www.paypal.me/zachstaryt join channel get access perks https //www.youtube.com/channel/ucpcsacbqs-sjevfk_hmfy9w/join ►follow instagram https //www.instagram.com/zachstar/ twitter https //twitter.com/imzachstar 2d graphing software https //www.desmos.com/calculator animations brainup studios http //brainup.in/ check spanish channel https //www.youtube.com/channel/ucnknu2xqblaspj6ckc8vtpa ►my setup space pictures https //amzn.to/2cc4kqj magnetic floating globe https //amzn.to/2vgpdn0 camera https //amzn.to/2rivyu5 mic https //amzn.to/35bkiri tripod https //amzn.to/2rgmtnl equilibrium tube https //amzn.to/2sowdrh ►check amazon store https //www.amazon.com/shop/zachstar,Machine Learning
ian goodfellow generative adversarial networks gans,ian goodfellow author popular textbook deep learning simply titled “ deep learning ” coined term generative adversarial networks gans 2014 paper responsible launching incredible growth research gans video version available youtube would like get information podcast go https //lexfridman.com/ai connect lexfridman twitter linkedin facebook medium youtube watch video versions conversations,Machine Learning
changes everything,changes everything,Machine Learning
80 – vitalik buterin ethereum cryptocurrency future money,vitalik buterin co-creator ethereum ether cryptocurrency currently second-largest digital currency bitcoin ethereum lot interesting technical ideas defining future blockchain technology vitalik one brilliant people innovating space today support podcast supporting sponsors special code – get expressvpn https //www.expressvpn.com/lexpod – sign masterclass https //masterclass.com/lex episode links vitalik blog https //vitalik.ca ethereum whitepaper http //bit.ly/3cvdtpj casper ffg paper http //bit.ly/2u6j7dj quadratic funding paper http //bit.ly/3auz8wd bitcoin whitepaper https //bitcoin.org/bitcoin.pdf mastering ethereum book https //amzn.to/2xejwme conversation part,Machine Learning
dan gable olympic wrestling mental toughness making champions lex fridman podcast 152,dan gable one greatest olympic athletes wrestling coaches time please support podcast checking sponsors tryolabs https //tryolabs.com/lex expressvpn https //expressvpn.com/lexpod use code lexpod get 3 months free grammarly https //grammarly.com/lex get 20 premium simplisafe https //simplisafe.com/lex use code lex get free security camera episode links dan 's twitter https //twitter.com/dannygable dan 's website https //dangable.com/ dan 's books https //amzn.to/2vk5nbn podcast info podcast website https //lexfridman.com/podcast apple podcasts https //apple.co/2lwqzir spotify https //spoti.fi/2newcf8 rss https //lexfridman.com/feed/podcast/ full episodes playlist https //www.youtube.com/playlist list=plraxtmerzgodp_8gztsuki9nrranbkkp4 clips playlist https //www.youtube.com/playlist list=plraxtmerzgoecifp3cbcieelojeitor41 outline 0:00 introduction 2:56 russian wrestling 4:34 coaching science art toughness wrestling 11:30 pain defeat tattoo hawk clawing heart 14:29 roger bannister 4 minute mile 17:35 dream becoming olympic champion 20:03 day 1972 olympic final 23:35 sauna story 25:05 match russian 30:38 role fear wrestling 35:40 line physical wrestling anger 40:18 tragic loss dan 's sister 47:46 role family wrestling 53:08 wrestling voted olympics 57:52 beat best must study best 1:03:05 role luck old man sea connect subscribe youtube channel twitter https //twitter.com/lexfridman linkedin https //www.linkedin.com/in/lexfridman facebook https //www.facebook.com/lexfridmanpage instagram https //www.instagram.com/lexfridman medium https //medium.com/ lexfridman support patreon https //www.patreon.com/lexfridman,Machine Learning
🎉new video🎉 clip openai huge step connecting images text beats fully supervised models zero-shot imagenet🔥trained 400 million scraped samples w/ huge potential applications💪 https //youtu.be/t9xsu0pkx2e alecrad _jongwook_kim ilyasut,🎉new video🎉 clip openai huge step connecting images text beats fully supervised models zero-shot imagenet🔥trained 400 million scraped samples w/ huge potential applications💪 invidious.snopyta.org/t9xsu0pkx2e alecrad _jongwook_kim ilyasut,Machine Learning
quan sun finishing second place predict grant applications,’ phd student machine learning group university waikato hamilton new zealand ’ also part-time software developer 11ants analytics phd research focuses meta-learning full model selection problem 2009 2010 participated ucsd/fico data mining contests tried ended working tried many different algorithms mainly weka matlab implementations feature sets nearly 80 submissions report briefly introduce two approaches worked competition discussed sequentially order submissions first 10 testing submissions realised concept drift happening 2007 2008 success rates decline gradually 2007 also information page contest states “ australia success rates fallen 20–25 per cent… ” probably means decision rules grant applications somehow changed 2007 2008 consequences could think including limited overall success rates continue drop successful applications 2005/2006 would declined 2007/2008 2009/2010 success patterns becoming “ ” random decision rules year 2009/2010 close 2007/2008 compared rules year 2006 prior based information assumptions decided mainly use data points 2007 2008 training classifiers turns reasonable choice approach ensemble selection transformed feature set used first 20 submissions data engineering/transformation part start.date numeric year month day numbers rfcd.code.x x=1 5 nominal person.id.x x=1 15 nominal number.of.grant.x x=1 15 total number successful/unsuccessful grants per application publications aa b c total number aa b c publications per application role.x total number chief_investigators principal_supervisors delegated_researcher ext_chief_investigators per application country.of.birth.x total number asia_pacific born australia great_britain western_europe eastern_europe north_america new_zealand middle_east_and_africa per application with.phd total number phds per application years.in.uni total number people university 5 years transformations done also java program transform nominal attributes corresponding frequency frequency counting based available data points final feature set consists original features transformed features frequency modeling part main method called ensemble selection originally proposed rich caruana co-authors cornell university http //portal.acm.org/citation.cfm id=1015432 following pseudocode demonstrates basic idea ensemble selection 0 split data two parts build set hillclimb set 1 start empty ensemble 2 add ensemble model trained “ build ” set library maximizes ensemble ’ performance error metric auc contest “ hillclimb ” validation set 3 repeat step 2 ﬁxed number iterations models used 4 return ensemble nested set ensembles maximum performance hillclimb validation set model library used ensemble selection system adaboost logitboost realadaboost decisiontable rotationforest bayesnet naivebayes 7 algorithms different parameters total 28 base classifiers building set hillclimb set ensemble selection data points year 2007 used “ build set ” data points year 2008 used “ hillclimb set ” data points year 2007/01/01 2008/04/30 used “ build set ” data points year 2008/04/30 used “ hillclimb set ” setups worked well ensemble selection approach summary final system approach consists three main components data points 2007 training 2008 hillclimbing ensemble selection num bags 10 hillclimb iterations size model library total 352 features learderboard auc 0.956x best final test set auc 0.961x submission 20 end competition following features added approach feature set number missing values number non-missing values missing value rate transform “ contract.value.band ” numeric values average contract value rfcd.code mean sum max min standard deviation per application based rfcd.pct mean sum max min std per application based seo.code mean sum max min std per application based seo.pct mean sum max min std per application based successful.grant mean sum max min std per application based unsuccessful.grant mean sum max min std per application based successful.grant mean average per application based successful.grant sum average per application based features first three applicants features unsuccessful.grant success rate applicant 1 applicant 2 applicant 3 per application based success rate applicants per application based mean max std success rates applicants per application based number publications mean sum max min std per application based except frequency counting described approach “ row-based per-application-based ” statistical features gradually introduced system competition thought compared “ time based/column based features ” “ row-based ” statistical features would reduce chance overfitting also following algorithms different/diverse parameter settings gradually added model library competition randomforest racedincrementallogitboost bagging trees adtree linear regression randomcommittee random trees dagging j48 approach b rotation forest feature set approach tried using rotation forest http //www.computer.org/portal/web/csdl/doi/10.1109/tpami.2006.211 following setup base classifier m5p model tree weka default j48 rotation method random projection gaussian distribution weka default pca rotation forest classifier trained data points 2007 2008 feature set approach results leaderboard auc 0.947x final test set auc 0.962x averaging two approaches could improve final test set auc 0.963x tools used software/tools used modelling data analysis weka 3.7.1 used modelling improved version ensemble selection algorithm matlab sas used data visualization statistical analysis java used main programming language project experiments done home pc amd 6-core 16g ram windows system originally published blog.kaggle.com february 22 2011 quan sun finishing second place predict grant applications originally published kaggle blog medium people continuing conversation highlighting responding story,Machine Learning
using applied statistics expand human knowledge,background includes working scientific projects data guy positions responsible establishing valid data collection procedures collecting usable data statistically analyzing presenting results post describe excitement statistician helping expand limits human knowledge learned … post using applied statistics expand human knowledge appeared first statistics jim,Machine Learning
grant sanderson 3blue1brown beauty mathematics,grant sanderson math educator creator 3blue1brown popular youtube channel uses programmatically-animated visualizations explain concepts linear algebra calculus fields mathematics conversation part artificial intelligence podcast would like get information podcast go https //lexfridman.com/ai connect lexfridman twitter linkedin facebook medium youtube watch video versions conversations enjoy podcast please rate 5 stars apple podcasts follow spotify support patreon episode presented cash,Machine Learning
jim gates supersymmetry string theory proving einstein right,jim gates james gates jr. theoretical physicist professor brown university working supersymmetry supergravity superstring theory served former president obama ’ council advisors science technology co-author new book titled proving einstein right scientists set prove einstein ’ theory relativity conversation part artificial intelligence podcast would like get information podcast go https //lexfridman.com/ai connect lexfridman twitter linkedin facebook medium youtube watch video versions,Machine Learning
72 – scott aaronson quantum computing,scott aaronson professor ut austin director quantum information center previously professor mit research interests center around capabilities limits quantum computers computational complexity theory generally conversation part artificial intelligence podcast would like get information podcast go https //lexfridman.com/ai connect lexfridman twitter linkedin facebook medium youtube watch video versions conversations enjoy podcast please rate 5 stars apple podcasts follow spotify support,Machine Learning
intuition implications complex derivative,get free access 2500 documentaries curiositystream https //curiositystream.thld.co/zachstarnov3 use code `` zachstar '' sign stemerch store https //stemerch.com/ support channel https //www.patreon.com/zachstar paypal one time donation https //www.paypal.me/zachstaryt graphing software https //www.desmos.com/calculator see video complex mapping files https //www.youtube.com/watch v=jgdwdwfs9fm join channel get access perks https //www.youtube.com/channel/ucpcsacbqs-sjevfk_hmfy9w/join ►follow instagram https //www.instagram.com/zachstar/ twitter https //twitter.com/imzachstar check spanish channel https //www.youtube.com/channel/ucnknu2xqblaspj6ckc8vtpa ►my setup space pictures https //amzn.to/2cc4kqj magnetic floating globe https //amzn.to/2vgpdn0 camera https //amzn.to/2rivyu5 mic https //amzn.to/35bkiri tripod https //amzn.to/2rgmtnl equilibrium tube https //amzn.to/2sowdrh ►check amazon store https //www.amazon.com/shop/zachstar,Machine Learning
best nlp competitions kaggle learn,video talk nlp competitions kaggle find best learn full list discussed video https //www.kaggle.com/c/predict-closed-questions-on-stack-overflow/ https //www.kaggle.com/c/stumbleupon https //www.kaggle.com/c/crowdflower-search-relevance/ https //www.kaggle.com/c/dato-native/ https //www.kaggle.com/c/home-depot-product-search-relevance https //www.kaggle.com/c/avito-duplicate-ads-detection/ https //www.kaggle.com/c/quora-question-pairs https //www.kaggle.com/c/quora-insincere-questions-classification https //www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge https //www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification https //www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification https //www.kaggle.com/c/tensorflow2-question-answering https //www.kaggle.com/c/tweet-sentiment-extraction please subscribe like video help keep motivated make awesome videos like one buy book approaching almost machine learning problem please visit https //bit.ly/buyaaml follow twitter https //twitter.com/abhi1thakur linkedin https //www.linkedin.com/in/abhi1thakur/ kaggle https //kaggle.com/abhishek instagram https //instagram.com/abhi4ml,Machine Learning
cristos goodrow youtube algorithm,cristos goodrow vp engineering google head search discovery youtube aka youtube algorithm conversation part artificial intelligence podcast would like get information podcast go https //lexfridman.com/ai connect lexfridman twitter linkedin facebook medium youtube watch video versions conversations enjoy podcast please rate 5 stars apple podcasts follow spotify support patreon episode presented cash app download app store google play use code “ lexpodcast ” ’,Machine Learning
p big sleep text-to-image generation using biggan openai 's clip via google colab notebook twitter user adverb,https //twitter.com/advadnoun/status/1351038053033406468 big sleep 's notebook generating images using clip guide biggan 's much unstable prototype 's also fair place start 'll likely update time goes colab.research.google.com/drive/1nccex2mbikoslad_o7iu7na9uskkn5wr usp=sharing change text notebook parameters section desired text first output image usually seems poor match desired text second output image often seems decent match desired text system non-deterministic words different runs using inputs seemingly usually result different outputs steps follow want generate different image colab instance click menu item runtime-lilili,Machine Learning
p machine learning flow-based visual coding environment using ryven,hello everyone recently worked creating fork visual coding tool python implements machine learning algorithms scikit-learn called ryven managed use create solution titanic problem kaggle wrote fairly detailed notebook working process goal make easier use data science machine learning people limited coding experience think machine learning could simplified made accessible way would really appreciate feedback tool concept especially early stage project would like test ryven available https //github.com/frecklebars/ryven kaggle notebook solving titanic problem https //www.kaggle.com/frecklebars/ml-flow-based-visual-coding-using-ryven questions comments suggestions project please reach either discussion thread directly message tweeter /frecklebars thank ​ edit correcting misspelling link comments,Machine Learning
83 – nick bostrom simulation superintelligence,nick bostrom philosopher university oxford director future humanity institute worked fascinating important ideas existential risks simulation hypothesis human enhancement ethics risks superintelligent ai systems including book superintelligence see talking nick multiple times podcast many hours time start somewhere support podcast signing sponsors – cash app – use code “ lexpodcast ” download – cash app app store https //apple.co/2spruhe – cash app google play https //bit.ly/2mlvp5w episode links nick ’,Machine Learning
michael littman reinforcement learning future ai lex fridman podcast 144,michael littman computer scientist brown university please support podcast checking sponsors simplisafe https //simplisafe.com/lex use code lex get free security camera expressvpn https //expressvpn.com/lexpod use code lexpod get 3 months free masterclass https //masterclass.com/lex get 2 price 1 betterhelp https //betterhelp.com/lex get 10 episode links michael 's twitter https //twitter.com/mlittmancs michael 's website https //www.littmania.com/ michael 's youtube https //www.youtube.com/user/mlittman podcast info podcast website https //lexfridman.com/podcast apple podcasts https //apple.co/2lwqzir spotify https //spoti.fi/2newcf8 rss https //lexfridman.com/feed/podcast/ full episodes playlist https //www.youtube.com/playlist list=plraxtmerzgodp_8gztsuki9nrranbkkp4 clips playlist https //www.youtube.com/playlist list=plraxtmerzgoecifp3cbcieelojeitor41 outline 0:00 introduction 2:30 robot frank 4:50 music 8:01 starring turbotax commercial 18:14 existential risks ai 36:36 reinforcement learning 1:02:24 alphago david silver 1:12:03 neural networks achieve agi 1:24:30 bitter lesson 1:37:20 driving require theory mind 1:46:46 book recommendations 1:52:08 meaning life connect subscribe youtube channel twitter https //twitter.com/lexfridman linkedin https //www.linkedin.com/in/lexfridman facebook https //www.facebook.com/lexfridmanpage instagram https //www.instagram.com/lexfridman medium https //medium.com/ lexfridman support patreon https //www.patreon.com/lexfridman,Machine Learning
directions ml ai adaptive experiment design caltech professor yisong yue,experiment design hallmark virtually research disciplines many settings one important challenge automatically design experiments large action/design spaces furthermore also important procedure adaptive i.e. adapt outcomes previous experiments talk describe recent progress using data-driven algorithmic techniques adaptive experiment design also known active learning bayesian optimization machine learning community building upon gaussian process gp framework describe case studies personalized clinical therapy nanophotonic structure design motivated applications show incorporate real-world considerations safety preference elicitation multi-fidelity experiment design gp framework new algorithms theoretical guarantees empirical validation time permitting also briefly overview case studies well learn 2020-2021 directions ml automl automating algorithms virtual speaker series https //aka.ms/diml,Machine Learning
rohit prasad amazon alexa conversational ai,rohit prasad vice president head scientist amazon alexa one original creators conversation part artificial intelligence podcast would like get information podcast go https //lexfridman.com/ai connect lexfridman twitter linkedin facebook medium youtube watch video versions conversations enjoy podcast please rate 5 stars apple podcasts support patreon episode presented cash app download app store google play use code “ lexpodcast ” episode also supported,Machine Learning
welcome microsoft phd summit 2020 fireside chat johannes gehrke,day 1 december 1 2020 neel joshi principal research manager microsoft gonzalo ramos principal researcher microsoft give warm welcome phd students attendance third microsoft phd summit following welcome fireside chat johannes gehrke technical fellow managing director research microsoft redmond microsoft 's third phd summit two-day virtual workshop opportunity top phd students enhance skills build network discuss research within community peers notable microsoft researchers speakers neel joshi principal research manager microsoft gonzalo ramos principal researcher microsoft johannes gehrke technical fellow managing director research redmond information workshop found https //www.microsoft.com/en-us/research/event/phd-summit-2020/,Machine Learning
74 – michael i. jordan machine learning recommender systems future ai,"michael i. jordan professor berkeley one influential people history machine learning statistics artificial intelligence cited 170,000 times mentored many world-class researchers defining field ai today including andrew ng zoubin ghahramani ben taskar yoshua bengio episode links blog post artificial intelligence—the revolution ’ happened yet conversation part artificial intelligence podcast would like get information podcast go https //lexfridman.com/ai connect lexfridman twitter linkedin facebook medium youtube",Machine Learning
chris urmson self-driving cars aurora google cmu darpa,chris urmson cto google self-driving car team key engineer leader behind carnegie mellon autonomous vehicle entries darpa grand challenges winner darpa urban challenge today ceo aurora innovation autonomous vehicle software company started sterling anderson former director tesla autopilot drew bagnell uber ’ former autonomy perception lead conversation part artificial intelligence podcast would like get information podcast go https //lexfridman.com/ai connect lexfridman twitter linkedin,Machine Learning
swav unsupervised learning visual features contrasting cluster assignments mathilde caron,week dr. tim scarfe yannic lightspeed kicher sayak paul ayush takur interview mathilde caron facebook research fair discuss mathilde 's paper wrote collaborators `` swav unsupervised learning visual features contrasting cluster assignments '' https //arxiv.org/pdf/2006.09882.pdf paper latest unsupervised contrastive visual representations algorithm new data augmentation strategy also new online clustering strategy note authors ishan misra julien mairal priya goyal piotr bojanowski armand joulin sayak paul risingsayak https //www.linkedin.com/in/sayak-paul/ ayush thakur ayushthakur0 https //www.linkedin.com/in/ayush-thakur-731914149/ article wrote https //app.wandb.ai/authors/swav-tf/reports/unsupervised-visual-representation-learning-with-swav -- vmlldzoymjg3mzg 00:00:00 yannic probability challenge solve 00:01:29 intro topic tim 00:08:18 yannic take 00:09:33 intro show guests 00:11:29 swav elevator pitch 00:17:31 clustering approach general 00:21:17 sayak ayush 's article swav 00:23:49 optional transport problem sinkhorn-knopp algorithm 00:31:43 clustering natural approach 00:44:19 image augmentations 00:46:20 priors vs experience data 00:48:32 life fair 00:52:33 progress image augmentation 00:56:10 things go plan research 01:01:04 question architecture 01:01:43 swav results 01:06:26 reproducing matilde 's code 01:14:51 need whole dataset set clustering loss 01:16:40 self-supervised learning transfer learning 01:23:25 link attention mechanism 01:24:41 sayak final thought unsupervised better 01:25:56 outro abstract `` unsupervised image representations significantly reduced gap supervised pretraining notably recent achievements contrastive learning methods contrastive methods typically work online rely large number explicit pairwise feature comparisons computationally challenging paper propose online algorithm swav takes advantage contrastive methods without requiring compute pairwise comparisons specifically method simultaneously clusters data enforcing consistency cluster assignments produced different augmentations “ views ” image instead comparing features directly contrastive learning simply put use “ swapped ” prediction mechanism predict cluster assignment view representation another view method trained large small batches scale unlimited amounts data compared previous contrastive methods method memory efficient since require large memory bank special momentum network addition also propose new data augmentation strategy multi-crop uses mix views different resolutions place two full-resolution views without increasing memory compute requirements much validate findings achieving 75.3 top-1 accuracy imagenet resnet-50 well surpassing supervised pretraining considered transfer tasks '',Machine Learning
r flavr fast efficient technique video frame interpolation,paper highlights flow-free single-shot prediction approach video frame interpolation 384x faster current accurate approach 23x faster current fastest 8x interpolation paper video project https //preview.redd.it/2f4aa60vl1c61.png width=1110 format=png auto=webp s=d5140ffc6378ef74d7d4f142159d007c8c2359d6 abstract majority approaches solve problem video frame interpolation computing bidirectional optical flow adjacent frames video followed suitable warping algorithm generate output frames however methods relying optical flow often fail model occlusions complex non-linear motions directly video introduce additional bottlenecks unsuitable real time deployment overcome limitations propose flexible efficient architecture makes use 3d space-time convolutions enable end end learning inference task video frame interpolation method efficiently learns reason non-linear motions complex occlusions temporal abstractions resulting improved performance video interpolation requiring additional inputs form optical flow depth maps due simplicity proposed method improves inference speed 384x compared current accurate method 23x compared current fastest 8x interpolation addition evaluate model wide range challenging settings consistently demonstrate superior qualitative quantitative results compared current methods various popular benchmarks including vimeo-90k ucf101 davis adobe gopro finally demonstrate video frame interpolation serve useful self-supervised pretext task action recognition optical flow estimation motion magnification link comments,Machine Learning
038 professor kenneth stanley greatness planned,professor kenneth stanley currently research science manager openai san fransisco 've dreaming getting kenneth show since begininning machine learning street talk might recall first ever show enhanced poet paper course kenneth hands 's cited 16000 times popular paper 3k citations neat algorithm interests neuroevolution open-endedness nns artificial life ai invented concept novelty search clearly defined objective key idea tyranny objectives prevailing every aspect lives society indeed algorithms crucially objectives produce convergent behaviour thinking distract us discovering stepping stones lead greatness thinks monotonic objective obsession idea need continue improve benchmarks every year dangerous wrote detail recent book `` greatness planned '' main topic discussion show also cover ideas open endedness machine learning 00:00:00 intro kenneth 00:01:16 show structure disclaimer 00:04:16 passionate discussion 00:06:26 greatness cant planned tyranny objectives 00:14:40 chinese finger trap 00:16:28 perverse incentives feedback loops 00:18:17 deception 00:23:29 maze example 00:24:44 define curiosity interestingness 00:26:59 open endedness 00:33:01 icml 2019 yannic poet first mslst 00:36:17 evolutionary algorithms++ 00:43:18 poet first mlst 00:45:39 lesson gofai people 00:48:46 machine learning -- great stagnation 00:54:34 actual scientific successes usually luck odds -- biontech 00:56:21 picbreeder neat 01:10:47 tim applies ideas life runs mlst 01:14:58 keith skit ucf 01:15:13 main show kick 01:18:02 kenneth value serindipitous exploration much 01:24:10 scientific support keneths ideas normal life 01:27:12 drop objectives achieve oxymoron 01:33:13 isnt resource allocation exploration exploitation 01:39:06 objectives merely matter degree 01:42:38 allocate funds treasure hunting society 01:47:34 keen nose interesting voting dangerous 01:53:00 committees antithesis innovation 01:56:21 kenneth apply ideas real life 01:59:48 divergence vs interestingness vs novelty vs complexity 02:08:13 picbreeder 02:12:39 isnt everything novel sense 02:16:35 imagine selection pressure 02:18:31 innovation == environment exploitation 02:20:37 possible take shortcuts already knew innovations 02:21:11 go explore -- algorithm encode stepping stones 02:24:41 mean things interestingly different 02:26:11 behavioral characterization diversity measure broad interests 02:30:54 shaping objectives 02:32:49 ambitious objectives deception picbreeder analogy 02:35:59 exploration vs exploitation science vs engineering 02:43:18 schools thought ml could search lead agi 02:45:49 official ending,Machine Learning
explainability reasoning priors gpt-3,week dr. tim scarfe dr. keith duggar discuss explainability reasoning priors gpt-3 check christoph molnar 's book intepretability talk priors vs experience nns whether nns reasoning also cover articles gary marcus walid saba critiquing deep learning finish brief discussion chollet 's arc challenge intelligence paper 00:00:00 intro 00:01:17 explainability christoph molnars book intepretability 00:26:45 explainability feature visualisation 00:33:28 architecture cppns 00:36:10 invariance data parsimony priors experience manifolds 00:42:04 nns learn logical view modern ai walid saba article 00:47:10 core knowledge 00:55:33 priors vs experience 00:59:44 mathematical reasoning 01:01:56 gary marcus gpt-3 01:09:14 nns reason 01:18:05 chollet intelligence paper/arc challenge,Machine Learning
dags grow people trust data source 'll ask,blog post refresh talk james gave strata back 2017 recap 3-year-old conference talk well core ideas aged well ’ never actually put writing ’ learned new things meantime enjoy read full story,Machine Learning
rt ykilcher ⏰video psa⏰end week healthy dose new deep learning memes 🧙 collaboration infamous orvieto_antonio think best one far 🎉 watch enjoy https //youtu.be/hhzsa9z_abe science ai memes machinelearning technology,⏰video psa⏰end week healthy dose new deep learning memes 🧙 collaboration infamous orvieto_antonio think best one far 🎉 watch enjoy invidious.snopyta.org/hhzsa9z_abe science ai memes machinelearning technology,Machine Learning
139 – andrew huberman neuroscience optimal performance,andrew huberman neuroscientist stanford please support podcast checking sponsors – eight sleep https //www.eightsleep.com/lex use code lex get 200 – semrush https //www.semrush.com/partner/lex/ get free month guru – cash app https //cash.app/ use code lexpodcast get 10 episode links andrew ’ instagram https //www.instagram.com/hubermanlab andrew ’ wikipedia https //en.wikipedia.org/wiki/andrew_d._huberman andrew ’ website http //www.hubermanlab.com/ podcast info podcast website https //lexfridman.com/podcast apple podcasts https //apple.co/2lwqzir spotify https //spoti.fi/2newcf8 rss https //lexfridman.com/feed/podcast/ youtube full episodes https //youtube.com/lexfridman youtube clips https //youtube.com/lexclips support connect – check sponsors ’ best way support podcast – support patreon,Machine Learning
witnessed malpractices ml/cv research papers,doctoral student working field computer vision cv deep learning dl tier-2 university europe noticing alarmingly wrong practices within small group people let 's call group x around spans accross different labs/universities related papers submitted top tier ml/cv conferences cvpr/iccv/eccv/bmvc etc seeing pattern literally overstating results papers get peer review process people gourp x actually proper research come seemingly intutive ideas combining existing papers extremely hasty implementation algorithm never works practice sometimes implement small part however utilize significant amount time upto 3-4 months writing paper great care nice story good english paper sets storyline makes claims much larger magnitude support claims overstate significant amount result including manipulated diagrams/figures looks appealing peer review process obvious reasons open-source codes data people exploit fact computer vision applied domain mostly realtively easy come models/algorithms without theoretical/mathy works much specifically choose topics reviewers less suspicious overstated results worrying thing good amount papers getting accepted top venues like cvpr/iccv/eccv/bmvc sometime also orals really bothers could n't help sharing everyone talked people group x understood people interested research sole purpose phd get job company handsome salary banners like `` need first-author papers top conferences '' ml/cv job-listings big companies whats encouraging wrong practices also managerial level supervisors/pis involved publications indirectly supportive practices question obvious tackle problem others also see similar practices ps 1 generalizing everyone observations based people personally interacted aware fact large number people actually care research thankful keeping community alive high quality research ps 2 disclose identity including mine obvious reasons link comments,Machine Learning
poncelet 's porism numberphile,featuring daniel litt check brilliant get 20 premium service https //brilliant.org/numberphile sponsor links stuff full description ↓↓↓ daniel litt https //www.daniellitt.com/ see daniel previously numberphile https //youtu.be/eyfpsaxgaki numberphile supported mathematical sciences research institute msri http //bit.ly/msrinumberphile also supported science sandbox simons foundation initiative dedicated engaging everyone process science https //www.simonsfoundation.org/outreach/science-sandbox/ support math america https //www.mathforamerica.org/ numberphile website http //www.numberphile.com/ numberphile facebook http //www.facebook.com/numberphile numberphile tweets https //twitter.com/numberphile subscribe http //bit.ly/numberphile_sub video brady haran pete mcpartlan patreon http //www.patreon.com/numberphile numberphile t-shirts merch https //teespring.com/stores/numberphile brady 's videos subreddit http //www.reddit.com/r/bradyharan/ brady 's latest videos across channels http //www.bradyharanblog.com/ sign occasional emails http //eepurl.com/ydjl9,Machine Learning
scikit-learn library machine learning nutshell,read full story,Machine Learning
n learnings deployments synthetic data,https //youtu.be/ydacc20gyu0 dr. nathan kundtz presentation nga learnings rendered.ai common applications framework synthetic data physics based synthetic data simulation libraries represent part solution link comments,Machine Learning
lex solo 1 – seven levels coronavirus impact,coronavirus pandemic global tragedy also moment unites us reveals strength community human capacity compassionate work hard face danger episode describe might 7 levels attack society fight back level describe pain challenge hope positive future side fill one-question survey whether want see solo episodes like,Machine Learning
visualize network activation non-classification models,seen lot great work comes visualizing attention maps activation maps heat maps cnn based architectures eg https //github.com/raghakot/keras-vis however research around image synthesis wondering anyone experience visualizing networks comes non classification task link comments,Machine Learning
rt karpathy https //berthub.eu/articles/posts/reverse-engineering-source-code-of-the-biontech-pfizer-vaccine/ great job breaking pfizer vaccine,berthub.eu/articles/posts/re… great job breaking pfizer vaccine,Machine Learning
143 – john clarke art fighting pursuit excellence,john clarke bjj black belt mma coach please support podcast checking sponsors – theragun https //theragun.com/lex get 30 day trial – magic spoon https //magicspoon.com/lex use code lex get free shipping – eight sleep https //www.eightsleep.com/lex use code lex get 200 – cash app https //cash.app/ use code lexpodcast get 10 episode links broadway jiu jitsu website https //www.broadwayjiujitsu.com broadway jiu jitsu instagram https //www.instagram.com/broadwayjiujitsu please allow podcast https //podcasts.apple.com/us/podcast/please-allow-me/id1531735873 please allow instagram https //www.instagram.com/please_allow_me_podcast/ podcast info podcast website https //lexfridman.com/podcast apple podcasts https //apple.co/2lwqzir spotify https //spoti.fi/2newcf8 rss https //lexfridman.com/feed/podcast/ youtube full episodes https //youtube.com/lexfridman,Machine Learning
hypothesis testing null hypothesis,one basic concepts statistics hypothesis testing something called null hypothesis video breaks concepts easy understand pieces understand motivation uses time 're done video hypothesis testing null hypothesis clearly explained note 'd like learn alternative hypothesis check ... 'd like learn p-values check ... p-values interpret https //youtu.be/vemztem63gy calculate p-values https //youtu.be/jqc3yx0-q9e ⭐ note code use kite free ai-powered coding assistant help code faster smarter kite plugin integrates top editors ides give smart completions documentation ’ typing love https //www.kite.com/get-kite/ utm_medium=referral utm_source=youtube utm_campaign=statquest utm_content=description-only complete index statquest videos check https //statquest.org/video-index/ 'd like support statquest please consider ... patreon https //www.patreon.com/statquest ... ... youtube membership https //www.youtube.com/channel/uctyluttgs3k1fg4y5tahlbw/join ... cool statquest t-shirt sweatshirt usa/europe https //teespring.com/stores/statquest everywhere https //www.redbubble.com/people/starmer/works/40421224-statquest-double-bam asc=u p=t-shirt ... buying one two songs go large get whole album https //joshuastarmer.bandcamp.com/ ... donating statquest https //www.paypal.me/statquest lastly want keep research create new statquests follow twitter https //twitter.com/joshuastarmer 0:00 awesome song introduction 0:22 background 2:32 first hypothesis 4:09 rejecting hypothesis 4:40 second hypothesis 7:14 failing reject hypothesis 8:21 rejecting vs failing reject 9:01 motivation null hypothesis 9:51 null hypothesis 13:44 next steps statquest nullhypothesis,Machine Learning
social dilemma part 2,week machine learning street talk dr. tim scarfe dr. keith duggar alex stenlake yannic kilcher conversation founder principal researcher montreal ai ethics institute -- abhishek gupta cover several topics social dilemma film ai ethics general 00:00:00 introduction 00:03:57 overcome weaknesses 00:14:30 threat landscape blind spots 00:18:35 differential reality vs universal shaping 00:24:21 shared reality incentives tools 00:32:01 transparency knowledge avoid pathology 00:40:09 federated informed autonomy 00:49:48 diversity metric inclusion strategy 00:59:58 locally aligned pockets stabilize global diversity 01:10:58 making inclusion easier tools 01:23:35 enabling community feedback 01:26:16 open source algorithms 01:33:02 n+1 cost inclusion 01:38:08 broader impact statement https //atg-abhishek.github.io https //www.linkedin.com/in/abhishekguptamcgill/,Machine Learning
rt ykilcher openai dall·e fighter jet mind ✈️ ▶️full video https //youtu.be/c7d5ezkht6a 📜source post https //openai.com/blog/dall-e/ dalle openai gpt3 ai deeplearning machinelearning twominutepapers whatatimetobealive,openai dall·e fighter jet mind ✈️ ▶️full video invidious.snopyta.org/c7d5ezkht6a 📜source post openai.com/blog/dall-e/ dalle openai gpt3 ai deeplearning machinelearning twominutepapers whatatimetobealive,Machine Learning
image,nan,Machine Learning
openai gpt-3 language models few-shot learners,episode machine learning street talk tim scarfe yannic kilcher connor shorten discuss takeaways openai ’ gpt-3 language model help microsoft ’ zero-2 deepspeed optimiser openai trained 175 billion parameter autoregressive language model paper demonstrates self-supervised language modelling scale perform many downstream tasks without fine-tuning 00:00:00 intro 00:00:54 zero1+2 model data parallelism connor 00:03:17 recent history nlp tim 00:06:04 yannic `` light-speed '' kilcher 's brief overview gpt-3 00:14:25 reviewing yannic 's yt comments gpt-3 video tim 00:20:26 main show intro 00:23:03 gpt-3 reasoning 00:28:15 architecture discussion autoregressive gpt vs denoising autoencoder bert 00:36:18 utility gpt-3 industry 00:43:03 gpt-3 math reasoning/system 1/system 2 00:51:03 generalisation 00:56:48 esoterics language models 00:58:46 architectural trade-offs 01:07:37 memorization machines intepretability 01:17:16 nearest neighbour probes watermarks 01:20:03 youtube comments gpt-3 video 01:21:50 gpt-3 news article generation issue 01:27:36 sampling data language models bias fairness politics 01:51:12 outro paradigms task adaptation divided zero one shot learning zero-shot learning extreme case expect language model perform task sentiment classification extractive question answering without additional supervision one few-shot learning provide examples model however gpt-3s definition diverges bit conventional literature gpt-3 provides one few-shot examples form “ in-context learning ” instead fine-tuning model examples model use input infer downstream task example gpt-3 transformer input sequence 2048 tokens demonstrations task yelp sentiment reviews would fit input sequence well new review thanks watching please subscribe paper links gpt-3 https //arxiv.org/abs/2005.14165 zero https //arxiv.org/abs/1910.02054 zero blog post https //www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/ zero-2 blog post https //www.microsoft.com/en-us/research/blog/zero-2-deepspeed-shattering-barriers-of-deep-learning-speed-scale/ ocid=msr_blog_deepspeed2_build_tw machinelearning naturallanguageprocessing deeplearning gpt3,Machine Learning
n questions john leonard- mit csail,hello guys ieee ras robotics podcast going john leonard- mit csail research focused marine robotics visual slam let us know questions also send directly https //docs.google.com/forms/d/e/1faipqlsfzvwpho7xpz1ymfg1pzrw7007gztz9-xp_euja4u44myxh4a/viewform vc=0 c=0 w=1 flr=0 gxids=7628 link comments,Machine Learning
code adam gradient descent optimization scratch,"gradient descent optimization algorithm follows negative gradient objective function order locate minimum function limitation gradient descent single step size learning rate used input variables extensions gradient descent like adagrad rmsprop update algorithm use separate step size input variable may result step size rapidly decreases small values adaptive movement estimation algorithm adam short extension gradient descent natural successor techniques like adagrad rmsprop automatically adapts learning rate input variable objective function smooths search process using exponentially decreasing moving average gradient make updates variables tutorial discover develop gradient descent adam optimization algorithm scratch completing tutorial know gradient descent optimization algorithm uses gradient objective function navigate search space gradient descent updated use automatically adaptive step size input variable using decaying average partial derivatives called adam implement adam optimization algorithm scratch apply objective function evaluate results let ’ get started gradient descent optimization adam scratch photo graham rights reserved tutorial overview tutorial divided three parts gradient descent adam optimization algorithm gradient descent adam two-dimensional test problem gradient descent optimization adam visualization adam gradient descent gradient descent optimization algorithm technically referred first-order optimization algorithm explicitly makes use first-order derivative target objective function first-order methods rely gradient information help direct search minimum … — page 69 algorithms optimization 2019 first-order derivative simply “ derivative ” rate change slope target function specific point e.g specific input target function takes multiple input variables referred multivariate function input variables thought vector turn derivative multivariate target function may also taken vector referred generally gradient gradient first-order derivative multivariate objective function derivative gradient points direction steepest ascent target function specific input gradient descent refers minimization optimization algorithm follows negative gradient downhill target function locate minimum function gradient descent algorithm requires target function optimized derivative function objective function target function f returns score given set inputs derivative function f gives derivative target function given set inputs gradient descent algorithm requires starting point x problem randomly selected point input space derivative calculated step taken input space expected result downhill movement target function assuming minimizing target function downhill movement made first calculating far move input space calculated step size called alpha learning rate multiplied gradient subtracted current point ensuring move gradient target function x x t-1 – step_size f x t-1 steeper objective function given point larger magnitude gradient turn larger step taken search space size step taken scaled using step size hyperparameter step size alpha hyperparameter controls far move search space gradient iteration algorithm step size small movement search space small search take long time step size large search may bounce around search space skip optima familiar gradient descent optimization algorithm let ’ take look adam algorithm adam optimization algorithm adaptive movement estimation algorithm adam short extension gradient descent optimization algorithm algorithm described 2014 paper diederik kingma jimmy lei ba titled “ adam method stochastic optimization. ” adam designed accelerate optimization process e.g decrease number function evaluations required reach optima improve capability optimization algorithm e.g result better final result achieved calculating step size input parameter optimized importantly step size automatically adapted throughput search process based gradients partial derivatives encountered variable propose adam method efficient stochastic optimization requires first-order gradients little memory requirement method computes individual adaptive learning rates different parameters estimates first second moments gradients name adam derived adaptive moment estimation — adam method stochastic optimization involves maintaining first second moment gradient e.g exponentially decaying mean gradient first moment variance second moment input variable moving averages estimates 1st moment mean 2nd raw moment uncentered variance gradient — adam method stochastic optimization let ’ step element algorithm first must maintain moment vector exponentially weighted infinity norm parameter optimized part search referred v really greek letter nu respectively initialized 0.0 start search 0 v 0 algorithm executed iteratively time starting t=1 iteration involves calculating new set parameter values x e.g going x t-1 x perhaps easy understand algorithm focus updating one parameter generalizes updating parameters via vector operations first gradient partial derivatives calculated current time step g f x t-1 next first moment updated using gradient hyperparameter beta1 beta1 t-1 1 – beta1 g second moment updated using squared gradient hyperparameter beta2 v beta2 v t-1 1 – beta2 g ^2 first second moments biased initialized zero values … moving averages initialized vectors 0 ’ leading moment estimates biased towards zero especially initial timesteps especially decay rates small i.e betas close 1 good news initialization bias easily counteracted resulting bias-corrected estimates … — adam method stochastic optimization next first second moments bias-corrected starring first moment mhat 1 – beta1 second moment vhat v 1 – beta2 note beta1 beta2 refer beta1 beta2 hyperparameters decayed schedule iterations algorithm static decay schedule used although paper recommend following beta1 beta1^t beta2 beta2^t finally calculate value parameter iteration x x t-1 – alpha mhat sqrt vhat eps alpha step size hyperparameter eps small value epsilon 1e-8 ensures encounter divide zero error sqrt square root function note efficient reordering update rule listed paper used alpha alpha sqrt 1 – beta2 1 – beta1 x x t-1 – alpha sqrt v eps review three hyperparameters algorithm alpha initial step size learning rate typical value 0.001. beta1 decay factor first momentum typical value 0.9. beta2 decay factor infinity norm typical value 0.999 ’ full derivation adam algorithm context adam algorithm recommend reading paper adam method stochastic optimization next let ’ look might implement algorithm scratch python gradient descent adam section explore implement gradient descent optimization algorithm adam two-dimensional test problem first let ’ define optimization function use simple two-dimensional function squares input dimension define range valid inputs -1.0 1.0 objective function implements function objective function def objective x return x 2.0 2.0 create three-dimensional plot dataset get feeling curvature response surface complete example plotting objective function listed 3d plot test function numpy import arange numpy import meshgrid matplotlib import pyplot objective function def objective x return x 2.0 2.0 define range input r_min r_max -1.0 1.0 sample input range uniformly 0.1 increments xaxis arange r_min r_max 0.1 yaxis arange r_min r_max 0.1 create mesh axis x meshgrid xaxis yaxis compute targets results objective x create surface plot jet color scheme figure pyplot.figure axis figure.gca projection='3d axis.plot_surface x results cmap='jet show plot pyplot.show running example creates three-dimensional surface plot objective function see familiar bowl shape global minima f 0 0 0 three-dimensional plot test objective function also create two-dimensional plot function helpful later want plot progress search example creates contour plot objective function contour plot test function numpy import asarray numpy import arange numpy import meshgrid matplotlib import pyplot objective function def objective x return x 2.0 2.0 define range input bounds asarray -1.0 1.0 -1.0 1.0 sample input range uniformly 0.1 increments xaxis arange bounds 0,0 bounds 0,1 0.1 yaxis arange bounds 1,0 bounds 1,1 0.1 create mesh axis x meshgrid xaxis yaxis compute targets results objective x create filled contour plot 50 levels jet color scheme pyplot.contourf x results levels=50 cmap='jet show plot pyplot.show running example creates two-dimensional contour plot objective function see bowl shape compressed contours shown color gradient use plot plot specific points explored progress search two-dimensional contour plot test objective function test objective function let ’ look might implement adam optimization algorithm gradient descent optimization adam apply gradient descent adam test problem first need function calculates derivative function f x x^2 f x x 2 derivative x^2 x 2 dimension derivative function implements derivative objective function def derivative x return asarray x 2.0 2.0 next implement gradient descent optimization first select random point bounds problem starting point search assumes array defines bounds search one row dimension first column defines minimum second column defines maximum dimension ... generate initial point x bounds 0 rand len bounds bounds 1 bounds 0 score objective x 0 x 1 next need initialize first second moments zero ... initialize first second moments 0.0 range bounds.shape 0 v 0.0 range bounds.shape 0 run fixed number iterations algorithm defined “ n_iter ” hyperparameter ... run iterations gradient descent range n_iter ... first step calculate gradient current solution using derivative function ... calculate gradient gradient derivative solution 0 solution 1 first step calculate derivative current set parameters ... calculate gradient g g derivative x 0 x 1 next need perform adam update calculations perform calculations one variable time using imperative programming style readability practice recommend using numpy vector operations efficiency ... build solution one variable time range x.shape 0 ... first need calculate moment ... beta1 t-1 1 beta1 g beta1 1.0 beta1 g second moment ... v beta2 v t-1 1 beta2 g ^2 v beta2 v 1.0 beta2 g 2 bias correction first second moments ... mhat 1 beta1 mhat 1.0 beta1 t+1 vhat v 1 beta2 vhat v 1.0 beta2 t+1 finally updated variable value ... x x t-1 alpha mhat sqrt vhat eps x x alpha mhat sqrt vhat eps repeated parameter optimized end iteration evaluate new parameter values report performance search ... evaluate candidate point score objective x 0 x 1 report progress print '/preem/empre class= '' urvanov-syntax-highlighter-plain-tag ''",Machine Learning
tomaso poggio brains minds machines,"tomaso poggio professor mit director center brains minds machines cited 100,000 times work profound impact understanding nature intelligence biological neural networks artificial ones advisor many highly-impactful researchers entrepreneurs ai including demis hassabis deepmind amnon shashua mobileeye christof koch allen institute brain science video version available youtube would like get information podcast go https //lexfridman.com/ai connect",Machine Learning
serp analysis google search console+python,read full story,Machine Learning
big data research ethics human rights collide mary l. gray microsoft phd summit 2020,day 2 december 2 2020 meredith ringel morris welcomes back attendees microsoft phd summit day 2 mary l. gray sr principal researcher microsoft gives talk big data research ethics human rights microsoft 's third phd summit two-day virtual workshop opportunity top phd students enhance skills build network discuss research within community peers notable microsoft researchers speakers meredith ringel morris sr principal researcher research area manager microsoft mary l. gray sr principal researcher microsoft information workshop found https //www.microsoft.com/en-us/research/event/phd-summit-2020/,Machine Learning
144 – michael littman reinforcement learning future ai,michael littman computer scientist brown university please support podcast checking sponsors – simplisafe https //simplisafe.com/lex use code lex get free security camera – expressvpn https //expressvpn.com/lexpod use code lexpod get 3 months free – masterclass https //masterclass.com/lex get 2 price 1 – betterhelp https //betterhelp.com/lex get 10 episode links michael ’ twitter https //twitter.com/mlittmancs michael ’ website https //www.littmania.com/ michael ’ youtube https //www.youtube.com/user/mlittman podcast info podcast website https //lexfridman.com/podcast apple podcasts https //apple.co/2lwqzir spotify https //spoti.fi/2newcf8 rss https //lexfridman.com/feed/podcast/ youtube full episodes https //youtube.com/lexfridman youtube clips https //youtube.com/lexclips support connect – check sponsors ’,Machine Learning
's conversation max tegmark tegmark first chat episode 1 podcast 's back talk intersection machine learning physics also avoid near-term long-term existential threats ai https //www.youtube.com/watch v=rl4j4kpwngm,'s conversation max tegmark tegmark first chat episode 1 podcast 's back talk intersection machine learning physics also avoid near-term long-term existential threats ai invidious.snopyta.org/watch v=rl4j4kpw…,Machine Learning
us census data contest,video describes two excellent websites accessing u.s. census data along competition 5k prizes `` let 's make count '' initiative effort funded national science foundation 's west big data innovation hub open u.s. high school students teachers ending january 1st 2021 join competition https //bit.ly/letsmakeitcount2020 lots links video https //censusreporter.org/ https //data.census.gov/ https //www.letsmakeitcount.org/ ... last least ... https //statquest.org/ ⭐ note code use kite free ai-powered coding assistant help code faster smarter kite plugin integrates top editors ides give smart completions documentation ’ typing love https //www.kite.com/get-kite/ utm_medium=referral utm_source=youtube utm_campaign=statquest utm_content=description-only complete index statquest videos check https //statquest.org/video-index/ 'd like support statquest please consider ... patreon https //www.patreon.com/statquest ... ... youtube membership https //www.youtube.com/channel/uctyluttgs3k1fg4y5tahlbw/join ... cool statquest t-shirt sweatshirt usa/europe https //teespring.com/stores/statquest everywhere https //www.redbubble.com/people/starmer/works/40421224-statquest-double-bam asc=u p=t-shirt ... buying one two songs go large get whole album https //joshuastarmer.bandcamp.com/ ... donating statquest https //www.paypal.me/statquest lastly want keep research create new statquests follow twitter https //twitter.com/joshuastarmer 0:00 awesome song introduction 1:13 censusreporter.org 2:56 data.census.gov 4:57 submitting story competition statquest uscensus letsmakeitcount,Machine Learning
vladimir vapnik statistical learning,"vladimir vapnik co-inventor support vector machines support vector clustering vc theory many foundational ideas statistical learning work cited 170,000 times interesting ideas artificial intelligence nature learning especially limits current approaches open problems field video version available youtube would like get information podcast go https //lexfridman.com/ai connect lexfridman twitter linkedin facebook youtube watch video versions conversations",Machine Learning
r evolving reinforcement learning algorithms,link comments,Machine Learning
yuanchen finishing third melbourne university competition,yuanchen senior engineer mcafee lab working large data analysis classification modeling network security problems method many thanks kaggle setting competition congratulations winners enjoyed learned lot working challenging data reading winners ’ posts sorry ’ find free time last week write report data came lot categorical features high number values beginning removed useless features weka.filters.unsupervised.attribute.removeuseless -m 99.0 removed features almost 100 missing values tried transform categorical features group binary features yes specific value also generated 4 quarter features 12 month features startdate generated binary indicator features missing values binary features date-based features indicator features well numerical features simply filling missing values mean fed r randomforest classifier rfe got 94.9x leaderboard kept tuning along way accuracy improved started suspect information loss process feature transformation feature selection tried build classifiers directly categorical features without transforming binary features simple frequency based pre-filtering applied raw categorical feature values presented less 10 instances data combined specific common value “ -1 ” however r randomforest accept categorical feature 32 values split categorical feature “ sub features ” 32 values way split values different sub features sorting values information gain first top 31 values assigned sub feature 1 next 31 values assigned sub feature 2 feature transformation strategy got 94.6x leaderboard next one tried simply combining top features two methods randomforest classifiers combined feature sets improve leaderboard roc 95.1x-95.3x depending instances used training best classifiers generated training instances 0606 instances 0612 instances 0706 finally observed prediction results classifiers different enough hence worth make major voting got best leaderboard auc 95.555 generalized 75 test instances final auc 96.1051 originally published blog.kaggle.com march 1 2011 yuanchen finishing third melbourne university competition originally published kaggle blog medium people continuing conversation highlighting responding story,Machine Learning
rt ykilcher week connossor explore history practical utility unique capabilities bayesian methods also discuss computational difficulties inherent bayesian methods https //www.youtube.com/watch v=rq31mhw84o0 tim ecsquendor alex keith,week connossor explore history practical utility unique capabilities bayesian methods also discuss computational difficulties inherent bayesian methods invidious.snopyta.org/watch v=rq31mhw8… tim ecsquendor alex keith,Machine Learning
ai-driven features highest potential enterprise development,enterprise players across industries eager optimization improvement business processes administration customer service marketing sales recruiting others today ai-driven software cover common enterprise needs like data security data processing resource optimization brand awareness forrester reported ai also able improve customer service quality existing products increase revenue streams customer lifetime value read full story,Machine Learning
use auroc ovr vs. auroc ovo,multiclass classification tasks scikit-learn 's documentation auroc score states 2 versions auroc score one-vs-rest ovr one-vs-one ovo version controlled parameter multi_class https //scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html anyone know particular cases would use ovr opposed ovo general preference given one link comments,Machine Learning
integrate ai data mapping drive business decision making,prior analyzing large chunks data enterprises must homogenize way makes available accessible decision-makers presently data comes many sources every particular source define similar data points different ways say example state field source system may exhibit “ illinois ” destination keeps “ il ” read full story,Machine Learning
p output gan match text 's input via clipping gradient clip openai 's new model,https //preview.redd.it/3o45ur0apdc61.png width=600 format=png auto=webp s=8e589725ebf5b9f2c7aa279944c73688f85e4bc1 ​ tl dr 's clean easy understand repo allows one make output gan match input text uses clip transfer layer https //github.com/cloneofsimo/clipping-clip-to-gan recently openai proposed clip multimodal transformer based model perform incredible wide-domain zero shot tasks read openai 's blog 's paper hand dall-e generative model also released date currently not-released probably end like gpt-3 recently ryan murdock proposed good feature visualization generate image matches text mainly used siren set parameters optimize used autograd learn best parameters generates image matches given set images general could done kind deterministic generative model gan ae vqvae etc think vqvae would really good dall-e used gradient ascent part still something implement repository contains test.py general takes generative model learnable latent vector find image matching input text link comments,Machine Learning
039 lena voita nlp,ena voita ph.d. student university edinburgh university amsterdam previously research scientist yandex research worked closely yandex translate team still teaches nlp yandex school data analysis created exciting new nlp course website lena-voita.github.io folks need check one well presented blogs ever seen discusses research easily digestable manner lena investigating many fascinating topics machine learning nlp today going talk three papers corresponding blog articles source target contributions nmt predictions -- talks influential dichotomy source prefix neural translation models https //arxiv.org/pdf/2010.10907.pdf https //lena-voita.github.io/posts/source_target_contributions_to_nmt.html information-theoretic probing mdl -- lena proposes technique evaluating model using minimum description length kolmogorov complexity labels given representations rather something basic like accuracy https //arxiv.org/pdf/2003.12298.pdf https //lena-voita.github.io/posts/mdl_probes.html evolution representations transformer lena investigates evolution representations individual tokens transformers -- trained different training objectives mt lm mlm https //arxiv.org/abs/1909.01380 https //lena-voita.github.io/posts/emnlp19_evolution.html panel dr. tim scarfe yannic kilcher sayak paul 00:00:00 kenneth stanley greatness planned house keeping 00:21:09 kilcher intro 00:28:54 hello lena 00:29:21 tim lenas nmt paper 00:35:26 tim minimum description length probe paper 00:40:12 tim evolution representations 00:46:40 lenas nlp course 00:49:18 peppermint tea situation 00:49:28 main show kick 00:50:22 hallucination vs exposure bias 00:53:04 lenas focus explaining models sota chasing 00:56:34 probes paper nlp intepretability 01:02:18 standard probing doesnt work 01:12:12 evolutions representations paper 01:23:53 bertscore bert rediscovers classical nlp pipeline paper 01:25:10 shifting encoding context bert bidirectionality 01:26:43 objective defines information lose input 01:27:59 influential dataset 01:29:42 community going wrong 01:31:55 thoughts gofai/understanding nlp 01:36:38 lena 's nlp course 01:47:40 foster better learning understanding 01:52:17 lena 's toolset languages 01:54:12 mathematics need 01:56:03 programming languages https //lena-voita.github.io/ https //www.linkedin.com/in/elena-voita/ https //scholar.google.com/citations user=ecn9o7kaaaaj hl=ja https //twitter.com/lena_voita,Machine Learning
get started recommender systems,tweet share share recommender systems may common type predictive model average person may encounter provide basis recommendations services amazon spotify youtube recommender systems huge daunting topic ’ getting started myriad data preparation techniques algorithms model evaluation methods techniques relevant fact state-of-the-art ignored likely get good results focusing fundamentals e.g treat straightforward classification regression problem important know basics laid systematic way recommend skimming reading standard books papers topic looking popular libraries tutorial discover resources use get started recommender systems completing tutorial know top review papers recommender systems use quickly understand state field top books recommender systems learn algorithms techniques required developing evaluating recommender systems top python libraries apis use prototype develop recommender systems let ’ get started get started recommender systems photo paul toogood right reserved tutorial overview tutorial divided three parts papers recommender systems books recommender systems recommender systems libraries papers recommender systems research papers recommender systems help quickly get speed state field specifically review papers use precise language define recommender system algorithms used standard datasets metrics comparing algorithms hints state art techniques skimming reading handful review papers recommender systems quickly develop foundation dive deeper start developing systems field change quickly techniques 10 20 years ago give solid results review papers recommender systems recommended establish foundational understanding include amazon.com recommendations item-to-item collaborative filtering 2003 matrix factorization techniques recommender systems 2009 recommender systems 2012 recommender systems survey 2013 advances collaborative filtering 2015 matrix factorization techniques recommender systems questions specific techniques find papers focus techniques dive deeper search papers specific techniques google scholar know additional good review papers recommender systems let know comments books recommender systems books recommender systems provide space lay field take tour techniques give detail need understand breadth detail much shorter review paper given field quite mature older books published decade ago immediately neglected top textbooks published key researchers field include following recommender systems introduction 2010 recommender systems textbook 2016 hard copy “ recommender systems introduction ” recommend highly enough book offers overview approaches developing state-of-the-art recommender systems authors present current algorithmic approaches generating personalized buying proposals collaborative content-based filtering well interactive knowledge- based approaches also discuss measure effectiveness recommender systems illustrate methods practical case studies — recommender systems introduction 2010 table contents book follows chapter 1 introduction chapter 2 collaborative recommendation chapter 3 content-based recommendation chapter 4 knowledge-based recommendation chapter 5 hybrid recommendation approaches chapter 6 explanations recommender systems chapter 7 evaluating recommender systems chapter 8 case study personalized game recommendations mobile internet chapter 9 attacks collaborative recommender systems chapter 10 online consumer decision making chapter 11 recommender systems next-generation web chapter 12 recommendations ubiquitous environments chapter 13 summary outlook recommender systems introduction good get handbook topic chapters written different academics summarizing championing preferred techniques methods recommend handbook recommender systems handbook 2015 looking hands-on book recommend practical recommender systems 2019 read one books know another great book topic let know comments recommender systems libraries probably ’ need dive start art least immediately standard machine learning libraries great place start example develop effective recommender system using matrix factorization methods svd even straight forward k-nearest neighbors model items users recommend starting experiments scikit-learn scikit-learn python machine learning library practice standard recommender system datasets data yet accessible available want get hang things first popular standard datasets recommender systems include movielens yahoo datasets music urls movies etc ready state-of-the-art techniques great place start “ papers code ” lists academic papers links source code methods described paper papers code recommendation systems number proprietary open-source libraries services recommender systems recommend sticking open-source python libraries beginning surprise python scikit building analyzing recommender systems case recommender flexible extensible python framework recommender systems used libraries develop recommender system let know comments summary tutorial discovered resources use get started recommender systems specifically learned top review papers recommender systems use quickly understand state field top books recommender systems learn algorithms techniques required developing evaluating recommender systems top python libraries apis use prototype develop recommender systems questions ask questions comments best answer tweet share share post get started recommender systems appeared first machine learning mastery,Machine Learning
2 rookie mistakes avoid training predictive analytics models,read full story,Machine Learning
sara hooker hardware lottery sparsity fairness,dr. tim scarfe yannic kilcher sayak paul chat sara hooker google brain team discuss recent hardware lottery paper pruning sparsity bias mitigation intepretability hardware lottery -- causes inertia friction marketplace ideas meritocracy ideas previous decisions made enslave us sara hooker calls lottery feels machine learning progress entirely beholdant hardware software landscape ideas succeed compatible hardware software time also existing inventions machine learning community exceptional pace innovation fast operate largely open largely n't build anything physical expensive slow cost scooped high get stuck basins attraction based technology decisions 's expensive jump outside basins story unique hardware ai algorithms really story innovation every great innovation must wait right stepping stone place really happen excited bring sara hooker give take youtube version including toc https //youtu.be/sqfxbq7ade0 show notes https //drive.google.com/file/d/1s_rhnhaovx4nzx_8e3esqq4usswasno7/view usp=sharing sara hooker page https //www.sarahooker.me,Machine Learning
lambdanetworks modeling long-range interactions without attention paper explained,ai research attention transformers already captured nlp recently started take field computer vision far size images input challenging transformers attention mechanism 's memory requirements grows quadratic input size lambdanetworks offer way around requirement capture long-range interactions without need build expensive attention maps reach new state-of-the-art imagenet compare favorably transformers cnns terms efficiency outline 0:00 introduction overview 6:25 attention mechanism memory requirements 9:30 lambda layers vs attention layers 17:10 lambda layers work 31:50 attention re-appears lambda layers 40:20 positional encodings 51:30 extensions experimental comparisons 58:00 code paper https //openreview.net/forum id=xtjen-ggl1b lucidrains code https //github.com/lucidrains/lambda-networks abstract present general framework capturing long-range interactions input structured contextual information e.g pixel surrounded pixels method called lambda layer captures interactions transforming available contexts linear functions termed lambdas applying linear functions input separately lambda layers versatile may implemented model content position-based interactions global local masked contexts bypass need expensive attention maps lambda layers routinely applied inputs length thousands en-abling applications long sequences high-resolution images resulting neural network architectures lambdanetworks computationally efficient simple implement using direct calls operations available modern neural network libraries experiments imagenet classification coco object detection instance segmentation demonstrate lambdanetworks significantly outperform convolutional attentional counterparts computationally efficient finally introduce lambdaresnets family lambdanetworks considerably improve speed-accuracy tradeoff image classification models lambdaresnets reach state-of-the-art accuracies imagenet ∼4.5x faster popular efficientnets modern machine learning accelerators authors anonymous links youtube https //www.youtube.com/c/yannickilcher twitter https //twitter.com/ykilcher discord https //discord.gg/4h8xxdf bitchute https //www.bitchute.com/channel/yannic-kilcher minds https //www.minds.com/ykilcher parler https //parler.com/profile/yannickilcher linkedin https //www.linkedin.com/in/yannic-kilcher-488534136/ want support best thing share content want support financially completely optional voluntary lot people asked subscribestar https //www.subscribestar.com/yannickilcher patreon https //www.patreon.com/yannickilcher bitcoin btc bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq ethereum eth 0x7ad3513e3b8f66799f507aa7874b1b0ebc7f85e2 litecoin ltc lqw2trykyetvc8wjfkhpphtpbdm4vw7r9m monero xmr 4acl8agreo5hair8a9cevrw8peauwvnp1wnsdzxw7tzicdlhzagsgzhrqabdnfy8yum9fwjdvijphkrjv4fwt19cjzn9d4n,Machine Learning
eric weinstein revolutionary ideas science math society,eric weinstein mathematician economist physicist managing director thiel capital formed “ intellectual dark web ” loosely assembled group public intellectuals including sam harris jordan peterson steven pinker joe rogan michael shermer others video version available youtube would like get information podcast go https //lexfridman.com/ai connect lexfridman twitter linkedin facebook medium youtube watch video versions conversations,Machine Learning
p ai telegram api cool bot,quick article creating recipe recommendation bot also recognize ingredients picture small tip telegram api super useful small projects poc frontend takes day ready link comments,Machine Learning
semi-supervised learning label spreading,"semi-supervised learning refers algorithms attempt make use labeled unlabeled training data semi-supervised learning algorithms unlike supervised learning algorithms able learn labeled training data popular approach semi-supervised learning create graph connects examples training dataset propagates known labels edges graph label unlabeled examples example approach semi-supervised learning label spreading algorithm classification predictive modeling tutorial discover apply label spreading algorithm semi-supervised learning classification dataset completing tutorial know intuition label spreading semi-supervised learning algorithm works develop semi-supervised classification dataset establish baseline performance supervised learning algorithm develop evaluate label spreading algorithm use model output train supervised learning algorithm let ’ get started semi-supervised learning label spreading photo jernej furman rights reserved tutorial overview tutorial divided three parts label spreading algorithm semi-supervised classification dataset label spreading semi-supervised learning label spreading algorithm label spreading semi-supervised learning algorithm algorithm introduced dengyong zhou et al 2003 paper titled “ learning local global consistency. ” intuition broader approach semi-supervised learning nearby points input space label points structure manifold input space label key semi-supervised learning problems prior assumption consistency means 1 nearby points likely label 2 points structure typically referred cluster manifold likely label — learning local global consistency 2003 label spreading inspired technique experimental psychology called spreading activation networks algorithm understood intuitively terms spreading activation networks experimental psychology — learning local global consistency 2003 points dataset connected graph based relative distances input space weight matrix graph normalized symmetrically much like spectral clustering information passed graph adapted capture structure input space approach similar label propagation algorithm semi-supervised learning another similar label propagation algorithm given zhou et al step node receives contribution neighbors j weighted normalized weight edge j additional small contribution given initial value — page 196 semi-supervised learning 2006 convergence labels applied based nodes passed information finally label unlabeled point set class received information iteration process — learning local global consistency 2003 familiar label spreading algorithm let ’ look might use project first must define semi-supervised classification dataset semi-supervised classification dataset section define dataset semis-supervised learning establish baseline performance dataset first define synthetic classification dataset using make_classification function define dataset two classes binary classification two input variables 1,000 examples ... define dataset x make_classification n_samples=1000 n_features=2 n_informative=2 n_redundant=0 random_state=1 next split dataset train test datasets equal 50-50 split e.g 500 rows ... split train test x_train x_test y_train y_test train_test_split x test_size=0.50 random_state=1 stratify=y finally split training dataset half portion labels portion pretend unlabeled ... split train labeled unlabeled x_train_lab x_test_unlab y_train_lab y_test_unlab train_test_split x_train y_train test_size=0.50 random_state=1 stratify=y_train tying together complete example preparing semi-supervised learning dataset listed prepare semi-supervised learning dataset sklearn.datasets import make_classification sklearn.model_selection import train_test_split define dataset x make_classification n_samples=1000 n_features=2 n_informative=2 n_redundant=0 random_state=1 split train test x_train x_test y_train y_test train_test_split x test_size=0.50 random_state=1 stratify=y split train labeled unlabeled x_train_lab x_test_unlab y_train_lab y_test_unlab train_test_split x_train y_train test_size=0.50 random_state=1 stratify=y_train summarize training set size print 'labeled train set x_train_lab.shape y_train_lab.shape print 'unlabeled train set x_test_unlab.shape y_test_unlab.shape summarize test set size print 'test set x_test.shape y_test.shape running example prepares dataset summarizes shape three portions results confirm test dataset 500 rows labeled training dataset 250 rows 250 rows unlabeled data labeled train set 250 2 250 unlabeled train set 250 2 250 test set 500 2 500 supervised learning algorithm 250 rows train model semi-supervised learning algorithm 250 labeled rows well 250 unlabeled rows could used numerous ways improve labeled training dataset next establish baseline performance semi-supervised learning dataset using supervised learning algorithm fit labeled training data important would expect semi-supervised learning algorithm outperform supervised learning algorithm fit labeled data alone case semi-supervised learning algorithm skill case use logistic regression algorithm fit labeled portion training dataset ... define model model logisticregression fit model labeled dataset model.fit x_train_lab y_train_lab model used make predictions entire holdout test dataset evaluated using classification accuracy ... make predictions hold test set yhat model.predict x_test calculate score test set score accuracy_score y_test yhat summarize score print 'accuracy .3f score 100 tying together complete example evaluating supervised learning algorithm semi-supervised learning dataset listed baseline performance semi-supervised learning dataset sklearn.datasets import make_classification sklearn.model_selection import train_test_split sklearn.metrics import accuracy_score sklearn.linear_model import logisticregression define dataset x make_classification n_samples=1000 n_features=2 n_informative=2 n_redundant=0 random_state=1 split train test x_train x_test y_train y_test train_test_split x test_size=0.50 random_state=1 stratify=y split train labeled unlabeled x_train_lab x_test_unlab y_train_lab y_test_unlab train_test_split x_train y_train test_size=0.50 random_state=1 stratify=y_train define model model logisticregression fit model labeled dataset model.fit x_train_lab y_train_lab make predictions hold test set yhat model.predict x_test calculate score test set score accuracy_score y_test yhat summarize score print 'accuracy .3f score 100 running algorithm fits model labeled training dataset evaluates holdout dataset prints classification accuracy note results may vary given stochastic nature algorithm evaluation procedure differences numerical precision consider running example times compare average outcome case see algorithm achieved classification accuracy 84.8 percent would expect effective semi-supervised learning algorithm achieve better accuracy accuracy 84.800 next let ’ explore apply label spreading algorithm dataset label spreading semi-supervised learning label spreading algorithm available scikit-learn python machine learning library via labelspreading class model fit like classification model calling fit function used make predictions new data via predict function ... define model model labelspreading fit model training dataset model.fit ... ... make predictions hold test set yhat model.predict ... importantly training dataset provided fit function must include labeled examples ordinal encoded per normal unlabeled examples marked label -1 model determine label unlabeled examples part fitting model model fit estimated labels labeled unlabeled data training dataset available via “ transduction_ ” attribute labelspreading class ... get labels entire training dataset data tran_labels model.transduction_ familiar use label spreading algorithm scikit-learn let ’ look might apply semi-supervised learning dataset first must prepare training dataset concatenate input data training dataset single array ... create training dataset input x_train_mixed concatenate x_train_lab x_test_unlab create list -1 valued unlabeled row unlabeled portion training dataset ... create `` label '' unlabeled data nolabel -1 range len y_test_unlab list concatenated labels labeled portion training dataset correspond input array training dataset ... recombine training dataset labels y_train_mixed concatenate y_train_lab nolabel train labelspreading model entire training dataset ... define model model labelspreading fit model training dataset model.fit x_train_mixed y_train_mixed next use model make predictions holdout dataset evaluate model using classification accuracy ... make predictions hold test set yhat model.predict x_test calculate score test set score accuracy_score y_test yhat summarize score print 'accuracy .3f score 100 tying together complete example evaluating label spreading semi-supervised learning dataset listed evaluate label spreading semi-supervised learning dataset numpy import concatenate sklearn.datasets import make_classification sklearn.model_selection import train_test_split sklearn.metrics import accuracy_score sklearn.semi_supervised import labelspreading define dataset x make_classification n_samples=1000 n_features=2 n_informative=2 n_redundant=0 random_state=1 split train test x_train x_test y_train y_test train_test_split x test_size=0.50 random_state=1 stratify=y split train labeled unlabeled x_train_lab x_test_unlab y_train_lab y_test_unlab train_test_split x_train y_train test_size=0.50 random_state=1 stratify=y_train create training dataset input x_train_mixed concatenate x_train_lab x_test_unlab create `` label '' unlabeled data nolabel -1 range len y_test_unlab recombine training dataset labels y_train_mixed concatenate y_train_lab nolabel define model model labelspreading fit model training dataset model.fit x_train_mixed y_train_mixed make predictions hold test set yhat model.predict x_test calculate score test set score accuracy_score y_test yhat summarize score print 'accuracy .3f score 100 running algorithm fits model entire training dataset evaluates holdout dataset prints classification accuracy note results may vary given stochastic nature algorithm evaluation procedure differences numerical precision consider running example times compare average outcome case see label spreading model achieves classification accuracy 85.4 percent slightly higher logistic regression fit labeled training dataset achieved accuracy 84.8 percent accuracy 85.400 far good another approach use semi-supervised model take estimated labels training dataset fit supervised learning model recall retrieve labels entire training dataset label spreading model follows ... get labels entire training dataset data tran_labels model.transduction_ use labels along input data train evaluate supervised learning algorithm logistic regression model hope supervised learning model fit entire training dataset would achieve even better performance semi-supervised learning model alone ... define supervised learning model model2 logisticregression fit supervised learning model entire training dataset model2.fit x_train_mixed tran_labels make predictions hold test set yhat model2.predict x_test calculate score test set score accuracy_score y_test yhat summarize score print 'accuracy .3f score 100 tying together complete example using estimated training set labels train evaluate supervised learning model listed evaluate logistic regression fit label spreading semi-supervised learning numpy import concatenate sklearn.datasets import make_classification sklearn.model_selection import train_test_split sklearn.metrics import accuracy_score sklearn.semi_supervised import labelspreading sklearn.linear_model import logisticregression define dataset x make_classification n_samples=1000 n_features=2 n_informative=2 n_redundant=0 random_state=1 split train test x_train x_test y_train y_test train_test_split x test_size=0.50 random_state=1 stratify=y split train labeled unlabeled x_train_lab x_test_unlab y_train_lab y_test_unlab train_test_split x_train y_train test_size=0.50 random_state=1 stratify=y_train create training dataset input x_train_mixed concatenate x_train_lab x_test_unlab create `` label '' unlabeled data nolabel -1 range len y_test_unlab recombine training dataset labels y_train_mixed concatenate y_train_lab nolabel define model model labelspreading fit model training dataset model.fit x_train_mixed y_train_mixed get labels entire training dataset data tran_labels model.transduction_ define supervised learning model model2 logisticregression fit supervised learning model entire training dataset model2.fit x_train_mixed tran_labels make predictions hold test set yhat model2.predict x_test calculate score test set score accuracy_score y_test yhat summarize score print 'accuracy .3f score 100 running algorithm fits semi-supervised model entire training dataset fits supervised learning model entire training dataset inferred labels evaluates holdout dataset printing classification accuracy note results may vary given stochastic nature algorithm evaluation procedure differences numerical precision consider running example times compare average outcome case see hierarchical approach semi-supervised model followed supervised model achieves classification accuracy 85.8 percent holdout dataset slightly better semi-supervised learning algorithm used alone achieved accuracy 85.6 percent accuracy 85.800 achieve better results tuning hyperparameters labelspreading model let know discover comments reading section provides resources topic looking go deeper books introduction semi-supervised learning 2009 chapter 11 label propagation quadratic criterion semi-supervised learning 2006 papers learning local global consistency 2003 apis sklearn.semi_supervised.labelspreading api section 1.14 semi-supervised scikit-learn user guide sklearn.model_selection.train_test_split api sklearn.linear_model.logisticregression api sklearn.datasets.make_classification api articles semi-supervised learning wikipedia summary tutorial discovered apply label spreading algorithm semi-supervised learning classification dataset specifically learned intuition label spreading semi-supervised learning algorithm works develop semi-supervised classification dataset establish baseline performance supervised learning algorithm develop evaluate label spreading algorithm use model output train supervised learning algorithm questions ask questions comments best answer post semi-supervised learning label spreading appeared first machine learning mastery",Machine Learning
valuing expertise people serve,day 2 november 18 2020 theme building pipeline research impact donna auguste auguste research group accessible computer science education fall workshop hosted microsoft university washington create university colorado ’ coleman institute took place november 17-19 2020 consisted three half-days talks discussions planning new research dedicated making computer science education learning experiences accessible people disabilities information workshop found https //www.microsoft.com/en-us/research/event/accessible-cs-education-fall-workshop/,Machine Learning
rethinking attention performers paper explained,ai research attention transformers huge memory compute requirements construct attention matrix grows quadratically size input performer model uses random positive orthogonal features construct unbiased estimator attention matrix obtains arbitrarily good approximation linear time method generalizes beyond attention opens door next generation deep learning architectures outline 0:00 intro outline 6:15 quadratic bottleneck attention mechanisms 10:00 decomposing attention matrix 15:30 approximating softmax kernel 24:45 different choices different kernels 28:00 naive approach work 31:30 better approximation via positive features 36:55 positive features infinitely better 40:10 orthogonal features even better 43:25 experiments 49:20 broader impact statement 50:00 causal attention via prefix sums 52:10 code 53:50 final remarks conclusion paper https //arxiv.org/abs/2009.14794 code https //github.com/google-research/google-research/tree/master/performer blog https //ai.googleblog.com/2020/10/rethinking-attention-with-performers.html kernels ml street talk https //www.youtube.com/watch v=y_rjsdhl5y4 video linformer https //www.youtube.com/watch v=-_2af9lhweo video reformer https //www.youtube.com/watch v=i4h0kjxrias video attention https //www.youtube.com/watch v=idulhoq2pro abstract introduce performers transformer architectures estimate regular softmax full-rank-attention transformers provable accuracy using linear opposed quadratic space time complexity without relying priors sparsity low-rankness approximate softmax attention-kernels performers use novel fast attention via positive orthogonal random features approach favor+ may independent interest scalable kernel methods favor+ also used efficiently model kernelizable attention mechanisms beyond softmax representational power crucial accurately compare softmax kernels first time large-scale tasks beyond reach regular transformers investigate optimal attention-kernels performers linear architectures fully compatible regular transformers strong theoretical guarantees unbiased nearly-unbiased estimation attention matrix uniform convergence low estimation variance tested performers rich set tasks stretching pixel-prediction text models protein sequence modeling demonstrate competitive results examined efficient sparse dense attention methods showcasing effectiveness novel attention-learning paradigm leveraged performers authors krzysztof choromanski valerii likhosherstov david dohan xingyou song andreea gane tamas sarlos peter hawkins jared davis afroz mohiuddin lukasz kaiser david belanger lucy colwell adrian weller links youtube https //www.youtube.com/c/yannickilcher twitter https //twitter.com/ykilcher discord https //discord.gg/4h8xxdf bitchute https //www.bitchute.com/channel/yannic-kilcher minds https //www.minds.com/ykilcher parler https //parler.com/profile/yannickilcher linkedin https //www.linkedin.com/in/yannic-kilcher-488534136/ want support best thing share content want support financially completely optional voluntary lot people asked subscribestar https //www.subscribestar.com/yannickilcher patreon https //www.patreon.com/yannickilcher bitcoin btc bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq ethereum eth 0x7ad3513e3b8f66799f507aa7874b1b0ebc7f85e2 litecoin ltc lqw2trykyetvc8wjfkhpphtpbdm4vw7r9m monero xmr 4acl8agreo5hair8a9cevrw8peauwvnp1wnsdzxw7tzicdlhzagsgzhrqabdnfy8yum9fwjdvijphkrjv4fwt19cjzn9d4n,Machine Learning
looking inspiration bachelor thesis,currently choosing task bachelor thesis tips really anything preferably rather complex unexplored question within machine learning good link comments,Machine Learning
eric schmidt google,eric schmidt ceo google 2001 2011 executive chairman 2011 2017 guiding company period incredible growth series world-changing innovations video version available youtube would like get information podcast go https //lexfridman.com/ai connect lexfridman twitter linkedin facebook youtube watch video versions conversations,Machine Learning
rt karpathy new blog post great stagnation ending technologies watching decade ahead going get life extension treatments soon 5700+ words please read share https //elidourado.com/blog/notes-on-technology-2020s/,new blog post great stagnation ending technologies watching decade ahead going get life extension treatments soon 5700+ words please read share elidourado.com/blog/notes-on…,Machine Learning
gentle introduction machine learning modeling pipelines,applied machine learning typically focused finding single model performs well best given dataset effective use model require appropriate preparation input data hyperparameter tuning model collectively linear sequence steps required prepare data tune model transform predictions called modeling pipeline modern machine learning libraries like scikit-learn python library allow sequence steps defined used correctly without data leakage consistently evaluation prediction nevertheless working modeling pipelines confusing beginners requires shift perspective applied machine learning process tutorial discover modeling pipelines applied machine learning completing tutorial know applied machine learning concerned finding good performing model also requires finding appropriate sequence data preparation steps steps post-processing predictions collectively operations required address predictive modeling problem considered atomic unit called modeling pipeline approaching applied machine learning lens modeling pipelines requires change thinking evaluating specific model configurations sequences transforms algorithms let ’ get started gentle introduction machine learning modeling pipelines photo jay huang rights reserved tutorial overview tutorial divided three parts finding skillful model enough modeling pipeline implications modeling pipeline finding skillful model enough applied machine learning process discovering model performs best given predictive modeling dataset fact ’ addition discovering model performs best dataset must also discover data transforms best expose unknown underlying structure problem learning algorithms model hyperparameters result good best configuration chosen model may also additional considerations techniques transform predictions made model like threshold moving model calibration predicted probabilities common think applied machine learning large combinatorial search problem across data transforms models model configurations quite challenging practice requires sequence one data preparation schemes model model configuration prediction transform schemes must evaluated consistently correctly given test harness although tricky may manageable simple train-test split becomes quite unmanageable using k-fold cross-validation even repeated k-fold cross-validation solution use modeling pipeline keep everything straight modeling pipeline pipeline linear sequence data preparation options modeling operations prediction transform operations allows sequence steps specified evaluated used atomic unit pipeline linear sequence data preparation modeling steps treated atomic unit make idea clear let ’ look two simple examples first example uses data normalization input variables fits logistic regression model input normalization logistic regression predictions second example standardizes input variables applies rfe feature selection fits support vector machine input standardization rfe svm predictions imagine examples modeling pipelines atomic unit pipeline evaluated using preferred resampling scheme train-test split k-fold cross-validation important two main reasons avoid data leakage consistency reproducibility modeling pipeline avoids common type data leakage data preparation techniques scaling input values applied entire dataset data leakage shares knowledge test dataset observations contribute mean maximum known value training dataset turn may result overly optimistic model performance instead data transforms must prepared training dataset applied training dataset test dataset validation dataset datasets require transform prior used model modeling pipeline ensures sequence data preparation operations performed reproducible without modeling pipeline data preparation steps may performed manually twice evaluating model making predictions changes sequence must kept consistent cases otherwise differences impact capability skill model pipeline ensures sequence operations defined consistent used model evaluation making predictions python scikit-learn machine learning library provides machine learning modeling pipeline via pipeline class learn use pipeline api tutorial avoid data leakage performing data preparation implications modeling pipeline modeling pipeline important tool machine learning practitioners nevertheless important implications must considered using main confusion beginners using pipelines comes understanding pipeline learned specific configuration discovered pipeline example pipeline may use data transform configures automatically rfecv technique feature selection evaluating pipeline uses automatically-configured data transform configuration choose fitting pipeline final model making predictions configuration choose answer ’ matter another example use hyperparameter tuning final step pipeline grid search performed data provided prior transform steps pipeline search best combination hyperparameters model using data fit model hyperparameters data evaluating pipeline grid searches model hyperparameters configuration choose fitting pipeline final model making predictions configuration choose answer ’ matter answer applies using threshold moving probability calibration step end pipeline reason reason concerned specific internal structure coefficients chosen model example evaluating logistic regression model ’ need inspect coefficients chosen k-fold cross-validation round order choose model instead focus out-of-fold predictive skill similarly using logistic regression model final model making predictions new data need inspect coefficients chosen fitting model entire dataset making predictions inspect discover coefficients used model exercise analysis impact selection use model answer generalizes considering modeling pipeline concerned features may automatically selected data transform pipeline also concerned hyperparameters chosen model using grid search final step modeling pipeline three cases single model pipeline automatic feature selection pipeline grid search evaluating “ model ” “ modeling pipeline ” atomic unit pipeline allows us machine learning practitioners move one level abstraction less concerned specific outcomes algorithms concerned capability sequence procedures focus evaluating capability algorithms dataset product algorithms i.e model estimate pipeline apply confident get similar performance average shift thinking may take time get used also philosophy behind modern automl automatic machine learning techniques treat applied machine learning large combinatorial search problem reading section provides resources topic looking go deeper avoid data leakage performing data preparation summary tutorial discovered modeling pipelines applied machine learning specifically learned applied machine learning concerned finding good performing model also requires finding appropriate sequence data preparation steps steps post-processing predictions collectively operations required address predictive modeling problem considered atomic unit called modeling pipeline approaching applied machine learning lens modeling pipelines requires change thinking evaluating specific model configurations sequences transforms algorithms questions ask questions comments best answer post gentle introduction machine learning modeling pipelines appeared first machine learning mastery,Machine Learning
rt ykilcher yearning little openai dall·e action make sure check ykilcher 's video exhaustive think 's best one good https //www.youtube.com/watch v=j4xgkjwlfl4,yearning little openai dall·e action make sure check ykilcher 's video exhaustive think 's best one good invidious.snopyta.org/watch v=j4xgkjwl…,Machine Learning
actually think barely even scratches surface weirdness inverted computers tenet universe legitimately breaks brain love quite bit depends understandably skimmed physics interaction fwd/bwd objects,actually think barely even scratches surface weirdness inverted computers tenet universe legitimately breaks brain love quite bit depends understandably skimmed physics interaction fwd/bwd objects nitter.net/karpathy/status/1346134712817864708,Machine Learning
wikipedia arguably greatest website ever made happy 20th,wikipedia arguably greatest website ever made happy 20th,Machine Learning
111 – richard karp algorithms computational complexity,richard karp professor berkeley one important figures history theoretical computer science 1985 received turing award research theory algorithms including development edmonds–karp algorithm solving maximum flow problem networks hopcroft–karp algorithm finding maximum cardinality matchings bipartite graphs landmark paper complexity theory called “ reducibility among combinatorial problems ” proved 21 problems np-complete paper probably important catalyst explosion interest study np-completeness,Machine Learning
could avoid hit laser room mirrors,get free access 2500 documentaries curiositystream https //curiositystream.thld.co/zachstaroct16 use code `` zachstar '' sign stemerch store https //stemerch.com/ book recommendation list available support channel https //www.patreon.com/zachstar paypal one time donation https //www.paypal.me/zachstaryt join channel get access perks https //www.youtube.com/channel/ucpcsacbqs-sjevfk_hmfy9w/join ►follow instagram https //www.instagram.com/zachstar/ twitter https //twitter.com/imzachstar ►puzzle books affiliates martin gardener book puzzles https //amzn.to/3jfedau peter winkler mathematical puzzles https //amzn.to/34cuy54 peter winkler video mentions problem 1:05:10 https //www.youtube.com/watch v=bgqkgiex1do animations brainup studios http //brainup.in/ check spanish channel https //www.youtube.com/channel/ucnknu2xqblaspj6ckc8vtpa ►my setup space pictures https //amzn.to/2cc4kqj magnetic floating globe https //amzn.to/2vgpdn0 camera https //amzn.to/2rivyu5 mic https //amzn.to/35bkiri tripod https //amzn.to/2rgmtnl equilibrium tube https //amzn.to/2sowdrh ►check amazon store https //www.amazon.com/shop/zachstar,Machine Learning
5 top tech careers consider studying towards 2021,gain entry knowledge data science engineering cloud computing cybersecurity devops read full story,Machine Learning
98 – kate darling emotional connection humans robots,kate darling researcher mit interested social robotics robot ethics generally technology intersects society explores emotional connection human beings life-like machines one exciting topics artificial intelligence support podcast signing sponsors – expressvpn https //www.expressvpn.com/lexpod – masterclass https //masterclass.com/lex episode links kate ’ website http //www.katedarling.org/ kate ’ twitter https //twitter.com/grok_ conversation part artificial intelligence podcast would like get information podcast go https //lexfridman.com/ai connect lexfridman twitter linkedin facebook,Machine Learning
86 – david silver alphago alphazero deep reinforcement learning,david silver leads reinforcement learning research group deepmind lead researcher alphago alphazero co-lead alphastar muzero lot important work reinforcement learning support podcast signing sponsors – masterclass https //masterclass.com/lex – cash app – use code “ lexpodcast ” download – cash app app store https //apple.co/2spruhe – cash app google play https //bit.ly/2mlvp5w episode links reinforcement learning book https //amzn.to/2jwp5zg conversation part artificial intelligence podcast would like get information podcast go https //lexfridman.com/ai connect lexfridman twitter linkedin facebook,Machine Learning
banned social media trump found way communicate duolingo,banned social media trump found way communicate duolingo,Machine Learning
5 simple ways kickstart freelance data science career,’ itching get feet wet field steps provide lots valuable ideas suggestions kickstart career read full story,Machine Learning
new improvements h2o 3.32.0.2,new minor release h2o introduces two useful improvements xgboost integration interaction constraints feature interactions interaction constraints feature interaction constraints allow users decide variables allowed interact potential benefits better predictive performance focusing interactions work – whether domain-specific knowledge algorithms rank interactions less noise predictions better generalization control given user model fit example user may want exclude interactions even perform well due regulatory constraints source https //xgboost.readthedocs.io/en/latest/tutorials/feature_interaction_constraint.html h2o documentation available xgbfi-like tool revealing feature interactions implemented ranks features feature interactions various measures xgbfi style thanks tool h2o provides insights higher-order interactions features trees user-friendly manner additionally leaf statistics split value histograms provided measures used either one gain implies relative contribution corresponding feature model calculated taking feature ’ contribution tree model higher value metric compared another feature implies important generating prediction cover metric measure number observations affected split counted specific feature measures relative quantity observations concerned feature frequency fscore number times feature used generated trees please note take tree-depth tree-index splits feature occurs consideration neither amount possible splits feature hence often suboptimal measure importance averaged weighed ranked alternatives h2o documentation available example jupyter notebook demo example codes presented available train xgboostestimator interaction_constraints parameter start h2o import h2o h2o.init h2o.estimators.xgboost import check h2o xgboostestimator available assert h2oxgboostestimator.available true import data data h2o.import_file path `` .. .. /smalldata/logreg/prostate.csv '' x list range 1 data.ncol-2 data.names len data.names 1 ntree 5 h2o_params 'eta 0.3 'max_depth 3 'ntrees ntree 'tree_method 'hist define interactions list list names colums lists defines allowed interaction interactions column always allowed specified list one column e.g `` psa '' h2o_params `` interaction_constraints '' `` capsule '' `` age '' `` psa '' `` dpros '' train h2o xgboost model h2o_model h2oxgboostestimator h2o_params h2o_model.train x=x y=y training_frame=data result display feature interactions calculate multi-level feature interactions h2o_model.feature_interaction credits new h2o release brought veronika maurerova zuzana olajcova hannah tillman get started download h2o-3 follow steps example notebook also check training center self-paced tutorials instructor-led courses post new improvements h2o 3.32.0.2 appeared first open source leader ai ml,Machine Learning
abandon ml research project,phd student 've resubmitting paper past 2 years since january 2019 3 separate conferences got 3 rejections row far time address reviews add new set experiments reviewers find reason reject paper 'm kind burned project want quit unsure keep chugging along abandon project link comments,Machine Learning
python beginners learning one-liners practice,like lists comprehensions lambda functions python one line codes save lot time space master read full story,Machine Learning
dan kokotov speech recognition ai humans lex fridman podcast 151,dan kokotov vp engineering rev.ai automatic speech recognition company please support podcast checking sponsors athletic greens https //athleticgreens.com/lex use code lex get 1 month fish oil blinkist https //blinkist.com/lex use code lex get 25 premium business wars https //wondery.com/business-wars/ cash app https //cash.app/ use code lexpodcast get 10 episode links rev https //www.rev.com rev.ai https //rev.ai podcast info podcast website https //lexfridman.com/podcast apple podcasts https //apple.co/2lwqzir spotify https //spoti.fi/2newcf8 rss https //lexfridman.com/feed/podcast/ full episodes playlist https //www.youtube.com/playlist list=plraxtmerzgodp_8gztsuki9nrranbkkp4 clips playlist https //www.youtube.com/playlist list=plraxtmerzgoecifp3cbcieelojeitor41 outline 0:00 introduction 3:23 dune 6:39 rev 12:39 translation 19:28 gig economy 28:08 automatic speech recognition 38:58 create products people love 47:08 future podcasts spotify 1:08:46 book recommendations 1:10:08 stories dystopian future 1:13:50 movies stalin hitler 1:19:05 interviewing putin 1:25:02 meaning life connect subscribe youtube channel twitter https //twitter.com/lexfridman linkedin https //www.linkedin.com/in/lexfridman facebook https //www.facebook.com/lexfridmanpage instagram https //www.instagram.com/lexfridman medium https //medium.com/ lexfridman support patreon https //www.patreon.com/lexfridman,Machine Learning
welcomeaioverlords zak jost,welcome zak jost welcomeaioverlords channel zak ml research scientist amazon great blog http //blog.zakjost.com also discord channel https //discord.gg/xh2chkx welcomeaioverlords https //www.youtube.com/channel/ucxw9_wymlqlj5pyxu2awu_g 00:00:00 intro start 00:01:07 main show start 00:01:59 zak 's story 00:05:06 youtube discussion 00:24:12 understanding papers 00:29:53 contrastive learning intro 00:33:00 bring latent paper 01:03:13 graphs ml knowledge graphs 01:21:36 graph use cases fraud 01:30:15 knowledge graphs 01:34:22 graphs ml 01:38:53 automated ml 01:57:32 outro,Machine Learning
ethical ai libraries critical every data scientist know,exponential rise applications ai data science machine learning critical ethical ai libraries know read full story,Machine Learning
sean carroll quantum mechanics many-worlds interpretation,sean carroll theoretical physicist caltech santa fe institute specializing quantum mechanics arrow time cosmology gravitation author something deeply hidden several popular books host great podcast called mindscape second time sean podcast watch first time youtube listen first time episode page conversation part artificial intelligence podcast would like get information podcast go https //lexfridman.com/ai connect lexfridman,Machine Learning
happy birthday andrewyang 46 new 42,happy birthday andrewyang 46 new 42,Machine Learning
`` n't join book burners n't think 're going conceal faults concealing evidence ever existed n't afraid go library read every book '' dwight d. eisenhower,`` n't join book burners n't think 're going conceal faults concealing evidence ever existed n't afraid go library read every book '' dwight d. eisenhower,Machine Learning
`` love '' new boston dynamics video 🤖 https //youtu.be/fn3kwm1kuaw,`` love '' new boston dynamics video 🤖 invidious.snopyta.org/fn3kwm1kuaw,Machine Learning
105 – robert langer edison medicine,robert langer professor mit one cited researchers history specializing biotechnology fields drug delivery systems tissue engineering bridged theory practice key member driving force launching many successful biotech companies mit support podcast supporting sponsors – masterclass https //masterclass.com/lex – cash app – use code “ lexpodcast ” download – cash app app store https //apple.co/2spruhe – cash app google play https //bit.ly/2mlvp5w conversation part artificial intelligence podcast would like get information podcast,Machine Learning
rt ykilcher openai clip https //openai.com/blog/clip/ came got curious adversarial examples turns easy find generalize semantically related adversary classes blog post https //stanislavfort.github.io/2021/01/12/openai_clip_adversarial_examples.html googlecolab https //github.com/stanislavfort/openai_clip_adversarial_examples/blob/main/openai_clip_adversarial_images_playground.ipynb,openai clip openai.com/blog/clip/ came got curious adversarial examples turns easy find generalize semantically related adversary classes blog post stanislavfort.github.io/2021… googlecolab github.com/stanislavfort/ope…,Machine Learning
76 – john hopfield physics view mind neurobiology,john hopfield professor princeton whose life ’ work weaved beautifully biology chemistry neuroscience physics crucially saw messy world biology piercing eyes physicist perhaps best known work associate neural networks known hopfield networks one early ideas catalyzed development modern field deep learning episode links article http //bit.ly/3843leu john wikipedia https //en.wikipedia.org/wiki/john_hopfield books mentioned – einstein ’ dreams https //amzn.to/2pba96x – mind flat https //amzn.to/2i3yb84 conversation part artificial intelligence podcast would like,Machine Learning
trained model next,kaggle ’ excited showcase work grandmasters post written vladimir iglovikov filled advice wishes someone shared active kaggle original post found vlad ’ ternaus blog introduction participated machine learning ml competitions kaggle platforms build machine learning muscles 19th global rating got kaggle grandmaster title every ml challenge ended new knowledge code model weights loved new learnings ignored value old ml pipelines could bring code stayed private github repositories weights scattered hard drive end deleted situation unique kaggle story academia student trains model writes paper accepted conference pipelines abandoned training artifacts deleted student moves article talk small steps end every ml challenge steps boost technical knowledge build personal brand improve career opportunities make world better place example use repository https //github.com/ternaus/retinaface part kaggle challenge created illustrate story +5 min release code public github repository likely code already github private repo lose make public situations private stay private pet project kaggle solution paper may case common obstacle seen people assume public code perfect judged case reality one cares release without polishing making code public important psychological step releasing non-perfect code confident bold move besides later steps based one example https //github.com/ternaus/retinaface ii +20 min improve readability improve readability python code adding syntax formatters checkers hard time-consuming checkers formatters transform bad code good readability go think fixing syntax basic hygiene like brushing teeth code wrote blog post topic called nine simple steps better looking python code feel free check step 1 configuration files add files root repository setup.cfg — configuration flake8 mypy pyproject.toml — configuration black step 2 requirements install required libraries pip install black flake8 mypystep 3 black 100500 ways format code formatters like black yapf modify code satisfy pre-defined set rules easier read codebase standards work code hours need switch context different coding styles drains “ willpower energy ” — need without good reason running black reformat python files follow set rules black step 4 flake8 running flake8 modify code check code syntax issues output screen fix step 5 mypy python mandatory static typization recommended add types function arguments return types example class mymodel nn.module .... def forward x torch.tensor -/preollili/olpre/preh4/h4strong/strongstrong/strongstrong/strongstrong/stronga href= '' https //github.com/ternaus/retinaface/blob/master/.pre-commit-config.yaml '' /apre/prepre/preh4/h4a href= '' https //github.com/ternaus/retinaface/blob/master/.github/workflows/ci.yml '' /apre/prea href= '' https //amzn.to/32fiaio '' /ah3/h3ullili/ulullililili/ulstrong/stronga href= '' https //github.com/ternaus/retinaface/blob/master/retinaface/predict_single.py '' /a,Machine Learning
optimization models subset cover,recent newsletter article complained researchers mislead applicability work gave sat solvers example people provided interesting examples response new concept smt satisfiability modulo theories extension sat smt seems practical uses vanilla sat see newsletter details wanted take time explore smt solvers landed z3 open-source smt solver microsoft particular wanted compare ilp integer linear programing solvers know relatively well picked problem thought would work better sat-ish solvers ilps subset covering explained next section ilp still wins z3 would great claim smt production strength solver code used post github subset covering subset covering kind combinatorial design explained terms magic rings adventurer stumbles upon chest full magic rings ring magical property pairs rings worn together hand produce combined special magical effect distinct pair adventurer would like try pairs rings catalogue magical interactions five fingers minimize time spent trying rings mathematically rings described set size want choose family subsets subset size 5 five fingers subset size 2 pairs rings contained subset want small possible subset covering “ production worthy ” problem rather could imagine ’ useful production settings ’ heard one actually used imagine instance cluster machines bug occurring seemingly random point-to-point rpcs tracking problem want deploy test change subsets servers observe bug occurring something like experiment design problem generalize “ 5 ” “ 5 fingers ” arbitrary positive integer “ 2 ” “ 2 rings ” solvestatus.solved 1/preimg src= '' https //s0.wp.com/latex.php latex=n 3d12 2c+k 3d6 bg=ffffff fg=36312d s=0 c=20201002 '' alt= '' n=12 k=6 '' title= '' n=12 k=6 '' class= '' latex '' /pre class= '' brush plain title notranslate '' /prepre class= '' brush plain title notranslate '' /preh2 class= '' has-text-align-center '' /h2a href= '' https //www.scipopt.org/ '' /aa href= '' https //www.gurobi.com/ '' /aa href= '' https //github.com/j2kun/subset-cover/blob/main/subset_cover_ilp.py '' /aimg src= '' https //s0.wp.com/latex.php latex= 5ctextup 7bmember 7d_ 7bs 2ci 7d+ 5cin+ 5c 7b+0 2c+1+ 5c 7d bg=ffffff fg=36312d s=0 c=20201002 '' alt= '' \textup member \in 0 1 '' title= '' \textup member \in 0 1 '' class= '' latex '' /img src= '' https //s0.wp.com/latex.php latex=i bg=ffffff fg=36312d s=0 c=20201002 '' alt= '' '' title= '' '' class= '' latex '' /img src= '' https //s0.wp.com/latex.php latex=s bg=ffffff fg=36312d s=0 c=20201002 '' alt= '' '' title= '' '' class= '' latex '' /img src= '' https //s0.wp.com/latex.php latex= 5ctextup 7bishit 7d_ 7bt 2c+s 7d+ 5cin+ 5c 7b0 2c+1 5c 7d bg=ffffff fg=36312d s=0 c=20201002 '' alt= '' \textup ishit \in 0 1\ '' title= '' \textup ishit \in 0 1\ '' class= '' latex '' /img src= '' https //s0.wp.com/latex.php latex=t bg=ffffff fg=36312d s=0 c=20201002 '' alt= '' '' title= '' '' class= '' latex '' /img src= '' https //s0.wp.com/latex.php latex=s bg=ffffff fg=36312d s=0 c=20201002 '' alt= '' '' title= '' '' class= '' latex '' /img src= '' https //s0.wp.com/latex.php latex=s bg=ffffff fg=36312d s=0 c=20201002 '' alt= '' '' title= '' '' class= '' latex '' /img src= '' https //s0.wp.com/latex.php latex=s bg=ffffff fg=36312d s=0 c=20201002 '' alt= '' '' title= '' '' class= '' latex '' /img src= '' https //s0.wp.com/latex.php latex=s bg=ffffff fg=36312d s=0 c=20201002 '' alt= '' '' title= '' '' class= '' latex '' /img src= '' https //s0.wp.com/latex.php latex=k bg=ffffff fg=36312d s=0 c=20201002 '' alt= '' k '' title= '' k '' class= '' latex '' /p class= '' has-text-align-center '' img src= '' https //s0.wp.com/latex.php latex= 5cdisplaystyle+ 5csum_ 7bi+ 5cin+x 7d+ 5ctextup 7bmember 7d_ 7bs 2c+i 7d+ 3d+k bg=ffffff fg=36312d s=0 c=20201002 '' alt= '' \displaystyle \sum_ \in x \textup member k '' title= '' \displaystyle \sum_ \in x \textup member k '' class= '' latex '' /img src= '' https //s0.wp.com/latex.php latex=t bg=ffffff fg=36312d s=0 c=20201002 '' alt= '' '' title= '' '' class= '' latex '' /p class= '' has-text-align-center '' img src= '' https //s0.wp.com/latex.php latex= 5cdisplaystyle+ 5csum_ 7bs 7d+ 5ctextup 7bishit 7d_ 7bt 2c+s 7d+ 5cgeq+1 bg=ffffff fg=36312d s=0 c=20201002 '' alt= '' \displaystyle \sum_ \textup ishit \geq 1 '' title= '' \displaystyle \sum_ \textup ishit \geq 1 '' class= '' latex '' /img src= '' https //s0.wp.com/latex.php latex=t bg=ffffff fg=36312d s=0 c=20201002 '' alt= '' '' title= '' '' class= '' latex '' /img src= '' https //s0.wp.com/latex.php latex=s bg=ffffff fg=36312d s=0 c=20201002 '' alt= '' '' title= '' '' class= '' latex '' /img src= '' https //s0.wp.com/latex.php latex= 5ctextup 7bishit 7d_ 7bt 2c+s 7d+ 3d+1 bg=ffffff fg=36312d s=0 c=20201002 '' alt= '' \textup ishit 1 '' title= '' \textup ishit 1 '' class= '' latex '' /img src= '' https //s0.wp.com/latex.php latex=t bg=ffffff fg=36312d s=0 c=20201002 '' alt= '' '' title= '' '' class= '' latex '' /img src= '' https //s0.wp.com/latex.php latex=s bg=ffffff fg=36312d s=0 c=20201002 '' alt= '' '' title= '' '' class= '' latex '' /p class= '' has-text-align-center '' img src= '' https //s0.wp.com/latex.php latex= 5cdisplaystyle+ 5csum_ 7bi+ 5cin+t 7d+ 5ctextup 7bmember 7d_ 7bs 2c+i 7d+ 5cgeq+l+ 5ccdot+ 5ctextup 7bishit 7d_ 7bt 2c+s 7d bg=ffffff fg=36312d s=0 c=20201002 '' alt= '' \displaystyle \sum_ \in \textup member \geq l \cdot \textup ishit '' title= '' \displaystyle \sum_ \in \textup member \geq l \cdot \textup ishit '' class= '' latex '' /img src= '' https //s0.wp.com/latex.php latex=l+ 3d+ 7ct 7c bg=ffffff fg=36312d s=0 c=20201002 '' alt= '' l |t| '' title= '' l |t| '' class= '' latex '' /img src= '' https //s0.wp.com/latex.php latex= 5ctextup 7bishit 7d_ 7bt 2c+s 7d bg=ffffff fg=36312d s=0 c=20201002 '' alt= '' \textup ishit '' title= '' \textup ishit '' class= '' latex '' /img src= '' https //s0.wp.com/latex.php latex= 5ctextup 7bishit 7d_ 7bt 2c+s 7d+ 3d+1 bg=ffffff fg=36312d s=0 c=20201002 '' alt= '' \textup ishit 1 '' title= '' \textup ishit 1 '' class= '' latex '' /img src= '' https //s0.wp.com/latex.php latex=l+ 3d+ 7ct 7c bg=ffffff fg=36312d s=0 c=20201002 '' alt= '' l |t| '' title= '' l |t| '' class= '' latex '' /a href= '' https //github.com/j2kun/subset-cover/blob/main/subset_cover_ilp.py '' /apre class= '' brush plain title notranslate '',Machine Learning
curl contrastive unsupervised representations reinforcement learning,according yann le cun next big thing machine learning unsupervised learning self-supervision changed entire game last years deep learning first transforming language world word2vec bert -- 's turning computer vision upside week yannic connor spoke one authors aravind srinivas recently co-led hot-off-the-press curl contrastive unsupervised representations reinforcement learning alongside michael misha laskin curl incredible reception ml community last month remember deep mind paper solved atari games using raw pixels aravind 's approach uses contrastive unsupervised learning featurise pixels applying rl curl first image-based algorithm nearly match sample-efficiency performance methods use state-based features huge step forwards able apply rl real world explore rl self-supervision computer vision detail find aravind got machine learning original youtube video https //youtu.be/1mprzvynpy8 paper curl contrastive unsupervised representations reinforcement learning aravind srinivas michael laskin pieter abbeel https //arxiv.org/pdf/2004.04136.pdf yannic 's analysis video https //www.youtube.com/watch v=hg2q_o5b9w4 machinelearning reinforcementlearning curl timscarfe yannickilcher connorshorten music credit https //soundcloud.com/errxrmusic/in-my-mind,Machine Learning
bjarne stroustrup c++,bjarne stroustrup creator c++ programming language 40 years still one popular powerful languages world focus fast stable robust code underlies many biggest systems world come rely society ’ watching youtube many critical back-end component youtube written c++ goes google facebook amazon twitter microsoft applications adobe applications database systems physical systems operate real-world like cars robots rockets,Machine Learning
hope everyone stays safe today,hope everyone stays safe today,Machine Learning
reproducible ml struggle,'ve ml research production settings years 've noticed lot discussion reproducibility ml results research little found way achieving reproducibility production setting many overlaps two production might even superset certainly aspects reproducibility production feel require community-wide effort really get going personally found way easier write reproducible ml code research production break reproducible ml achieved setting systems track version code configuration environment data get point one establish solid system across fast-moving ml team industry quite difficult requires systems overhead e.g writing reproducible train.py paper ca n't one looking 6 month old jupyter notebook wondering heck certain model produced company even trying re-deploy ml model production sometimes gives different result expected 'd curious see practitioners struggle reproduce machine learning production war stories share thoughts processes/tools need used ensure reproducibility production disclaimer 'm part team building zenml open-source mlops framework whose design centers around enabling reproducibility machine learning gave two cents blog post wrote think important aspects blog post https //blog.maiot.io/is-your-ml-reproducible/ link comments,Machine Learning
google coral mini pcie clustering,computer accommodate 3 mini pcie corals install 3 cluster would set os ubuntu link comments,Machine Learning
leonard susskind quantum mechanics string theory black holes,leonard susskind professor theoretical physics stanford university founding director stanford institute theoretical physics widely regarded one fathers string theory general one greatest physicists time researcher educator conversation part artificial intelligence podcast would like get information podcast go https //lexfridman.com/ai connect lexfridman twitter linkedin facebook medium youtube watch video versions conversations enjoy podcast,Machine Learning
predators prey numberphile,tom crawford talks foxes rabbits links stuff full description ↓↓↓ tom numberphile http //bit.ly/tomplaylist tom 's website https //tomrocksmaths.com numberphile supported mathematical sciences research institute msri http //bit.ly/msrinumberphile also supported science sandbox simons foundation initiative dedicated engaging everyone process science https //www.simonsfoundation.org/outreach/science-sandbox/ support math america https //www.mathforamerica.org/ numberphile website http //www.numberphile.com/ numberphile facebook http //www.facebook.com/numberphile numberphile tweets https //twitter.com/numberphile subscribe http //bit.ly/numberphile_sub videos brady haran animated pete mcpartlan patreon http //www.patreon.com/numberphile numberphile t-shirts merch https //teespring.com/stores/numberphile brady 's videos subreddit http //www.reddit.com/r/bradyharan/ brady 's latest videos across channels http //www.bradyharanblog.com/ sign occasional emails http //eepurl.com/ydjl9 thanks patreon supporters ben delo arjun chakroborty jeff straathof andy b yana chernobilsky ken baron gnare tom marshall jesús saucedo john zelinka ubiquity ventures mateusz swiatkowski garrett smith ron hochsprung jeremy buchanan nat tyce matthew schuster mitch harding rad donato andrei burke ben white adam savage steve crutchfield james bissonette jubal john jordan w oja anna mirik gogri asymptote alex khein kermit norlund bodhisattva debnath bernd sing charles southerland alfred wallace valentin arnas kristian joensen tracy parry ian george walker,Machine Learning
vijay kumar flying robots,vijay kumar one top roboticists world professor university pennsylvania dean penn engineering former director grasp lab general robotics automation sensing perception laboratory penn established back 1979 40 years ago vijay perhaps best known work multi-robot systems robot swarms micro aerial vehicles robots elegantly cooperate flight uncertainty challenges real-world conditions present conversation part artificial intelligence podcast would like get information podcast go,Machine Learning
lex plays stanley parable,simulated existential crisis form video game please check sponsors tryolabs https //tryolabs.com/lex vincero https //vincerowatches.com/lex get 25 free shipping outline 0:00 introduction 0:48 round 1 matrix 8:12 round 2 reincarnation 9:58 round 3 winning 12:39 round 4 adventure line 16:30 round 5 confusion 19:58 round 6 mind control 35:09 round 7 dream within dream 42:28 round 8 ego death 47:49 death becomes meaningless connect subscribe youtube channel twitter https //twitter.com/lexfridman linkedin https //www.linkedin.com/in/lexfridman facebook https //www.facebook.com/lexfridmanpage instagram https //www.instagram.com/lexfridman medium https //medium.com/ lexfridman support patreon https //www.patreon.com/lexfridman,Machine Learning
univariate function optimization python,tweet share share optimize function one variable univariate function optimization involves finding input function results optimal output objective function common procedure machine learning fitting model one parameter tuning model single hyperparameter efficient algorithm required solve optimization problems type find best solution minimum number evaluations objective function given evaluation objective function could computationally expensive fitting evaluating model dataset excludes expensive grid search random search algorithms favor efficient algorithms like brent ’ method tutorial discover perform univariate function optimization python completing tutorial know univariate function optimization involves finding optimal input objective function takes single continuous argument perform univariate function optimization unconstrained convex function perform univariate function optimization unconstrained non-convex function let ’ get started univariate function optimization python photo robert haandrikman rights reserved tutorial overview tutorial divided three parts univariate function optimization convex univariate function optimization non-convex univariate function optimization univariate function optimization may need find optimal value function takes single parameter machine learning may occur many situations finding coefficient model fit training dataset finding value single hyperparameter results best model performance called univariate function optimization may interested minimum outcome maximum outcome function although simplified minimization maximizing function made minimizing adding negative sign outcomes function may may limits inputs function so-called unconstrained constrained optimization assume small changes input correspond small changes output function e.g smooth function may may single optima although prefer single optima shape function looks like large basin case know sample function one point find path minima function technically referred convex function minimization concave maximization functions ’ basin shape referred non-convex convex target function single optima shape target function leads optima nevertheless target function sufficiently complex ’ know derivative meaning use calculus analytically compute minimum maximum function gradient zero referred function non-differentiable although might able sample function candidate values ’ know input result best outcome may many reasons expensive evaluate candidate solutions therefore require algorithm efficiently samples input values function one approach solving univariate function optimization problems use brent ’ method brent ’ method optimization algorithm combines bisecting algorithm dekker ’ method inverse quadratic interpolation used constrained unconstrained univariate function optimization brent-dekker method extension bisection method root-finding algorithm combines elements secant method inverse quadratic interpolation reliable fast convergence properties univariate optimization algorithm choice many popular numerical optimization packages — pages 49-51 algorithms optimization 2019 bisecting algorithms use bracket lower upper input values split input domain bisecting order locate domain optima located much like binary search dekker ’ method one way achieved efficiently continuous domain dekker ’ method gets stuck non-convex problems brent ’ method modifies dekker ’ method avoid getting stuck also approximates second derivative objective function called secant method effort accelerate search brent ’ method univariate function optimization generally preferred univariate function optimization algorithms given efficiency brent ’ method available python via minimize_scalar scipy function takes name function minimized target function constrained range specified via “ bounds ” argument returns optimizeresult object dictionary containing solution importantly ‘ x ‘ key summarizes input optima ‘ fun ‘ key summarizes function output optima ‘ nfev ‘ summarizes number evaluations target function performed ... minimize function result minimize_scalar objective method='brent know perform univariate function optimization python let ’ look examples convex univariate function optimization section explore solve convex univariate function optimization problem first define function implements function case use simple offset version x^2 function e.g simple parabola u-shape function minimization objective function optima -5.0 objective function def objective x return 5.0 x 2.0 plot coarse grid function input values -10 10 get idea shape target function complete example listed plot convex target function numpy import arange matplotlib import pyplot objective function def objective x return 5.0 x 2.0 define range r_min r_max -10.0 10.0 prepare inputs inputs arange r_min r_max 0.1 compute targets targets objective x x inputs plot inputs vs target pyplot.plot inputs targets -- pyplot.show running example evaluates input values specified range using target function creates plot function inputs function outputs see u-shape function objective -5.0 line plot convex objective function note real optimization problem would able perform many evaluations objective function easily simple function used demonstration purposes learn use optimization algorithm next use optimization algorithm find optima ... minimize function result minimize_scalar objective method='brent optimized summarize result including input evaluation optima number function evaluations required locate optima ... summarize result opt_x opt_y result x result 'fun print 'optimal input x .6f opt_x print 'optimal output f x .6f opt_y print 'total evaluations n result 'nfev finally plot function mark optima confirm located place expected function ... define range r_min r_max -10.0 10.0 prepare inputs inputs arange r_min r_max 0.1 compute targets targets objective x x inputs plot inputs vs target pyplot.plot inputs targets -- plot optima pyplot.plot opt_x opt_y 's color= r show plot pyplot.show complete example optimizing unconstrained convex univariate function listed optimize convex objective function numpy import arange scipy.optimize import minimize_scalar matplotlib import pyplot objective function def objective x return 5.0 x 2.0 minimize function result minimize_scalar objective method='brent summarize result opt_x opt_y result x result 'fun print 'optimal input x .6f opt_x print 'optimal output f x .6f opt_y print 'total evaluations n result 'nfev define range r_min r_max -10.0 10.0 prepare inputs inputs arange r_min r_max 0.1 compute targets targets objective x x inputs plot inputs vs target pyplot.plot inputs targets -- plot optima pyplot.plot opt_x opt_y 's color= r show plot pyplot.show running example first solves optimization problem reports result note results may vary given stochastic nature algorithm evaluation procedure differences numerical precision consider running example times compare average outcome case see optima located 10 evaluations objective function input -5.0 achieving objective function value 0.0 optimal input x -5.000000 optimal output f x 0.000000 total evaluations n 10 plot function created time optima marked red square line plot convex objective function optima marked non-convex univariate function optimization convex function one resemble basin meaning may one hill valley make challenging locate global optima multiple hills valleys cause search get stuck report false local optima instead define non-convex univariate function follows objective function def objective x return x 2.0 x x 2.0 2.0 sample function create line plot input values objective values complete example listed plot non-convex univariate function numpy import arange matplotlib import pyplot objective function def objective x return x 2.0 x x 2.0 2.0 define range r_min r_max -3.0 2.5 prepare inputs inputs arange r_min r_max 0.1 compute targets targets objective x x inputs plot inputs vs target pyplot.plot inputs targets -- pyplot.show running example evaluates input values specified range using target function creates plot function inputs function outputs see function one false optima around -2.0 global optima around 1.2 line plot non-convex objective function note real optimization problem would able perform many evaluations objective function easily simple function used demonstration purposes learn use optimization algorithm next use optimization algorithm find optima call minimize_scalar function optimize function summarize result plot optima line plot complete example optimization unconstrained non-convex univariate function listed optimize non-convex objective function numpy import arange scipy.optimize import minimize_scalar matplotlib import pyplot objective function def objective x return x 2.0 x x 2.0 2.0 minimize function result minimize_scalar objective method='brent summarize result opt_x opt_y result x result 'fun print 'optimal input x .6f opt_x print 'optimal output f x .6f opt_y print 'total evaluations n result 'nfev define range r_min r_max -3.0 2.5 prepare inputs inputs arange r_min r_max 0.1 compute targets targets objective x x inputs plot inputs vs target pyplot.plot inputs targets -- plot optima pyplot.plot opt_x opt_y 's color= r show plot pyplot.show running example first solves optimization problem reports result want get started ensemble learning take free 7-day email crash course sample code click sign-up also get free pdf ebook version course download free mini-course case see optima located 15 evaluations objective function input 1.28 achieving objective function value -9.91 optimal input x 1.280776 optimal output f x -9.914950 total evaluations n 15 plot function created time optima marked red square see optimization deceived false optima successfully located global optima line plot non-convex objective function optima marked reading section provides resources topic looking go deeper books algorithms optimization 2019 apis optimization scipy.optimize optimization root finding scipy.optimize scipy.optimize.minimize_scalar api articles brent ’ method wikipedia secant method wikipedia summary tutorial discovered perform univariate function optimization python specifically learned univariate function optimization involves finding optimal input objective function takes single continuous argument perform univariate function optimization unconstrained convex function perform univariate function optimization unconstrained non-convex function questions ask questions comments best answer tweet share share post univariate function optimization python appeared first machine learning mastery,Machine Learning
119 – david eagleman neuroplasticity livewired brain,david eagleman neuroscientist stanford support podcast supporting sponsors – athletic greens https //athleticgreens.com/lex – betterhelp https //betterhelp.com/lex – cash app download app use code “ lexpodcast ” episode links david ’ website https //www.eagleman.com/ david ’ twitter https //twitter.com/davideagleman livewired book https //amzn.to/3ba4ezv would like get information podcast go https //lexfridman.com/podcast connect lexfridman twitter linkedin facebook medium youtube watch video versions conversations enjoy podcast please rate 5 stars apple podcasts follow spotify support patreon ’ outline,Machine Learning
73 – andrew ng deep learning education real-world ai,andrew ng one impactful educators researchers innovators leaders artificial intelligence technology space general co-founded coursera google brain launched deeplearning.ai landing.ai ai fund chief scientist baidu stanford professor coursera deeplearning.ai helped educate inspire millions students including episode links andrew twitter https //twitter.com/andrewyng andrew facebook https //www.facebook.com/andrew.ng.96 andrew linkedin https //www.linkedin.com/in/andrewyng/ deeplearning.ai https //www.deeplearning.ai landing.ai https //landing.ai ai fund https //aifund.ai/ ai everyone https //www.coursera.org/learn/ai-for-everyone batch newsletter https //www.deeplearning.ai/thebatch/ conversation part artificial intelligence podcast would like,Machine Learning
recommendations startups solid ml,hello redditors wanted recommendations startups really cool ml keep finding generic content web wanted know startups solid value creation via application ml know lot work big companies wanted understand niches startups applying ml thanks link comments,Machine Learning
r degenerative adversarial neuroimage nets brain scan simulations application ageing dementia,link paper anyone interested paper and/or code please n't hesitate message ​ accurate realistic simulation high-dimensional medical images become important research area relevant many ai-enabled healthcare applications however current state-of-the-art approaches lack ability produce satisfactory high-resolution accurate subject-specific images work present deep learning framework namely 4d-degenerative adversarial neuroimage net 4d-dani-net generate high-resolution longitudinal mri scans mimic subject-specific neurodegeneration ageing dementia 4d-dani-net modular framework based adversarial training set novel spatiotemporal biologically-informed constraints ensure efficient training overcome memory limitations affecting high-dimensional problems rely three key technological advances new 3d training consistency mechanism called profile weight functions pwfs ii 3d super-resolution module iii transfer learning strategy fine-tune system given individual evaluate approach trained framework 9852 t1-weighted mri scans 876 participants alzheimer 's disease neuroimaging initiative dataset held separate test set 1283 mri scans 170 participants quantitative qualitative assessment personalised time series synthetic images performed three evaluations image quality assessment ii quantifying accuracy regional brain volumes benchmark models iii quantifying visual perception synthetic images medical experts overall quantitative qualitative results show 4d-dani-net produces realistic low-artefact personalised time series synthetic t1 mri outperforms benchmark models link comments,Machine Learning
automl conversation gnosis data analysis,statquest sponsored jadbio add data automatic machine learning algorithms work details see https //bit.ly/3bxtheb automl rage days statquest interview ceo co-founder gnosis data analysis find latest info ⭐ note code use kite free ai-powered coding assistant help code faster smarter kite plugin integrates top editors ides give smart completions documentation ’ typing love https //www.kite.com/get-kite/ utm_medium=referral utm_source=youtube utm_campaign=statquest utm_content=description-only complete index statquest videos check https //statquest.org/video-index/ 'd like support statquest please consider ... patreon https //www.patreon.com/statquest ... ... youtube membership https //www.youtube.com/channel/uctyluttgs3k1fg4y5tahlbw/join ... cool statquest t-shirt sweatshirt usa/europe https //teespring.com/stores/statquest everywhere https //www.redbubble.com/people/starmer/works/40421224-statquest-double-bam asc=u p=t-shirt ... buying one two songs go large get whole album https //joshuastarmer.bandcamp.com/ ... donating statquest https //www.paypal.me/statquest lastly want keep research create new statquests follow twitter https //twitter.com/joshuastarmer 0:00 awesome song introduction 0:43 automl 3:04 automl 5:04 need know automl 7:38 manual ml vs automl 10:46 automl replace data science jobs 12:28 future automl statquest automl,Machine Learning
video tutorial visualizing activations forward hooks,https //youtu.be/1zbla7ofasy hi filmed video forward hooks pytorch would happy get feedback constructive criticism planning `` applied ml/dl videos '' regular basis big proponent free online education learned bunch life feel like giving back little know link comments,Machine Learning
machine learning laptops becoming worth,'ve come realize workflow somewhat inefficient come debugging gpu code access cluster small cost moving code requesting gpu schedule manager etc adds 're debugging code debug cpu 're limited toy models/datasets intuition build often wrong let 's assume 's worth get gpu locally covid times many us working several locations ca n't move rig around finally new gaming laptops pack surprisingly good gpus get rtx 2080 alienware r3 new version laptop available end jan expected offer rtx 3080 understand laptop gpus less efficient desktop counterparts still enough run small wrn 10 epochs cifar10 ~1 min thoughts link comments,Machine Learning
last bi vendor please turn lights,read full story,Machine Learning
chain rule,chain rule method finding complex derivatives used time statistics machine learning video breaks two simple pieces shows easily come together use chain rule solve common machine learning problem optimizing residual squared loss function ⭐ note code use kite free ai-powered coding assistant help code faster smarter kite plugin integrates top editors ides give smart completions documentation ’ typing love https //www.kite.com/get-kite/ utm_medium=referral utm_source=youtube utm_campaign=statquest utm_content=description-only complete index statquest videos check https //statquest.org/video-index/ 'd like support statquest please consider ... patreon https //www.patreon.com/statquest ... ... youtube membership https //www.youtube.com/channel/uctyluttgs3k1fg4y5tahlbw/join ... cool statquest t-shirt sweatshirt usa/europe https //teespring.com/stores/statquest everywhere https //www.redbubble.com/people/starmer/works/40421224-statquest-double-bam asc=u p=t-shirt ... buying one two songs go large get whole album https //joshuastarmer.bandcamp.com/ ... donating statquest https //www.paypal.me/statquest lastly want keep research create new statquests follow twitter https //twitter.com/joshuastarmer 0:00 awesome song introduction 2:02 super simple example 6:32 slightly complicated example 9:16 chain rule relationship obvious 11:47 chain rule residual sum squares statquest thechainrule,Machine Learning
p preview pdf arxiv abstract page,script working tampermonkey chrome greasemonkey firefox filesave/arxiv_pdf_preview.js master · onemoresecond/filesave github.com motivations prefer abstract page pdf page sometimes title pdf page something like 2006.04664.pdf informational enough saved browser favorites sometimes pdf link old version one like 2006.04664_v1.pdf abstract page helps view latest submission link comments,Machine Learning
think shortcomings arguments argue artificial intelligence never created important penrose 's theories academic responses,penrose making mistakes pursuing impossible dream link comments,Machine Learning
gavin miller adobe research,gavin miller head adobe research adobe empowered artists designers creative minds professions working digital medium 30 years software photoshop illustrator premiere effects indesign audition work images video audio adobe research working define future evolution products way makes life creatives easier automates tedious tasks gives time operate idea space instead pixel space cutting-edge deep learning methods past decade,Machine Learning
rt ykilcher great video summary recent work thanks ykilcher,great video summary recent work thanks ykilcher nitter.net/ykilcher/status/1352704316080091139,Machine Learning
ayanna howard human-robot interaction ethics safety-critical systems,ayanna howard roboticist professor georgia tech director human-automation systems lab research interests human-robot interaction assistive robots home therapy gaming apps remote robotic exploration extreme environments conversation part artificial intelligence podcast would like get information podcast go https //lexfridman.com/ai connect lexfridman twitter linkedin facebook medium youtube watch video versions conversations enjoy podcast please rate 5 stars apple podcasts follow spotify support patreon,Machine Learning
colouring knots numberphile,featuring professor sylvain cappell nyu extra footage https //youtu.be/nv3eeagyu0y links stuff full description ↓↓↓ merch based video https //teespring.com/numberphile-knots https //teespring.com/numberphile-figure-eight numberphile supported mathematical sciences research institute msri http //bit.ly/msrinumberphile also supported science sandbox simons foundation initiative dedicated engaging everyone process science https //www.simonsfoundation.org/outreach/science-sandbox/ support math america https //www.mathforamerica.org/ numberphile website http //www.numberphile.com/ numberphile facebook http //www.facebook.com/numberphile numberphile tweets https //twitter.com/numberphile subscribe http //bit.ly/numberphile_sub video brady haran pete mcpartlan patreon http //www.patreon.com/numberphile numberphile t-shirts merch https //teespring.com/stores/numberphile brady 's videos subreddit http //www.reddit.com/r/bradyharan/ brady 's latest videos across channels http //www.bradyharanblog.com/ sign occasional emails http //eepurl.com/ydjl9,Machine Learning
applications python programming language,python trending second popular programming language world grabbed position edging java read full story,Machine Learning
neural networks part 2 backpropagation main ideas,backpropagation method use optimize parameters neural network ideas behind backpropagation quite simple tons details statquest focuses explaining main ideas way easy understand ⭐ note code use kite free ai-powered coding assistant help code faster smarter kite plugin integrates top editors ides give smart completions documentation ’ typing love https //www.kite.com/get-kite/ utm_medium=referral utm_source=youtube utm_campaign=statquest utm_content=description-only note statquest assumes already know main ideas behind ... neural networks https //youtu.be/cqofi41lfdw chain rule https //youtu.be/wl1myxrtqhq gradient descent https //youtu.be/sdv4f4s2sb8 last note researching 'quest found page sebastian raschka helpful https //sebastianraschka.com/faq/docs/backprop-arbitrary.html complete index statquest videos check https //statquest.org/video-index/ 'd like support statquest please consider ... patreon https //www.patreon.com/statquest ... ... youtube membership https //www.youtube.com/channel/uctyluttgs3k1fg4y5tahlbw/join ... cool statquest t-shirt sweatshirt usa/europe https //teespring.com/stores/statquest everywhere https //www.redbubble.com/people/starmer/works/40421224-statquest-double-bam asc=u p=t-shirt ... buying one two songs go large get whole album https //joshuastarmer.bandcamp.com/ ... donating statquest https //www.paypal.me/statquest lastly want keep research create new statquests follow twitter https //twitter.com/joshuastarmer 0:00 awesome song introduction 3:55 fitting neural network data 6:04 sum squared residuals 7:23 testing different values parameter 8:38 using chain rule calculate derivative 13:28 using gradient descent 16:05 summary statquest neuralnetworks backpropagation,Machine Learning
88 – eric weinstein geometric unity call new ideas leaders institutions,eric weinstein mathematician bold piercing intelligence unafraid explore biggest questions universe shine light darkest corners society host portal podcast part recently released 2013 oxford lecture theory geometric unity center lifelong efforts arriving theory everything unifies fundamental laws physics support podcast signing sponsors – cash app – use code “ lexpodcast ” download – cash app,Machine Learning
finding increasingly hard keep activity deep learning right tabs \infty tab width 0 n't even decade since alexnet ~sep 2012 ~8.5yrs lot happened another 8.5 ~2030 wonder 's like,finding increasingly hard keep activity deep learning right tabs,Machine Learning
butterflies gyroids numberphile,dr sabetta matsumoto discusses mathematics colour nature including butterfly wings soap films links stuff full description ↓↓↓ dr matsumoto 's group georgia tech http //matsumoto.gatech.edu twitter https //twitter.com/sabetta_ previous video sabetta numberphile https //youtu.be/0e5vztslh2s brady looks butterflies objectivity https //youtu.be/ljwhppdupbi numberphile supported mathematical sciences research institute msri http //bit.ly/msrinumberphile also supported science sandbox simons foundation initiative dedicated engaging everyone process science https //www.simonsfoundation.org/outreach/science-sandbox/ support math america https //www.mathforamerica.org/ numberphile website http //www.numberphile.com/ numberphile facebook http //www.facebook.com/numberphile numberphile tweets https //twitter.com/numberphile subscribe http //bit.ly/numberphile_sub videos brady haran patreon http //www.patreon.com/numberphile numberphile t-shirts merch https //teespring.com/stores/numberphile brady 's videos subreddit http //www.reddit.com/r/bradyharan/ brady 's latest videos across channels http //www.bradyharanblog.com/ sign occasional emails http //eepurl.com/ydjl9,Machine Learning
rt ykilcher hilarious lot truth ykilcher,hilarious lot truth ykilcher,Machine Learning
david ferrucci ibm watson jeopardy deep conversations ai,david ferrucci led team built watson ibm question-answering system beat top humans world game jeopardy also founder ceo chief scientist elemental cognition company working engineer ai systems understand world way people conversation part artificial intelligence podcast would like get information podcast go https //lexfridman.com/ai connect lexfridman twitter linkedin facebook medium youtube watch video versions conversations enjoy podcast please,Machine Learning
4 ways data science helps streamline business operations,data science changed way organizations collect analyze process different types information read full story,Machine Learning
variance inflation factors vifs,variance inflation factors vifs measure correlation among independent variables least squares regression models statisticians refer type correlation multicollinearity excessive multicollinearity cause problems regression models post focus vifs detect multicollinearity ’ better pairwise correlations calculate vifs … post variance inflation factors vifs appeared first statistics jim,Machine Learning
noam chomsky language cognition deep learning,noam chomsky one greatest minds time one cited scholars history linguist philosopher cognitive scientist historian social critic political activist spent 60 years mit recently also joined university arizona conversation part artificial intelligence podcast would like get information podcast go https //lexfridman.com/ai connect lexfridman twitter linkedin facebook medium youtube watch video versions conversations enjoy podcast please rate,Machine Learning
99 – karl friston neuroscience free energy principle,"karl friston one greatest neuroscientists history cited 245,000 times known many influential ideas brain imaging neuroscience theoretical neurobiology including fascinating idea free-energy principle action perception support podcast signing sponsors – cash app – use code “ lexpodcast ” download – cash app app store https //apple.co/2spruhe – cash app google play https //bit.ly/2mlvp5w episode links karl ’ website https //www.fil.ion.ucl.ac.uk/~karl/ karl ’ wiki https //en.wikipedia.org/wiki/karl_j._friston conversation part artificial intelligence podcast would like get information podcast go https //lexfridman.com/ai connect lexfridman",Machine Learning
natural language processing brief overview,natural language processing nlp subfield artificial intelligence ability analyze process natural language read full story,Machine Learning
106 – matt botvinick neuroscience psychology ai deepmind,matt botvinick director neuroscience research deepmind brilliant cross-disciplinary mind navigating effortlessly cognitive psychology computational neuroscience artificial intelligence support podcast supporting sponsors – jordan harbinger show https //www.jordanharbinger.com/lex – magic spoon https //magicspoon.com/lex use code lex checkout would like get information podcast go https //lexfridman.com/ai connect lexfridman twitter linkedin facebook medium youtube watch video versions conversations enjoy podcast please rate 5 stars apple podcasts follow spotify,Machine Learning
oriol vinyals deepmind alphastar starcraft language sequences,"oriol vinyals senior research scientist google deepmind google brain berkeley research cited 39,000 times one brilliant impactful minds field deep learning behind biggest papers ideas ai including sequence sequence learning audio generation image captioning neural machine translation reinforcement learning co-lead david silver alphastar project creating agent defeated top professional game starcraft would like get",Machine Learning
stemming lemmatization,first video nlp series talk stemming lemmatization hope like video please subscribe like video help keep motivated make awesome videos like one buy book approaching almost machine learning problem please visit https //bit.ly/buyaaml follow twitter https //twitter.com/abhi1thakur linkedin https //www.linkedin.com/in/abhi1thakur/ kaggle https //kaggle.com/abhishek instagram https //instagram.com/abhi4ml,Machine Learning
conform,conform nitter.net/viacristiano/status/1347705178699558912,Machine Learning
fintech 2021 fintech companies use big data effectively,according study 90 whole world ’ data created last two years sounds quite cool world data one analyze read full story,Machine Learning
greg brockman openai agi,greg brockman co-founder cto openai research organization developing ideas ai lead eventually safe friendly artificial general intelligence benefits empowers humanity video version available youtube would like get information podcast go https //lexfridman.com/ai connect lexfridman twitter linkedin facebook medium youtube watch video versions conversations,Machine Learning
10 best image classification datasets ml projects,help build object recognition models scene recognition models ’ compiled list best image classification datasets datasets vary scope magnitude suit variety use cases furthermore datasets divided following categories medical imaging agriculture scene recognition others read full story,Machine Learning
likely live one 18 universes,get free access 2500 documentaries curiositystream http //go.thoughtleaders.io/1621220200713 use promo code `` zachstar '' sign stemerch store https //stemerch.com/ support channel https //www.patreon.com/zachstar paypal one time donation https //www.paypal.me/zachstaryt download `` curved spaces '' application http //www.geometrygames.org/curvedspaces/index.html.en previous video https //youtu.be/lmct2mp2bfe follow video spherical universe https //youtu.be/iige2x8t6ma possible shapes universe article https //www.americanscientist.org/sites/americanscientist.org/files/200522415348_306.pdf ►follow instagram https //www.instagram.com/zachstar/ twitter https //twitter.com/imzachstar animations brainup studios http //brainup.in/ ►my setup space pictures https //amzn.to/2cc4kqj magnetic floating globe https //amzn.to/2vgpdn0 camera https //amzn.to/2rivyu5 mic https //amzn.to/35bkiri tripod https //amzn.to/2rgmtnl equilibrium tube https //amzn.to/2sowdrh ►check amazon store https //www.amazon.com/shop/zachstar,Machine Learning
152 – dan gable olympic wrestling mental toughness making champions,dan gable one greatest olympic athletes wrestling coaches time please support podcast checking sponsors – tryolabs https //tryolabs.com/lex – expressvpn https //expressvpn.com/lexpod use code lexpod get 3 months free – grammarly https //grammarly.com/lex get 20 premium – simplisafe https //simplisafe.com/lex use code lex get free security camera episode links dan ’ twitter https //twitter.com/dannygable dan ’ website https //dangable.com/ dan ’ books https //amzn.to/2vk5nbn podcast info podcast website https //lexfridman.com/podcast apple podcasts https //apple.co/2lwqzir spotify https //spoti.fi/2newcf8 rss https //lexfridman.com/feed/podcast/ youtube full episodes https //youtube.com/lexfridman youtube clips https //youtube.com/lexclips support connect – check sponsors ’ best way support podcast – support patreon https //www.patreon.com/lexfridman – twitter https //twitter.com/lexfridman – instagram https //www.instagram.com/lexfridman – linkedin https //www.linkedin.com/in/lexfridman – facebook https //www.facebook.com/lexfridmanpage – medium https //medium.com/ lexfridman outline ’ timestamps episode podcast players able click timestamp jump time 00:00 – introduction 09:31 – russian wrestling 11:09 – coaching science art toughness wrestling 18:05 – pain defeat tattoo hawk clawing heart 21:04 – roger bannister 4 minute mile 24:09 – dream becoming olympic champion 26:38 – day 1972 olympic final 30:10 – sauna story 31:39 – match russian 37:13 – role fear wrestling 42:14 – line physical wrestling anger 46:53 – tragic loss dan ’ sister 54:21 – role family wrestling 59:43 – wrestling voted olympics 1:04:26 – beat best must study best 1:09:39 – role luck old man sea,Machine Learning
nlp nlu gpt-3 walid saba,machinelearning week dr. tim scarfe dr. keith duggar yannic kilcher speak veteran nlu expert dr. walid saba walid old-school ai expert polymath neuroscientist psychologist linguist philosopher statistician logician thinks missing information problem lack typed ontology key issue nlu sample efficiency generalisation big critic deep learning movement bertology also cover gpt-3 detail today 's session covering luciano floridi 's recent article `` gpt‑3 nature scope limits consequences '' commentary incredible power gpt-3 perform tasks examples including yann lecun commentary facebook hackernews time stamps youtube version 0:00:00 walid intro 00:05:03 knowledge acquisition bottleneck 00:06:11 language ambiguous 00:07:41 language learned 00:08:32 language formal language 00:08:55 learning data ’ work 00:14:01 intelligence 00:15:07 lack domain knowledge days 00:16:37 yannic kilcher thuglife comment 00:17:57 deep learning assault 00:20:07 way evaluate language models flawed 00:20:47 humans type checking 00:23:02 ontologic 00:25:48 comments gpt3 00:30:54 yann lecun reddit 00:33:57 minds machines luciano 00:35:55 main show introduction 00:39:02 walid introduces 00:40:20 science advances one funeral time 00:44:58 deep learning obsession syndrome inception 00:46:14 bertology empirical methods nlu 00:49:55 pattern recognition vs domain reasoning knowledge data 00:56:04 natural language understanding decoding compression 's learnable 01:01:46 intelligence needing infinite amounts time 01:04:23 need explicit ontological structure understand anything 01:06:40 ontological concepts 01:09:38 word embeddings 01:12:20 power structure 01:15:16 language models trained pronoun disambiguation resolving scopes 01:17:33 information data 01:19:03 generate rules fly rules data 01:20:39 missing data problem key 01:21:19 problem empirical methods lecunn reference 01:22:45 comparison meatspace brains 01:28:16 knowledge graph game knowledge constructed discovered 01:29:41 small ontology world 01:33:08 walids taxonomy understanding 01:38:49 trend seems less rules better othe way around 01:40:30 testing latest nlp models entailment 01:42:25 problems way evaluate nlp 01:44:10 winograd schema challenge 01:45:56 need know build neural networks lack rigour ml research 01:50:47 everything learnable 01:53:02 elevate language systems 01:54:04 10 big problems language missing information 01:55:59 multiple inheritance wrong 01:58:19 language ambiguous 02:01:14 big would world ontology need 02:05:49 learn nlu 02:09:10 alphago walid 's blog https //medium.com/ ontologik linkedin https //www.linkedin.com/in/walidsaba/,Machine Learning
euler 's formula actually saying lockdown math ep 4,mean compute e^ pi full playlist https //www.youtube.com/playlist list=plzhqobowtqdp5cveljj1bndouqrahvpev home page https //www.3blue1brown.com brought https //3b1b.co/ldm-thanks beautiful pictorial summary thuynganvu https //twitter.com/thuynganvu/status/1258220129327800320 https //twitter.com/thuynganvu/status/1258220541686628353 -- -- -- -- -- -- -- -- -- video timeline thanks user `` tieriffic '' 0:00:00 welcome 0:00:20 ending animation preview 0:01:15 reminders previous lecture 0:03:30 q1 prompt relationship e^iθ=… 0:05:40 q1 results 0:07:15 wtf whats function 0:10:00 exploring exp x 0:11:45 exploring exp x python 0:14:45 important exp x property 0:15:55 q2 prompt given f a+b f f b … 0:17:30 ask interesting special cases general case 0:20:00 q2 results 0:23:50 zero break q2 0:25:40 e^x convention 0:27:10 q3 prompt i^2 -1 i^n -1 0:27:45 ask zero break q2 0:30:20 q3 results 0:31:05 comparison rotation 0:33:00 visualizing relationship 0:36:50 special case π 0:39:20 periodic nature relationship 0:39:40 q4 prompt e^3i 0:41:35 q4 results 0:43:55 explaining celebrity equation 0:45:55 homework things think 0:49:15 ask zero break q2 0:50:30 closing remarks -- -- -- -- -- -- -- -- -- live question setup stats on-screen powered itempool https //itempool.com/ curious animations https //www.3blue1brown.com/faq manim music vincent rubinetti download music bandcamp https //vincerubinetti.bandcamp.com/album/the-music-of-3blue1brown stream music spotify https //open.spotify.com/album/1dvyjws8fbqxhrunag5w5u want contribute translated subtitles help review already made others need approval click gear icon video go subtitles/cc `` add subtitles/cc '' really appreciate helps make lessons accessible people -- -- -- -- -- -- -- -- -- 3blue1brown channel animating math senses word animate know drill youtube want stay posted new videos subscribe http //3b1b.co/subscribe various social media stuffs website https //www.3blue1brown.com twitter https //twitter.com/3blue1brown reddit https //www.reddit.com/r/3blue1brown instagram https //www.instagram.com/3blue1brown_animations/ patreon https //patreon.com/3blue1brown facebook https //www.facebook.com/3blue1brown,Machine Learning
`` easy denounce evildoer difficult understand '' dostoevsky,`` easy denounce evildoer difficult understand '' dostoevsky,Machine Learning
126 – james gosling java jvm emacs early days computing,james gosling founder lead designer java programming language please check sponsors get discount support podcast – public goods https //publicgoods.com/lex use code lex – betterhelp https //betterhelp.com/lex – expressvpn https //www.expressvpn.com/lexpod would like get information podcast go https //lexfridman.com/podcast connect lexfridman twitter linkedin facebook medium youtube watch video versions conversations enjoy podcast please rate 5 stars apple podcasts follow spotify support patreon ’ outline,Machine Learning
data science love letters,read full story,Machine Learning
discussion someone donated gaming pc streaming video content creation ’ overwhelmed joy ’ know anything pcs gaming could guys tell good specs give advice general gaming machine,link comments,Machine Learning
r paper reinforcement learning solving mazes,could n't find wondering anyone familiar link comments,Machine Learning
actors behind flash fill,sumit acknowledges 5 sets people 5-minute award acceptance talk `` influential '' popl 2011 paper describes technology behind popular flash fill feature excel shares inside stories role actors played invention impact influential test time paper https //www.microsoft.com/en-us/research/publication/automating-string-processing-spreadsheets-using-input-output-examples/ prose research engineering team https //www.microsoft.com/en-us/research/group/prose/,Machine Learning
137 – alex filippenko supernovae dark energy aliens expanding universe,alex filippenko astrophysicist professor astronomy berkeley please support podcast checking sponsors – neuro https //www.getneuro.com use code lex get 15 – betterhelp https //betterhelp.com/lex get 10 – masterclass https //masterclass.com/lex get 15 annual sub – cash app https //cash.app/ use code lexpodcast get 10 episode links alex ’ website https //astro.berkeley.edu/people/alex-filippenko/ podcast info podcast website https //lexfridman.com/podcast apple podcasts https //apple.co/2lwqzir spotify https //spoti.fi/2newcf8 rss https //lexfridman.com/feed/podcast/ youtube full episodes https //youtube.com/lexfridman youtube clips https //youtube.com/lexclips support connect – check sponsors ’ best way support podcast –,Machine Learning
"group theory 196,883-dimensional monster","introduction group theory monster group viewer supported https //3b1b.co/monster-thanks minor error corrections below↓↓↓ video part megafavnumbers project https //www.youtube.com/playlist list=plar4u0v66viodqt3kszpsyyuuld5meoao join gang upload video favorite number 1,000,000 hashtag megafavnumbers word megafavnumbers title september 2nd 2020 'll added playlist errors typo `` hard problem '' 14:11 a/ b+c b/ a+c c/ a+b 4 typo-turned-speako classification quasithin groups 1221 pages long 12,000 full collection papers proving cfsg theorem make tens thousands pages one paper quite _that_ crazy thanks richard borcherds helpful comments putting video together wonderful hidden gem channel https //youtu.be/a9k_qmzbwx8 may also enjoy brief article giving overview monster http //www.ams.org/notices/200209/what-is.pdf want learn group theory check expository papers https //kconrad.math.uconn.edu/blurbs/ videos john conway talking monster https //youtu.be/jsseogpiwsw https //youtu.be/lbn8emcoh5o noether 's theorem https //youtu.be/cxlhlqj9i0a https //youtu.be/04ersb06dog symmetry ambigram designed punya mishra https //punyamishra.com/2013/05/31/symmetry-new-ambigram/ monster image comes noun project via nicky knicky -- -- -- -- -- -- -- -- -- animations largely made using manim scrappy open-source python library https //github.com/3b1b/manim want check feel compelled warn 's well-documented tool many quirks might expect library someone wrote use mind music vincent rubinetti download music bandcamp https //vincerubinetti.bandcamp.com/album/the-music-of-3blue1brown stream music spotify https //open.spotify.com/album/1dvyjws8fbqxhrunag5w5u want contribute translated subtitles help review already made others need approval click gear icon video go subtitles/cc `` add subtitles/cc '' really appreciate helps make lessons accessible people -- -- -- -- -- -- -- -- -- 3blue1brown channel animating math senses word animate know drill youtube want stay posted new videos subscribe http //3b1b.co/subscribe various social media stuffs website https //www.3blue1brown.com twitter https //twitter.com/3blue1brown reddit https //www.reddit.com/r/3blue1brown instagram https //www.instagram.com/3blue1brown_animations/ patreon https //patreon.com/3blue1brown facebook https //www.facebook.com/3blue1brown",Machine Learning
6 tips working analysts data engineers,work data engineer actually let tell one thing ’ think especially part running around collecting data building yet another one dashboards used weeks read full story,Machine Learning
regina barzilay deep learning cancer diagnosis treatment,regina barzilay professor mit world-class researcher natural language processing applications deep learning chemistry oncology use deep learning early diagnosis prevention treatment cancer also recognized teaching several successful ai-related courses mit including popular introduction machine learning course conversation part artificial intelligence podcast would like get information podcast go https //lexfridman.com/ai connect lexfridman twitter linkedin facebook medium youtube watch video,Machine Learning
tuomas sandholm poker game theory,tuomas sandholm professor cmu co-creator libratus first ai system beat top human players game heads-up no-limit texas hold ’ em published 450 papers game theory machine learning including best paper 2017 nips neurips research companies wide-reaching impact real world especially group propose new ideas also build systems prove ideas work real world video version available youtube would like get,Machine Learning
142 – manolis kellis meaning life universe everything,manolis kellis computational biologist mit please support podcast checking sponsors – grammarly https //grammarly.com/lex get 20 premium – athletic greens https //athleticgreens.com/lex use code lex get 1 month fish oil – cash app https //cash.app/ use code lexpodcast get 10 episode links manolis website http //web.mit.edu/manoli/ manolis twitter https //twitter.com/manoliskellis manolis youtube https //www.youtube.com/channel/uckklj5lhre3c7fgbnpa5dga podcast info podcast website https //lexfridman.com/podcast apple podcasts https //apple.co/2lwqzir spotify https //spoti.fi/2newcf8 rss https //lexfridman.com/feed/podcast/ youtube full episodes https //youtube.com/lexfridman youtube clips https //youtube.com/lexclips support connect – check sponsors ’ best way support podcast – support,Machine Learning
17 open crime datasets data science machine learning projects,looking analyze crime rates trends specific area time period compiled list 16 best crime datasets made available public use read full story,Machine Learning
82 – simon sinek leadership hard work optimism infinite game,simon sinek author several books including start leaders eat last latest infinite game one best communicators takes good leader inspire build businesses solve big difficult challenges support podcast signing sponsors – masterclass https //masterclass.com/lex – cash app – use code “ lexpodcast ” download – cash app app store https //apple.co/2spruhe – cash app google play https //bit.ly/2mlvp5w episode links simon twitter https //twitter.com/simonsinek simon facebook https //www.facebook.com/simonsinek simon website https //simonsinek.com/ books – infinite game https //amzn.to/2wxbh1i – leaders eat,Machine Learning
using moving averages smooth time series data,moving averages smooth time series data reveal underlying trends identify components use statistical modeling smoothing process removing random variations appear coarseness plot raw time series data reduces noise emphasize signal contain trends cycles analysts also refer … post using moving averages smooth time series data appeared first statistics jim,Machine Learning
retweeting one time excellent describes nicely mrna vaccines direct hacking life 's assembly code spike protein headers metadata tweaking stable likely evade immune system defenses 👌👌,retweeting one time excellent describes nicely mrna vaccines direct hacking life 's assembly code spike protein headers metadata tweaking stable likely evade immune system defenses 👌👌 nitter.net/powerdns_bert/status/1342568484946010116,Machine Learning
keoki jackson lockheed martin,keoki jackson cto lockheed martin company long history created incredible engineering marvels human beings ever built including planes fly fast undetected defense systems intersect threats could take lives millions case nuclear weapons spacecraft systems venture space moon mars beyond without humans on-board conversation part artificial intelligence podcast would like get information podcast go https //lexfridman.com/ai connect lexfridman,Machine Learning
lex solo 2 – future neuralink,thoughts 8 possible long-term futures neuralink attending august 2020 progress update solo episode 2 podcast hopefully ’ interesting folks aim episodes focused particular topic times challenging times personal times exciting technical philosophical level like episode today would like get information podcast go https //lexfridman.com/podcast connect lexfridman twitter linkedin facebook medium youtube watch video versions conversations,Machine Learning
123 – manolis kellis origin life humans ideas suffering happiness,manolis kellis professor mit head mit computational biology group please check sponsors get discount support podcast – public goods https //publicgoods.com/lex use code lex – magic spoon https //magicspoon.com/lex link using code lex checkout – expressvpn https //www.expressvpn.com/lexpod lex fridman podcast survey mentioned intro https //www.surveygizmo.com/s3/5833660/lex-fridman-podcast-survey would like get information podcast go https //lexfridman.com/podcast connect lexfridman twitter linkedin facebook medium youtube watch video versions conversations enjoy podcast please rate,Machine Learning
live 2020-06-15 bootstrapping main ideas,today 're going talk bootstrapping one fancy sounding things actually quite simple crazy powerful today walk bootstrapping used test hypotheses p-values information using bootstrapping calculate p-values see https //web.stanford.edu/class/archive/cs/cs109/cs109.1178/lecturehandouts/170-samples.pdf ... ... http //www.stat.ucla.edu/~rgould/110as02/bshypothesis.pdf learn minimum number measurements required bootstrapping see https //stats.stackexchange.com/questions/33300/determining-sample-size-necessary-for-bootstrap-method-proposed-method complete index statquest videos check https //statquest.org/video-index/ 'd like support statquest please consider ... patreon https //www.patreon.com/statquest ... ... youtube membership https //www.youtube.com/channel/uctyluttgs3k1fg4y5tahlbw/join ... buying statquest study guide ... https //statquest.org/studyguides/ ... cool statquest t-shirt sweatshirt usa/europe https //teespring.com/stores/statquest everywhere https //www.redbubble.com/people/starmer/works/40421224-statquest-double-bam asc=u p=t-shirt ... buying one two songs go large get whole album https //joshuastarmer.bandcamp.com/ ... donating statquest https //www.paypal.me/statquest lastly want keep research create new statquests follow twitter https //twitter.com/joshuastarmer,Machine Learning
r strange interview question feature selection using test null hypothesis definition,"suppose x_i i=1,2 ... n attribute values n samples class w_1 mean m_1 y_i i=1,2 ... n attribute values n samples class w_2 mean m_2.for feature selection using hypothesis testing define h_0 hypothesis option m_1 m_2=0 option b m_1 -m_2 /codea href= '' https //preview.redd.it/mokxgy18f4d61.png width=882 format=png auto=webp s=4511114cc43c526a53060ed49608d78464a8603e '' /a -- sc_on -- href= '' https //teddit.net/r/machinelearning/comments/l3h038/d_r_very_strange_interview_question_on_feature/ '' /aa href= '' https //teddit.net/r/machinelearning/comments/l3h038/d_r_very_strange_interview_question_on_feature/ '' /a",Machine Learning
john clarke art fighting pursuit excellence lex fridman podcast 143,john clarke bjj black belt mma coach please support podcast checking sponsors theragun https //theragun.com/lex get 30 day trial magic spoon https //magicspoon.com/lex use code lex get free shipping eight sleep https //www.eightsleep.com/lex use code lex get 200 cash app https //cash.app/ use code lexpodcast get 10 episode links broadway jiu jitsu website https //www.broadwayjiujitsu.com broadway jiu jitsu instagram https //www.instagram.com/broadwayjiujitsu please allow podcast https //podcasts.apple.com/us/podcast/please-allow-me/id1531735873 please allow instagram https //www.instagram.com/please_allow_me_podcast/ podcast info podcast website https //lexfridman.com/podcast apple podcasts https //apple.co/2lwqzir spotify https //spoti.fi/2newcf8 rss https //lexfridman.com/feed/podcast/ full episodes playlist https //www.youtube.com/playlist list=plraxtmerzgodp_8gztsuki9nrranbkkp4 clips playlist https //www.youtube.com/playlist list=plraxtmerzgoecifp3cbcieelojeitor41 outline 0:00 introduction 2:43 great american road trip 20:13 martial arts philosophy 23:13 real vs fake success instagram 33:58 brutal honesty mike tyson 38:44 breaking opponent wrestling 46:51 genghis khan 57:57 's okay change mind 1:02:34 politicians become inauthentic 1:09:11 greatness requires sacrifice 1:11:54 whiplash 1:20:02 relationships 1:25:39 greatest fighters time 1:33:20 greatest fight time 1:47:43 khabib nurmagomedov 1:49:31 conor mcgregor beat khabib nurmagomedov 2:03:47 conor vs khabib 2 2:10:23 always war 2:11:59 future civilization 2:14:10 kids 2:20:55 meaning `` like '' social media 2:29:52 starting podcast 2:48:34 book recommendations 2:52:04 keeping independence solitude connect subscribe youtube channel twitter https //twitter.com/lexfridman linkedin https //www.linkedin.com/in/lexfridman facebook https //www.facebook.com/lexfridmanpage instagram https //www.instagram.com/lexfridman medium https //medium.com/ lexfridman support patreon https //www.patreon.com/lexfridman,Machine Learning
semi-supervised learning label spreading,"tweet share share semi-supervised learning refers algorithms attempt make use labeled unlabeled training data semi-supervised learning algorithms unlike supervised learning algorithms able learn labeled training data popular approach semi-supervised learning create graph connects examples training dataset propagates known labels edges graph label unlabeled examples example approach semi-supervised learning label spreading algorithm classification predictive modeling tutorial discover apply label spreading algorithm semi-supervised learning classification dataset completing tutorial know intuition label spreading semi-supervised learning algorithm works develop semi-supervised classification dataset establish baseline performance supervised learning algorithm develop evaluate label spreading algorithm use model output train supervised learning algorithm let ’ get started semi-supervised learning label spreading photo jernej furman rights reserved tutorial overview tutorial divided three parts label spreading algorithm semi-supervised classification dataset label spreading semi-supervised learning label spreading algorithm label spreading semi-supervised learning algorithm algorithm introduced dengyong zhou et al 2003 paper titled “ learning local global consistency. ” intuition broader approach semi-supervised learning nearby points input space label points structure manifold input space label key semi-supervised learning problems prior assumption consistency means 1 nearby points likely label 2 points structure typically referred cluster manifold likely label — learning local global consistency 2003 label spreading inspired technique experimental psychology called spreading activation networks algorithm understood intuitively terms spreading activation networks experimental psychology — learning local global consistency 2003 points dataset connected graph based relative distances input space weight matrix graph normalized symmetrically much like spectral clustering information passed graph adapted capture structure input space approach similar label propagation algorithm semi-supervised learning another similar label propagation algorithm given zhou et al step node receives contribution neighbors j weighted normalized weight edge j additional small contribution given initial value — page 196 semi-supervised learning 2006 convergence labels applied based nodes passed information finally label unlabeled point set class received information iteration process — learning local global consistency 2003 familiar label spreading algorithm let ’ look might use project first must define semi-supervised classification dataset semi-supervised classification dataset section define dataset semis-supervised learning establish baseline performance dataset first define synthetic classification dataset using make_classification function define dataset two classes binary classification two input variables 1,000 examples ... define dataset x make_classification n_samples=1000 n_features=2 n_informative=2 n_redundant=0 random_state=1 next split dataset train test datasets equal 50-50 split e.g 500 rows ... split train test x_train x_test y_train y_test train_test_split x test_size=0.50 random_state=1 stratify=y finally split training dataset half portion labels portion pretend unlabeled ... split train labeled unlabeled x_train_lab x_test_unlab y_train_lab y_test_unlab train_test_split x_train y_train test_size=0.50 random_state=1 stratify=y_train tying together complete example preparing semi-supervised learning dataset listed prepare semi-supervised learning dataset sklearn.datasets import make_classification sklearn.model_selection import train_test_split define dataset x make_classification n_samples=1000 n_features=2 n_informative=2 n_redundant=0 random_state=1 split train test x_train x_test y_train y_test train_test_split x test_size=0.50 random_state=1 stratify=y split train labeled unlabeled x_train_lab x_test_unlab y_train_lab y_test_unlab train_test_split x_train y_train test_size=0.50 random_state=1 stratify=y_train summarize training set size print 'labeled train set x_train_lab.shape y_train_lab.shape print 'unlabeled train set x_test_unlab.shape y_test_unlab.shape summarize test set size print 'test set x_test.shape y_test.shape running example prepares dataset summarizes shape three portions results confirm test dataset 500 rows labeled training dataset 250 rows 250 rows unlabeled data labeled train set 250 2 250 unlabeled train set 250 2 250 test set 500 2 500 supervised learning algorithm 250 rows train model semi-supervised learning algorithm 250 labeled rows well 250 unlabeled rows could used numerous ways improve labeled training dataset next establish baseline performance semi-supervised learning dataset using supervised learning algorithm fit labeled training data important would expect semi-supervised learning algorithm outperform supervised learning algorithm fit labeled data alone case semi-supervised learning algorithm skill case use logistic regression algorithm fit labeled portion training dataset ... define model model logisticregression fit model labeled dataset model.fit x_train_lab y_train_lab model used make predictions entire holdout test dataset evaluated using classification accuracy ... make predictions hold test set yhat model.predict x_test calculate score test set score accuracy_score y_test yhat summarize score print 'accuracy .3f score 100 tying together complete example evaluating supervised learning algorithm semi-supervised learning dataset listed baseline performance semi-supervised learning dataset sklearn.datasets import make_classification sklearn.model_selection import train_test_split sklearn.metrics import accuracy_score sklearn.linear_model import logisticregression define dataset x make_classification n_samples=1000 n_features=2 n_informative=2 n_redundant=0 random_state=1 split train test x_train x_test y_train y_test train_test_split x test_size=0.50 random_state=1 stratify=y split train labeled unlabeled x_train_lab x_test_unlab y_train_lab y_test_unlab train_test_split x_train y_train test_size=0.50 random_state=1 stratify=y_train define model model logisticregression fit model labeled dataset model.fit x_train_lab y_train_lab make predictions hold test set yhat model.predict x_test calculate score test set score accuracy_score y_test yhat summarize score print 'accuracy .3f score 100 running algorithm fits model labeled training dataset evaluates holdout dataset prints classification accuracy note results may vary given stochastic nature algorithm evaluation procedure differences numerical precision consider running example times compare average outcome case see algorithm achieved classification accuracy 84.8 percent would expect effective semi-supervised learning algorithm achieve better accuracy accuracy 84.800 next let ’ explore apply label spreading algorithm dataset label spreading semi-supervised learning label spreading algorithm available scikit-learn python machine learning library via labelspreading class model fit like classification model calling fit function used make predictions new data via predict function ... define model model labelspreading fit model training dataset model.fit ... ... make predictions hold test set yhat model.predict ... importantly training dataset provided fit function must include labeled examples ordinal encoded per normal unlabeled examples marked label -1 model determine label unlabeled examples part fitting model model fit estimated labels labeled unlabeled data training dataset available via “ transduction_ ” attribute labelspreading class ... get labels entire training dataset data tran_labels model.transduction_ familiar use label spreading algorithm scikit-learn let ’ look might apply semi-supervised learning dataset first must prepare training dataset concatenate input data training dataset single array ... create training dataset input x_train_mixed concatenate x_train_lab x_test_unlab create list -1 valued unlabeled row unlabeled portion training dataset ... create `` label '' unlabeled data nolabel -1 range len y_test_unlab list concatenated labels labeled portion training dataset correspond input array training dataset ... recombine training dataset labels y_train_mixed concatenate y_train_lab nolabel train labelspreading model entire training dataset ... define model model labelspreading fit model training dataset model.fit x_train_mixed y_train_mixed next use model make predictions holdout dataset evaluate model using classification accuracy ... make predictions hold test set yhat model.predict x_test calculate score test set score accuracy_score y_test yhat summarize score print 'accuracy .3f score 100 tying together complete example evaluating label spreading semi-supervised learning dataset listed evaluate label spreading semi-supervised learning dataset numpy import concatenate sklearn.datasets import make_classification sklearn.model_selection import train_test_split sklearn.metrics import accuracy_score sklearn.semi_supervised import labelspreading define dataset x make_classification n_samples=1000 n_features=2 n_informative=2 n_redundant=0 random_state=1 split train test x_train x_test y_train y_test train_test_split x test_size=0.50 random_state=1 stratify=y split train labeled unlabeled x_train_lab x_test_unlab y_train_lab y_test_unlab train_test_split x_train y_train test_size=0.50 random_state=1 stratify=y_train create training dataset input x_train_mixed concatenate x_train_lab x_test_unlab create `` label '' unlabeled data nolabel -1 range len y_test_unlab recombine training dataset labels y_train_mixed concatenate y_train_lab nolabel define model model labelspreading fit model training dataset model.fit x_train_mixed y_train_mixed make predictions hold test set yhat model.predict x_test calculate score test set score accuracy_score y_test yhat summarize score print 'accuracy .3f score 100 running algorithm fits model entire training dataset evaluates holdout dataset prints classification accuracy note results may vary given stochastic nature algorithm evaluation procedure differences numerical precision consider running example times compare average outcome case see label spreading model achieves classification accuracy 85.4 percent slightly higher logistic regression fit labeled training dataset achieved accuracy 84.8 percent accuracy 85.400 far good another approach use semi-supervised model take estimated labels training dataset fit supervised learning model recall retrieve labels entire training dataset label spreading model follows ... get labels entire training dataset data tran_labels model.transduction_ use labels along input data train evaluate supervised learning algorithm logistic regression model hope supervised learning model fit entire training dataset would achieve even better performance semi-supervised learning model alone ... define supervised learning model model2 logisticregression fit supervised learning model entire training dataset model2.fit x_train_mixed tran_labels make predictions hold test set yhat model2.predict x_test calculate score test set score accuracy_score y_test yhat summarize score print 'accuracy .3f score 100 tying together complete example using estimated training set labels train evaluate supervised learning model listed evaluate logistic regression fit label spreading semi-supervised learning numpy import concatenate sklearn.datasets import make_classification sklearn.model_selection import train_test_split sklearn.metrics import accuracy_score sklearn.semi_supervised import labelspreading sklearn.linear_model import logisticregression define dataset x make_classification n_samples=1000 n_features=2 n_informative=2 n_redundant=0 random_state=1 split train test x_train x_test y_train y_test train_test_split x test_size=0.50 random_state=1 stratify=y split train labeled unlabeled x_train_lab x_test_unlab y_train_lab y_test_unlab train_test_split x_train y_train test_size=0.50 random_state=1 stratify=y_train create training dataset input x_train_mixed concatenate x_train_lab x_test_unlab create `` label '' unlabeled data nolabel -1 range len y_test_unlab recombine training dataset labels y_train_mixed concatenate y_train_lab nolabel define model model labelspreading fit model training dataset model.fit x_train_mixed y_train_mixed get labels entire training dataset data tran_labels model.transduction_ define supervised learning model model2 logisticregression fit supervised learning model entire training dataset model2.fit x_train_mixed tran_labels make predictions hold test set yhat model2.predict x_test calculate score test set score accuracy_score y_test yhat summarize score print 'accuracy .3f score 100 running algorithm fits semi-supervised model entire training dataset fits supervised learning model entire training dataset inferred labels evaluates holdout dataset printing classification accuracy note results may vary given stochastic nature algorithm evaluation procedure differences numerical precision consider running example times compare average outcome case see hierarchical approach semi-supervised model followed supervised model achieves classification accuracy 85.8 percent holdout dataset slightly better semi-supervised learning algorithm used alone achieved accuracy 85.6 percent accuracy 85.800 achieve better results tuning hyperparameters labelspreading model let know discover comments reading section provides resources topic looking go deeper books introduction semi-supervised learning 2009 chapter 11 label propagation quadratic criterion semi-supervised learning 2006 papers learning local global consistency 2003 apis sklearn.semi_supervised.labelspreading api section 1.14 semi-supervised scikit-learn user guide sklearn.model_selection.train_test_split api sklearn.linear_model.logisticregression api sklearn.datasets.make_classification api articles semi-supervised learning wikipedia summary tutorial discovered apply label spreading algorithm semi-supervised learning classification dataset specifically learned intuition label spreading semi-supervised learning algorithm works develop semi-supervised classification dataset establish baseline performance supervised learning algorithm develop evaluate label spreading algorithm use model output train supervised learning algorithm questions ask questions comments best answer tweet share share post semi-supervised learning label spreading appeared first machine learning mastery",Machine Learning
rt karpathy cc amprobotics https //www.rollingstone.com/culture/culture-features/plastic-problem-recycling-myth-big-oil-950957/ fbclid=iwar0g5nbjlzeg7ukymretyevtpw9bvtrvbopuryfctnv1chmj51lqgtdie6i,cc amprobotics rollingstone.com/culture/cul…,Machine Learning
automate model documentation using h2o autodoc,create model documentation supervised learning models h2o-3 scikit-learn — minutes federal reserve ’ 2011 guidelines state without adequate documentation model risk assessment management would ineffective similar requirement put forward today many regulatory corporate governance bodies thus model documentation today necessity choice however still denying fact one time-consuming jobs data scientist opposed building validating machine learning models describing model works detail tedious takes considerable amount time effort also issues consistency clarity collaboration way automate entire documentation process well precisely issue h2o autodoc tries address creating comprehensive high-quality model documentation minutes h2o autodoc frees user time-consuming task documenting summarizing workflow building machine learning models additionally also increases consistency model documentation applying standard template across models essential model governance reproducibility regulatory compliance way using ai explain ai challenges creating robust documentation model documentation includes model created training test data characteristics alternatives considered model evaluated information model performance etc today documenting models necessary best practice vital requirement business point view challenges associating manually documenting models creating good documentation ’ piece cake times many teams struggle process often tedious time-consuming business data scientist could using time build additional models create value additionally inconsistent inaccurate model documentation issue model validation governance regulatory compliance better idea automate documentation process h2o autodoc h2o autodoc automated model documentation h2o autodoc new time-saving ml documentation product h2o.ai h2o autodoc automatically generate model documentation supervised learning models created h2o-3 scikit-learn interestingly automated documentation already used production h2o driverless ai industry-leading capability available new standalone commercial module key features h2o autodoc h2o autodoc python package creating automatic reports supervised learning models distributed automatic document generation microsoft word .docx markdown .md formats out-of-the-box documentation template included customizable templates fit unique business needs internal best practices compliance requirements support variety supervised models generated h2o-3 scikit-learn advantages using h2o autodoc h2o autodoc provides various advantages traditional method manual documentation h2o autodoc ensures compliance provides consistent accurate thorough approach model documentation shared production teams data scientists thereby improving collaboration amongst teams saves time money automatically creating model documents instead valuable resources writing editing documents h2o autodoc action know h2o autodoc automatically generate model documentation supervised learning models created h2o-3 scikit-learn let ’ see ways generate automatic report 1 h2o autodoc models created h2o-3 h2o-3 fully open-source distributed in-memory machine learning platform linear scalability speed quality ease-of-use model-deployment various cutting-edge algorithms make h2o highly sought-after api big data data science h2o also industry-leading automl functionality used automating machine learning workflow documentation generated editable word markdown format follows h2o_autodoc import config h2o_autodoc import render_autodoc get h2o-3 model object required create h2o autodoc model h2o.get_model “ my_gbm_model ” configure render autodoc config config output_path= ” full/path/autodoc_h2o3.docx ” render_autodoc h2o config model 2 h2o autodoc models created scikit-learn scikit-learn open-source software machine learning library python programming language features various classification regression clustering algorithms process create automatic documentation models created scikit learn also pretty similar ones created h2o-3 follows h2o_autodoc import config h2o_autodoc.scikit.autodoc import render_autodoc build logistic regression model model logisticregression model.fit x_train y_train configure render autodoc config config output_path= ” full/path/autodoc_scikitlearn.docx ” render_autodoc config model x_train y_train 3 steam h2o autodoc h2o autodoc currently available steam another h2o.ai product allows launch connect h2o cluster securely version h2o autodoc leverages steam python api code follows structure h2o autodoc python api generated report almost identical one difference steam version supports h2o-3 models import h2osteam h2osteam.clients import h2oclient login steam h2osteam.login url= ” https //steam.h2o.ai:9555 '' username= ” user01 '' password= ” token-here ” verify_ssl=true cluster h2oclient.get_cluster “ test-cluster ” h2osteam import autodocconfig get h2o-3 objects using keys model h2o.get_model “ gbm_model ” train h2o.get_frame “ creditcard_train ” use default configuration settings config autodocconfig specify path output file output_file_path “ autodoc_report.docx ” download h2o autodoc cluster.download_autodoc model config train output_file_path documentation features h2o autodoc generates editable word document based automated template includes several features important ones tabulated supported algorithms try h2o autodocdo want get hands dirty experience power h2o autodoc brings machine learning project made easy register trial license try h2o autodoc environment team reach provide 30-day trial license help get running experiment use h2o-3 scikit-learn models conclusion h2o autodoc automatically generates comprehensive model documentation minutes using out-of-the-box custom templates h2o autodoc saves data science teams weeks tedious work increases data science productivity allowing focus model building h2o autodoc increases consistency model documentation applying standard template across models teams essential model governance reproducibility compliance regulations post automate model documentation using h2o autodoc appeared first open source leader ai ml,Machine Learning
impressive surprising https //openai.com/blog/dall-e/ use words sparingly,impressive surprising openai.com/blog/dall-e/ use words sparingly,Machine Learning
uk algoshambles neuralink gpt-3 intelligence,week dr. tim scarfe dr. keith duggar yannic `` lightspeed '' kilcher respond `` algoshambles '' exam fiasco uk government forced step standardise grades grossly inflated schools schools teachers paid metrics related grades received students could possibly go wrong result end grades lost value students coached exams n't actually learn subject also cover second francois chollet interview lex fridman podcast cover gpt-3 neuralink discussion intelligence 00:00:00 algoshambles 00:45:40 lex fridman/chollet intro 00:55:21 lex fridman/chollet neuralink 01:06:28 lex fridman/chollet gpt-3 01:23:43 lex fridman/chollet intelligence discussion,Machine Learning
code adam gradient descent optimization scratch,"tweet share share last updated january 16 2021 gradient descent optimization algorithm follows negative gradient objective function order locate minimum function limitation gradient descent single step size learning rate used input variables extensions gradient descent like adagrad rmsprop update algorithm use separate step size input variable may result step size rapidly decreases small values adaptive movement estimation algorithm adam short extension gradient descent natural successor techniques like adagrad rmsprop automatically adapts learning rate input variable objective function smooths search process using exponentially decreasing moving average gradient make updates variables tutorial discover develop gradient descent adam optimization algorithm scratch completing tutorial know gradient descent optimization algorithm uses gradient objective function navigate search space gradient descent updated use automatically adaptive step size input variable using decaying average partial derivatives called adam implement adam optimization algorithm scratch apply objective function evaluate results let ’ get started gradient descent optimization adam scratch photo graham rights reserved tutorial overview tutorial divided three parts gradient descent adam optimization algorithm gradient descent adam two-dimensional test problem gradient descent optimization adam visualization adam gradient descent gradient descent optimization algorithm technically referred first-order optimization algorithm explicitly makes use first-order derivative target objective function first-order methods rely gradient information help direct search minimum … — page 69 algorithms optimization 2019 first-order derivative simply “ derivative ” rate change slope target function specific point e.g specific input target function takes multiple input variables referred multivariate function input variables thought vector turn derivative multivariate target function may also taken vector referred generally gradient gradient first-order derivative multivariate objective function derivative gradient points direction steepest ascent target function specific input gradient descent refers minimization optimization algorithm follows negative gradient downhill target function locate minimum function gradient descent algorithm requires target function optimized derivative function objective function target function f returns score given set inputs derivative function f gives derivative target function given set inputs gradient descent algorithm requires starting point x problem randomly selected point input space derivative calculated step taken input space expected result downhill movement target function assuming minimizing target function downhill movement made first calculating far move input space calculated step size called alpha learning rate multiplied gradient subtracted current point ensuring move gradient target function x x t-1 – step_size f x t-1 steeper objective function given point larger magnitude gradient turn larger step taken search space size step taken scaled using step size hyperparameter step size alpha hyperparameter controls far move search space gradient iteration algorithm step size small movement search space small search take long time step size large search may bounce around search space skip optima familiar gradient descent optimization algorithm let ’ take look adam algorithm adam optimization algorithm adaptive movement estimation algorithm adam short extension gradient descent optimization algorithm algorithm described 2014 paper diederik kingma jimmy lei ba titled “ adam method stochastic optimization. ” adam designed accelerate optimization process e.g decrease number function evaluations required reach optima improve capability optimization algorithm e.g result better final result achieved calculating step size input parameter optimized importantly step size automatically adapted throughput search process based gradients partial derivatives encountered variable propose adam method efficient stochastic optimization requires first-order gradients little memory requirement method computes individual adaptive learning rates different parameters estimates first second moments gradients name adam derived adaptive moment estimation — adam method stochastic optimization involves maintaining first second moment gradient e.g exponentially decaying mean gradient first moment variance second moment input variable moving averages estimates 1st moment mean 2nd raw moment uncentered variance gradient — adam method stochastic optimization let ’ step element algorithm first must maintain moment vector exponentially weighted infinity norm parameter optimized part search referred v really greek letter nu respectively initialized 0.0 start search 0 v 0 algorithm executed iteratively time starting t=1 iteration involves calculating new set parameter values x e.g going x t-1 x perhaps easy understand algorithm focus updating one parameter generalizes updating parameters via vector operations first gradient partial derivatives calculated current time step g f x t-1 next first moment updated using gradient hyperparameter beta1 beta1 t-1 1 – beta1 g second moment updated using squared gradient hyperparameter beta2 v beta2 v t-1 1 – beta2 g ^2 first second moments biased initialized zero values … moving averages initialized vectors 0 ’ leading moment estimates biased towards zero especially initial timesteps especially decay rates small i.e betas close 1 good news initialization bias easily counteracted resulting bias-corrected estimates … — adam method stochastic optimization next first second moments bias-corrected starring first moment mhat 1 – beta1 second moment vhat v 1 – beta2 note beta1 beta2 refer beta1 beta2 hyperparameters decayed schedule iterations algorithm static decay schedule used although paper recommend following beta1 beta1^t beta2 beta2^t finally calculate value parameter iteration x x t-1 – alpha mhat sqrt vhat eps alpha step size hyperparameter eps small value epsilon 1e-8 ensures encounter divide zero error sqrt square root function note efficient reordering update rule listed paper used alpha alpha sqrt 1 – beta2 1 – beta1 x x t-1 – alpha sqrt v eps review three hyperparameters algorithm alpha initial step size learning rate typical value 0.001. beta1 decay factor first momentum typical value 0.9. beta2 decay factor infinity norm typical value 0.999 ’ full derivation adam algorithm context adam algorithm recommend reading paper adam method stochastic optimization next let ’ look might implement algorithm scratch python gradient descent adam section explore implement gradient descent optimization algorithm adam two-dimensional test problem first let ’ define optimization function use simple two-dimensional function squares input dimension define range valid inputs -1.0 1.0 objective function implements function objective function def objective x return x 2.0 2.0 create three-dimensional plot dataset get feeling curvature response surface complete example plotting objective function listed 3d plot test function numpy import arange numpy import meshgrid matplotlib import pyplot objective function def objective x return x 2.0 2.0 define range input r_min r_max -1.0 1.0 sample input range uniformly 0.1 increments xaxis arange r_min r_max 0.1 yaxis arange r_min r_max 0.1 create mesh axis x meshgrid xaxis yaxis compute targets results objective x create surface plot jet color scheme figure pyplot.figure axis figure.gca projection='3d axis.plot_surface x results cmap='jet show plot pyplot.show running example creates three-dimensional surface plot objective function see familiar bowl shape global minima f 0 0 0 three-dimensional plot test objective function also create two-dimensional plot function helpful later want plot progress search example creates contour plot objective function contour plot test function numpy import asarray numpy import arange numpy import meshgrid matplotlib import pyplot objective function def objective x return x 2.0 2.0 define range input bounds asarray -1.0 1.0 -1.0 1.0 sample input range uniformly 0.1 increments xaxis arange bounds 0,0 bounds 0,1 0.1 yaxis arange bounds 1,0 bounds 1,1 0.1 create mesh axis x meshgrid xaxis yaxis compute targets results objective x create filled contour plot 50 levels jet color scheme pyplot.contourf x results levels=50 cmap='jet show plot pyplot.show running example creates two-dimensional contour plot objective function see bowl shape compressed contours shown color gradient use plot plot specific points explored progress search two-dimensional contour plot test objective function test objective function let ’ look might implement adam optimization algorithm gradient descent optimization adam apply gradient descent adam test problem first need function calculates derivative function f x x^2 f x x 2 derivative x^2 x 2 dimension derivative function implements derivative objective function def derivative x return asarray x 2.0 2.0 next implement gradient descent optimization first select random point bounds problem starting point search assumes array defines bounds search one row dimension first column defines minimum second column defines maximum dimension ... generate initial point x bounds 0 rand len bounds bounds 1 bounds 0 score objective x 0 x 1 next need initialize first second moments zero ... initialize first second moments 0.0 range bounds.shape 0 v 0.0 range bounds.shape 0 run fixed number iterations algorithm defined “ n_iter ” hyperparameter ... run iterations gradient descent range n_iter ... first step calculate gradient current solution using derivative function ... calculate gradient gradient derivative solution 0 solution 1 first step calculate derivative current set parameters ... calculate gradient g g derivative x 0 x 1 next need perform adam update calculations perform calculations one variable time using imperative programming style readability practice recommend using numpy vector operations efficiency ... build solution one variable time range x.shape 0 ... first need calculate moment ... beta1 t-1 1 beta1 g beta1 1.0 beta1 g second moment ... v beta2 v t-1 1 beta2 g ^2 v beta2 v 1.0 beta2 g 2 bias correction first second moments ... mhat 1 beta1 mhat 1.0 beta1 t+1 vhat v 1 beta2 vhat v 1.0 beta2 t+1 finally updated variable value ... x x t-1 alpha mhat sqrt vhat eps x x alpha mhat sqrt vhat eps repeated parameter optimized end iteration evaluate new parameter values report performance search ... evaluate candidate point score objective x 0 x 1 report progress print '/preem/empre class= '' urvanov-syntax-highlighter-plain-tag ''",Machine Learning
top 3 things forget building saas product,number product management roles us grown 30 two years according linkedin responsibilities job morphing read full story,Machine Learning
chris lattner compilers llvm swift tpu ml accelerators,chris lattner senior director google working several projects including cpu gpu tpu accelerators tensorflow swift tensorflow kinds machine learning compiler magic going behind scenes one top experts world compiler technologies means deeply understands intricacies hardware software come together create efficient code created llvm compiler infrastructure project clang compiler led major engineering efforts apple including creation swift programming language also briefly spent time tesla,Machine Learning
top 20 image datasets machine learning computer vision,computer vision enables computers understand content images videos goal computer vision automate tasks human visual system read full story,Machine Learning
96 – stephen schwarzman going big business investing ai,stephen schwarzman ceo co-founder blackstone one world ’ leading investment firms 530 billion dollars assets management one successful business leaders history humble beginnings back philly recommend recent book called takes tells stories lessons personal journey support podcast signing sponsors – expressvpn https //www.expressvpn.com/lexpod – masterclass https //masterclass.com/lex episode links takes book https //amzn.to/2wx9czu conversation part artificial intelligence podcast would like get information,Machine Learning
133 – manolis kellis biology disease,manolis kellis computational biologist mit please support podcast checking sponsors – semrush https //www.semrush.com/partner/lex/ get free month guru – pessimists archive https //pessimists.co/ – eight sleep https //www.eightsleep.com/lex use code lex get 200 – betterhelp https //betterhelp.com/lex get 10 episode links manolis website http //web.mit.edu/manoli/ manolis twitter https //twitter.com/manoliskellis manolis youtube https //www.youtube.com/channel/uckklj5lhre3c7fgbnpa5dga manolis wikipedia https //en.wikipedia.org/wiki/manolis_kellis podcast info podcast website https //lexfridman.com/podcast apple podcasts https //apple.co/2lwqzir spotify https //spoti.fi/2newcf8 rss https //lexfridman.com/feed/podcast/ youtube full episodes https //youtube.com/lexfridman youtube clips https //youtube.com/lexclips support connect – check sponsors ’ best way support podcast,Machine Learning
7 challenges marketing ai machine learning solutions,article help readers identify understand challenges faced ai development companies market ai ml products read full story,Machine Learning
helping young students build career research msr india research fellow program,episode 007 december 21 2020 one microsoft research india ’ goals help strengthen research ecosystem encourage young students look research career always easy students understand research figure research right career research fellow program microsoft research india enables bright young students work real-world research problems top notch researchers across research lifecycle including ideation implementation evaluation deployment many students part program gone become researchers engineers entrepreneurs show notes transcript https //www.microsoft.com/en-us/research/lab/microsoft-research-india/articles/podcast-helping-young-students-build-a-career-in-research-through-the-msr-india-research-fellow-program-with-shruti-rijhwani-and-dr-vivek-seshadri/ research fellows program microsoft research india https //www.microsoft.com/en-us/research/academic-program/research-fellows-program-at-microsoft-research-india/ see microsoft research india podcast episodes learn research https //www.microsoft.com/en-us/research/lab/microsoft-research-india/,Machine Learning
109 – brian kernighan unix c awk ampl go programming,brian kernighan professor computer science princeton university co-authored c programming language dennis ritchie creator c written lot books programming computers life including practice programming go programming language latest unix history memoir co-created awk text processing language used linux folks like co-designed ampl algebraic modeling language large-scale optimization support podcast supporting sponsors – eight sleep https //eightsleep.com/lex – raycon http //buyraycon.com/lex would like get information podcast,Machine Learning
michael malice white pill freedom hope happiness amidst chaos lex fridman podcast 150,michael malice political thinker podcaster author please support podcast checking sponsors netsuite http //netsuite.com/strategy get free product tour athletic greens https //athleticgreens.com/lex use code lex get 1 month fish oil sun basket https //sunbasket.com/lex use code lex get 35 cash app https //cash.app/ use code lexpodcast get 10 episode links michael 's twitter https //twitter.com/michaelmalice michael 's community https //malice.locals.com/ michael 's youtube https //www.youtube.com/channel/uc5tj5qcpjkil-kia4gib5xw michael 's website http //michaelmalice.com/about/ welcome podcast https //bit.ly/30q8oz1 new right book https //amzn.to/34gxlo3 dear reader book https //amzn.to/2hpplhs podcast round 1 https //www.youtube.com/watch v=bik1zuy8ehu podcast info podcast website https //lexfridman.com/podcast apple podcasts https //apple.co/2lwqzir spotify https //spoti.fi/2newcf8 rss https //lexfridman.com/feed/podcast/ full episodes playlist https //www.youtube.com/playlist list=plraxtmerzgodp_8gztsuki9nrranbkkp4 clips playlist https //www.youtube.com/playlist list=plraxtmerzgoecifp3cbcieelojeitor41 outline 0:00 introduction 3:25 conversation alex jones tim pool 12:10 michael 's outfit 20:31 self-publishing book 30:19 white pill 41:43 volcano say true love 43:06 myth sisyphus 46:47 journalism failed stop stalin hitler 54:31 good germans 58:27 richard wolff 1:01:58 could united states stayed world war ii 1:04:50 trump derangement syndrome 1:06:36 nazism antisemitism 1:09:18 knock knock 1:15:58 putin 1:23:38 evil kim jong-il north korea 1:32:10 dark humor 1:36:56 comedy tragedy plus timing 1:44:12 interviewing difficult guests 1:53:44 curtis yarvin mencius moldbug 2:10:02 violence anarchism 2:25:36 ayn rand 2:28:45 secession united states 2:38:24 politics next 4 years 2:45:52 mars 2:49:55 ufos 2:52:50 psychedelics 2:56:46 love connect subscribe youtube channel twitter https //twitter.com/lexfridman linkedin https //www.linkedin.com/in/lexfridman facebook https //www.facebook.com/lexfridmanpage instagram https //www.instagram.com/lexfridman medium https //medium.com/ lexfridman support patreon https //www.patreon.com/lexfridman,Machine Learning
materials learning gpu programming computer vision,good resources recommend learn gpu programming computer vision tasks link comments,Machine Learning
r karpathy 8.5 years ago training restricted boltzmann machines matlab cpu machine desk,8.5 years ago training restricted boltzmann machines matlab cpu machine desk,Machine Learning
choose activation function deep learning,activation functions critical part design neural network choice activation function hidden layer control well network model learns training dataset choice activation function output layer define type predictions model make careful choice activation function must made deep learning neural network project tutorial discover choose activation functions neural network models completing tutorial know activation functions key part neural network design modern default activation function hidden layers relu function activation function output layers depends type prediction problem let ’ get started choose activation function deep learning photo peter dowley rights reserved tutorial overview tutorial divided three parts activation functions activation hidden layers activation output layers activation functions activation function neural network defines weighted sum input transformed output node nodes layer network sometimes activation function called “ transfer function. ” output range activation function limited may called “ squashing function. ” many activation functions nonlinear may referred “ nonlinearity ” layer network design choice activation function large impact capability performance neural network different activation functions may used different parts model technically activation function used within internal processing node network although networks designed use activation function nodes layer network may three types layers input layers take raw input domain hidden layers take input another layer pass output another layer output layers make prediction hidden layers typically use activation function output layer typically use different activation function hidden layers dependent upon type prediction required model activation functions also typically differentiable meaning first-order derivative calculated given input value required given neural networks typically trained using backpropagation error algorithm requires derivative prediction error order update weights model many different types activation functions used neural networks although perhaps small number functions used practice hidden output layers let ’ take look activation functions used type layer turn activation hidden layers hidden layer neural network layer receives input another layer another hidden layer input layer provides output another layer another hidden layer output layer hidden layer directly contact input data produce outputs model least general neural network may zero hidden layers typically differentiable nonlinear activation function used hidden layers neural network allows model learn complex functions network trained using linear activation function order get access much richer hypothesis space would benefit deep representations need non-linearity activation function — page 72 deep learning python 2017 perhaps three activation functions may want consider use hidden layers rectified linear activation relu logistic sigmoid hyperbolic tangent tanh exhaustive list activation functions used hidden layers commonly used let ’ take closer look turn relu hidden layer activation function rectified linear activation function relu activation function perhaps common function used hidden layers common simple implement effective overcoming limitations previously popular activation functions sigmoid tanh specifically less susceptible vanishing gradients prevent deep models trained although suffer problems like saturated “ dead ” units relu function calculated follows max 0.0 x means input value x negative value 0.0 returned otherwise value returned learn details relu activation function tutorial gentle introduction rectified linear unit relu get intuition shape function worked example example plot relu activation function matplotlib import pyplot rectified linear function def rectified x return max 0.0 x define input data inputs x x range -10 10 calculate outputs outputs rectified x x inputs plot inputs vs outputs pyplot.plot inputs outputs pyplot.show running example calculates outputs range values creates plot inputs versus outputs see familiar kink shape relu activation function plot inputs vs. outputs relu activation function using relu function hidden layers good practice use “ normal ” “ uniform ” weight initialization scale input data range 0-1 normalize prior training sigmoid hidden layer activation function sigmoid activation function also called logistic function function used logistic regression classification algorithm function takes real value input outputs values range 0 1 larger input positive closer output value 1.0 whereas smaller input negative closer output 0.0 sigmoid activation function calculated follows 1.0 1.0 e^-x e mathematical constant base natural logarithm get intuition shape function worked example example plot sigmoid activation function math import exp matplotlib import pyplot sigmoid activation function def sigmoid x return 1.0 1.0 exp -x define input data inputs x x range -10 10 calculate outputs outputs sigmoid x x inputs plot inputs vs outputs pyplot.plot inputs outputs pyplot.show running example calculates outputs range values creates plot inputs versus outputs see familiar s-shape sigmoid activation function plot inputs vs. outputs sigmoid activation function using sigmoid function hidden layers good practice use “ xavier normal ” “ xavier uniform ” weight initialization also referred glorot initialization named xavier glorot scale input data range 0-1 e.g range activation function prior training tanh hidden layer activation function hyperbolic tangent activation function also referred simply tanh also “ tanh ” “ tanh “ function similar sigmoid activation function even s-shape function takes real value input outputs values range -1 1 larger input positive closer output value 1.0 whereas smaller input negative closer output -1.0 tanh activation function calculated follows e^x – e^-x e^x e^-x e mathematical constant base natural logarithm get intuition shape function worked example example plot tanh activation function math import exp matplotlib import pyplot tanh activation function def tanh x return exp x exp -x exp x exp -x define input data inputs x x range -10 10 calculate outputs outputs tanh x x inputs plot inputs vs outputs pyplot.plot inputs outputs pyplot.show running example calculates outputs range values creates plot inputs versus outputs see familiar s-shape tanh activation function plot inputs vs. outputs tanh activation function using tanh function hidden layers good practice use “ xavier normal ” “ xavier uniform ” weight initialization also referred glorot initialization named xavier glorot scale input data range -1 1 e.g range activation function prior training choose hidden layer activation function neural network almost always activation function hidden layers unusual vary activation function network model traditionally sigmoid activation function default activation function 1990s perhaps mid late 1990s 2010s tanh function default activation function hidden layers … hyperbolic tangent activation function typically performs better logistic sigmoid — page 195 deep learning 2016 sigmoid tanh functions make model susceptible problems training via so-called vanishing gradients problem learn problem tutorial gentle introduction rectified linear unit relu activation function used hidden layers typically chosen based type neural network architecture modern neural network models common architectures mlp cnn make use relu activation function extensions modern neural networks default recommendation use rectified linear unit relu … — page 174 deep learning 2016 recurrent networks still commonly use tanh sigmoid activation functions even example lstm commonly uses sigmoid activation recurrent connections tanh activation output multilayer perceptron mlp relu activation function convolutional neural network cnn relu activation function recurrent neural network tanh and/or sigmoid activation function ’ unsure activation function use network try compare results figure summarizes choose activation function hidden layers neural network model choose hidden layer activation function activation output layers output layer layer neural network model directly outputs prediction feed-forward neural network models output layer perhaps three activation functions may want consider use output layer linear logistic sigmoid softmax exhaustive list activation functions used output layers commonly used let ’ take closer look turn linear output activation function linear activation function also called “ identity ” multiplied 1.0 “ activation. ” linear activation function change weighted sum input way instead returns value directly get intuition shape function worked example example plot linear activation function matplotlib import pyplot linear activation function def linear x return x define input data inputs x x range -10 10 calculate outputs outputs linear x x inputs plot inputs vs outputs pyplot.plot inputs outputs pyplot.show running example calculates outputs range values creates plot inputs versus outputs see diagonal line shape inputs plotted identical outputs plot inputs vs. outputs linear activation function target values used train model linear activation function output layer typically scaled prior modeling using normalization standardization transforms sigmoid output activation function sigmoid logistic activation function described previous section nevertheless add symmetry review shape function worked example example plot sigmoid activation function math import exp matplotlib import pyplot sigmoid activation function def sigmoid x return 1.0 1.0 exp -x define input data inputs x x range -10 10 calculate outputs outputs sigmoid x x inputs plot inputs vs outputs pyplot.plot inputs outputs pyplot.show running example calculates outputs range values creates plot inputs versus outputs see familiar s-shape sigmoid activation function plot inputs vs. outputs sigmoid activation function target labels used train model sigmoid activation function output layer values 0 1 softmax output activation function softmax function outputs vector values sum 1.0 interpreted probabilities class membership related argmax function outputs 0 options 1 chosen option softmax “ softer ” version argmax allows probability-like output winner-take-all function input function vector real values output vector length values sum 1.0 like probabilities softmax function calculated follows e^x sum e^x x vector outputs e mathematical constant base natural logarithm learn details softmax function tutorial softmax activation function python plot softmax function give example calculating python numpy import exp softmax activation function def softmax x return exp x exp x .sum define input data inputs 1.0 3.0 2.0 calculate outputs outputs softmax inputs report probabilities print outputs report sum probabilities print outputs.sum running example calculates softmax output input vector confirm sum outputs softmax indeed sums value 1.0 0.09003057 0.66524096 0.24472847 1.0 target labels used train model softmax activation function output layer vectors 1 target class 0 classes choose output activation function must choose activation function output layer based type prediction problem solving specifically type variable predicted example may divide prediction problems two main groups predicting categorical variable classification predicting numerical variable regression problem regression problem use linear activation function regression one node linear activation problem classification problem three main types classification problems may use different activation function predicting probability regression problem classification cases classification model predict probability class membership e.g probability example belongs class convert crisp class label rounding sigmoid argmax softmax two mutually exclusive classes binary classification output layer one node sigmoid activation function used two mutually exclusive classes multiclass classification output layer one node per class softmax activation used two mutually inclusive classes multilabel classification output layer one node class sigmoid activation function used binary classification one node sigmoid activation multiclass classification one node per class softmax activation multilabel classification one node per class sigmoid activation figure summarizes choose activation function output layer neural network model choose output layer activation function reading section provides resources topic looking go deeper tutorials gentle introduction rectified linear unit relu softmax activation function python 4 types classification tasks machine learning fix vanishing gradients problem using relu books deep learning 2016 neural smithing supervised learning feedforward artificial neural networks 1999 neural networks pattern recognition 1996 deep learning python 2017 articles activation function wikipedia summary tutorial discovered choose activation functions neural network models specifically learned activation functions key part neural network design modern default activation function hidden layers relu function activation function output layers depends type prediction problem questions ask questions comments best answer post choose activation function deep learning appeared first machine learning mastery,Machine Learning
complete introduction graph data structure,data structures important storing data efficient ways article discuss graph data structure definition types examples read full story,Machine Learning
fourier neural operator parametric partial differential equations paper explained,ai research engineering numerical solvers partial differential equations notoriously slow need evolve state tiny steps order stay accurate need repeat new problem neural fourier operators architecture proposed paper evolve pde time single forward pass entire family pdes long training set covers well performing crucial operations fourier space new architecture also independent discretization sampling underlying signal potential speed many scientific applications outline 0:00 intro overview 6:15 navier stokes problem statement 11:00 formal problem definition 15:00 neural operator 31:30 fourier neural operator 48:15 experimental examples 50:35 code walkthrough 1:01:00 summary conclusion paper https //arxiv.org/abs/2010.08895 blog https //zongyi-li.github.io/blog/2020/fourier-pde/ code https //github.com/zongyi-li/fourier_neural_operator/blob/master/fourier_3d.py mit technology review https //www.technologyreview.com/2020/10/30/1011435/ai-fourier-neural-network-cracks-navier-stokes-and-partial-differential-equations/ abstract classical development neural networks primarily focused learning mappings finite-dimensional euclidean spaces recently generalized neural operators learn mappings function spaces partial differential equations pdes neural operators directly learn mapping functional parametric dependence solution thus learn entire family pdes contrast classical methods solve one instance equation work formulate new neural operator parameterizing integral kernel directly fourier space allowing expressive efficient architecture perform experiments burgers equation darcy flow navier-stokes equation including turbulent regime fourier neural operator shows state-of-the-art performance compared existing neural network methodologies three orders magnitude faster compared traditional pde solvers authors zongyi li nikola kovachki kamyar azizzadenesheli burigede liu kaushik bhattacharya andrew stuart anima anandkumar links youtube https //www.youtube.com/c/yannickilcher twitter https //twitter.com/ykilcher discord https //discord.gg/4h8xxdf bitchute https //www.bitchute.com/channel/yannic-kilcher minds https //www.minds.com/ykilcher parler https //parler.com/profile/yannickilcher linkedin https //www.linkedin.com/in/yannic-kilcher-488534136/ want support best thing share content want support financially completely optional voluntary lot people asked subscribestar https //www.subscribestar.com/yannickilcher patreon https //www.patreon.com/yannickilcher bitcoin btc bc1q49lsw3q325tr58ygf8sudx2dqfguclvngvy2cq ethereum eth 0x7ad3513e3b8f66799f507aa7874b1b0ebc7f85e2 litecoin ltc lqw2trykyetvc8wjfkhpphtpbdm4vw7r9m monero xmr 4acl8agreo5hair8a9cevrw8peauwvnp1wnsdzxw7tzicdlhzagsgzhrqabdnfy8yum9fwjdvijphkrjv4fwt19cjzn9d4n,Machine Learning
r facial recognition expose political orientation facial images,hi guys recently came along read bizarre scientific article published nature https //www.nature.com/articles/s41598-020-79310-1 claims machine learning model capable predicting people 's political orientation facial expressions 's even worse images collected facebook dating apps face conservativist liberal look like political views natural way humans evolved since time immemorial thus must possible present someone 's face really reminds clever hans effect detecting criminals images detecting presence absence smile https //www.callingbullshit.org/case_studies/case_study_criminal_machine_learning.html society polarized let 's make even thoughts link comments,Machine Learning
create bubble map javascript visualize election results,beginner level tutorial get started data visualization creating interesting intuitive javascript bubble map read full story,Machine Learning
jeremy howard winning predict grant applications competition,recently started employment kaggle eligible win prizes means prize-winner comp quan sun team ‘ student1 ’ congratulations approach competition first analyze data excel pivottables looked groups high low application success rates way found large number strong predictors — including date new years day strong predictor applications processed sunday many fields null value highly predictive used c normalize data grants persons objects constructed dataset modeling including features catcode numperperson personid numondate anyhasphd country dept dayofweek hasphd isny month noclass nospons rfcd role seo sponsor valueband hasid anyhasid anyhassucc hassucc people.count astarpapers apapers bpapers cpapers papers maxastarpapers maxcpapers maxpapers numsucc numunsucc minnumsucc minnumunsucc pctrfcd pctseo maxyearbirth minyearuni yearbirth yearuni fairly obvious mean field names starting ‘ ’ true person attached grant feature e.g ‘ anyhasphd ’ fields one predictor looks person 1 e.g ‘ apapers ’ number papers person 1 one maximum people application e.g ‘ maxapapers ’ created features used generalization random forest algorithm build model ’ try write detail algorithm works time really difference regular random forest great pre-processed data running model grouping small groups categorical variables replacing continuous columns null values 2 columns one containing binary predictor true continuous column null containing original column nulls replaced median excel pivottables start pre-processing modelling done c using libraries developed competition hope document release libraries point — perhaps tuning future comps originally published blog.kaggle.com february 21 2011 jeremy howard winning predict grant applications competition originally published kaggle blog medium people continuing conversation highlighting responding story,Machine Learning
138 – yaron brook ayn rand philosophy objectivism,yaron brook objectivist philosopher podcaster author please support podcast checking sponsors – blinkist https //blinkist.com/lex use code lex get 25 premium – expressvpn https //expressvpn.com/lexpod use code lexpod get 3 months free – cash app https //cash.app/ use code lexpodcast get 10 episode links yaron ’ twitter https //twitter.com/yaronbrook yaron brook show youtube https //www.youtube.com/user/ybrook free market revolution book https //amzn.to/32h0olb equal unfair book https //amzn.to/32k3nsc podcast info podcast website https //lexfridman.com/podcast apple podcasts https //apple.co/2lwqzir spotify https //spoti.fi/2newcf8 rss https //lexfridman.com/feed/podcast/ youtube full episodes https //youtube.com/lexfridman youtube clips https //youtube.com/lexclips support connect – check sponsors,Machine Learning
117 – sheldon solomon death meaning,sheldon solomon social psychologist philosopher co-developer terror management theory co-author worm core please support channel supporting sponsors – blinkist https //blinkist.com/lex – expressvpn https //www.expressvpn.com/lexpod – cash app download app use code “ lexpodcast ” episode links sheldon ’ website https //www.skidmore.edu/psychology/faculty/solomon.php worm core book https //amzn.to/31hqaxh denial death book https //amzn.to/329zxl4 would like get information podcast go https //lexfridman.com/podcast connect lexfridman twitter linkedin facebook medium youtube watch video versions conversations enjoy podcast please,Machine Learning
xgboost python start finish,note support statquest purchasing jupyter notebook python code seen video https //statquest.org/product/jupyter-notebook-xgboost-in-python/ ⭐ note code use kite free ai-powered coding assistant help code faster smarter kite plugin integrates top editors ides give smart completions documentation ’ typing love https //www.kite.com/get-kite/ utm_medium=referral utm_source=youtube utm_campaign=statquest utm_content=description-only note statquest assumes already familiar xgboost regression https //youtu.be/otd8wvafm6e xgboost classification https //youtu.be/8b1jedvenqu xgboost crazy cool optimizations https //youtu.be/orrkeucebq8 regularization https //youtu.be/q81rr3ykn30 cross validation https //youtu.be/fsytzgwwbvw confusion matrices https //youtu.be/kdsp6soqa7o complete index statquest videos check https //statquest.org/video-index/ 'd like support statquest please consider ... patreon https //www.patreon.com/statquest ... ... youtube membership https //www.youtube.com/channel/uctyluttgs3k1fg4y5tahlbw/join ... cool statquest t-shirt sweatshirt usa/europe https //teespring.com/stores/statquest everywhere https //www.redbubble.com/people/starmer/works/40421224-statquest-double-bam asc=u p=t-shirt ... buying one two songs go large get whole album https //joshuastarmer.bandcamp.com/ ... donating statquest https //www.paypal.me/statquest lastly want keep research create new statquests follow twitter https //twitter.com/joshuastarmer 0:00 awesome song introduction 2:56 import modules 4:34 import data 13:43 missing data part 1 identifying 18:37 missing data part 2 dealing 24:03 format data part 1 x 25:55 format data part 2 one-hot encoding 33:25 xgboost missing data one-hot encoding 36:43 build preliminary xgboost model 45:01 optimize parameters cross validation gridsearchcv 49:44 build draw final xgboost model statquest ml xgboost,Machine Learning
new name lex fridman podcast,new podcast name new russian hitman thumbnail everything else stays ai still passion gives bit freedom talk interesting folks thanks support love would like get information podcast go https //lexfridman.com/podcast connect lexfridman twitter linkedin facebook medium youtube watch video versions conversations enjoy podcast please rate 5 stars apple podcasts follow spotify support patreon,Machine Learning
michael mina rapid testing viruses engineering mindset lex fridman podcast 146,michael mina immunologist epidemiologist physician harvard please support podcast checking sponsors brave https //brave.com/lex athletic greens https //athleticgreens.com/lex use code lex get 1 month fish oil expressvpn https //expressvpn.com/lexpod use code lexpod get 3 months free cash app https //cash.app/ use code lexpodcast get 10 episode links michael 's twitter https //twitter.com/michaelmina_lab michael 's time article https //time.com/5912705/covid-19-stop-spread-christmas/ rapid tests https //www.rapidtests.org/ podcast info podcast website https //lexfridman.com/podcast apple podcasts https //apple.co/2lwqzir spotify https //spoti.fi/2newcf8 rss https //lexfridman.com/feed/podcast/ full episodes playlist https //www.youtube.com/playlist list=plraxtmerzgodp_8gztsuki9nrranbkkp4 clips playlist https //www.youtube.com/playlist list=plraxtmerzgoecifp3cbcieelojeitor41 outline 0:00 introduction 2:32 interacting viruses bacteria 6:45 deadlier viruses 10:17 covid-19 mutate 11:51 rapid testing 29:15 pcr vs rapid antigen tests 38:59 medical industrial complex 42:51 lex takes covid test 49:35 fda cheap tests 52:21 explanation elon musk 's positive covid tests 59:29 role testing vaccine deployment 1:02:58 public health policy 1:12:38 weather system viruses 1:29:30 virus kill humans 1:35:09 engineering deadly virus 1:39:51 alphafold 2 viruses 1:45:46 advice young people 1:53:54 time buddhist monk 1:59:58 meditation 2:07:36 meaning life connect subscribe youtube channel twitter https //twitter.com/lexfridman linkedin https //www.linkedin.com/in/lexfridman facebook https //www.facebook.com/lexfridmanpage instagram https //www.instagram.com/lexfridman medium https //medium.com/ lexfridman support patreon https //www.patreon.com/lexfridman,Machine Learning
h2o driverless ai 1.9.1 continuing push boundaries responsible ai,h2o.ai busy significant new software launch coming details also thrilled announce latest release flagship enterprise platform h2o driverless ai 1.9.1 said let ’ jump new faster python scoring pipelines embedded mojos real-time inference interpretability explainability production models improved user experience shapley feature importances original transformed feature space greatly improved time-series modeling short-horizon forecasting problems new out-of-the-box recipe monotonic gradient boosted machines gbm various new controls enforcing custom monotonic models enhanced mojo visualization pipeline displaying first tree lightgbm xgboost models expanded autodoc configuration options shapley values monotonicity constraints imbalanced models enhanced string/numeric feature detection conversion improved automatic feature engineering date/time differences preview multi-gpu multi-node dask/rapids xgboost lightgbm optuna hyper-parameter tuning additional machine learning interpretability mli features mli bring recipe byor customizability machine learning interpretability code exposed sampling parameter explainers added mojo support k-lime download option added ability download raw k-lime data mli ui added ability change threshold disparate impact analysis dia expert settings added ability run pdp range data added max runtime parameter kernel shapley mli expert settings added ability run pd/ice multinomial models dai added ability run mli ts typical mli view iid added ability see rules decision tree surrogate model enhanced mli autodoc configuration options k-lime surrogate decision tree explanations lot ’ dive individual bullet absolutely features called mli bring recipe custom recipes data preprocessing models feature engineering metrics one major features h2o driverless ai provides user-mode customizations flexible python code snippets entire machine learning pipeline opens platform entire python ecosystem version 1.9.1 custom recipes available mli well recognize spend lot time trying discover latest greatest explainable ai machine learning interpretability methods users might want try others bring recipe framework mli enables users bring explainability interpretability method driverless ai check open-source recipes custom explainers mojo metrics obviously primary goal machine learning exercises ultimately get production start automatically making better decisions driving value often immediate secondary goal understand models performing production time continue expand number metrics embedded production model objects mojos users see predictions model see progressive changes shapley values k-lime real-time autodoc documentation explainability metrics determine model changing focus time potentially getting less bias time better constraints visualizations many h2o.ai ’ longest-standing customers partners investors financial services highly regulated industries reason always customer-centric build better product industries requiring simple interpretable models continue add host constraining options model universe feature engineering h2o driverless ai 1.9.1 built-in monotonic recipes enable interpretable feature engineering strict monotonicity constraints additionally enhanced mojo visualization pipeline light gbms xgboost users better visualize first tree model see essential features important characteristics model structure tree depth get started new h2o driverless ai recommend risk-free web-based test drive h2o aquarium cloud lab session lasts two hours keep trying software free license key required also self-paced tutorials guide journey note process updating materials h2o driverless ai 1.9 new tutorials available coming weeks existing users license keys please download latest version website also find links different cloud marketplaces page hope enjoy reading quick overview please give spin share experience us learning resources self-paced tutorials instructor-led courses learning center h2o documentation h2o blogs part community find meetup group near h2o.ai events overview related webinars july 9 future ai looks like arno candel cto july 16 use cases value automated computer vision modeling july 23 state art nlp models h2o driverless ai 1.9 july 30 exploration model explainability h2o driverless ai 1.9 july 30 making production machine learning post h2o driverless ai 1.9.1 continuing push boundaries responsible ai appeared first open source leader ai ml,Machine Learning
searching rh counterexamples — unbounded integers,’ ironically searching counterexamples riemann hypothesis setting pytest adding database search strategies last article improved naive search “ try positive integers ” enumerate subset integers superabundant numbers rh counterexamples guaranteed among numbers grow large fast quickly reached limit 64 bit integers store unbounded integer arithmetic possible computers requires special software implementation brief represent numbers base-n large n say use 32-bit integer digit arithmetic quantities emulates ripple-carry adder naturally requires linear time number digits operand artem golubin nice explanation python internally python handle unbounded integer arithmetic neither numba database engine crash exceeding 64-bit integers problem ’ able store results search without able put database leaves us classic software engineering problem ’ path forward exploring alternatives impulse answer little possible make damn thing work situation software ’ writing prototype expect rewritten scratch future acceptable attitude said experienced engineers would caution often “ prototypes ” copy-pasted become janky mission-critical systems years pretending “ real thing ” let ’ real engineers would scope alternatives diving two aspects database use numba performance let ’ start database quick dirty option store numbers text strings database ’ limit size number case benefit ’ need use different database engine code stays cost ’ use numeric operations database queries would make analysis fetching awkward particular ’ even apply sorting operations since text strings sorted lexicographically e.g. 100 25 numbers sorted magnitude 25 100 note applied “ numbers text ” idea problem serializing search state hacky second option find database engine direct support unbounded-integer arithmetic benefit fast database queries confidence support future use cases well cost existing sqlite-based interface ’ work new database engine ’ write another implementation database interface numba least three options first fall back native python arithmetic slow second implement arbitrary-precision arithmetic python way numba compile third find implement c-implementation arbitrary precision integer arithmetic provide python bindings optionally see work replace numba write ’ yet tried options intuition tells best way go would find “ proper ” support arbitrary precision integers database recall postgres database engine supports various extensions example extension adds support geographic objects postgres ’ extension framework demonstrates important software engineering principle many best projects follow “ closed modification open extension. ” postgres designed others contribute new features postgres without requiring postgres team anything special—specifically ’ change postgres accommodate name sometimes goes extensions plug-ins hooks lower level callbacks github actions good example geographic objects almost certainly complicated arbitrary precision integers chances good postgres extension exists latter incorporating would involve migrating postgres finding installing extension converting c library representation whatever representation postgres accepts query good route also ensure need change tests much since ’ modifying implementations ’ see well holds gmp pgmp digging found gmp gnu multiple precision c library written torbjörn granlund python bindings library called gmpy allows python use “ mpz ” “ multiple precision “ type drop-in replacement python integers found postgresql extension called pgmp standard python library postgres psycopg2 written person wrote pgmp daniele varrazzo start ran timing test gmpy proves fast numba pull request details took small bit kicking get pgmp install made test database uses new column type mpz stores value postgres= create database pgmp_test create database postgres= \connect pgmp_test connected database `` pgmp_test '' user `` jeremy '' pgmp_test= create extension pgmp create extension pgmp_test= create table test_table id int4 value mpz create table pgmp_test= insert test_table pgmp_test- values 1 2 :mpz 513 insert 0 1 pgmp_test= select test_table id value -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 1 26815615859885194199148049996411692254958731641184786755447122887443528060147093953603748596333806855380063716372972101707507765623893139892867298012168192 1 row ’ pretty confident approach work pull request includes necessary commits add postgres implementation database interface add tests minor nuisance pull request converts main divisor computation functions use gmpy final commit converts main program use postgres database exposed one bug ’ converting new mpz types properly postgres sql query commit fixes commit adds regression test catch specific error going forward results next steps work ran counterexample search hours stopped checked possibly-superabundant numbers whose prime factorizations 75 prime factors including multiplicity since possible counterexamples rh must superabundant superabundant numbers aforementioned special prime factorization say simply ruled positive integers whose prime factorization 75 factors top 10 divisor= select n witness_value riemanndivisorsums witness_value,Machine Learning
jeremy howard fast.ai deep learning courses research,jeremy howard founder fast.ai research institute dedicated make deep learning accessible also distinguished research scientist university san francisco former president kaggle well top-ranking competitor general ’ successful entrepreneur educator research inspiring personality ai community conversation part artificial intelligence podcast would like get information podcast go https //lexfridman.com/ai connect lexfridman twitter linkedin facebook medium youtube watch video versions,Machine Learning
would like join ml slack,hi organise slack group small team ml enthusiasts including students researchers academia industry originally got together due common interest graph representation learning hold online weekly journal club grl us also work number ml specialisations including nlp computer vision friendly group try help thought others might like join feel like something want involved feel free join clicking cheers link comments,Machine Learning
p prompty mcpromptface telegram writing prompt bot gpt-2,https //t.me/mcpromptface_bot silly project figured 'd share case anyone finds fun 's based gpt-2 uses largest available public model give short,Machine Learning
035 christmas community edition,welcome christmas special community edition mlst discuss recent interesting papers pedro domingos nns kernel machines deepmind nns out-reason symbolic machines anna rodgers bert plays lottery tickets winning prof. mark bishop even causal methods wo n't deliver understanding also cover favourite bits recent montreal ai event run prof. gary marcus including rich sutton danny kahneman christof koch respond reader mail capsule networks deep dive type theory lambda calculus community member alex mattick final hour discuss inductive priors label information density another one discord community members panel dr. tim scarfe yannic kilcher alex stenlake dr. keith duggar enjoy show n't forget subscribe 00:00:00 welcome christmas special 00:00:44 sota meme 00:01:30 happy christmas 00:03:11 paper -- deepmind outperforming neuro-symbolic models nns ding et al 00:08:57 mean understand 00:17:37 paper prof. mark bishop artificial intelligence stupid causal reasoning wont fix 00:25:39 paper -- pedro domingos every model learned gradient descent approximately kernel machine 00:31:07 paper bengio inductive biases deep learning higher-level cognition 00:32:54 anna rodgers bert plays lottery tickets winning 00:37:16 montreal ai event gary marcus reasoning 00:40:37 montreal ai event -- rich sutton universal theory ai 00:49:45 montreal ai event -- danny kahneman system 1 vs 2 generative models ala free energy principle 01:02:57 montreal ai event -- christof koch neuroscience hard 01:10:55 markus carr -- reader letter capsule networks 01:13:21 alex response marcus carr 01:22:06 type theory segment -- alex mattick discord 01:24:45 type theory segment -- type theory 01:28:12 type theory segment -- difference functional oop languages 01:29:03 type theory segment -- lambda calculus 01:30:46 type theory segment -- closures 01:35:05 type theory segment -- term rewriting confluency termination 01:42:02 mtype theory segment -- eta term rewritig system lambda calculus 01:54:44 type theory segment -- types semantics 02:06:26 type theory segment -- calculus constructions 02:09:27 type theory segment -- homotopy type theory 02:11:02 type theory segment -- deep learning link 02:17:27 jan discord segment -- chrome mru skit 02:18:56 jan discord segment -- inductive priors xmaster96/jan discord 02:37:59 jan discord segment -- label information density xmaster96/jan discord 02:55:13 outro,Machine Learning
evidence based cs education,day 2 november 18 2020 theme building pipeline research impact andreas stefik university nevada las vegas accessible computer science education fall workshop hosted microsoft university washington create university colorado ’ coleman institute took place november 17-19 2020 consisted three half-days talks discussions planning new research dedicated making computer science education learning experiences accessible people disabilities information workshop found https //www.microsoft.com/en-us/research/event/accessible-cs-education-fall-workshop/,Machine Learning
use marketing automation acquiring qualifying e-commerce leads,acquire new customers using sales funnel marketing automation read full story,Machine Learning
107 – peter singer suffering humans animals ai,peter singer professor bioethics princeton best known 1975 book animal liberation makes ethical case eating meat written brilliantly ethical perspective extreme poverty euthanasia human genetic selection sports doping sale kidneys happiness including books ethics real world life save key popularizer effective altruism movement generally considered one influential philosophers world support podcast supporting sponsors – masterclass https //masterclass.com/lex – cash app –,Machine Learning
146 – michael mina rapid testing viruses engineering mindset,michael mina immunologist epidemiologist physician harvard please support podcast checking sponsors – brave https //brave.com/lex – athletic greens https //athleticgreens.com/lex use code lex get 1 month fish oil – expressvpn https //expressvpn.com/lexpod use code lexpod get 3 months free – cash app https //cash.app/ use code lexpodcast get 10 episode links michael ’ twitter https //twitter.com/michaelmina_lab michael ’ time article https //time.com/5912705/covid-19-stop-spread-christmas/ rapid tests https //www.rapidtests.org/ podcast info podcast website https //lexfridman.com/podcast apple podcasts https //apple.co/2lwqzir spotify https //spoti.fi/2newcf8 rss https //lexfridman.com/feed/podcast/ youtube full episodes https //youtube.com/lexfridman youtube clips https //youtube.com/lexclips support connect – check sponsors ’ best way support podcast – support patreon https //www.patreon.com/lexfridman – twitter https //twitter.com/lexfridman – instagram https //www.instagram.com/lexfridman – linkedin https //www.linkedin.com/in/lexfridman – facebook https //www.facebook.com/lexfridmanpage – medium https //medium.com/ lexfridman outline ’ timestamps episode podcast players able click timestamp jump time 00:00 – introduction 07:28 – interacting viruses bacteria 11:42 – deadlier viruses 15:13 – covid-19 mutate 16:47 – rapid testing 34:11 – pcr vs rapid antigen tests 43:55 – medical industrial complex 47:47 – lex takes covid test 54:32 – fda cheap tests 57:17 – explanation elon musk ’ positive covid tests 1:04:25 – role testing vaccine deployment 1:07:54 – public health policy 1:17:34 – weather system viruses 1:34:26 – virus kill humans 1:40:05 – engineering deadly virus 1:44:47 – alphafold 2 viruses 1:50:42 – advice young people 1:58:50 – time buddhist monk 2:04:54 – meditation 2:12:32 – meaning life,Machine Learning
microsoft research reinforcement learning day 2021,reinforcement learning day 2021 hosted microsoft research designed bring together rl community build latest knowledge hear professor doina precup mcgill university discussed `` new advances hierarchical reinforcement learning '' addition listen professor yoshua bengio mila quebec ai institute dr. john langford microsoft research nyc engaged lively debate `` reinforcement learning state rl theory-practice divide '' 4:50 “ new advances hierarchical reinforcement learning ” keynote presented professor doina precup mcgill university moderated ida momennejad microsoft research nyc 54:56 “ reinforcement learning day debate rl theory-practice divide ” featuring professor yoshua bengio mila quebec ai institute dr. john langford microsoft research nyc moderated katja hofmann microsoft research cambridge uk learn reinforcement learning day 2021 https //www.microsoft.com/en-us/research/event/reinforcement-learning-day-2021/,Machine Learning
10 computer vision startups product hunt upvotes,self-driving cars facial recognition ai surveillance gans computer vision tech poster child ai industry recent years collaborative global data science community advancements come research teams big tech computer vision startups alike read full story,Machine Learning
nick lane 's books good https //www.amazon.com/vital-question-evolution-origins-complex/dp/0393352978,nick lane 's books good amazon.com/vital-question-ev…,Machine Learning
data used shape future media entertainment,read full story,Machine Learning
92 – harry cliff particle physics large hadron collider,harry cliff particle physicist university cambridge working large hadron collider beauty experiment specializes searching hints new particles forces studying type particle called “ beauty quark ” “ b quark ” way part group physicists searching answers biggest questions modern physics also exceptional communicator science clearest captivating explanations basic concepts particle physics ’ ever heard support podcast signing,Machine Learning
regression metrics machine learning,"regression refers predictive modeling problems involve predicting numeric value different classification involves predicting class label unlike classification use classification accuracy evaluate predictions made regression model instead must use error metrics specifically designed evaluating predictions made regression problems tutorial discover calculate error metrics regression predictive modeling projects completing tutorial know regression predictive modeling problems involve predicting numeric value metrics regression involve calculating error score summarize predictive skill model calculate report mean squared error root mean squared error mean absolute error let ’ get started regression metrics machine learning photo gael varoquaux rights reserved tutorial overview tutorial divided three parts regression predictive modeling evaluating regression models metrics regression mean squared error root mean squared error mean absolute error regression predictive modeling predictive modeling problem developing model using historical data make prediction new data answer predictive modeling described mathematical problem approximating mapping function f input variables x output variables called problem function approximation job modeling algorithm find best mapping function given time resources available approximating functions applied machine learning see post machine learning algorithms work regression predictive modeling task approximating mapping function f input variables x continuous output variable regression different classification involves predicting category class label difference classification regression see tutorial difference classification regression machine learning continuous output variable real-value integer floating point value often quantities amounts sizes example house may predicted sell specific dollar value perhaps range 100,000 200,000 regression problem requires prediction quantity regression real-valued discrete input variables problem multiple input variables often called multivariate regression problem regression problem input variables ordered time called time series forecasting problem familiar regression predictive modeling let ’ look might evaluate regression model evaluating regression models common question beginners regression predictive modeling projects calculate accuracy regression model accuracy e.g classification accuracy measure classification regression calculate accuracy regression model skill performance regression model must reported error predictions makes sense think predicting numeric value like height dollar amount ’ want know model predicted value exactly might intractably difficult practice instead want know close predictions expected values error addresses exactly summarizes average close predictions expected values three error metrics commonly used evaluating reporting performance regression model mean squared error mse root mean squared error rmse mean absolute error mae many metrics regression although commonly used see full list regression metrics supported scikit-learn python machine learning library scikit-learn api regression metrics next section let ’ take closer look turn metrics regression section take closer look popular metrics regression models calculate predictive modeling project mean squared error mean squared error mse short popular error metric regression problems also important loss function algorithms fit optimized using least squares framing regression problem “ least squares ” refers minimizing mean squared error predictions expected values mse calculated mean average squared differences predicted expected target values dataset mse 1 n sum n y_i – yhat_i ^2 y_i ’ th expected value dataset yhat_i ’ th predicted value difference two values squared effect removing sign resulting positive error value squaring also effect inflating magnifying large errors larger difference predicted expected values larger resulting squared positive error effect “ punishing ” models larger errors mse used loss function also effect “ punishing ” models inflating average error score used metric create plot get feeling change prediction error impacts squared error example gives small contrived dataset 1.0 values predictions range perfect 1.0 wrong 0.0 0.1 increments squared error prediction expected value calculated plotted show quadratic increase squared error ... calculate error err expected predicted 2 complete example listed example increase mean squared error matplotlib import pyplot sklearn.metrics import mean_squared_error real value expected 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 predicted value predicted 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 calculate errors errors list range len expected calculate error err expected predicted 2 store error errors.append err report error print '/prepre class= '' urvanov-syntax-highlighter-plain-tag ''",Machine Learning
p datasets behave like git repositories,let 's talk datasets machine learning change time real-life projects datasets rarely static grow change evolve time fact reflected datasets maintained taking inspiration software dev codebases managed using git create living git repositories datasets well means dataset becomes easily manageable sharing collaborating updating downstream consumers changes data done similar manage pip npm packages wrote blog project showcasing transform dataset living-dataset use machine learning project https //dagshub.com/blog/datasets-should-behave-like-git-repositories/ example project living dataset https //dagshub.com/simon/baby-yoda-segmentation-dataset project using living dataset dependency https //dagshub.com/simon/baby-yoda-segmentor would love hear thoughts ​ https //preview.redd.it/cvpu2j7ovac61.png width=588 format=png auto=webp s=15d1fe9cfacf282427e4394b3c729082710d2b99 link comments,Machine Learning
rt ykilcher 🔥new video🔥 everybody talking openai 's new dall·e model 👀 takes piece text turns image absolutely crazy 😱 watch video learn more💪 https //youtu.be/j4xgkjwlfl4 dalle ilyasut _jongwook_kim mikhailpavlov5 gabeeegoooh scottgray76,🔥new video🔥 everybody talking openai 's new dall·e model 👀 takes piece text turns image absolutely crazy 😱 watch video learn more💪 invidious.snopyta.org/j4xgkjwlfl4 dalle ilyasut _jongwook_kim mikhailpavlov5 gabeeegoooh scottgray76,Machine Learning
future capitalism age ai,read full story,Machine Learning
proposed dot-product attention,tried trace back history dot-product attention today successfully find origin current understanding bahdanau et al proposed original attention mechanism neural machine translation jointly learning align translate famous paper attention need came seemed dot-product attention already standard since cited multiple prior works using mechanism however papers seem advertise dot-product attention original contribution also seem cite someone else proposed maybe missed something since skimming papers anyone know originally proposed dot-product attention link comments,Machine Learning
029 gpt-3 prompt engineering trading ai alignment intelligence,week dr. tim scarfe dr. keith duggar yannic kilcher connor leahy cover broad range topics ranging academia gpt-3 whether prompt engineering could next in-demand skill markets economics including trading whether predict stock market ai alignment utilitarian philosophy randomness intelligence even whether universe infinite 00:00:00 show introduction 00:12:49 academia ph.d 00:15:49 academia wall street 00:17:08 quants -- smoke mirrors tail risk 00:19:46 previous results dont indicate future success markets 00:23:23 making money social media signals 00:24:41 predicting stock market 00:27:20 things predictable 00:31:40 tim postscript comment predicting markets 00:32:37 connor take markets 00:35:16 market become efficient .. 00:36:38 snake oil ml 00:39:20 gpt-3 changed minds 00:52:34 prompt engineering new form software development 01:06:07 gpt-3 prompt engineering 01:12:33 emergent intelligence increasingly weird abstractions 01:27:29 wireheading economy 01:28:54 free markets dragon story price vs value 01:33:59 utilitarian philosophy good look like 01:41:39 randomness intelligence 01:44:55 different schools thought ml 01:46:09 universe infinite thanks lot connor leahy guest today 's show https //twitter.com/npcollapse -- join eleutherai community discord https //discord.com/invite/vtrgjbm,Machine Learning
'm running 48 miles 48 hours davidgoggins march 5-7 starting 8pm pst join us challenge see page live videos details podcast 's throwing stuff refuses tell think thinks break good luck 😎,'m running 48 miles 48 hours davidgoggins march 5-7 starting 8pm pst join us challenge see page live videos details podcast 's throwing stuff refuses tell think thinks break good luck 😎,Machine Learning
psa buggy caching behaviour huggingfaces old run_glue script,chasing index bug glue evaluation script found caching behaviour run_glue.py would expect huggingface downloads glue data tokenizes featurizes text using supplied tokenizer caches result problem cache treats tokenizers class identical see issue major problem like us evaluating models using different tokenizers type glue task data featurized using whichever tokenizer happened use first worst case scenario means glue evaluation scores based nonsensical data perspective pre-trained model best case scenario means got cryptic index bug seems like fixed recently run_glue script updated use datasets library gather bug existed quite tl dr 've used huggingface´s run_glue script evaluate models happen use different tokenizers class results probably invalid link comments,Machine Learning
cuda work tensorflow,cuda specifally cudnn library implement layers tensorflow python api gpu instance import conv2d tf.keras.layers run gpu sequence imports tf.keras.layers lead cuda code actually implements convolution layer gpu broader sense python api interact c++ based low level implementations link comments,Machine Learning
data science people trouble buying superheroine movies,read full story,Machine Learning
rt lexfridman feel honored excited back lexfridman 's podcast 's really deep unique knack getting important exciting questions tech life reality https //youtu.be/rl4j4kpwngm,feel honored excited back lexfridman 's podcast 's really deep unique knack getting important exciting questions tech life reality invidious.snopyta.org/rl4j4kpwngm nitter.net/lexfridman/status/1351053401438773248,Machine Learning
inverted pendulum puzzle,sign backblaze get unlimited storage mac pc 6/month plus 15 day free trial https //thld.co/backblaze_zachstar ►items video support channel double pendulum https //stemerch.com/collections/science-toys/products/chaos-double-pendulum floating globe https //stemerch.com/collections/science-toys/products/led-magnetic-levitation-globe n't jerk shirt https //stemerch.com/collections/dont-be-a-jerk sierpinski triangle poster https //stemerch.com/collections/sierpinski-triangle-1 `` mathematics '' affiliate link https //amzn.to/2kydcxg book recommendations https //stemerch.com/collections/books support channel https //www.patreon.com/zachstar paypal one time donation https //www.paypal.me/zachstaryt join channel get access perks https //www.youtube.com/channel/ucpcsacbqs-sjevfk_hmfy9w/join ►follow instagram https //www.instagram.com/zachstar/ twitter https //twitter.com/imzachstar 2d graphing software https //www.desmos.com/calculator animations brainup studios http //brainup.in/ check spanish channel https //www.youtube.com/channel/ucnknu2xqblaspj6ckc8vtpa ►my setup space pictures https //amzn.to/2cc4kqj magnetic floating globe https //amzn.to/2vgpdn0 camera https //amzn.to/2rivyu5 mic https //amzn.to/35bkiri tripod https //amzn.to/2rgmtnl equilibrium tube https //amzn.to/2sowdrh ►check amazon store https //www.amazon.com/shop/zachstar,Machine Learning
image,nitter.net/ykilcher/status/1349006655439179781,Machine Learning
